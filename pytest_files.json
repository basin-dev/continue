[
  {
    "prompt": "class GroupFactory\n\nclass UserFactory\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from importlib import import_module, reload\nimport pytest\nimport sys\nfrom unittest.mock import patch\n\nfrom rest_framework import status\n\nfrom django.contrib.auth.models import Permission, Group\nfrom django.conf import settings\nfrom django.urls import clear_url_caches\nfrom django.urls import reverse\n\nfrom .factories import UserFactory\nfrom groups.models import GroupInfo\nfrom users.models import UserProfile\n\nBLANK_CHOICE = ('', '---------')\n\nUSER_DETAILS = {\n    'username': 'test',\n    'email': 'test@test.com',\n    'first_name': 'Foo',\n    'last_name': 'Bar',\n}\n\nUSER_DETAILS_CREATE = USER_DETAILS.copy()\nUSER_DETAILS_CREATE.update(password1='pass', password2='pass')\n\nUSER_DETAILS_CHANGING = {\n    'username': 'johnsmith',\n    'email': 'john@smiths.com',\n    'first_name': 'John',\n    'last_name': 'Smith',\n}\n\n\n@pytest.mark.django_db\ndef test_create_user_view_get(admin_client):\n    url = reverse('wagtailusers_users:add')\n    response = admin_client.get(url)\n    assert response.status_code == status.HTTP_200_OK\n\n\n@pytest.mark.django_db\ndef test_create_user_view(admin_client):\n    url = reverse('wagtailusers_users:add')\n\n    response = admin_client.post(url, data=USER_DETAILS_CREATE)\n\n    assert response.context['message'] == 'User test created.'\n    assert response.status_code == status.HTTP_302_FOUND\n    assert response.url == reverse('wagtailusers_users:index')\n\n\n@pytest.mark.django_db\ndef test_create_user_view_invalid_form(admin_client):\n    url = reverse('wagtailusers_users:add')\n\n    post_data = USER_DETAILS.copy()\n    post_data.update(email='This is not an email address')\n    response = admin_client.post(url, post_data)\n\n    message = response.context['message']\n    assert message == 'The user could not be created due to errors.'\n    assert response.status_code == status.HTTP_200_OK\n\n\n@pytest.mark.django_db\ndef test_get_edit_user_view(admin_client):\n    user = UserFactory(**USER_DETAILS)\n    url = reverse('wagtailusers_users:edit', kwargs={'pk': user.pk})\n    response = admin_client.get(url)\n    assert response.status_code == status.HTTP_200_OK\n    assert response.context['can_delete'] is True\n\n\n@pytest.mark.django_db\ndef test_edit_user_view(team_leaders_group, admin_client):\n    user = UserFactory(**USER_DETAILS)\n    url = reverse('wagtailusers_users:edit', kwargs={'pk': user.pk})\n\n    # We'll add the user to a group, as well as changing their details\n    post_data = USER_DETAILS_CHANGING.copy()\n    post_data['groups'] = [team_leaders_group.pk]\n    response = admin_client.post(url, data=post_data)\n\n    assert response.context['message'] == 'User johnsmith updated.'\n    assert response.status_code == status.HTTP_302_FOUND\n    assert response.url == reverse('wagtailusers_users:index')\n\n    # The user's details should have changed to reflect the posted values\n    user.refresh_from_db()\n    for field_name, changed_value in USER_DETAILS_CHANGING.items():\n        assert getattr(user, field_name) == changed_value\n\n    # And they should have been added to a group\n    group_ids = set(user.groups.values_list('id', flat=True))\n    assert group_ids == {team_leaders_group.pk}\n\n\n@pytest.mark.django_db\ndef test_edit_user_view_invalid_form(admin_client, approved_user):\n    url = reverse('wagtailusers_users:edit', kwargs={'pk': approved_user.pk})\n\n    post_data = USER_DETAILS.copy()\n    post_data.update(email='This is not an email address')\n\n    response = admin_client.post(url, post_data)\n\n    message = response.context['message']\n    assert message == 'The user could not be saved due to errors.'\n    assert response.status_code == status.HTTP_200_OK\n\n\n@pytest.mark.django_db\ndef test_edit_user_view_cannot_change_personal_details_when_sso_enforced(\n    admin_client\n):\n    # Set this flag to True and repeat previous test\n    settings.FEATURE_FLAGS['ENFORCE_STAFF_SSO_ON'] = True\n\n    user = UserFactory(**USER_DETAILS)\n\n    # Post changes to the view\n    url = reverse('wagtailusers_users:edit', kwargs={'pk': user.pk})\n    admin_client.post(url, data=USER_DETAILS_CHANGING)\n\n    # The users details should remain unchanged, because the\n    # personal detail fields should all be disabled\n    user.refresh_from_db()\n    for field_name, original_value in USER_DETAILS.items():\n        assert getattr(user, field_name) == original_value\n\n    # Change this back to avoid cross-test pollution\n    settings.FEATURE_FLAGS['ENFORCE_STAFF_SSO_ON'] = False\n    reload_urlconf()\n\n\n@pytest.mark.django_db\ndef test_edit_user_view_preserves_ability_to_update_is_active(admin_client):\n    # Set this flag to True and actions if previous test\n    settings.FEATURE_FLAGS['ENFORCE_STAFF_SSO_ON'] = True\n\n    # Create an 'inactive' user to test with\n    user = UserFactory(**USER_DETAILS)\n    user.is_active = False\n    user.save()\n\n    # Post using the same details + 'is_active=on'\n    post_data = USER_DETAILS.copy()\n    post_data.update(is_active='on')\n    url = reverse('wagtailusers_users:edit', kwargs={'pk': user.pk})\n    admin_client.post(url, data=post_data)\n\n    # The change to 'is_active' should have been applied, because that field\n    # is not disabled along with the personal detail ones\n    user.refresh_from_db()\n    assert user.is_active is True\n\n    # Reset flag to avoid cross-test pollution\n    settings.FEATURE_FLAGS['ENFORCE_STAFF_SSO_ON'] = False\n    reload_urlconf()\n\n\n@pytest.mark.django_db\ndef test_edit_user_view_warns_administrator_if_user_is_awaiting_approval(\n    admin_client, user_awaiting_approval\n):\n    # This flag must be set for the warning to show\n    settings.FEATURE_FLAGS['ENFORCE_STAFF_SSO_ON'] = True\n\n    user = user_awaiting_approval\n    url = reverse('wagtailusers_users:edit', kwargs={'pk': user.pk})\n    response = admin_client.get(url)\n\n    message = response.context['message']\n    assert \"This user is awaiting approval\" in message\n    assert \"requested to be added to the 'Moderators' group\" in message\n\n    # Reset flag to avoid cross-test pollution\n    settings.FEATURE_FLAGS['ENFORCE_STAFF_SSO_ON'] = False\n    reload_urlconf()\n\n\n@pytest.mark.django_db\ndef test_edit_user_view_marks_user_as_approved_if_added_to_group(\n    admin_client, admin_user, user_awaiting_approval\n):\n    # This flag must be set for the warning to show\n    settings.FEATURE_FLAGS['ENFORCE_STAFF_SSO_ON'] = True\n\n    user = user_awaiting_approval\n    profile = user_awaiting_approval.userprofile\n    url = reverse('wagtailusers_users:edit', kwargs={'pk': user.pk})\n\n    group = Group.objects.get(pk=profile.self_assigned_group_id)\n    group.permissions.add(Permission.objects.get(codename='access_admin'))\n\n    with patch('users.views.notify_user_of_access_request_approval', autospec=True) as mocked_method:\n        response = admin_client.post(url, {\n            'username': user.username,\n            'first_name': user.first_name,\n            'last_name': user.last_name,\n            'email': user.email,\n            'is_active': True,\n            'groups': [group.pk],\n        })\n\n    # Ensure the post was successful\n    assert response.context['message'] == 'User %s updated.' % user.username\n    assert response.status_code == status.HTTP_302_FOUND\n    assert response.url == reverse('wagtailusers_users:index')\n\n    # The UserProfile should have been updated\n    profile.refresh_from_db()\n    assert profile.assignment_status == UserProfile.STATUS_APPROVED\n    assert profile.approved_by_id == admin_user.id\n    assert profile.approved_at is not None\n\n    # A notification should have been triggered for the user\n    expected_call_args = dict(\n        request=response.wsgi_request,\n        user_email=user.email,\n        user_name=user.first_name,\n        reviewer_name=admin_user.get_full_name(),\n    )\n    mocked_method.assert_called_with(**expected_call_args)\n\n    # Reset flag to avoid cross-test pollution\n    settings.FEATURE_FLAGS['ENFORCE_STAFF_SSO_ON'] = False\n    reload_urlconf()\n\n\n@pytest.mark.django_db\ndef test_edit_user_view_does_not_mark_user_as_approved_if_not_added_to_a_group(admin_client, groups_with_info):\n    user = UserFactory(username='some-user')\n    profile = user.userprofile\n    profile.assignment_status = UserProfile.STATUS_AWAITING_APPROVAL\n    profile.self_assigned_group_id = groups_with_info[0].id\n    profile.save()\n    url = reverse('wagtailusers_users:edit', kwargs={'pk': user.pk})\n\n    with patch(\n        'users.views.notify_user_of_access_request_approval'\n    ) as mocked_method:\n        response = admin_client.post(url, {\n            'username': user.username,\n            'first_name': user.first_name,\n            'last_name': user.last_name,\n            'email': user.email,\n            'is_active': True,\n        })\n\n    # Ensure the post was successful\n    assert response.context['message'] == 'User %s updated.' % user.username\n    assert response.status_code == status.HTTP_302_FOUND\n    assert response.url == reverse('wagtailusers_users:index')\n\n    # The UserProfile should NOT have been updated\n    profile.refresh_from_db()\n    assert profile.assignment_status == UserProfile.STATUS_AWAITING_APPROVAL\n    assert profile.approved_by_id is None\n    assert profile.approved_at is None\n\n    # no notification should have been triggered\n    mocked_method.assert_not_called()\n\n\ndef reload_urlconf(urlconf=None):\n    clear_url_caches()\n    if urlconf is None:\n        urlconf = settings.ROOT_URLCONF\n    if urlconf in sys.modules:\n        reload(sys.modules[urlconf])\n    else:\n        import_module(urlconf)\n\n\n@pytest.mark.django_db\ndef test_force_staff_sso(client):\n    \"\"\"Test that URLs and redirects are in place.\"\"\"\n    settings.FEATURE_FLAGS['ENFORCE_STAFF_SSO_ON'] = True\n    settings.AUTHBROKER_CLIENT_ID = 'debug'\n    settings.AUTHBROKER_CLIENT_SECRET = 'debug'\n    settings.AUTHBROKER_URL = 'https://test.com'\n    reload_urlconf()\n\n    assert reverse('authbroker_client:login') == '/auth/login/'\n    assert reverse('authbroker_client:callback') == '/auth/callback/'\n    response = client.get('/admin/login/')\n    assert response.status_code == 302\n    assert response.url == '/auth/login/'\n\n    settings.FEATURE_FLAGS['ENFORCE_STAFF_SSO_ON'] = False\n    reload_urlconf()\n\n\n@pytest.mark.parametrize('assignment_status, expected_status_code', (\n    (UserProfile.STATUS_CREATED, 200),\n    (UserProfile.STATUS_AWAITING_APPROVAL, 302),\n    (UserProfile.STATUS_APPROVED, 302)\n))\n@pytest.mark.django_db\ndef test_ssorequestaccessview_responds_based_on_assignment_status(\n    admin_client, admin_user, assignment_status, expected_status_code\n):\n    url = reverse('sso:request_access')\n    profile = admin_user.userprofile\n    profile.assignment_status = assignment_status\n    profile.save()\n    response = admin_client.get(url)\n    assert response.status_code == expected_status_code\n\n\n@pytest.mark.django_db\ndef test_ssorequestaccessview_shows_unlimited_visibilty_groups_only(\n    admin_client, groups_with_info\n):\n    url = reverse('sso:request_access')\n\n    # Visbility is set to 'unrestricted' for all groups in `groups_with_info`,\n    # so choices should reflect that by default\n    expected_choices = tuple(\n        (g.id, g.info.name_singular) for g in groups_with_info\n    )\n\n    # Confirm the choices in the form are as expected\n    response = admin_client.get(url)\n    group_field = response.context['form']['self_assigned_group'].field\n    actual_choices = tuple(group_field.choices)\n    assert actual_choices == expected_choices\n\n    # Change the visibility of groups and try again\n    GroupInfo.objects.all().update(\n        visibility=GroupInfo.VISIBILITY_MANAGERS_ONLY)\n\n    # Choices should be empty now\n    response = admin_client.get(url)\n    group_field = response.context['form']['self_assigned_group'].field\n    assert tuple(group_field.choices) == ()\n\n\n@pytest.mark.django_db\ndef test_ssorequestaccessview_with_no_team_leaders_group(admin_client):\n    # If no 'team leaders group' has been designated, the 'team_leaders'\n    # field should only have a 'blank' option\n    url = reverse('sso:request_access')\n    response = admin_client.get(url)\n    team_leader_field = response.context['form']['team_leader'].field\n    assert tuple(team_leader_field.choices) == (BLANK_CHOICE,)\n\n\n@pytest.mark.django_db\ndef test_ssorequestaccessview_with_team_leaders_group_but_no_members(\n    admin_client, team_leaders_group\n):\n    # If the designated 'team leaders group' has no members, the 'team_leaders'\n    # field should only have a 'blank' option\n    url = reverse('sso:request_access')\n    response = admin_client.get(url)\n    team_leader_field = response.context['form']['team_leader'].field\n    assert team_leaders_group.user_set.all().exists() is False\n    assert tuple(team_leader_field.choices) == (BLANK_CHOICE,)\n\n\n@pytest.mark.django_db\ndef test_ssorequestaccessview_with_team_leaders(\n    admin_client, team_leaders_group, team_leaders\n):\n    url = reverse('sso:request_access')\n\n    # When team leaders are defined, they will appear as choices\n    # for the 'team_leaders' field\n    expected_choices = [BLANK_CHOICE]\n    expected_choices.extend(list(\n        (tl.id, \"{} <{}>\".format(tl.get_full_name(), tl.email))\n        for tl in team_leaders\n    ))\n\n    # Confirm the choices in the form are as expected\n    response = admin_client.get(url)\n    team_leader_field = response.context['form']['team_leader'].field\n    actual_choices = list(team_leader_field.choices)\n    assert actual_choices == expected_choices\n\n\n@pytest.mark.django_db\ndef test_ssorequestaccessview_fails_validation_if_form_incomplete(\n    admin_client, groups_with_info, team_leaders\n):\n    url = reverse('sso:request_access')\n    response = admin_client.post(url, data={})\n\n    # Should still be on the same view\n    assert response.status_code == 200\n\n    # Both form fields should have errors\n    assert 'self_assigned_group' in response.context['form'].errors\n    assert 'team_leader' in response.context['form'].errors\n\n\n@pytest.mark.django_db\ndef test_ssorequestaccessview_post_with_complete_data(\n    admin_client, admin_user, groups_with_info, team_leaders\n):\n    group = groups_with_info[0]\n    team_leader = team_leaders[0]\n\n    with patch(\n        'users.views.notify_team_leader_of_pending_access_request',\n        autospec=True\n    ) as mocked_method:\n        response = admin_client.post(\n            reverse('sso:request_access'),\n            data={\n                'self_assigned_group': group.id,\n                'team_leader': team_leader.id,\n            }\n        )\n\n    # Should be redirected to the success url\n    success_url = reverse('sso:request_access_success')\n    assert response.url == success_url\n\n    # The UserProfile for `admin_user` should have been updated\n    profile = admin_user.userprofile\n    assert profile.self_assigned_group_id == group.id\n    assert profile.team_leader_id == team_leader.id\n    assert profile.assignment_status == UserProfile.STATUS_AWAITING_APPROVAL # noqa\n\n    # A notification should have been triggered for the user\n    expected_call_args = dict(\n        request=response.wsgi_request,\n        team_leader_email=team_leader.email,\n        team_leader_name=team_leader.first_name,\n        user_id=admin_user.id,\n        user_name=admin_user.get_full_name(),\n        user_email=admin_user.email,\n        user_role=group.info.name_singular,\n    )\n    mocked_method.assert_called_with(**expected_call_args)\n\n\n@pytest.mark.django_db\n@pytest.mark.parametrize('url', (\n    reverse('sso:request_access'),\n    reverse('sso:request_access_success'),\n))\ndef test_ssorequestaccess_views_only_available_to_authenticated_users(\n    client, admin_client, url\n):\n    # When not authenticated, the user is redirected to the login page\n    response = client.get(url)\n    assert response.status_code == 302\n    assert response.url.startswith(settings.LOGIN_URL)\n\n    # When authenticated, things work fine\n    response = admin_client.get(url)\n    assert response.status_code == 200\n"
  },
  {
    "prompt": "\"\"\"All EU ISO-2 codes\n\nReturns:\n    data (list): List of ISO-2 codes)\"\"\"\nfn get_eu_countries()\n\n\"\"\"Retrieves continent ISO2 code to continent name mapping from a static open URL.\n\nReturns:\n    data (dict): Key-value pairs of continent-codes and names.\"\"\"\nfn get_continent_lookup()\n\n\"\"\"Retrieves subregions (around 18 in total)\nlookups for all world countries, by ISO2 code,\nform a static open URL.\n\nReturns:\n    data (dict): Values are country_name-region_name pairs.\"\"\"\nfn get_country_region_lookup()\n\n\"\"\"Geocoder using the Open Street Map Nominatim API.\n\nIf there are multiple results the first one is returned (they are ranked by importance).\nThe API usage policy allows maximum 1 request per second and no multithreading:\nhttps://operations.osmfoundation.org/policies/nominatim/\n\nArgs:\n    request_kwargs (dict): Parameters for OSM API.\nReturns:\n    JSON from API response.\"\"\"\nfn geocode()\n\n\"\"\"Forces retry to exit if a valueError is returned. Supplied to the\n'retry_on_exception' argument in the retry decorator.\n\nArgs:\n    exception (Exception): the raised exception, to check\n\nReturns:\n    (bool): False if a ValueError, else True\"\"\"\nfn retry_if_not_value_error(exception: Exception) -> (bool)\n\n\"\"\"Extension of geocode to catch invalid requests to the api and handle errors.\nfailure.\n\nArgs:\n    q (str): query string, multiple words should be separated with +\n    kwargs (str): name and value of any other valid query parameters\n\nReturns:\n    dict: lat and lon\"\"\"\nfn _geocode(q: str) -> dict\n\n\"\"\"A wrapper for the geocode function to process a supplied dataframe using\nthe city and country.\n\nArgs:\n    df (dataframe): a dataframe containing city and country fields.\nReturns:\n    a dataframe with a 'coordinates' column appended.\"\"\"\nfn geocode_dataframe(df: dataframe)\n\n\"\"\"Geocodes a dataframe, first by supplying the city and country to the api, if this\nfails a second attempt is made supplying the combination using the q= method.\nThe supplied dataframe df is returned with additional columns appended, containing\nthe latitude and longitude as floats.\n\nArgs:\n    df (:obj:`pandas.DataFrame`): input dataframe\n    city (str): name of the input column containing the city\n    country (str): name of the input column containing the country\n    latitude (str): name of the output column containing the latitude\n    longitude (str): name of the output column containing the longitude\n    query_method (int): query methods to attempt:\n                                'city_country_only': city and country only\n                                'query_only': q method only\n                                'both': city, country with fallback to q method\n\nReturns:\n    (:obj:`pandas.DataFrame`): original dataframe with lat and lon appended as floats\"\"\"\nfn geocode_batch_dataframe(df, city: str, country: str, latitude: str, longitude: str, query_method: int) -> (\n\n\"\"\"Generates a composite key to use as the primary key for the geographic data.\n\nArgs:\n    city (str): name of the city\n    country (str): name of the country\n\nReturns:\n    (str): composite key\"\"\"\nfn generate_composite_key(city: str, country: str) -> (str)\n\n\"\"\"Look up the ISO 3166 codes for countries.\nhttps://www.iso.org/glossary-for-iso-3166.html\n\nWraps the pycountry module to attempt lookup with all name options.\n\nArgs:\n    country (str): name of the country to lookup\nReturns:\n    Country object from the pycountry module\"\"\"\nfn country_iso_code(country: str)\n\n\"\"\"A wrapper for the country_iso_code function to apply it to a whole dataframe,\nusing the country name. Also appends the continent code based on the country.\n\nArgs:\n    df (:obj:`pandas.DataFrame`): a dataframe containing a country field.\n    country (str): field in df containing the country name\nReturns:\n    a dataframe with country_alpha_2, country_alpha_3, country_numeric, and\n    continent columns appended.\"\"\"\nfn country_iso_code_dataframe(df, country: str)\n\n\"\"\"Converts country alpha_3 into name and catches error so this can be used with\n   pd.apply.\n\nArgs:\n    code (str): iso alpha 3 code\n    iso2 (bool): use alpha 2 code instead\nReturns:\n    str: name of the country or None if not valid\"\"\"\nfn country_iso_code_to_name(code: str, iso2: bool) -> str\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pandas as pd\nfrom pandas.testing import assert_frame_equal\nimport pytest\nfrom unittest import mock\n\nfrom nesta.packages.geo_utils.geocode import geocode\nfrom nesta.packages.geo_utils.geocode import _geocode\nfrom nesta.packages.geo_utils.geocode import geocode_dataframe\nfrom nesta.packages.geo_utils.geocode import geocode_batch_dataframe\nfrom nesta.packages.geo_utils.geocode import generate_composite_key\nfrom nesta.packages.geo_utils.country_iso_code import country_iso_code\nfrom nesta.packages.geo_utils.country_iso_code import country_iso_code_dataframe\nfrom nesta.packages.geo_utils.country_iso_code import country_iso_code_to_name\nfrom nesta.packages.geo_utils.lookup import get_continent_lookup\nfrom nesta.packages.geo_utils.lookup import get_country_region_lookup\nfrom nesta.packages.geo_utils.lookup import get_country_continent_lookup\n\nREQUESTS = 'nesta.packages.geo_utils.geocode.requests.get'\nPYCOUNTRY = 'nesta.packages.geo_utils.country_iso_code.pycountry.countries.get'\nGEOCODE = 'nesta.packages.geo_utils.geocode.geocode'\n_GEOCODE = 'nesta.packages.geo_utils.geocode._geocode'\nCOUNTRY_ISO_CODE = 'nesta.packages.geo_utils.country_iso_code.country_iso_code'\n\n\nclass TestGeocoding():\n    @staticmethod\n    @pytest.fixture\n    def mocked_osm_response():\n        mocked_response = mock.Mock()\n        mocked_response.json.return_value = [{'lat': '12.923432', 'lon': '-75.234569'}]\n        return mocked_response\n\n    def test_error_raised_when_arguments_missing(self):\n        with pytest.raises(ValueError) as e:\n            geocode()\n        assert \"No geocode match\" in str(e.value)\n\n    @mock.patch(REQUESTS)\n    def test_request_includes_user_agent_in_header(self, mocked_request, mocked_osm_response):\n        mocked_request.return_value = mocked_osm_response\n        geocode(something='a')\n        assert mocked_request.call_args[1]['headers'] == {'User-Agent': 'Nesta health data geocode'}\n\n    @mock.patch(REQUESTS)\n    def test_url_correct_with_city_and_country(self, mocked_request, mocked_osm_response):\n        mocked_request.return_value = mocked_osm_response\n        kwargs = dict(city='london', country='UK')\n        geocode(**kwargs)\n        assert mocked_request.call_args[1]['params'] == dict(format=\"json\", **kwargs)\n\n    @mock.patch(REQUESTS)\n    def test_url_correct_with_query(self, mocked_request, mocked_osm_response):\n        mocked_request.return_value = mocked_osm_response\n        kwargs = dict(q='my place')\n        geocode(**kwargs)\n        assert mocked_request.call_args[1]['params'] == dict(format=\"json\", **kwargs)\n\n    @mock.patch(REQUESTS)\n    def test_error_returned_if_no_match(self, mocked_request):\n        mocked_response = mock.Mock()\n        mocked_response.json.return_value = []\n        mocked_request.return_value = mocked_response\n        with pytest.raises(ValueError) as e:\n            geocode(q=\"Something bad\")\n        assert \"No geocode match\" in str(e.value)\n\n    @mock.patch(REQUESTS)\n    def test_coordinates_extracted_from_json_with_one_result(self, mocked_request, mocked_osm_response):\n        mocked_request.return_value = mocked_osm_response\n        assert geocode(q='somewhere') == [{'lat': '12.923432', 'lon': '-75.234569'}]\n\n    @mock.patch(GEOCODE)\n    def test_geocode_wrapper_rejects_invalid_query_parameters(self, mocked_geocode):\n        with pytest.raises(ValueError) as e:\n            _geocode(cat='dog', city='Nice')\n        assert \"Invalid query parameter\" in str(e.value)\n\n    @mock.patch(GEOCODE)\n    def test_geocode_wrapper_rejects_both_q_and_kwargs_supplied(self, mocked_geocode):\n        with pytest.raises(ValueError) as e:\n            _geocode(city='London', q='somewhere')\n        assert \"Supply either q OR other query parameters, they cannot be combined.\" in str(e.value)\n\n    @mock.patch(GEOCODE)\n    def test_geocode_wrapper_errors_if_no_query_parameters_supplied(self, mocked_geocode):\n        with pytest.raises(ValueError) as e:\n            _geocode()\n        assert \"No query parameters supplied\" in str(e.value)\n\n    @mock.patch(GEOCODE)\n    def test_geocode_wrapper_calls_geocode_properly(self, mocked_geocode):\n        mocked_geocode.return_value = [{'lat': 1.1, 'lon': 2.2}]\n\n        _geocode('my place')\n        _geocode(q='somewhere')\n        _geocode(city='London', country='UK')\n        _geocode(postalcode='ABC 123')\n\n        expected_calls = [mock.call(q='my place'),\n                          mock.call(q='somewhere'),\n                          mock.call(city='London', country='UK'),\n                          mock.call(postalcode='ABC 123')\n                          ]\n        assert mocked_geocode.mock_calls == expected_calls\n\n\nclass TestGeocodeDataFrame():\n    @staticmethod\n    @pytest.fixture\n    def test_dataframe():\n        df = pd.DataFrame({'index': [0, 1, 2],\n                           'city': ['London', 'Sheffield', 'Brussels'],\n                           'country': ['UK', 'United Kingdom', 'Belgium'],\n                           })\n        return df\n\n    @mock.patch(_GEOCODE)\n    def test_underlying_geocoding_function_called_with_city_country(self, mocked_geocode,\n                                                                    test_dataframe):\n        # Generate dataframe using a mocked output\n        mocked_geocode.side_effect = ['cat', 'dog', 'squirrel']\n        geocoded_dataframe = geocode_dataframe(test_dataframe)\n\n        # Expected outputs\n        expected_dataframe = pd.DataFrame({'index': [0, 1, 2],\n                                           'city': ['London', 'Sheffield', 'Brussels'],\n                                           'country': ['UK', 'United Kingdom', 'Belgium'],\n                                           'coordinates': ['cat', 'dog', 'squirrel']\n                                           })\n        expected_calls = [mock.call(city='London', country='UK'),\n                          mock.call(city='Sheffield', country='United Kingdom'),\n                          mock.call(city='Brussels', country='Belgium')]\n\n        # Check expected behaviours\n        assert geocoded_dataframe.to_dict(orient=\"records\") == expected_dataframe.to_dict(orient=\"records\")\n        assert mocked_geocode.mock_calls == expected_calls\n\n    @mock.patch(_GEOCODE)\n    def test_underlying_geocoding_function_called_with_query_fallback(self, mocked_geocode,\n                                                                      test_dataframe):\n        mocked_geocode.side_effect = [None, None, None, 'dog', 'cat', 'squirrel']\n        geocoded_dataframe = geocode_dataframe(test_dataframe)\n        # Expected outputs\n        expected_dataframe = pd.DataFrame({'index': [0, 1, 2],\n                                           'city': ['London', 'Sheffield', 'Brussels'],\n                                           'country': ['UK', 'United Kingdom', 'Belgium'],\n                                           'coordinates': ['dog', 'cat', 'squirrel']\n                                           })\n        expected_calls = [mock.call(city='London', country='UK'),\n                          mock.call(city='Sheffield', country='United Kingdom'),\n                          mock.call(city='Brussels', country='Belgium'),\n                          mock.call('London UK'),\n                          mock.call('Sheffield United Kingdom'),\n                          mock.call('Brussels Belgium')]\n\n        # Check expected behaviours\n        assert geocoded_dataframe.to_dict(orient=\"records\") == expected_dataframe.to_dict(orient=\"records\")\n        assert mocked_geocode.mock_calls == expected_calls\n\n    @mock.patch(_GEOCODE)\n    def test_duplicates_are_only_geocoded_once(self, mocked_geocode):\n        test_dataframe = pd.DataFrame({'index': [0, 1, 2, 3],\n                                       'city': ['London', 'Brussels', 'London', 'Brussels'],\n                                       'country': ['UK', 'Belgium', 'UK', 'Belgium']\n                                       })\n\n        mocked_geocode.side_effect = ['LON', 'BRU']\n        geocoded_dataframe = geocode_dataframe(test_dataframe)\n\n        expected_dataframe = pd.DataFrame({'index': [0, 1, 2, 3],\n                                           'city': ['London', 'Brussels', 'London', 'Brussels'],\n                                           'country': ['UK', 'Belgium', 'UK', 'Belgium'],\n                                           'coordinates': ['LON', 'BRU', 'LON', 'BRU']\n                                           })\n\n        assert geocoded_dataframe.to_dict(orient=\"records\") == expected_dataframe.to_dict(orient=\"records\")\n        assert mocked_geocode.call_count == 2\n\n\nclass TestGeocodeBatchDataframe():\n    @staticmethod\n    @pytest.fixture\n    def test_dataframe():\n        df = pd.DataFrame({'index': [0, 1, 2],\n                           'city': ['London', 'Sheffield', 'Brussels'],\n                           'country': ['UK', 'United Kingdom', 'Belgium'],\n                           })\n        return df\n\n    @mock.patch(_GEOCODE)\n    def test_underlying_geocoding_function_called_with_city_country(self, mocked_geocode,\n                                                                    test_dataframe):\n        # Generate dataframe using a mocked output\n        mocked_geocode.side_effect = [{'lat': '12.923432', 'lon': '-75.234569'},\n                                      {'lat': '99.999999', 'lon': '-88.888888'},\n                                      {'lat': '-2.202022', 'lon': '0.000000'}\n                                      ]\n\n        # Expected outputs\n        expected_dataframe = pd.DataFrame({'index': [0, 1, 2],\n                                           'city': ['London', 'Sheffield', 'Brussels'],\n                                           'country': ['UK', 'United Kingdom', 'Belgium'],\n                                           'latitude': [12.923432, 99.999999, -2.202022],\n                                           'longitude': [-75.234569, -88.888888, 0.0]\n                                           })\n        expected_calls = [mock.call(city='London', country='UK'),\n                          mock.call(city='Sheffield', country='United Kingdom'),\n                          mock.call(city='Brussels', country='Belgium')]\n\n        geocoded_dataframe = geocode_batch_dataframe(test_dataframe)\n\n        # Check expected behaviours\n        assert_frame_equal(geocoded_dataframe, expected_dataframe,\n                           check_like=True, check_dtype=False)\n        assert mocked_geocode.mock_calls == expected_calls\n\n    @mock.patch(_GEOCODE)\n    def test_underlying_geocoding_function_called_with_query_fallback(self,\n                                                                      mocked_geocode,\n                                                                      test_dataframe):\n        mocked_geocode.side_effect = [None,\n                                      {'lat': 1, 'lon': 4},\n                                      None,\n                                      {'lat': 2, 'lon': 5},\n                                      None,\n                                      {'lat': 3, 'lon': 6}\n                                      ]\n        # Expected outputs\n        expected_dataframe = pd.DataFrame({'index': [0, 1, 2],\n                                           'city': ['London', 'Sheffield', 'Brussels'],\n                                           'country': ['UK', 'United Kingdom', 'Belgium'],\n                                           'latitude': [1.0, 2.0, 3.0],\n                                           'longitude': [4.0, 5.0, 6.0],\n                                           })\n        expected_calls = [mock.call(city='London', country='UK'),\n                          mock.call(q='London UK'),\n                          mock.call(city='Sheffield', country='United Kingdom'),\n                          mock.call(q='Sheffield United Kingdom'),\n                          mock.call(city='Brussels', country='Belgium'),\n                          mock.call(q='Brussels Belgium')]\n\n        geocoded_dataframe = geocode_batch_dataframe(test_dataframe, query_method='both')\n\n        # Check expected behaviours\n        assert_frame_equal(geocoded_dataframe, expected_dataframe,\n                           check_like=True, check_dtype=False)\n        assert mocked_geocode.mock_calls == expected_calls\n\n    @mock.patch(_GEOCODE)\n    def test_underlying_geocoding_function_called_with_query_method_only(self,\n                                                                         mocked_geocode,\n                                                                         test_dataframe):\n        mocked_geocode.side_effect = [{'lat': 1, 'lon': 4},\n                                      {'lat': 2, 'lon': 5},\n                                      {'lat': 3, 'lon': 6}\n                                      ]\n        # Expected outputs\n        expected_dataframe = pd.DataFrame({'index': [0, 1, 2],\n                                           'city': ['London', 'Sheffield', 'Brussels'],\n                                           'country': ['UK', 'United Kingdom', 'Belgium'],\n                                           'latitude': [1.0, 2.0, 3.0],\n                                           'longitude': [4.0, 5.0, 6.0],\n                                           })\n        expected_calls = [mock.call(q='London UK'),\n                          mock.call(q='Sheffield United Kingdom'),\n                          mock.call(q='Brussels Belgium')]\n\n        geocoded_dataframe = geocode_batch_dataframe(test_dataframe, query_method='query_only')\n\n        # Check expected behaviours\n        assert_frame_equal(geocoded_dataframe, expected_dataframe,\n                           check_like=True, check_dtype=False)\n        assert mocked_geocode.mock_calls == expected_calls\n\n    @mock.patch(_GEOCODE)\n    def test_valueerror_raised_when_invalid_query_method_passed(self,\n                                                                mocked_geocode,\n                                                                test_dataframe):\n        with pytest.raises(ValueError):\n            geocode_batch_dataframe(test_dataframe, query_method='cats')\n\n        with pytest.raises(ValueError):\n            geocode_batch_dataframe(test_dataframe, query_method='test')\n\n        with pytest.raises(ValueError):\n            geocode_batch_dataframe(test_dataframe, query_method=1)\n\n    @mock.patch(_GEOCODE)\n    def test_output_column_names_are_applied(self, mocked_geocode, test_dataframe):\n\n        # Generate dataframe using a mocked output\n        mocked_geocode.side_effect = [{'lat': '12.923432', 'lon': '-75.234569'},\n                                      {'lat': '99.999999', 'lon': '-88.888888'},\n                                      {'lat': '-2.202022', 'lon': '0.000000'}\n                                      ]\n\n        # Expected outputs\n        expected_dataframe = pd.DataFrame({'index': [0, 1, 2],\n                                           'city': ['London', 'Sheffield', 'Brussels'],\n                                           'country': ['UK', 'United Kingdom', 'Belgium'],\n                                           'lat': [12.923432, 99.999999, -2.202022],\n                                           'lon': [-75.234569, -88.888888, 0.0]\n                                           })\n\n        geocoded_dataframe = geocode_batch_dataframe(test_dataframe,\n                                                     latitude='lat',\n                                                     longitude='lon')\n\n        # Check expected behaviours\n        assert_frame_equal(geocoded_dataframe, expected_dataframe,\n                           check_like=True, check_dtype=False)\n\n\nclass TestCountryIsoCode():\n    @mock.patch(PYCOUNTRY)\n    def test_lookup_via_name(self, mocked_pycountry):\n        mocked_pycountry.return_value = 'country_object'\n        expected_calls = [mock.call(name='United Kingdom')]\n\n        assert country_iso_code('United Kingdom') == 'country_object'\n        assert mocked_pycountry.mock_calls == expected_calls\n        assert mocked_pycountry.call_count == 1\n        country_iso_code.cache_clear()\n\n    @mock.patch(PYCOUNTRY)\n    def test_lookup_via_common_name(self, mocked_pycountry):\n        mocked_pycountry.side_effect = [KeyError(), 'country_object']\n        expected_calls = [mock.call(name='United Kingdom'),\n                          mock.call(common_name='United Kingdom')\n                          ]\n\n        assert country_iso_code('United Kingdom') == 'country_object'\n        assert mocked_pycountry.mock_calls == expected_calls\n        assert mocked_pycountry.call_count == 2\n        country_iso_code.cache_clear()\n\n    @mock.patch(PYCOUNTRY)\n    def test_lookup_via_official_name(self, mocked_pycountry):\n        mocked_pycountry.side_effect = [KeyError(), KeyError(), 'country_object']\n        expected_calls = [mock.call(name='United Kingdom'),\n                          mock.call(common_name='United Kingdom'),\n                          mock.call(official_name='United Kingdom')\n                          ]\n\n        assert country_iso_code('United Kingdom') == 'country_object'\n        assert mocked_pycountry.mock_calls == expected_calls\n        assert mocked_pycountry.call_count == 3\n        country_iso_code.cache_clear()\n\n    @mock.patch(PYCOUNTRY)\n    def test_invalid_lookup_raises_keyerror(self, mocked_pycountry):\n        mocked_pycountry.side_effect = [KeyError(), KeyError(), KeyError()]*2\n\n        with pytest.raises(KeyError) as e:\n            country_iso_code('Fake Country')\n        assert 'Fake Country not found' in str(e.value)\n        country_iso_code.cache_clear()\n\n    @mock.patch(PYCOUNTRY)\n    def test_title_case_is_applied(self, mocked_pycountry):\n        expected_calls = []\n        names = ['united kingdom', 'UNITED KINGDOM',\n                 'United kingdom']\n        mocked_pycountry.side_effect = [KeyError(), KeyError(), KeyError(), 'blah'] * len(names)\n        for name in names:\n            country_iso_code(name)  # Find the iso codes\n            raw_call = mock.call(name=name)\n            common_call = mock.call(common_name=name)\n            official_call = mock.call(official_name=name)\n            title_call = mock.call(name='United Kingdom')\n            expected_calls.append(raw_call)  # The initial call\n            expected_calls.append(common_call)  # Tries common name call\n            expected_calls.append(official_call)  # Tries official name\n            expected_calls.append(title_call) # The title case call\n        assert mocked_pycountry.mock_calls == expected_calls\n    country_iso_code.cache_clear()\n\n\nclass TestCountryIsoCodeDataframe():\n    @staticmethod\n    def _mocked_response(alpha_2, alpha_3, numeric, continent):\n        '''Builds a mocked response for the patched country_iso_code function.'''\n        response = mock.Mock()\n        response.alpha_2 = alpha_2\n        response.alpha_3 = alpha_3\n        response.numeric = numeric\n        response.continent = continent\n        return response\n\n    @mock.patch(COUNTRY_ISO_CODE)\n    def test_valid_countries_coded(self, mocked_country_iso_code):\n        test_df = pd.DataFrame({'index': [0, 1, 2],\n                                'country': ['United Kingdom', 'Belgium', 'United States']\n                                })\n        mocked_response_uk = self._mocked_response('GB', 'GBR', '123', 'EU')\n        mocked_response_be = self._mocked_response('BE', 'BEL', '875', 'EU')\n        mocked_response_us = self._mocked_response('US', 'USA', '014', 'NA')\n        mocked_country_iso_code.side_effect = [mocked_response_uk,\n                                               mocked_response_be,\n                                               mocked_response_us\n                                               ]\n        expected_dataframe = pd.DataFrame(\n                            {'index': [0, 1, 2],\n                             'country': ['United Kingdom', 'Belgium', 'United States'],\n                             'country_alpha_2': ['GB', 'BE', 'US'],\n                             'country_alpha_3': ['GBR', 'BEL', 'USA'],\n                             'country_numeric': ['123', '875', '014'],\n                             'continent': ['EU', 'EU', 'NA']\n                             })\n        coded_df = country_iso_code_dataframe(test_df)\n        assert coded_df.to_dict(orient=\"records\") == expected_dataframe.to_dict(orient=\"records\")\n\n    @mock.patch(COUNTRY_ISO_CODE)\n    def test_invalid_countries_data_is_none(self, mocked_country_iso_code):\n        test_df = pd.DataFrame({'index': [0, 1, 2],\n                                'country': ['United Kingdom', 'Belgium', 'United States']\n                                })\n        mocked_country_iso_code.side_effect = KeyError\n        expected_dataframe = pd.DataFrame(\n                            {'index': [0, 1, 2],\n                             'country': ['United Kingdom', 'Belgium', 'United States'],\n                             'country_alpha_2': [None, None, None],\n                             'country_alpha_3': [None, None, None],\n                             'country_numeric': [None, None, None],\n                             'continent': [None, None, None]\n                             })\n        coded_df = country_iso_code_dataframe(test_df)\n        assert coded_df.to_dict(orient=\"records\") == expected_dataframe.to_dict(orient=\"records\")\n\n\nclass TestCountryIsoCodeToName():\n    def test_valid_iso_code_returns_name(self):\n        assert country_iso_code_to_name('ITA') == 'Italy'\n        assert country_iso_code_to_name('DEU') == 'Germany'\n        assert country_iso_code_to_name('GBR') == 'United Kingdom'\n\n    def test_invalid_iso_code_returns_none(self):\n        assert country_iso_code_to_name('FOO') is None\n        assert country_iso_code_to_name('ABC') is None\n        assert country_iso_code_to_name('ZZZ') is None\n\n\ndef test_generate_composite_key():\n    assert generate_composite_key('London', 'United Kingdom') == 'london_united-kingdom'\n    assert generate_composite_key('Paris', 'France') == 'paris_france'\n    assert generate_composite_key('Name-with hyphen', 'COUNTRY') == 'name-with-hyphen_country'\n\n\ndef test_generate_composite_key_raises_error_with_invalid_input():\n    with pytest.raises(ValueError):\n        generate_composite_key(None, 'UK')\n\n    with pytest.raises(ValueError):\n        generate_composite_key('city_only')\n\n    with pytest.raises(ValueError):\n        generate_composite_key(1, 2)\n\n\ndef test_get_continent_lookup():\n    continents = get_continent_lookup()\n    assert None in continents\n    assert '' in continents\n    assert continents['NA'] == 'North America'\n    assert len(continents) == 9  # 2 nulls + 7 continents\n\ndef test_get_country_region_lookup():\n    countries = get_country_region_lookup()\n    assert len(countries) > 100\n    assert len(countries) < 1000\n    assert all(len(k) == 2 for k in countries.keys())\n    assert all(type(v) is tuple for v in countries.values())\n    assert all(len(v) == 2 for v in countries.values())\n    all_regions = {v[1] for v in countries.values()}\n    assert len(all_regions) == 18\n\n\ndef test_country_continent_lookup():\n    lookup = get_country_continent_lookup()\n    non_nulls = {k: v for k, v in lookup.items()\n                 if k is not None and k != ''}\n    # All iso2, so length == 2\n    assert all(len(k) == 2 for k in non_nulls.items())\n    assert all(len(v) == 2 for v in non_nulls.values())\n    # Either strings or Nones\n    country_types = set(type(v) for v in lookup.values())\n    assert country_types == {str, type(None)}\n    # Right ball-park of country and continent numbers\n    assert len(non_nulls) > 100  # num countries\n    assert len(non_nulls) < 1000 # num countries\n    assert len(set(non_nulls.values())) == 7 # num continents\n"
  },
  {
    "prompt": "class IncidentType\n\nclass IncidentTypeCount\n\nclass Incident\n\nclass Get\n\nclass GetList\n\nclass Resolve\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# -*- coding: utf-8 -*-\n\nimport unittest.mock\n\nimport pytest\n\nimport pycamunda.incident\nfrom tests.mock import raise_requests_exception_mock, not_ok_response_mock\n\n\ndef test_get_params(engine_url):\n    get_incident = pycamunda.incident.Get(url=engine_url, id_='anId')\n\n    assert get_incident.url == engine_url + '/incident/anId'\n    assert get_incident.query_parameters() == {}\n    assert get_incident.body_parameters() == {}\n\n\n@unittest.mock.patch('pycamunda.incident.Incident.load', unittest.mock.MagicMock())\n@unittest.mock.patch('requests.Session.request')\ndef test_get_calls_requests(mock, engine_url):\n    get_incident = pycamunda.incident.Get(url=engine_url, id_='anId')\n    get_incident()\n\n    assert mock.called\n    assert mock.call_args[1]['method'].upper() == 'GET'\n\n\n@unittest.mock.patch('requests.Session.request', raise_requests_exception_mock)\ndef test_get_raises_pycamunda_exception(engine_url):\n    get_incident = pycamunda.incident.Get(url=engine_url, id_='anId')\n    with pytest.raises(pycamunda.PyCamundaException):\n        get_incident()\n\n\n@unittest.mock.patch('requests.Session.request', not_ok_response_mock)\n@unittest.mock.patch('pycamunda.incident.Incident', unittest.mock.MagicMock())\n@unittest.mock.patch('pycamunda.base._raise_for_status')\ndef test_get_raises_for_status(mock, engine_url):\n    get_incident = pycamunda.incident.Get(url=engine_url, id_='anId')\n    get_incident()\n\n    assert mock.called\n\n\n@unittest.mock.patch('requests.Session.request', unittest.mock.MagicMock())\n@unittest.mock.patch('pycamunda.base.from_isoformat', unittest.mock.MagicMock())\n@unittest.mock.patch('pycamunda.incident.IncidentType', unittest.mock.MagicMock())\ndef test_get_returns_incident(engine_url):\n    get_incident = pycamunda.incident.Get(url=engine_url, id_='anId')\n    incident = get_incident()\n\n    assert isinstance(incident, pycamunda.incident.Incident)\n"
  },
  {
    "prompt": "class SecretaryTool\n\nclass OutcomeStatus\n\nfn get_job_type(record)\n\nclass OutcomeChecker\n\nclass FailuresQuerySet\n\nclass NamedModel\n\nclass Product\n\nclass BuildPlatform\n\nclass Option\n\nclass RepositoryGroup\n\nclass Repository\n\nclass Push\n\nclass Commit\n\nclass MachinePlatform\n\nclass Bugscache\n\nclass Machine\n\nclass JobGroup\n\nclass OptionCollectionManager\n\nclass OptionCollection\n\nclass JobType\n\nclass FailureClassification\n\nclass ReferenceDataSignatures\n\nclass JobManager\n\nclass Job\n\nclass TaskclusterMetadata\n\nclass JobLog\n\nclass BugJobMap\n\nclass JobNote\n\nclass FailureLine\n\nclass Group\n\nclass GroupStatus\n\nclass ClassifiedFailure\n\nclass TextLogStep\n\nclass TextLogError\n\nclass TextLogErrorMetadata\n\nclass TextLogErrorMatch\n\nclass InvestigatedTests\n\nclass PerformanceFramework\n\nclass PerformanceSignature\n\nclass PerformanceDatum\n\nclass MultiCommitDatum\n\nclass IssueTracker\n\nclass PerformanceAlertSummary\n\nclass PerformanceAlert\n\nclass PerformanceTag\n\nclass PerformanceBugTemplate\n\nclass BackfillReport\n\nclass BackfillRecord\n\nclass PerformanceSettings\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import uuid\nfrom datetime import datetime, timedelta\n\nimport pytest\nimport simplejson as json\nfrom django.db.models import Q\nfrom mock import Mock, patch\n\nfrom treeherder.config.settings import IS_WINDOWS\nfrom treeherder.perf.auto_perf_sheriffing.secretary_tool import SecretaryTool\nfrom treeherder.model.models import Push, Job\nfrom treeherder.perf.models import BackfillRecord, BackfillReport, PerformanceSettings\nfrom treeherder.perf.auto_perf_sheriffing.outcome_checker import OutcomeChecker, OutcomeStatus\n\n# we're testing against this (automatically provided by fixtures)\nJOB_TYPE_ID = 1\n\n\ndef get_middle_index(successful_jobs):\n    # get middle index to make sure the push is in range\n    index_in_range = int((len(successful_jobs) + 1) / 2)\n    return index_in_range\n\n\n@pytest.fixture\ndef record_backfilled(test_perf_alert, record_context_sample):\n    report = BackfillReport.objects.create(summary=test_perf_alert.summary)\n    record = BackfillRecord.objects.create(\n        alert=test_perf_alert,\n        report=report,\n        status=BackfillRecord.BACKFILLED,\n    )\n    record.set_context(record_context_sample)\n    record.save()\n    return record\n\n\n@pytest.fixture\ndef range_dates(record_context_sample):\n\n    from_date = datetime.fromisoformat(record_context_sample[0]['push_timestamp'])\n    to_date = datetime.fromisoformat(record_context_sample[-1]['push_timestamp'])\n\n    return {\n        'before_date': from_date - timedelta(days=5),\n        'from_date': from_date,\n        'in_range_date': from_date + timedelta(hours=13),\n        'to_date': to_date,\n        'after_date': to_date + timedelta(days=3),\n    }\n\n\n@pytest.fixture\ndef outcome_checking_pushes(\n    create_push, range_dates, record_context_sample, test_repository, test_repository_2\n):\n    from_push_id = record_context_sample[0]['push_id']\n    to_push_id = record_context_sample[-1]['push_id']\n\n    pushes = [\n        create_push(test_repository, revision=uuid.uuid4(), time=range_dates['before_date']),\n        create_push(\n            test_repository,\n            revision=uuid.uuid4(),\n            time=range_dates['from_date'],\n            explicit_id=from_push_id,\n        ),\n        create_push(test_repository, revision=uuid.uuid4(), time=range_dates['in_range_date']),\n        create_push(test_repository, revision=uuid.uuid4(), time=range_dates['in_range_date']),\n        create_push(test_repository, revision=uuid.uuid4(), time=range_dates['in_range_date']),\n        create_push(test_repository, revision=uuid.uuid4(), time=range_dates['in_range_date']),\n        create_push(\n            test_repository,\n            revision=uuid.uuid4(),\n            time=range_dates['to_date'],\n            explicit_id=to_push_id,\n        ),\n        create_push(test_repository, revision=uuid.uuid4(), time=range_dates['after_date']),\n    ]\n\n    return pushes\n\n\n@pytest.fixture\ndef successful_jobs(outcome_checking_pushes, eleven_jobs_stored):\n    jobs = Job.objects.all()\n    _successful_jobs = []\n    pairs = zip(outcome_checking_pushes, jobs)\n    for push, job in pairs:\n        job.push = push\n        job.result = 'success'\n        job.job_type_id = JOB_TYPE_ID\n        job.save()\n        _successful_jobs.append(job)\n    return _successful_jobs\n\n\n@pytest.fixture\ndef jobs_with_one_failed(successful_jobs):\n    index_in_range = get_middle_index(successful_jobs)\n    job_to_fail = successful_jobs[index_in_range]\n    job_to_fail.result = 'testfailed'\n    job_to_fail.save()\n\n\n@pytest.fixture\ndef jobs_with_one_pending(successful_jobs):\n    index_in_range = get_middle_index(successful_jobs)\n    job_pending = successful_jobs[index_in_range]\n    job_pending.result = 'unknown'\n    job_pending.save()\n\n\n@pytest.fixture\ndef get_outcome_checker_mock():\n    def get_outcome_checker_mock(outcome: OutcomeStatus):\n        return type('', (), {'check': lambda *params: outcome})\n\n    return get_outcome_checker_mock\n\n\n@pytest.mark.skipif(IS_WINDOWS, reason=\"datetime logic does not work when OS not on GMT\")\ndef test_secretary_tool_updates_only_matured_reports(\n    test_perf_alert, test_perf_alert_2, create_record\n):\n    # create new report with records\n    create_record(test_perf_alert)\n    # create mature report with records\n    date_past = datetime.utcnow() - timedelta(hours=5)\n    with patch('django.utils.timezone.now', Mock(return_value=date_past)):\n        create_record(test_perf_alert_2)\n\n    assert BackfillRecord.objects.count() == 2\n    assert BackfillRecord.objects.filter(status=BackfillRecord.PRELIMINARY).count() == 2\n\n    SecretaryTool.mark_reports_for_backfill()\n    assert BackfillRecord.objects.filter(status=BackfillRecord.PRELIMINARY).count() == 1\n\n\ndef test_secretary_tool_uses_existing_settings(performance_settings):\n    assert PerformanceSettings.objects.count() == 1\n    last_reset_date_before = json.loads(performance_settings.settings)[\"last_reset_date\"]\n\n    SecretaryTool.validate_settings()\n\n    assert PerformanceSettings.objects.count() == 1\n    settings_after = PerformanceSettings.objects.filter(name=\"perf_sheriff_bot\").first()\n    assert json.loads(settings_after.settings)[\"last_reset_date\"] == last_reset_date_before\n\n\ndef test_secretary_tool_resets_settings_if_expired(expired_performance_settings):\n    assert PerformanceSettings.objects.count() == 1\n    expired_last_reset_date = json.loads(expired_performance_settings.settings)[\"last_reset_date\"]\n\n    SecretaryTool.validate_settings()\n\n    assert PerformanceSettings.objects.count() == 1\n    settings_after = PerformanceSettings.objects.filter(name=\"perf_sheriff_bot\").first()\n    assert json.loads(settings_after.settings)[\"last_reset_date\"] != expired_last_reset_date\n\n\ndef test_secretary_tool_creates_new_settings_if_none_exist(db):\n    assert PerformanceSettings.objects.count() == 0\n\n    SecretaryTool.validate_settings()\n\n    assert PerformanceSettings.objects.count() == 1\n\n\ndef test_check_outcome_after_success(get_outcome_checker_mock, record_backfilled):\n    outcome_checker_mock = get_outcome_checker_mock(OutcomeStatus.SUCCESSFUL)\n    secretary = SecretaryTool(outcome_checker_mock)\n\n    assert BackfillRecord.objects.filter(status=BackfillRecord.BACKFILLED).count() == 1\n    assert BackfillRecord.objects.filter(status=BackfillRecord.SUCCESSFUL).count() == 0\n    secretary.check_outcome()\n    assert BackfillRecord.objects.filter(status=BackfillRecord.BACKFILLED).count() == 0\n    assert BackfillRecord.objects.filter(status=BackfillRecord.SUCCESSFUL).count() == 1\n\n\ndef test_check_outcome_after_fail(get_outcome_checker_mock, record_backfilled):\n    outcome_checker_mock = get_outcome_checker_mock(OutcomeStatus.FAILED)\n    secretary = SecretaryTool(outcome_checker_mock)\n\n    assert BackfillRecord.objects.filter(status=BackfillRecord.BACKFILLED).count() == 1\n    assert BackfillRecord.objects.filter(status=BackfillRecord.FAILED).count() == 0\n    secretary.check_outcome()\n    assert BackfillRecord.objects.filter(status=BackfillRecord.BACKFILLED).count() == 0\n    assert BackfillRecord.objects.filter(status=BackfillRecord.FAILED).count() == 1\n\n\ndef test_no_action_when_in_progress(get_outcome_checker_mock, record_backfilled):\n    outcome_checker_mock = get_outcome_checker_mock(OutcomeStatus.IN_PROGRESS)\n    secretary = SecretaryTool(outcome_checker_mock)\n\n    assert BackfillRecord.objects.filter(status=BackfillRecord.BACKFILLED).count() == 1\n    secretary.check_outcome()\n    assert BackfillRecord.objects.filter(status=BackfillRecord.BACKFILLED).count() == 1\n\n\ndef test_outcome_checker_identifies_pushes_in_range(\n    record_backfilled, test_repository, test_repository_2, range_dates, outcome_checking_pushes\n):\n    # TODO: retarget this test to BackfillRecord.get_pushes_in_range()\n    outcome_checker = OutcomeChecker()\n    total_pushes = Push.objects.count()\n\n    from_time = range_dates['from_date']\n    to_time = range_dates['to_date']\n\n    total_outside_pushes = Push.objects.filter(\n        Q(repository=test_repository) & (Q(time__lt=from_time) | Q(time__gt=to_time))\n    ).count()\n\n    pushes_in_range = outcome_checker._get_pushes_in_range(from_time, to_time, test_repository.id)\n    assert len(pushes_in_range) == total_pushes - total_outside_pushes\n\n    # change repository for the first 2 pushes in range\n    assert test_repository.id != test_repository_2.id\n\n    total_changed_pushes = 2\n    for push in pushes_in_range[:total_changed_pushes]:\n        push.repository = test_repository_2\n        push.save()\n\n    total_other_repo_pushes = Push.objects.filter(repository=test_repository_2).count()\n    assert total_other_repo_pushes == total_changed_pushes\n\n    updated_pushes_in_range = outcome_checker._get_pushes_in_range(\n        from_time, to_time, test_repository.id\n    )\n\n    assert len(updated_pushes_in_range) == len(pushes_in_range) - total_other_repo_pushes\n\n\nclass TestOutcomeChecker:\n    @patch('treeherder.perf.auto_perf_sheriffing.outcome_checker.get_job_type')\n    def test_successful_jobs_mean_successful_outcome(\n        self, mock_get_job_type, record_backfilled, outcome_checking_pushes, successful_jobs\n    ):\n        # TODO: remove job type mock after soft launch lands\n        mock_get_job_type.return_value = JOB_TYPE_ID\n        outcome_checker = OutcomeChecker()\n\n        response = outcome_checker.check(record_backfilled)\n        assert response == OutcomeStatus.SUCCESSFUL\n\n    @patch('treeherder.perf.auto_perf_sheriffing.outcome_checker.get_job_type')\n    def test_failed_job_means_failed_outcome(\n        self, mock_get_job_type, record_backfilled, outcome_checking_pushes, jobs_with_one_failed\n    ):\n        mock_get_job_type.return_value = JOB_TYPE_ID\n        outcome_checker = OutcomeChecker()\n\n        response = outcome_checker.check(record_backfilled)\n        assert response == OutcomeStatus.FAILED\n\n    @patch('treeherder.perf.auto_perf_sheriffing.outcome_checker.get_job_type')\n    def test_pending_job_means_in_progress_outcome(\n        self, mock_get_job_type, record_backfilled, outcome_checking_pushes, jobs_with_one_pending\n    ):\n        mock_get_job_type.return_value = JOB_TYPE_ID\n        outcome_checker = OutcomeChecker()\n\n        response = outcome_checker.check(record_backfilled)\n        assert response == OutcomeStatus.IN_PROGRESS\n"
  },
  {
    "prompt": "fn OrbitalPeriod(primary_mass, secondary_mass, semimajor_axis, gravitational_constant)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom engine.constants import G\nfrom pytest import param as p\n\nfrom .orbit_derived_parameters import OrbitalPeriod\n\n\n@pytest.mark.parametrize(\n    (\"primary_mass\", \"secondary_mass\", \"semimajor_axis\", \"expected\"),\n    [p(10e10, 100, 10, 76.9102, id=\"arbitrary period\")],\n)\ndef test_orbital_period(primary_mass, secondary_mass, semimajor_axis, expected):\n    assert OrbitalPeriod(\n        primary_mass, secondary_mass, semimajor_axis, G\n    ).evalf() == pytest.approx(expected, 1e-3)\n"
  },
  {
    "prompt": "class User\n\nclass UserFactory\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom djcookie_demo_proj.users.models import User\nfrom djcookie_demo_proj.users.tests.factories import UserFactory\n\n\n@pytest.fixture(autouse=True)\ndef media_storage(settings, tmpdir):\n    settings.MEDIA_ROOT = tmpdir.strpath\n\n\n@pytest.fixture\ndef user() -> User:\n    return UserFactory()\n"
  },
  {
    "prompt": "class MeshRouter\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom receptor.router import MeshRouter\n\ntest_networks = [\n    (\n        [\n            (\"a\", \"b\", 1),\n            (\"a\", \"d\", 1),\n            (\"a\", \"f\", 1),\n            (\"b\", \"d\", 1),\n            (\"b\", \"c\", 1),\n            (\"c\", \"e\", 1),\n            (\"c\", \"h\", 1),\n            (\"c\", \"j\", 1),\n            (\"e\", \"f\", 1),\n            (\"e\", \"g\", 1),\n            (\"e\", \"h\", 1),\n            (\"f\", \"g\", 1),\n            (\"g\", \"h\", 1),\n            (\"h\", \"j\", 1),\n            (\"h\", \"k\", 1),\n            (\"j\", \"k\", 1),\n            (\"j\", \"m\", 1),\n            (\"l\", \"m\", 1),\n        ],\n        [(\"a\", \"f\", \"f\"), (\"a\", \"m\", \"b\"), (\"h\", \"d\", \"c\")],\n        [(\"a\", {\"b\", \"d\", \"f\"}), (\"f\", {\"a\", \"e\", \"g\"}), (\"j\", {\"c\", \"h\", \"k\", \"m\"})],\n    ),\n    (\n        [(\"a\", \"b\", 1), (\"b\", \"c\", 1), (\"c\", \"d\", 1), (\"d\", \"e\", 1), (\"e\", \"f\", 1)],\n        [(\"a\", \"f\", \"b\"), (\"c\", \"a\", \"b\"), (\"f\", \"c\", \"e\")],\n        [(\"a\", {\"b\"}), (\"f\", {\"e\"}), (\"c\", {\"b\", \"d\"})],\n    ),\n]\n\n\n@pytest.mark.parametrize(\"edges, expected_next_hops, expected_neighbors\", test_networks)\ndef test_next_hop(edges, expected_next_hops, expected_neighbors):\n    for node_id, remote, enh in expected_next_hops:\n        r = MeshRouter(node_id=node_id)\n        r.add_or_update_edges(edges)\n        assert r.next_hop(remote) == enh\n\n\n@pytest.mark.parametrize(\"edges, expected_next_hops, expected_neighbors\", test_networks)\ndef test_neighbors(edges, expected_next_hops, expected_neighbors):\n    r = MeshRouter(node_id=edges[0][0])\n    r.add_or_update_edges(edges)\n    for node_id, neighbors in expected_neighbors:\n        assert r.get_neighbors(node_id) == neighbors\n"
  },
  {
    "prompt": "fn create_temp_fn()\n\nfn search_diff(diff, phid, node)\n\nfn search_rev(rev, phid, bug, status, closed, author, diff, reviewers)\n\nfn assert_attributes(hunk, expected)\n\nfn hg_sha()\n\nfn git_sha()\n\nfn init_sha(in_process, git_repo_path, git_sha)\n\nfn reset_cache()\n\nfn repo_phab_url()\n\nfn data_file()\n\nfn git_command()\n\nfn git(m_read_json_field, m_which, m_os_path, m_git_get_current_head, m_git_git_out, repo_phab_url, git_command)\n\nfn hg(m_hglib_open, m_read_json_field, m_which, m_os_path, m_repository, m_os_chdir, safe_environ, repo_phab_url)\n\nfn hg_out()\n\nfn write_text(filename, content)\n\n\"\"\"Return the fully qualified path to an executable, preferring those installed\ninto the current virtualenv.\"\"\"\nfn find_script_path(name)\n\n\"\"\"Build a usable HG repository. Return the pathlib.Path to the repo.\"\"\"\nfn hg_repo_path(monkeypatch, tmp_path)\n\n\"\"\"Overrides global ~/.gitconfig.\n\nCreates a tiny gitconfig file in the temp repo and sets it as the $HOME\"\"\"\nfn fresh_global_config(tmp_path)\n\nfn git_out()\n\n\"\"\"Build a usable Git repository. Return the pathlib.Path to the repo.\"\"\"\nfn git_repo_path(monkeypatch, tmp_path)\n\nfn safe_environ(monkeypatch)\n\n\"\"\"Replace the original config with the fake one.\"\"\"\nfn config()\n\n\"\"\"Set up an environment to run moz-phab within the current process.\"\"\"\nfn in_process(monkeypatch, safe_environ, request, config)\n\nfn user_data(m_file)\n\nfn fixture_reset_glean()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# coding=utf-8\n# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\nimport shutil\nimport mock\nimport pytest\n\nfrom callee import Contains\nfrom .conftest import git_out, search_diff, search_rev\n\nfrom mozphab import environment, exceptions, mozphab\n\n\narc_call_conduit = mock.Mock()\n\ncall_conduit = mock.Mock()\n\n\ndef by_line_mock(*args, **_kwargs):\n    # join args to catch unicode errors\n    \" \".join(*args)\n    return [\"Revision URI: http://example.test/D123\"]\n\n\ncheck_call_by_line = mock.Mock()\ncheck_call_by_line.side_effect = by_line_mock\n\ninitial_sha = None\n\n\ndef test_submit_create_arc(in_process, git_repo_path, init_sha):\n    call_conduit.side_effect = (\n        dict(),\n        dict(data=[dict(phid=\"PHID-REPO-1\", fields=dict(vcs=\"git\"))]),\n        [{\"userName\": \"alice\", \"phid\": \"PHID-USER-1\"}],\n    )\n    testfile = git_repo_path / \"X\"\n    testfile.write_text(\"a\")\n    git_out(\"add\", \".\")\n    git_out(\"commit\", \"--message\", \"A r?alice\")\n    testfile = git_repo_path / \"untracked\"\n    testfile.write_text(\"a\")\n\n    mozphab.main(\n        [\"submit\", \"--arc\", \"--yes\", \"--bug\", \"1\", init_sha], is_development=True\n    )\n\n    log = git_out(\"log\", \"--format=%s%n%n%b\", \"-1\")\n    expected = \"\"\"\nBug 1 - A r?alice\n\nDifferential Revision: http://example.test/D123\n\"\"\"\n    assert log.strip() == expected.strip()\n\n\ndef test_submit_create(in_process, git_repo_path, init_sha):\n    call_conduit.side_effect = (\n        # ping\n        dict(),\n        # diffusion.repository.search\n        dict(data=[dict(phid=\"PHID-REPO-1\", fields=dict(vcs=\"git\"))]),\n        # user search\n        [dict(userName=\"alice\", phid=\"PHID-USER-1\")],\n        # differential.creatediff\n        dict(dict(phid=\"PHID-DIFF-1\", diffid=\"1\")),\n        # differential.setdiffproperty\n        dict(),\n        # differential.revision.edit\n        dict(object=dict(id=\"123\")),\n        # differential.setdiffproperty\n        dict(),\n    )\n    (git_repo_path / \"X\").write_text(u\"\u0105\\nb\\nc\\n\", encoding=\"utf-8\")\n    (git_repo_path / \"Y\").write_text(\"no line ending\")\n    git_out(\"add\", \".\")\n    (git_repo_path / \"msg\").write_text(u\"\u0104 r?alice\", encoding=\"utf-8\")\n    git_out(\"commit\", \"--file\", \"msg\")\n    (git_repo_path / \"untracked\").write_text(\"a\\n\")\n\n    mozphab.main([\"submit\", \"--yes\", \"--bug\", \"1\", init_sha], is_development=True)\n\n    log = git_out(\"log\", \"--format=%s%n%n%b\", \"-1\")\n    expected = \"\"\"\nBug 1 - \u0104 r?alice\n\nDifferential Revision: http://example.test/D123\n\n\"\"\"\n    assert log.strip() == expected.strip()\n    assert mock.call(\"conduit.ping\", {}) in call_conduit.call_args_list\n    assert (\n        mock.call(\"user.query\", dict(usernames=[\"alice\"]))\n        in call_conduit.call_args_list\n    )\n    assert (\n        mock.call(\n            \"diffusion.repository.search\",\n            dict(limit=1, constraints=dict(callsigns=[\"TEST\"])),\n        )\n        in call_conduit.call_args_list\n    )\n    assert (\n        mock.call(\n            \"differential.creatediff\",\n            {\n                \"sourceControlPath\": \"/\",\n                \"sourceControlSystem\": \"git\",\n                \"lintStatus\": \"none\",\n                \"sourcePath\": mock.ANY,\n                \"unitStatus\": \"none\",\n                \"sourceMachine\": \"http://example.test\",\n                \"sourceControlBaseRevision\": mock.ANY,\n                \"repositoryPHID\": \"PHID-REPO-1\",\n                \"branch\": \"HEAD\",\n                \"changes\": [\n                    {\n                        \"commitHash\": mock.ANY,\n                        \"awayPaths\": [],\n                        \"newProperties\": {\"unix:filemode\": \"100644\"},\n                        \"oldPath\": None,\n                        \"hunks\": [\n                            {\n                                \"oldOffset\": 0,\n                                \"oldLength\": 0,\n                                \"newOffset\": 1,\n                                \"newLength\": 3,\n                                \"addLines\": 3,\n                                \"delLines\": 0,\n                                \"corpus\": \"+\u0105\\n+b\\n+c\\n\",\n                                \"isMissingOldNewline\": False,\n                                \"isMissingNewNewline\": False,\n                            }\n                        ],\n                        \"oldProperties\": {},\n                        \"currentPath\": \"X\",\n                        \"fileType\": 1,\n                        \"type\": 1,\n                        \"metadata\": {},\n                    },\n                    {\n                        \"commitHash\": mock.ANY,\n                        \"awayPaths\": [],\n                        \"newProperties\": {\"unix:filemode\": \"100644\"},\n                        \"oldPath\": None,\n                        \"hunks\": [\n                            {\n                                \"oldOffset\": 0,\n                                \"oldLength\": 0,\n                                \"newOffset\": 1,\n                                \"newLength\": 1,\n                                \"addLines\": 1,\n                                \"delLines\": 0,\n                                \"corpus\": (\n                                    \"+no line ending\\n\\\\ No newline at end of file\\n\"\n                                ),\n                                \"isMissingOldNewline\": False,\n                                \"isMissingNewNewline\": True,\n                            }\n                        ],\n                        \"oldProperties\": {},\n                        \"currentPath\": \"Y\",\n                        \"fileType\": 1,\n                        \"type\": 1,\n                        \"metadata\": {},\n                    },\n                ],\n                \"creationMethod\": \"moz-phab-git\",\n            },\n        )\n        in call_conduit.call_args_list\n    )\n    assert (\n        mock.call(\n            \"differential.setdiffproperty\",\n            {\"diff_id\": \"1\", \"name\": \"local:commits\", \"data\": ~Contains('\"rev\":')},\n        )\n        in call_conduit.call_args_list\n    )\n    assert (\n        call_conduit.call_args_list.count(\n            mock.call(\"differential.setdiffproperty\", mock.ANY)\n        )\n        == 2\n    )\n\n\ndef test_submit_create_added_not_commited(in_process, git_repo_path, init_sha):\n    call_conduit.side_effect = (\n        # ping\n        dict(),\n        # diffusion.repository.search\n        dict(data=[dict(phid=\"PHID-REPO-1\", fields=dict(vcs=\"git\"))]),\n        # user search\n        [dict(userName=\"alice\", phid=\"PHID-USER-1\")],\n        # differential.creatediff\n        dict(dict(phid=\"PHID-DIFF-1\", diffid=\"1\")),\n        # differential.setdiffproperty\n        dict(),\n        # differential.revision.edit\n        dict(object=dict(id=\"123\")),\n        # differential.setdiffproperty\n        dict(),\n    )\n    (git_repo_path / \"X\").write_text(\"\u0105\\r\\nb\\nc\\n\", encoding=\"utf-8\")\n    (git_repo_path / \"Y\").write_text(\"no line ending\")\n    git_out(\"add\", \".\")\n    (git_repo_path / \"msg\").write_text(\"\u0104 r?alice\", encoding=\"utf-8\")\n    git_out(\"commit\", \"--file\", \"msg\")\n    (git_repo_path / \"untracked\").write_text(\"a\\n\")\n    git_out(\"add\", \"untracked\")\n\n    with pytest.raises(exceptions.Error) as excinfo:\n        mozphab.main([\"submit\", \"--yes\", \"--bug\", \"1\", init_sha], is_development=True)\n\n    assert \"Uncommitted changes present.\" in str(excinfo.value)\n\n\ndef test_submit_create_no_bug(in_process, git_repo_path, init_sha):\n    call_conduit.reset_mock()\n    call_conduit.side_effect = (\n        # ping\n        dict(),\n        # diffusion.repository.search\n        dict(data=[dict(phid=\"PHID-REPO-1\", fields=dict(vcs=\"git\"))]),\n        # user search\n        [dict(userName=\"alice\", phid=\"PHID-USER-1\")],\n        # differential.creatediff\n        dict(dict(phid=\"PHID-DIFF-1\", diffid=\"1\")),\n        # differential.setdiffproperty\n        dict(),\n        # differential.revision.edit\n        dict(object=dict(id=\"123\")),\n        # differential.setdiffproperty\n        dict(),\n    )\n    testfile = git_repo_path / \"X\"\n    testfile.write_text(\"a\\n\")\n    git_out(\"add\", \".\")\n    msgfile = git_repo_path / \"msg\"\n    msgfile.write_text(\"A r?alice\")\n    git_out(\"commit\", \"--file\", \"msg\")\n\n    mozphab.main([\"submit\", \"--yes\", \"--no-bug\", init_sha], is_development=True)\n\n    log = git_out(\"log\", \"--format=%s%n%n%b\", \"-1\")\n    expected = \"\"\"\nA r?alice\n\nDifferential Revision: http://example.test/D123\n\"\"\"\n    assert log.strip() == expected.strip()\n\n\ndef test_submit_create_binary_arc(in_process, git_repo_path, init_sha, data_file):\n    call_conduit.side_effect = (\n        dict(),\n        dict(data=[dict(phid=\"PHID-REPO-1\", fields=dict(vcs=\"git\"))]),\n        [{\"userName\": \"alice\", \"phid\": \"PHID-USER-1\"}],\n    )\n    shutil.copyfile(str(data_file), str(git_repo_path / \"img.png\"))\n    git_out(\"add\", \".\")\n    git_out(\"commit\", \"--message\", \"IMG\")\n\n    mozphab.main(\n        [\"submit\", \"--arc\", \"--yes\", \"--bug\", \"1\", init_sha], is_development=True\n    )\n    expected = \"\"\"\nBug 1 - IMG\n\nDifferential Revision: http://example.test/D123\n\"\"\"\n    log = git_out(\"log\", \"--format=%s%n%n%b\", \"-1\")\n    assert log.strip() == expected.strip()\n\n\ndef test_submit_create_binary(in_process, git_repo_path, init_sha, data_file):\n    call_conduit.reset_mock()\n    call_conduit.side_effect = (\n        # ping\n        dict(),\n        # diffusion.repository.search\n        dict(data=[dict(phid=\"PHID-REPO-1\", fields=dict(vcs=\"git\"))]),\n        # file.allocate\n        dict(dict(filePHID=None, upload=True)),\n        # file.upload\n        dict(),\n        # differential.creatediff\n        dict(dict(phid=\"PHID-DIFF-1\", diffid=\"1\")),\n        # differential.setdiffproperty\n        dict(),\n        # differential.revision.edit\n        dict(object=dict(id=\"123\")),\n        # differential.setdiffproperty\n        dict(),\n    )\n    shutil.copyfile(str(data_file), str(git_repo_path / \"img.png\"))\n    git_out(\"add\", \".\")\n    git_out(\"commit\", \"-m\", \"IMG\")\n\n    mozphab.main([\"submit\", \"--yes\", \"--bug\", \"1\", init_sha], is_development=True)\n\n    log = git_out(\"log\", \"--format=%s%n%n%b\", \"-1\")\n    expected = \"\"\"\nBug 1 - IMG\n\nDifferential Revision: http://example.test/D123\n\"\"\"\n    assert log.strip() == expected.strip()\n    assert (\n        mock.call(\n            \"file.allocate\",\n            {\"name\": \"img.png\", \"contentHash\": mock.ANY, \"contentLength\": 182},\n        )\n        in call_conduit.call_args_list\n    )\n    assert (\n        mock.call(\"file.upload\", {\"data_base64\": mock.ANY, \"name\": \"img.png\"})\n        in call_conduit.call_args_list\n    )\n\n\ndef test_submit_create_binary_existing(in_process, git_repo_path, init_sha, data_file):\n    call_conduit.reset_mock()\n    call_conduit.side_effect = (\n        # ping\n        dict(),\n        # diffusion.repository.search\n        dict(data=[dict(phid=\"PHID-REPO-1\", fields=dict(vcs=\"git\"))]),\n        # file.allocate\n        dict(dict(filePHID=\"PHID-FILE-1\", upload=False)),\n        # no file.upload call\n        # differential.creatediff\n        dict(dict(phid=\"PHID-DIFF-1\", diffid=\"1\")),\n        # differential.setdiffproperty\n        dict(),\n        # differential.revision.edit\n        dict(object=dict(id=\"123\")),\n        # differential.setdiffproperty\n        dict(),\n    )\n    shutil.copyfile(str(data_file), str(git_repo_path / \"img.png\"))\n    git_out(\"add\", \".\")\n    git_out(\"commit\", \"-m\", \"IMG\")\n\n    mozphab.main([\"submit\", \"--yes\", \"--bug\", \"1\", init_sha], is_development=True)\n\n    log = git_out(\"log\", \"--format=%s%n%n%b\", \"-1\")\n    expected = \"\"\"\nBug 1 - IMG\n\nDifferential Revision: http://example.test/D123\n\"\"\"\n    assert log.strip() == expected.strip()\n    assert (\n        mock.call(\n            \"file.allocate\",\n            {\"name\": \"img.png\", \"contentHash\": mock.ANY, \"contentLength\": 182},\n        )\n        in call_conduit.call_args_list\n    )\n    assert mock.call(\"file.upload\", mock.ANY) not in call_conduit.call_args_list\n\n\ndef test_submit_create_binary_chunked(in_process, git_repo_path, init_sha, data_file):\n    call_conduit.reset_mock()\n    call_conduit.side_effect = (\n        # ping\n        dict(),\n        # diffusion.repository.search\n        dict(data=[dict(phid=\"PHID-REPO-1\", fields=dict(vcs=\"git\"))]),\n        # file.allocate\n        dict(dict(filePHID=\"PHID-FILE-1\", upload=True)),\n        # file.querychunks\n        [\n            dict(byteStart=\"0\", byteEnd=\"4194304\", complete=False),\n            dict(byteStart=\"4194304\", byteEnd=\"8388608\", complete=False),\n            dict(byteStart=\"8388608\", byteEnd=\"8425160\", complete=False),\n        ],\n        # file.uploadchunk\n        dict(),\n        # file.uploadchunk\n        dict(),\n        # file.uploadchunk\n        dict(),\n        # differential.creatediff\n        dict(dict(phid=\"PHID-DIFF-1\", diffid=\"1\")),\n        # differential.setdiffproperty\n        dict(),\n        # differential.revision.edit\n        dict(object=dict(id=\"123\")),\n        # differential.setdiffproperty\n        dict(),\n    )\n    shutil.copyfile(str(data_file), str(git_repo_path / \"img.png\"))\n    git_out(\"add\", \".\")\n    git_out(\"commit\", \"-m\", \"IMG\")\n\n    mozphab.main([\"submit\", \"--yes\", \"--bug\", \"1\", init_sha], is_development=True)\n\n    log = git_out(\"log\", \"--format=%s%n%n%b\", \"-1\")\n    expected = \"\"\"\nBug 1 - IMG\n\nDifferential Revision: http://example.test/D123\n\"\"\"\n    assert log.strip() == expected.strip()\n    assert (\n        mock.call(\n            \"file.allocate\",\n            {\"name\": \"img.png\", \"contentHash\": mock.ANY, \"contentLength\": 182},\n        )\n        in call_conduit.call_args_list\n    )\n    assert (\n        mock.call(\"file.querychunks\", {\"filePHID\": \"PHID-FILE-1\"})\n        in call_conduit.call_args_list\n    )\n    assert (\n        mock.call(\n            \"file.uploadchunk\",\n            {\n                \"filePHID\": \"PHID-FILE-1\",\n                \"byteStart\": 0,\n                \"data\": mock.ANY,\n                \"dataEncoding\": \"base64\",\n            },\n        )\n        in call_conduit.call_args_list\n    )\n    assert (\n        mock.call(\n            \"file.uploadchunk\",\n            {\n                \"filePHID\": \"PHID-FILE-1\",\n                \"byteStart\": 4194304,\n                \"data\": mock.ANY,\n                \"dataEncoding\": \"base64\",\n            },\n        )\n        in call_conduit.call_args_list\n    )\n    assert (\n        mock.call(\n            \"file.uploadchunk\",\n            {\n                \"filePHID\": \"PHID-FILE-1\",\n                \"byteStart\": 8388608,\n                \"data\": mock.ANY,\n                \"dataEncoding\": \"base64\",\n            },\n        )\n        in call_conduit.call_args_list\n    )\n\n\ndef test_submit_update(in_process, git_repo_path, init_sha):\n    call_conduit.reset_mock()\n    call_conduit.side_effect = (\n        # ping\n        dict(),\n        # diffusion.repository.search\n        dict(data=[dict(phid=\"PHID-REPO-1\", fields=dict(vcs=\"git\"))]),\n        # diffusion.revision.search\n        dict(data=[search_rev(rev=123)]),\n        # diffusion.diff.search\n        dict(data=[search_diff()]),\n        # whoami\n        dict(phid=\"PHID-USER-1\"),\n        # differential.creatediff\n        dict(dict(phid=\"PHID-DIFF-1\", diffid=\"1\")),\n        # differential.setdiffproperty\n        dict(),\n        # differential.revision.edit\n        dict(object=dict(id=\"123\")),\n        # differential.setdiffproperty\n        dict(),\n    )\n    testfile = git_repo_path / \"X\"\n    testfile.write_text(\"\u0105\", encoding=\"utf-8\")\n    git_out(\"add\", \".\")\n    msgfile = git_repo_path / \"msg\"\n    msgfile.write_text(\n        \"\"\"\\\nBug 1 - \u0104\n\nDifferential Revision: http://example.test/D123\n\"\"\",\n        encoding=\"utf-8\",\n    )\n    git_out(\"commit\", \"--file\", \"msg\")\n\n    mozphab.main(\n        [\"submit\", \"--yes\"]\n        + [\"--bug\", \"1\"]\n        + [\"--message\", \"update message \u0107wik\u0142a\"]\n        + [init_sha],\n        is_development=True,\n    )\n\n    assert call_conduit.call_count == 9\n    log = git_out(\"log\", \"--format=%s%n%n%b\", \"-1\")\n    expected = \"\"\"\\\nBug 1 - \u0104\n\nDifferential Revision: http://example.test/D123\n\n\"\"\"\n    assert log == expected\n\n\ndef test_submit_update_no_change(in_process, git_repo_path, init_sha, git_sha):\n    testfile = git_repo_path / \"X\"\n    testfile.write_text(\"a\")\n    git_out(\"add\", \".\")\n    msgfile = git_repo_path / \"msg\"\n    msgfile.write_text(\n        \"\"\"\\\nBug 1 - A\n\nDifferential Revision: http://example.test/D123\n\"\"\"\n    )\n    git_out(\"commit\", \"--file\", \"msg\")\n    sha = git_sha()\n    call_conduit.reset_mock()\n    call_conduit.side_effect = (\n        # ping\n        dict(),\n        # diffusion.repository.search\n        dict(data=[dict(phid=\"PHID-REPO-1\", fields=dict(vcs=\"git\"))]),\n        # diffusion.revision.search\n        dict(data=[search_rev(rev=123)]),\n        # diffusion.diff.search\n        dict(data=[search_diff(node=sha)]),\n        # whoami\n        dict(phid=\"PHID-USER-1\"),\n    )\n\n    mozphab.main(\n        [\"submit\", \"--yes\"] + [init_sha],\n        is_development=True,\n    )\n    assert call_conduit.call_count == 5\n\n\ndef test_submit_remove_cr(in_process, git_repo_path, init_sha):\n    if environment.IS_WINDOWS:\n        pytest.skip(\"Removing CR will not work on Windows.\")\n\n    call_conduit.side_effect = (\n        # CREATE\n        # ping\n        dict(),\n        # diffusion.repository.search\n        dict(data=[dict(phid=\"PHID-REPO-1\", fields=dict(vcs=\"git\"))]),\n        # user.search\n        [dict(userName=\"alice\", phid=\"PHID-USER-1\")],\n        # differential.creatediff\n        dict(dict(phid=\"PHID-DIFF-1\", diffid=\"1\")),\n        # differential.setdiffproperty\n        dict(),\n        # differential.revision.edit\n        dict(object=dict(id=\"123\")),\n        # differential.setdiffproperty\n        dict(),\n        # UPDATE\n        # no need to ping (checked)\n        # no need to check reviewer\n        # no need to search for repository repository data is saved in .hg\n        # differential.creatediff\n        dict(dict(phid=\"PHID-DIFF-2\", diffid=\"2\")),\n        # differential.setdiffproperty\n        dict(),\n        # differential.revision.edit\n        dict(object=dict(id=\"124\")),\n        # differential.setdiffproperty\n        dict(),\n    )\n    test_a = git_repo_path / \"X\"\n    test_a.write_text(\"a\\r\\nb\\n\")\n    git_out(\"add\", \"X\")\n    git_out(\"commit\", \"-am\", \"A r?alice\")\n    mozphab.main([\"submit\", \"--yes\", \"--bug\", \"1\", init_sha], is_development=True)\n    call_conduit.reset_mock()\n    # removing CR, leaving LF\n    test_a.write_text(\"a\\nb\\n\")\n    git_out(\"commit\", \"-am\", \"B r?alice\")\n    mozphab.main([\"submit\", \"--yes\", \"--bug\", \"1\", \"HEAD~\"], is_development=True)\n\n    assert (\n        mock.call(\n            \"differential.creatediff\",\n            {\n                \"changes\": [\n                    {\n                        \"metadata\": {},\n                        \"oldPath\": \"X\",\n                        \"currentPath\": \"X\",\n                        \"awayPaths\": [],\n                        \"oldProperties\": {},\n                        \"newProperties\": {},\n                        \"commitHash\": mock.ANY,\n                        \"type\": 2,\n                        \"fileType\": 1,\n                        \"hunks\": [\n                            {\n                                \"oldOffset\": 1,\n                                \"oldLength\": 2,\n                                \"newOffset\": 1,\n                                \"newLength\": 2,\n                                \"addLines\": 1,\n                                \"delLines\": 1,\n                                \"isMissingOldNewline\": False,\n                                \"isMissingNewNewline\": False,\n                                \"corpus\": \"-a\\r\\n+a\\n b\\n\",\n                            }\n                        ],\n                    }\n                ],\n                \"sourceMachine\": \"http://example.test\",\n                \"sourceControlSystem\": \"git\",\n                \"sourceControlPath\": \"/\",\n                \"sourceControlBaseRevision\": mock.ANY,\n                \"creationMethod\": \"moz-phab-git\",\n                \"lintStatus\": \"none\",\n                \"unitStatus\": \"none\",\n                \"repositoryPHID\": \"PHID-REPO-1\",\n                \"sourcePath\": mock.ANY,\n                \"branch\": \"HEAD\",\n            },\n        )\n        in call_conduit.call_args_list\n    )\n    assert (\n        mock.call(\n            \"differential.setdiffproperty\",\n            {\n                \"diff_id\": \"2\",\n                \"name\": \"local:commits\",\n                \"data\": Contains('\"summary\": \"Bug 1 - B r?alice\"')\n                & Contains(\n                    '\"message\": \"'\n                    \"Bug 1 - B r?alice\\\\n\\\\n\"\n                    \"Summary:\\\\n\\\\n\\\\n\\\\n\\\\n\"\n                    \"Test Plan:\\\\n\\\\n\"\n                    \"Reviewers: alice\\\\n\\\\n\"\n                    \"Subscribers:\\\\n\\\\n\"\n                    'Bug #: 1\"'\n                ),\n            },\n        )\n        in call_conduit.call_args_list\n    )\n\n\ndef test_submit_single_last(in_process, git_repo_path, init_sha):\n    call_conduit.side_effect = (\n        # ping\n        dict(),\n        # diffusion.repository.search\n        dict(data=[dict(phid=\"PHID-REPO-1\", fields=dict(vcs=\"git\"))]),\n        # differential.creatediff\n        dict(dict(phid=\"PHID-DIFF-1\", diffid=\"1\")),\n        # differential.setdiffproperty\n        dict(),\n        # differential.revision.edit\n        dict(object=dict(id=\"123\")),\n        # differential.setdiffproperty\n        dict(),\n    )\n    (git_repo_path / \"X\").write_text(\"a\\n\")\n    git_out(\"add\", \"X\")\n    git_out(\"commit\", \"-am\", \"A\")\n    (git_repo_path / \"X\").write_text(\"b\\n\")\n    git_out(\"commit\", \"-am\", \"B\")\n\n    mozphab.main([\"submit\", \"--yes\", \"--bug\", \"1\", \"--single\"], is_development=True)\n\n    log = git_out(\"log\", \"--format=%s%n%n%b\", \"-2\")\n    expected = \"\"\"\\\nBug 1 - B\n\nDifferential Revision: http://example.test/D123\nA\n\n\n\"\"\"\n    assert log == expected\n\n\ndef test_submit_single_first(in_process, git_repo_path, init_sha, git_sha):\n    call_conduit.side_effect = (\n        # ping\n        dict(),\n        # diffusion.repository.search\n        dict(data=[dict(phid=\"PHID-REPO-1\", fields=dict(vcs=\"git\"))]),\n        # differential.creatediff\n        dict(dict(phid=\"PHID-DIFF-1\", diffid=\"1\")),\n        # differential.setdiffproperty\n        dict(),\n        # differential.revision.edit\n        dict(object=dict(id=\"123\")),\n        # differential.setdiffproperty\n        dict(),\n    )\n    (git_repo_path / \"X\").write_text(\"a\\n\")\n    git_out(\"add\", \"X\")\n    git_out(\"commit\", \"-am\", \"A\")\n    sha = git_sha()\n    (git_repo_path / \"X\").write_text(\"b\\n\")\n    git_out(\"commit\", \"-am\", \"B\")\n\n    mozphab.main(\n        [\"submit\", \"--yes\", \"--bug\", \"1\", \"--single\", sha], is_development=True\n    )\n\n    log = git_out(\"log\", \"--format=%s%n%n%b\", \"-2\")\n    expected = \"\"\"\\\nB\n\n\nBug 1 - A\n\nDifferential Revision: http://example.test/D123\n\"\"\"\n    assert log == expected\n\n\ndef test_submit_update_no_message(in_process, git_repo_path, init_sha):\n    call_conduit.reset_mock()\n    call_conduit.side_effect = (\n        # ping\n        dict(),\n        # diffusion.repository.search\n        dict(data=[dict(phid=\"PHID-REPO-1\", fields=dict(vcs=\"git\"))]),\n        dict(data=[search_rev(rev=123)]),\n        dict(data=[search_diff()]),\n        dict(phid=\"PHID-USER-1\"),\n        # differential.creatediff\n        dict(dict(phid=\"PHID-DIFF-1\", diffid=\"1\")),\n        # differential.setdiffproperty\n        dict(),\n        # differential.revision.edit\n        dict(object=dict(id=\"123\")),\n        # differential.setdiffproperty\n        dict(),\n    )\n    (git_repo_path / \"X\").write_text(u\"\u0105\", encoding=\"utf-8\")\n    git_out(\"add\", \".\")\n    (git_repo_path / \"msg\").write_text(\n        u\"\"\"\\\nBug 1 - \u0104\n\nDifferential Revision: http://example.test/D123\n\"\"\",\n        encoding=\"utf-8\",\n    )\n    git_out(\"commit\", \"--file\", \"msg\")\n\n    mozphab.main([\"submit\", \"--yes\", \"--bug\", \"1\", init_sha], is_development=True)\n\n    log = git_out(\"log\", \"--format=%s%n%n%b\", \"-1\")\n    expected = \"\"\"\\\nBug 1 - \u0104\n\nDifferential Revision: http://example.test/D123\n\n\"\"\"\n    assert log == expected\n\n\ndef test_submit_different_author_arc(in_process, git_repo_path, init_sha):\n    call_conduit.reset_mock()\n    call_conduit.side_effect = (\n        dict(),\n        dict(data=[dict(phid=\"PHID-REPO-1\", fields=dict(vcs=\"git\"))]),\n        [{\"userName\": \"alice\", \"phid\": \"PHID-USER-1\"}],\n    )\n    testfile = git_repo_path / \"X\"\n    testfile.write_text(\"a\")\n    git_out(\"add\", \".\")\n    git_out(\n        \"commit\",\n        \"--date\",\n        \"Tue, 22 Jan 2019 13:42:48 +0000\",\n        \"--author\",\n        \"foo <foo@bar.com>\",\n        \"--message\",\n        \"A r?alice\",\n    )\n    testfile.write_text(\"b\")\n    git_out(\n        \"commit\",\n        \"--date\",\n        \"Tue, 22 Jan 2019 13:43:48 +0000\",\n        \"--author\",\n        \"bar <bar@foo.com>\",\n        \"--all\",\n        \"--message\",\n        \"B r?alice\",\n    )\n\n    mozphab.main(\n        [\"submit\", \"--arc\", \"--yes\", \"--bug\", \"1\", init_sha], is_development=True\n    )\n\n    log = git_out(\"log\", \"--format=%aD+++%an+++%ae\", \"-2\")\n    expected = \"\"\"\\\nTue, 22 Jan 2019 13:43:48 +0000+++bar+++bar@foo.com\nTue, 22 Jan 2019 13:42:48 +0000+++foo+++foo@bar.com\n\"\"\"\n    assert log == expected\n\n\ndef test_submit_utf8_author_arc(in_process, git_repo_path, init_sha):\n    call_conduit.reset_mock()\n    call_conduit.side_effect = (\n        dict(),\n        dict(data=[dict(phid=\"PHID-REPO-1\", fields=dict(vcs=\"git\"))]),\n        [{\"userName\": \"alice\", \"phid\": \"PHID-USER-1\"}],\n    )\n    testfile = git_repo_path / \"X\"\n    testfile.write_text(\"a\")\n    git_out(\"add\", \".\")\n    git_out(\n        \"commit\",\n        \"--date\",\n        \"Tue, 22 Jan 2019 13:42:48 +0000\",\n        \"--author\",\n        \"\u0107wik\u0142a <\u0107wik\u0142a@bar.com>\",\n        \"--message\",\n        \"A r?alice\",\n    )\n\n    mozphab.main(\n        [\"submit\", \"--arc\", \"--yes\", \"--bug\", \"1\", init_sha], is_development=True\n    )\n\n    log = git_out(\"log\", \"--format=%aD+++%an+++%ae\", \"-1\")\n    expected = \"Tue, 22 Jan 2019 13:42:48 +0000+++\u0107wik\u0142a+++\u0107wik\u0142a@bar.com\\n\"\n    assert log == expected\n\n\ndef test_submit_update_arc(in_process, git_repo_path, init_sha):\n    call_conduit.reset_mock()\n    call_conduit.side_effect = (\n        {},  # ping\n        dict(data=[dict(phid=\"PHID-REPO-1\", fields=dict(vcs=\"git\"))]),\n        dict(data=[search_rev(rev=123)]),\n        dict(data=[search_diff()]),\n        dict(phid=\"PHID-USER-1\"),\n    )\n    testfile = git_repo_path / \"X\"\n    testfile.write_text(\"a\")\n    git_out(\"add\", \".\")\n\n    # Write out our commit message as if the program had already run and appended\n    # a Differential Revision keyword to the commit body for tracking.\n    git_out(\n        \"commit\",\n        \"--message\",\n        \"\"\"\\\nBug 1 - A\n\nDifferential Revision: http://example.test/D123\n\"\"\",\n    )\n\n    mozphab.main(\n        [\"submit\", \"--arc\", \"--yes\"]\n        + [\"--bug\", \"1\"]\n        + [\"--message\", \"update message \u0107wik\u0142a\"]\n        + [init_sha],\n        is_development=True,\n    )\n\n    log = git_out(\"log\", \"--format=%s%n%n%b\", \"-1\")\n    expected = \"\"\"\\\nBug 1 - A\n\nDifferential Revision: http://example.test/D123\n\n\"\"\"\n    assert log == expected\n\n\ndef test_submit_update_bug_id_arc(in_process, git_repo_path, init_sha):\n    call_conduit.reset_mock()\n    call_conduit.side_effect = (\n        dict(),\n        dict(data=[dict(phid=\"PHID-REPO-1\", fields=dict(vcs=\"git\"))]),\n        dict(data=[search_rev(rev=123)]),\n        dict(data=[search_diff()]),\n        # get reviewers for updated revision\n        dict(phid=\"PHID-USER-1\"),\n    )\n    arc_call_conduit.reset_mock()\n    arc_call_conduit.side_effect = (\n        {},\n        {\"data\": {}},\n    )\n    testfile = git_repo_path / \"X\"\n    testfile.write_text(\"a\")\n    git_out(\"add\", \".\")\n\n    # Write out our commit message as if the program had already run and appended\n    # a Differential Revision keyword to the commit body for tracking.\n    git_out(\n        \"commit\",\n        \"--message\",\n        \"\"\"\\\nBug 1 - A\n\nDifferential Revision: http://example.test/D123\n\"\"\",\n    )\n\n    mozphab.main(\n        [\"submit\", \"--arc\", \"--yes\", \"--bug\", \"2\", init_sha], is_development=True\n    )\n\n    arc_call_conduit.assert_called_with(\n        \"differential.revision.edit\",\n        {\n            \"objectIdentifier\": \"D123\",\n            \"transactions\": [{\"type\": \"bugzilla.bug-id\", \"value\": \"2\"}],\n        },\n        mock.ANY,\n    )\n\n\ndef test_submit_update_revision_not_found(in_process, git_repo_path, init_sha):\n    call_conduit.reset_mock()\n    call_conduit.side_effect = (\n        # ping\n        dict(),\n        dict(data=[dict(phid=\"PHID-REPO-1\", fields=dict(vcs=\"git\"))]),\n        # response for searching D123 and D124\n        dict(data=[search_rev(rev=123)]),\n        dict(data=[search_diff()]),\n        # moz-phab asks again for D124\n        dict(data=[]),\n        # whoami\n        dict(phid=\"PHID-USER-1\"),\n        # moz-phab asks again for D124\n        dict(data=[]),\n        # moz-phab asks again for D124\n        dict(data=[]),\n    )\n    testfile = git_repo_path / \"X\"\n    testfile.write_text(u\"\u0105\", encoding=\"utf-8\")\n    git_out(\"add\", \".\")\n    msgfile = git_repo_path / \"msg\"\n    msgfile.write_text(\n        u\"\"\"\\\nBug 1 - \u0104\n\nDifferential Revision: http://example.test/D123\n\"\"\",\n        encoding=\"utf-8\",\n    )\n    git_out(\"commit\", \"--file\", \"msg\")\n    testfile.write_text(u\"missing repo\")\n    msgfile.write_text(\n        u\"\"\"\\\nBug 1 - missing revision\n\nDifferential Revision: http://example.test/D124\n\"\"\"\n    )\n    git_out(\"commit\", \"--all\", \"--file\", \"./msg\")\n\n    with pytest.raises(exceptions.Error) as excinfo:\n        mozphab.main(\n            [\"submit\", \"--yes\"]\n            + [\"--bug\", \"1\"]\n            + [\"--message\", \"update message \u0107wik\u0142a\"]\n            + [init_sha],\n            is_development=True,\n        )\n    assert \"query result for revision D124\" in str(excinfo.value)\n\n\ndef test_empty_file(in_process, git_repo_path, init_sha):\n    # Add an empty file\n    call_conduit.side_effect = (\n        # ping\n        dict(),\n        # diffusion.repository.search\n        dict(data=[dict(phid=\"PHID-REPO-1\", fields=dict(vcs=\"git\"))]),\n        # differential.creatediff\n        dict(dict(phid=\"PHID-DIFF-1\", diffid=\"1\")),\n        # differential.setdiffproperty\n        dict(),\n        # differential.revision.edit\n        dict(object=dict(id=\"123\")),\n        # differential.setdiffproperty\n        dict(),\n    )\n    testfile = git_repo_path / \"X\"\n    testfile.touch()\n    git_out(\"add\", \".\")\n    git_out(\"commit\", \"--message\", \"A\")\n\n    mozphab.main([\"submit\", \"--yes\", \"--bug\", \"1\", init_sha], is_development=True)\n\n    log = git_out(\"log\", \"--format=%s%n%n%b\", \"-1\")\n    expected = \"\"\"\nBug 1 - A\n\nDifferential Revision: http://example.test/D123\n\"\"\"\n    assert log.strip() == expected.strip()\n\n    # Rempve an empty file\n    call_conduit.reset_mock()\n    call_conduit.side_effect = (\n        # differential.creatediff\n        dict(dict(phid=\"PHID-DIFF-2\", diffid=\"2\")),\n        # differential.setdiffproperty\n        dict(),\n        # differential.revision.edit\n        dict(object=dict(id=\"124\")),\n        # differential.setdiffproperty\n        dict(),\n    )\n    testfile.unlink()\n    git_out(\"commit\", \"-a\", \"--message\", \"B\")\n    mozphab.main([\"submit\", \"--yes\", \"--bug\", \"1\", \"HEAD~\"], is_development=True)\n    log = git_out(\"log\", \"--format=%s%n%n%b\", \"-1\")\n    expected = \"\"\"\nBug 1 - B\n\nDifferential Revision: http://example.test/D124\n\"\"\"\n    assert log.strip() == expected.strip()\n"
  },
  {
    "prompt": "class EnvironmentParseError\n\n\"\"\"Splits space-separated variable assignments into a list of individual assignments.\n\n>>> split_env_items('VAR=abc')\n['VAR=abc']\n>>> split_env_items('VAR=\"a string\" THING=3')\n['VAR=\"a string\"', 'THING=3']\n>>> split_env_items('VAR=\"a string\" THING=\\'single \"quotes\"\\'')\n['VAR=\"a string\"', 'THING=\\'single \"quotes\"\\'']\n>>> split_env_items('VAR=\"dont \\\\\"forget\\\\\" about backslashes\"')\n['VAR=\"dont \\\\\"forget\\\\\" about backslashes\"']\n>>> split_env_items('PATH=\"$PATH;/opt/cibw_test_path\"')\n['PATH=\"$PATH;/opt/cibw_test_path\"']\n>>> split_env_items('PATH2=\"something with spaces\"')\n['PATH2=\"something with spaces\"']\"\"\"\nfn split_env_items(env_string: str) -> List[str]\n\nclass EnvironmentAssignment\n\nclass ParsedEnvironment\n\nfn parse_environment(env_string: str) -> ParsedEnvironment\n\nclass DockerContainer\n\nfn shell_quote(path: PurePath) -> str\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import platform\nimport random\nimport shutil\nimport subprocess\nimport textwrap\nfrom pathlib import Path, PurePath\n\nimport pytest\n\nfrom cibuildwheel.docker_container import DockerContainer\nfrom cibuildwheel.environment import EnvironmentAssignment\n\n# for these tests we use manylinux2014 images, because they're available on\n# multi architectures and include python3.8\npm = platform.machine()\nif pm == \"x86_64\":\n    DEFAULT_IMAGE = \"quay.io/pypa/manylinux2014_x86_64:2020-05-17-2f8ac3b\"\nelif pm == \"aarch64\":\n    DEFAULT_IMAGE = \"quay.io/pypa/manylinux2014_aarch64:2020-05-17-2f8ac3b\"\nelif pm == \"ppc64le\":\n    DEFAULT_IMAGE = \"quay.io/pypa/manylinux2014_ppc64le:2020-05-17-2f8ac3b\"\nelif pm == \"s390x\":\n    DEFAULT_IMAGE = \"quay.io/pypa/manylinux2014_s390x:2020-05-17-2f8ac3b\"\n\n\n@pytest.mark.docker\ndef test_simple():\n    with DockerContainer(DEFAULT_IMAGE) as container:\n        assert container.call([\"echo\", \"hello\"], capture_output=True) == \"hello\\n\"\n\n\n@pytest.mark.docker\ndef test_no_lf():\n    with DockerContainer(DEFAULT_IMAGE) as container:\n        assert container.call([\"printf\", \"hello\"], capture_output=True) == \"hello\"\n\n\n@pytest.mark.docker\ndef test_environment():\n    with DockerContainer(DEFAULT_IMAGE) as container:\n        assert (\n            container.call(\n                [\"sh\", \"-c\", \"echo $TEST_VAR\"], env={\"TEST_VAR\": \"1\"}, capture_output=True\n            )\n            == \"1\\n\"\n        )\n\n\n@pytest.mark.docker\ndef test_cwd():\n    with DockerContainer(DEFAULT_IMAGE, cwd=\"/cibuildwheel/working_directory\") as container:\n        assert container.call([\"pwd\"], capture_output=True) == \"/cibuildwheel/working_directory\\n\"\n        assert container.call([\"pwd\"], capture_output=True, cwd=\"/opt\") == \"/opt\\n\"\n\n\n@pytest.mark.docker\ndef test_container_removed():\n    with DockerContainer(DEFAULT_IMAGE) as container:\n        docker_containers_listing = subprocess.run(\n            \"docker container ls\",\n            shell=True,\n            check=True,\n            stdout=subprocess.PIPE,\n            universal_newlines=True,\n        ).stdout\n        assert container.name in docker_containers_listing\n        old_container_name = container.name\n\n    docker_containers_listing = subprocess.run(\n        \"docker container ls\",\n        shell=True,\n        check=True,\n        stdout=subprocess.PIPE,\n        universal_newlines=True,\n    ).stdout\n    assert old_container_name not in docker_containers_listing\n\n\n@pytest.mark.docker\ndef test_large_environment():\n    # max environment variable size is 128kB\n    long_env_var_length = 127 * 1024\n    large_environment = {\n        \"a\": \"0\" * long_env_var_length,\n        \"b\": \"0\" * long_env_var_length,\n        \"c\": \"0\" * long_env_var_length,\n        \"d\": \"0\" * long_env_var_length,\n    }\n\n    with DockerContainer(DEFAULT_IMAGE) as container:\n        # check the length of d\n        assert (\n            container.call([\"sh\", \"-c\", \"echo ${#d}\"], env=large_environment, capture_output=True)\n            == f\"{long_env_var_length}\\n\"\n        )\n\n\n@pytest.mark.docker\ndef test_binary_output():\n    with DockerContainer(DEFAULT_IMAGE) as container:\n        # note: the below embedded snippets are in python2\n\n        # check that we can pass though arbitrary binary data without erroring\n        container.call(\n            [\n                \"/usr/bin/python2\",\n                \"-c\",\n                textwrap.dedent(\n                    \"\"\"\n                    import sys\n                    sys.stdout.write(''.join(chr(n) for n in range(0, 256)))\n                    \"\"\"\n                ),\n            ]\n        )\n\n        # check that we can capture arbitrary binary data\n        output = container.call(\n            [\n                \"/usr/bin/python2\",\n                \"-c\",\n                textwrap.dedent(\n                    \"\"\"\n                    import sys\n                    sys.stdout.write(''.join(chr(n % 256) for n in range(0, 512)))\n                    \"\"\"\n                ),\n            ],\n            capture_output=True,\n        )\n\n        data = bytes(output, encoding=\"utf8\", errors=\"surrogateescape\")\n\n        for i in range(512):\n            assert data[i] == i % 256\n\n        # check that environment variables can carry binary data, except null characters\n        # (https://www.gnu.org/software/libc/manual/html_node/Environment-Variables.html)\n        binary_data = bytes(n for n in range(1, 256))\n        binary_data_string = str(binary_data, encoding=\"utf8\", errors=\"surrogateescape\")\n        output = container.call(\n            [\"python2\", \"-c\", 'import os, sys; sys.stdout.write(os.environ[\"TEST_VAR\"])'],\n            env={\"TEST_VAR\": binary_data_string},\n            capture_output=True,\n        )\n        assert output == binary_data_string\n\n\n@pytest.mark.docker\ndef test_file_operations(tmp_path: Path):\n    with DockerContainer(DEFAULT_IMAGE) as container:\n        # test copying a file in\n        test_binary_data = bytes(random.randrange(256) for _ in range(1000))\n        original_test_file = tmp_path / \"test.dat\"\n        original_test_file.write_bytes(test_binary_data)\n\n        dst_file = PurePath(\"/tmp/test.dat\")\n\n        container.copy_into(original_test_file, dst_file)\n\n        output = container.call([\"cat\", dst_file], capture_output=True)\n        assert test_binary_data == bytes(output, encoding=\"utf8\", errors=\"surrogateescape\")\n\n\n@pytest.mark.docker\ndef test_dir_operations(tmp_path: Path):\n    with DockerContainer(DEFAULT_IMAGE) as container:\n        test_binary_data = bytes(random.randrange(256) for _ in range(1000))\n        original_test_file = tmp_path / \"test.dat\"\n        original_test_file.write_bytes(test_binary_data)\n\n        # test copying a dir in\n        test_dir = tmp_path / \"test_dir\"\n        test_dir.mkdir()\n        test_file = test_dir / \"test.dat\"\n        shutil.copyfile(original_test_file, test_file)\n\n        dst_dir = PurePath(\"/tmp/test_dir\")\n        dst_file = dst_dir / \"test.dat\"\n        container.copy_into(test_dir, dst_dir)\n\n        output = container.call([\"cat\", dst_file], capture_output=True)\n        assert test_binary_data == bytes(output, encoding=\"utf8\", errors=\"surrogateescape\")\n\n        # test glob\n        assert container.glob(dst_dir, \"*.dat\") == [dst_file]\n\n        # test copy dir out\n        new_test_dir = tmp_path / \"test_dir_new\"\n        container.copy_out(dst_dir, new_test_dir)\n\n        assert test_binary_data == (new_test_dir / \"test.dat\").read_bytes()\n\n\n@pytest.mark.docker\ndef test_environment_executor():\n    with DockerContainer(DEFAULT_IMAGE) as container:\n        assignment = EnvironmentAssignment(\"TEST=$(echo 42)\")\n        assert assignment.evaluated_value({}, container.environment_executor) == \"42\"\n"
  },
  {
    "prompt": "class ConservedVars\n\n\"\"\":arg leading_shape: a tuple with which ``ary.shape`` is expected to begin.\"\"\"\nfn _aux_shape(ary, leading_shape: None)\n\n\"\"\"Get the canonical conserved quantities.\n\nReturn a :class:`ConservedVars` that is the canonical conserved quantities,\nmass, energy, and momentum from the agglomerated object array extracted\nfrom the state vector *q*.\"\"\"\nfn split_conserved(dim, q)\n\n\"\"\"Create an agglomerated solution array from the conserved quantities.\"\"\"\nfn join_conserved(dim, mass, energy, momentum)\n\n\"\"\"Compute the inviscid flux vectors from flow solution *q*.\n\nThe inviscid fluxes are\n$(\\rho\\vec{V},(\\rho{E}+p)\\vec{V},\\rho(\\vec{V}\\otimes\\vec{V})+p\\mathbf{I})$\"\"\"\nfn inviscid_flux(discr, eos, q)\n\n\"\"\"Return the maximum wavespeed in for flow solution *q*.\"\"\"\nfn _get_wavespeed(dim, eos, cv: ConservedVars)\n\n\"\"\"Return the flux across a face given the solution on both sides *q_tpair*.\n\nParameters\n----------\neos: mirgecom.eos.GasEOS\n    Implementing the pressure and temperature functions for\n    returning pressure and temperature as a function of the state q.\n\nq_tpair: :class:`grudge.symbolic.TracePair`\n    Trace pair for the face upon which flux calculation is to be performed\n\nlocal: bool\n    Indicates whether to skip projection of fluxes to \"all_faces\" or not. If\n    set to *False* (the default), the returned fluxes are projected to\n    \"all_faces.\"  If set to *True*, the returned fluxes are not projected to\n    \"all_faces\"; remaining instead on the boundary restriction.\"\"\"\nfn _facial_flux(discr, eos: mirgecom.eos.GasEOS, q_tpair: :class:`grudge.symbolic.TracePair`, local: bool)\n\n\"\"\"Compute RHS of the Euler flow equations.\n\nReturns\n-------\nnumpy.ndarray\n    The right-hand-side of the Euler flow equations:\n\n    .. math::\n\n        \\dot{\\mathbf{q}} = \\mathbf{S} - \\nabla\\cdot\\mathbf{F} +\n            (\\mathbf{F}\\cdot\\hat{n})_{\\partial\\Omega}\n\nParameters\n----------\nq\n    State array which expects at least the canonical conserved quantities\n    (mass, energy, momentum) for the fluid at each point.\n\nboundaries\n    Dictionary of boundary functions, one for each valid btag\n\nt\n    Time\n\neos: mirgecom.eos.GasEOS\n    Implementing the pressure and temperature functions for\n    returning pressure and temperature as a function of the state q.\"\"\"\nfn inviscid_operator(discr, eos: mirgecom.eos.GasEOS, boundaries: None, q: None, t: None) -> numpy.ndarray\n\n\"\"\"Calculate and return CFL based on current state and timestep.\"\"\"\nfn get_inviscid_cfl(discr, eos, dt, q)\n\n\"\"\"Routine (will) return the (local) maximum stable inviscid timestep.\n\nCurrently, it's a hack waiting for the geometric_factor helpers port\nfrom grudge.\"\"\"\nfn get_inviscid_timestep(discr, eos, cfl, q)\n\nclass EOSDependentVars\n\nclass GasEOS\n\nclass IdealSingleGas\n\n\"\"\"Construct uniform, constant flow.\n\nConstruct a uniform, constant flow from mass, energy, pressure, and\nan EOS object.\n\nParameters\n----------\nx_vec: np.ndarray\n    Nodal positions\nmass: float\n    Value to set $\\rho$\nenergy: float\n    Optional value to set $\\rho{E}$\npressure: float\n    Value to use for calculating $\\rho{E}$\nvelocity: np.ndarray\n    Optional constant velocity to set $\\rho\\vec{V}$\n\nReturns\n-------\nq: Object array of DOFArrays\n    Agglomerated object array with the uniform flow\"\"\"\nfn _make_uniform_flow(x_vec: np.ndarray, mass: float, energy: float, pressure: float, velocity: np.ndarray, eos) -> Object array of DOFArrays\n\n\"\"\"Create a Gaussian pulse.\n\nThe Gaussian pulse is defined by:\n\n.. math::\n\n    G(\\vec{r}) = a_0*\\exp^{-(\\frac{(\\vec{r}-\\vec{r_0})}{\\sqrt{2}w})^{2}}\\\\\n\nWhere $\\vec{r}$ is the position, and the parameters are\nthe pulse amplitude $a_0$, the pulse location $\\vec{r_0}$, and the\nRMS width of the pulse, $w$.\n\nParameters\n----------\namp: float\n    specifies the value of $\\a_0$, the pulse amplitude\nr0: float array\n    specifies the value of $\\r_0$, the pulse location\nw: float\n    specifies the value of $w$, the rms pulse width\nr: Object array of DOFArrays\n    specifies the nodal coordinates\n\nReturns\n-------\nG: float array\n    The values of the exponential function\"\"\"\nfn _make_pulse(amp: float, r0: float array, w: float, r: Object array of DOFArrays) -> float array\n\nclass Vortex2D\n\nclass SodShock1D\n\nclass Lump\n\nclass AcousticPulse\n\nclass Uniform\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "__copyright__ = \"\"\"\nCopyright (C) 2020 University of Illinois Board of Trustees\n\"\"\"\n\n__license__ = \"\"\"\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\"\"\"\n\nimport logging\nimport numpy as np\nimport numpy.linalg as la  # noqa\nimport pyopencl as cl\nimport pyopencl.clrandom\nimport pyopencl.clmath\nimport pytest\n\nfrom meshmode.array_context import PyOpenCLArrayContext\nfrom meshmode.dof_array import thaw\nfrom meshmode.mesh import BTAG_ALL, BTAG_NONE  # noqa\n\nfrom mirgecom.initializers import Vortex2D\nfrom mirgecom.initializers import Lump\nfrom mirgecom.euler import split_conserved\nfrom mirgecom.initializers import SodShock1D\nfrom mirgecom.eos import IdealSingleGas\n\nfrom grudge.eager import EagerDGDiscretization\nfrom pyopencl.tools import (  # noqa\n    pytest_generate_tests_for_pyopencl as pytest_generate_tests,\n)\n\nlogger = logging.getLogger(__name__)\n\n\ndef test_lump_init(ctx_factory):\n    \"\"\"\n    Simple test to check that Lump initializer\n    creates the expected solution field.\n    \"\"\"\n    cl_ctx = ctx_factory()\n    queue = cl.CommandQueue(cl_ctx)\n    actx = PyOpenCLArrayContext(queue)\n    dim = 2\n    nel_1d = 4\n\n    from meshmode.mesh.generation import generate_regular_rect_mesh\n\n    mesh = generate_regular_rect_mesh(\n        a=[(0.0,), (-5.0,)], b=[(10.0,), (5.0,)], n=(nel_1d,) * dim\n    )\n\n    order = 3\n    logger.info(f\"Number of elements: {mesh.nelements}\")\n\n    discr = EagerDGDiscretization(actx, mesh, order=order)\n    nodes = thaw(actx, discr.nodes())\n\n    # Init soln with Vortex\n    center = np.zeros(shape=(dim,))\n    velocity = np.zeros(shape=(dim,))\n    center[0] = 5\n    velocity[0] = 1\n    lump = Lump(center=center, velocity=velocity)\n    lump_soln = lump(0, nodes)\n\n    cv = split_conserved(dim, lump_soln)\n    p = 0.4 * (cv.energy - 0.5 * np.dot(cv.momentum, cv.momentum) / cv.mass)\n    exp_p = 1.0\n    errmax = discr.norm(p - exp_p, np.inf)\n\n    logger.info(f\"lump_soln = {lump_soln}\")\n    logger.info(f\"pressure = {p}\")\n\n    assert errmax < 1e-15\n\n\ndef test_vortex_init(ctx_factory):\n    \"\"\"\n    Simple test to check that Vortex2D initializer\n    creates the expected solution field.\n    \"\"\"\n    cl_ctx = ctx_factory()\n    queue = cl.CommandQueue(cl_ctx)\n    actx = PyOpenCLArrayContext(queue)\n    dim = 2\n    nel_1d = 4\n\n    from meshmode.mesh.generation import generate_regular_rect_mesh\n\n    mesh = generate_regular_rect_mesh(\n        a=[(0.0,), (-5.0,)], b=[(10.0,), (5.0,)], n=(nel_1d,) * dim\n    )\n\n    order = 3\n    logger.info(f\"Number of elements: {mesh.nelements}\")\n\n    discr = EagerDGDiscretization(actx, mesh, order=order)\n    nodes = thaw(actx, discr.nodes())\n\n    # Init soln with Vortex\n    vortex = Vortex2D()\n    vortex_soln = vortex(0, nodes)\n    gamma = 1.4\n    cv = split_conserved(dim, vortex_soln)\n    p = 0.4 * (cv.energy - 0.5 * np.dot(cv.momentum, cv.momentum) / cv.mass)\n    exp_p = cv.mass ** gamma\n    errmax = discr.norm(p - exp_p, np.inf)\n\n    logger.info(f\"vortex_soln = {vortex_soln}\")\n    logger.info(f\"pressure = {p}\")\n\n    assert errmax < 1e-15\n\n\ndef test_shock_init(ctx_factory):\n    \"\"\"\n    Simple test to check that Shock1D initializer\n    creates the expected solution field.\n    \"\"\"\n    cl_ctx = ctx_factory()\n    queue = cl.CommandQueue(cl_ctx)\n    actx = PyOpenCLArrayContext(queue)\n\n    nel_1d = 10\n    dim = 2\n\n    from meshmode.mesh.generation import generate_regular_rect_mesh\n\n    mesh = generate_regular_rect_mesh(\n        a=[(0.0,), (1.0,)], b=[(-0.5,), (0.5,)], n=(nel_1d,) * dim\n    )\n\n    order = 3\n    print(f\"Number of elements: {mesh.nelements}\")\n\n    discr = EagerDGDiscretization(actx, mesh, order=order)\n    nodes = thaw(actx, discr.nodes())\n\n    initr = SodShock1D()\n    initsoln = initr(t=0.0, x_vec=nodes)\n    print(\"Sod Soln:\", initsoln)\n    xpl = 1.0\n    xpr = 0.1\n    tol = 1e-15\n    nodes_x = nodes[0]\n    eos = IdealSingleGas()\n    cv = split_conserved(dim, initsoln)\n    p = eos.pressure(cv)\n\n    assert discr.norm(actx.np.where(nodes_x < 0.5, p-xpl, p-xpr), np.inf) < tol\n\n\n@pytest.mark.parametrize(\"dim\", [1, 2, 3])\ndef test_uniform(ctx_factory, dim):\n    \"\"\"\n    Simple test to check that Uniform initializer\n    creates the expected solution field.\n    \"\"\"\n    cl_ctx = ctx_factory()\n    queue = cl.CommandQueue(cl_ctx)\n    actx = PyOpenCLArrayContext(queue)\n\n    nel_1d = 2\n\n    from meshmode.mesh.generation import generate_regular_rect_mesh\n\n    mesh = generate_regular_rect_mesh(\n        a=(-0.5,) * dim, b=(0.5,) * dim, n=(nel_1d,) * dim\n    )\n\n    order = 1\n    print(f\"Number of elements: {mesh.nelements}\")\n\n    discr = EagerDGDiscretization(actx, mesh, order=order)\n    nodes = thaw(actx, discr.nodes())\n    print(f\"DIM = {dim}, {len(nodes)}\")\n    print(f\"Nodes={nodes}\")\n\n    from mirgecom.initializers import Uniform\n    initr = Uniform(numdim=dim)\n    initsoln = initr(t=0.0, x_vec=nodes)\n    tol = 1e-15\n    ssoln = split_conserved(dim, initsoln)\n\n    assert discr.norm(ssoln.mass - 1.0, np.inf) < tol\n    assert discr.norm(ssoln.energy - 2.5, np.inf) < tol\n\n    print(f\"Uniform Soln:{initsoln}\")\n    eos = IdealSingleGas()\n    cv = split_conserved(dim, initsoln)\n    p = eos.pressure(cv)\n    print(f\"Press:{p}\")\n\n    assert discr.norm(p - 1.0, np.inf) < tol\n\n\n@pytest.mark.parametrize(\"dim\", [1, 2, 3])\ndef test_pulse(ctx_factory, dim):\n    \"\"\"\n    Test of Gaussian pulse generator.\n    If it looks, walks, and quacks like a duck, then ...\n    \"\"\"\n    cl_ctx = ctx_factory()\n    queue = cl.CommandQueue(cl_ctx)\n    actx = PyOpenCLArrayContext(queue)\n\n    nel_1d = 10\n\n    from meshmode.mesh.generation import generate_regular_rect_mesh\n\n    mesh = generate_regular_rect_mesh(\n        a=(-0.5,) * dim, b=(0.5,) * dim, n=(nel_1d,) * dim\n    )\n\n    order = 1\n    print(f\"Number of elements: {mesh.nelements}\")\n\n    discr = EagerDGDiscretization(actx, mesh, order=order)\n    nodes = thaw(actx, discr.nodes())\n    print(f\"DIM = {dim}, {len(nodes)}\")\n    print(f\"Nodes={nodes}\")\n\n    tol = 1e-15\n    from mirgecom.initializers import _make_pulse\n    amp = 1.0\n    w = .1\n    rms2 = w * w\n    r0 = np.zeros(dim)\n    r2 = np.dot(nodes, nodes) / rms2\n    pulse = _make_pulse(amp=amp, r0=r0, w=w, r=nodes)\n    print(f\"Pulse = {pulse}\")\n\n    # does it return the expected exponential?\n    pulse_check = actx.np.exp(-.5 * r2)\n    print(f\"exact: {pulse_check}\")\n    pulse_resid = pulse - pulse_check\n    print(f\"pulse residual: {pulse_resid}\")\n    assert(discr.norm(pulse_resid, np.inf) < tol)\n\n    # proper scaling with amplitude?\n    amp = 2.0\n    pulse = 0\n    pulse = _make_pulse(amp=amp, r0=r0, w=w, r=nodes)\n    pulse_resid = pulse - (pulse_check + pulse_check)\n    assert(discr.norm(pulse_resid, np.inf) < tol)\n\n    # proper scaling with r?\n    amp = 1.0\n    rcheck = np.sqrt(2.0) * nodes\n    pulse = _make_pulse(amp=amp, r0=r0, w=w, r=rcheck)\n    assert(discr.norm(pulse - (pulse_check * pulse_check), np.inf) < tol)\n\n    # proper scaling with w?\n    w = w / np.sqrt(2.0)\n    pulse = _make_pulse(amp=amp, r0=r0, w=w, r=nodes)\n    assert(discr.norm(pulse - (pulse_check * pulse_check), np.inf) < tol)\n"
  },
  {
    "prompt": "fn _vcr_integration_test(decorated)\n\nclass EngineOverrideConfigSource\n\nclass FallbackConfigSource\n\nclass IntegrationTestsV1\n\nclass IntegrationTestsV2\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"Integration tests for Glesys\"\"\"\nfrom unittest import TestCase\n\nimport pytest\nfrom lexicon.tests.providers.integration_tests import IntegrationTestsV1\n\n\n# Hook into testing framework by inheriting unittest.TestCase and reuse\n# the tests which *each and every* implementation of the interface must\n# pass, by inheritance from define_tests.TheTests\n# TODO: migrate to IntegrationTestsV2 and its extended test suite\nclass GlesysProviderTests(TestCase, IntegrationTestsV1):\n    \"\"\"TestCase for Glesys\"\"\"\n    provider_name = 'glesys'\n    domain = \"capsulecd.com\"\n\n    def _filter_headers(self):\n        return ['Authorization']\n\n    # TODO: enable the skipped tests\n    @pytest.mark.skip(reason=\"new test, missing recording\")\n    def test_provider_when_calling_update_record_should_modify_record_name_specified(self):\n        return\n"
  },
  {
    "prompt": "\"\"\"Create a jwt bearer token with the correct keys, roles and username.\"\"\"\nfn helper_create_jwt(jwt_manager, roles: List[str], username: str)\n\n\"\"\"Return a header containing a JWT bearer token.\"\"\"\nfn create_header(jwt_manager, roles: List[str], username: str)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright \u00a9 2019 Province of British Columbia\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests to assure the Authorization Services.\n\nTest-Suite to ensure that the Authorization Service is working as expected.\n\"\"\"\nfrom http import HTTPStatus\n\nimport pytest\nfrom flask import jsonify\nfrom legal_api.models.business import Business\n\nfrom legal_api.services.authz import BASIC_USER, COLIN_SVC_ROLE, STAFF_ROLE, authorized, get_allowed, is_allowed\nfrom tests import integration_authorization, not_github_ci\n\nfrom .utils import helper_create_jwt\n\n\ndef test_jwt_manager_initialized(jwt):\n    \"\"\"Assert that the jwt_manager is created as part of the fixtures.\"\"\"\n    assert jwt\n\n\n@not_github_ci\ndef test_jwt_manager_correct_test_config(app_request, jwt):\n    \"\"\"Assert that the test configuration for the JWT is working as expected.\"\"\"\n    message = 'This is a protected end-point'\n    protected_route = '/fake_jwt_route'\n\n    @app_request.route(protected_route)\n    @jwt.has_one_of_roles([STAFF_ROLE])\n    def get():\n        return jsonify(message=message)\n\n    # assert that JWT is setup correctly for a known role\n    token = helper_create_jwt(jwt, [STAFF_ROLE])\n    headers = {'Authorization': 'Bearer ' + token}\n    rv = app_request.test_client().get(protected_route, headers=headers)\n    assert rv.status_code == HTTPStatus.OK\n\n    # assert the JWT fails for an unknown role\n    token = helper_create_jwt(jwt, ['SHOULD-FAIL'])\n    headers = {'Authorization': 'Bearer ' + token}\n    rv = app_request.test_client().get(protected_route, headers=headers)\n    assert rv.status_code == HTTPStatus.UNAUTHORIZED\n\n\nTEST_AUTHZ_DATA = [\n    ('staff_role',  # test name\n     'CP1234567',  # business identifier\n     'happy-staff',  # username\n     [STAFF_ROLE],  # roles\n     ['view', 'edit'],  # allowed actions\n     ['edit'],  # requested action\n     HTTPStatus.OK),  # expected response\n    ('colin svc role', 'CP1234567', 'CP1234567', [COLIN_SVC_ROLE], ['view', 'edit'], ['edit'],\n     HTTPStatus.OK),\n    ('authorized_user', 'CP0001237', 'CP1234567', [BASIC_USER], ['view', 'edit'], ['edit'],\n     HTTPStatus.OK),\n    ('unauthorized_user', 'CP1234567', 'Not-Match-Identifier', [BASIC_USER], None, ['edit'],\n     HTTPStatus.METHOD_NOT_ALLOWED),\n    ('missing_action', 'CP1234567', 'Not-Match-Identifier', [BASIC_USER], None, None,\n     HTTPStatus.METHOD_NOT_ALLOWED),\n    ('invalid_action', 'CP1234567', 'Not-Match-Identifier', [BASIC_USER], None, ['scrum'],\n     HTTPStatus.METHOD_NOT_ALLOWED),\n    ('add_comment_not_allowed', 'CP0001237', 'CP1234567', [BASIC_USER], None, ['add_comment'],\n     HTTPStatus.METHOD_NOT_ALLOWED),\n    ('court_order_not_allowed', 'CP0001237', 'CP1234567', [BASIC_USER], None, ['court_order'],\n     HTTPStatus.METHOD_NOT_ALLOWED),\n    ('registrars_notation_not_allowed', 'CP0001237', 'CP1234567', [BASIC_USER], None, ['registrars_notation'],\n     HTTPStatus.METHOD_NOT_ALLOWED),\n    ('registrars_order_not_allowed', 'CP0001237', 'CP1234567', [BASIC_USER], None, ['registrars_order'],\n     HTTPStatus.METHOD_NOT_ALLOWED),\n]\n\n\n@not_github_ci\n@pytest.mark.parametrize('test_name,identifier,username,roles,allowed_actions,requested_actions,expected',\n                         TEST_AUTHZ_DATA)\ndef test_authorized_user(monkeypatch, app_request, jwt,\n                         test_name, identifier, username, roles, allowed_actions, requested_actions, expected):\n    \"\"\"Assert that the type of user authorization is correct, based on the expected outcome.\"\"\"\n    from requests import Response\n    print(test_name)\n\n    # mocks, the get and json calls for requests.Response\n    def mock_get(*args, **kwargs):  # pylint: disable=unused-argument; mocks of library methods\n        resp = Response()\n        resp.status_code = 200\n        return resp\n\n    def mock_json(self, **kwargs):  # pylint: disable=unused-argument; mocks of library methods\n        return {'roles': allowed_actions}\n\n    monkeypatch.setattr('requests.sessions.Session.get', mock_get)\n    monkeypatch.setattr('requests.Response.json', mock_json)\n\n    # setup\n    @app_request.route('/fake_jwt_route/<string:identifier>')\n    @jwt.requires_auth\n    def get_fake(identifier: str):\n        if not authorized(identifier, jwt, ['view']):\n            return jsonify(message='failed'), HTTPStatus.METHOD_NOT_ALLOWED\n        return jsonify(message='success'), HTTPStatus.OK\n\n    token = helper_create_jwt(jwt, roles=roles, username=username)\n    headers = {'Authorization': 'Bearer ' + token}\n\n    # test it\n    rv = app_request.test_client().get(f'/fake_jwt_route/{identifier}', headers=headers)\n\n    # check it\n    assert rv.status_code == expected\n\n\nTEST_INTEG_AUTHZ_DATA = [\n    ('staff_role',  # test name\n     'CP1234567',  # business identifier\n     'happy-staff',  # username\n     [STAFF_ROLE],  # roles\n     ['view', 'edit'],  # allowed actions\n     ['edit'],  # requested action\n     HTTPStatus.OK),  # expected response\n    ('colin svc role', 'CP1234567', 'CP1234567', [COLIN_SVC_ROLE], ['view', 'edit'], ['edit'],\n     HTTPStatus.OK),\n    ('unauthorized_user', 'CP1234567', 'Not-Match-Identifier', [BASIC_USER], None, ['edit'],\n     HTTPStatus.METHOD_NOT_ALLOWED),\n    ('missing_action', 'CP1234567', 'Not-Match-Identifier', [BASIC_USER], None, None,\n     HTTPStatus.METHOD_NOT_ALLOWED),\n    ('invalid_action', 'CP1234567', 'Not-Match-Identifier', [BASIC_USER], None, ['scrum'],\n     HTTPStatus.METHOD_NOT_ALLOWED),\n    ('add_comment_not_allowed', 'CP0001237', 'CP1234567', [BASIC_USER], None, ['add_comment'],\n     HTTPStatus.METHOD_NOT_ALLOWED),\n    ('court_order_not_allowed', 'CP0001237', 'CP1234567', [BASIC_USER], None, ['court_order'],\n     HTTPStatus.METHOD_NOT_ALLOWED),\n    ('registrars_notation_not_allowed', 'CP0001237', 'CP1234567', [BASIC_USER], None, ['registrars_notation'],\n     HTTPStatus.METHOD_NOT_ALLOWED),\n    ('registrars_order_not_allowed', 'CP0001237', 'CP1234567', [BASIC_USER], None, ['registrars_order'],\n     HTTPStatus.METHOD_NOT_ALLOWED),\n]\n\n\n@integration_authorization\n@pytest.mark.parametrize('test_name,identifier,username,roles,allowed_actions,requested_actions,expected',\n                         TEST_INTEG_AUTHZ_DATA)\ndef test_authorized_user_integ(monkeypatch, app, jwt,\n                               test_name, identifier, username, roles, allowed_actions, requested_actions, expected):\n    \"\"\"Assert that the type of user authorization is correct, based on the expected outcome.\"\"\"\n    import flask  # noqa: F401; import actually used in mock\n    # setup\n    token = helper_create_jwt(jwt, roles=roles, username=username)\n    headers = {'Authorization': 'Bearer ' + token}\n\n    def mock_auth(one, two):  # pylint: disable=unused-argument; mocks of library methods\n        return headers['Authorization']\n\n    with app.test_request_context():\n        monkeypatch.setattr('flask.request.headers.get', mock_auth)\n\n        rv = authorized(identifier, jwt, ['view'])\n\n# check it\n    if expected == HTTPStatus.OK:\n        assert rv\n    else:\n        assert not rv\n\n\ndef test_authorized_missing_args():\n    \"\"\"Assert that the missing args return False.\"\"\"\n    identifier = 'a corp'\n    jwt = 'fake'\n    action = 'fake'\n\n    rv = authorized(identifier, jwt, None)\n    assert not rv\n\n    rv = authorized(identifier, None, action)\n    assert not rv\n\n    rv = authorized(None, jwt, action)\n    assert not rv\n\n\ndef test_authorized_bad_url(monkeypatch, app, jwt):\n    \"\"\"Assert that an invalid auth service URL returns False.\"\"\"\n    import flask  # noqa: F401; import actually used in mock\n    # setup\n    identifier = 'CP1234567'\n    username = 'username'\n    roles = [BASIC_USER]\n    token = helper_create_jwt(jwt, roles=roles, username=username)\n    headers = {'Authorization': 'Bearer ' + token}\n\n    def mock_auth(one, two):  # pylint: disable=unused-argument; mocks of library methods\n        return headers['Authorization']\n\n    with app.test_request_context():\n        monkeypatch.setattr('flask.request.headers.get', mock_auth)\n        auth_svc_url = app.config['AUTH_SVC_URL']\n        app.config['AUTH_SVC_URL'] = 'http://no.way.this.works/dribble'\n\n        rv = authorized(identifier, jwt, ['view'])\n\n        app.config['AUTH_SVC_URL'] = auth_svc_url\n\n    assert not rv\n\n\ndef test_authorized_invalid_roles(monkeypatch, app, jwt):\n    \"\"\"Assert that an invalid role returns False.\"\"\"\n    import flask  # noqa: F401 ; import actually used in mock\n    # setup noqa: I003\n    identifier = 'CP1234567'\n    username = 'username'\n    roles = ['NONE']\n    token = helper_create_jwt(jwt, roles=roles, username=username)\n    headers = {'Authorization': 'Bearer ' + token}\n\n    def mock_auth(one, two):  # pylint: disable=unused-argument; mocks of library methods\n        return headers['Authorization']\n\n    with app.test_request_context():\n        monkeypatch.setattr('flask.request.headers.get', mock_auth)\n        rv = authorized(identifier, jwt, ['view'])\n\n    assert not rv\n\n\n@pytest.mark.parametrize(\n    'test_name,state,legal_type,username,roles,expected',\n    [\n        # active business\n        ('staff_active_cp', Business.State.ACTIVE, 'CP', 'staff', [STAFF_ROLE],\n         ['annualReport', 'changeOfAddress', 'changeOfDirectors', 'correction',\n            'courtOrder', 'dissolution', 'incorporationApplication',\n            'specialResolution', 'registrarsNotation', 'registrarsOrder']),\n        ('staff_active_bc', Business.State.ACTIVE, 'BC', 'staff', [STAFF_ROLE],\n         ['alteration', 'courtOrder', 'dissolution', 'incorporationApplication',\n            'transition', 'registrarsNotation', 'registrarsOrder']),\n        ('staff_active_ben', Business.State.ACTIVE, 'BEN', 'staff', [STAFF_ROLE],\n         ['alteration', 'annualReport', 'changeOfAddress', 'changeOfDirectors', 'conversion', 'correction',\n            'courtOrder', 'dissolution', 'incorporationApplication',\n            'transition', 'registrarsNotation', 'registrarsOrder']),\n        ('staff_active_cc', Business.State.ACTIVE, 'CC', 'staff', [STAFF_ROLE],\n         ['courtOrder', 'dissolution',\n         'registrarsNotation', 'registrarsOrder']),\n        ('staff_active_ulc', Business.State.ACTIVE, 'ULC', 'staff', [STAFF_ROLE],\n         ['alteration', 'courtOrder', 'dissolution',\n         'registrarsNotation', 'registrarsOrder']),\n        ('staff_active_llc', Business.State.ACTIVE, 'LLC', 'staff', [STAFF_ROLE],\n         ['courtOrder', 'dissolution',\n         'registrarsNotation', 'registrarsOrder']),\n        ('staff_active_sp', Business.State.ACTIVE, 'SP', 'staff', [STAFF_ROLE],\n         ['changeOfRegistration', 'conversion', 'dissolution', 'registration']),\n        ('staff_active_gp', Business.State.ACTIVE, 'GP', 'staff', [STAFF_ROLE],\n         ['changeOfRegistration', 'conversion', 'dissolution', 'registration']),\n\n        ('user_active_cp', Business.State.ACTIVE, 'CP', 'user', [BASIC_USER],\n         ['annualReport', 'changeOfAddress', 'changeOfDirectors',\n          'dissolution', 'incorporationApplication', 'specialResolution']),\n        ('user_active_bc', Business.State.ACTIVE, 'BC', 'user', [BASIC_USER],\n         ['alteration', 'dissolution', 'incorporationApplication', 'transition']),\n        ('user_active_ben', Business.State.ACTIVE, 'BEN', 'user', [BASIC_USER],\n         ['alteration', 'annualReport', 'changeOfAddress', 'changeOfDirectors',\n            'dissolution', 'incorporationApplication', 'transition']),\n        ('user_active_cc', Business.State.ACTIVE, 'CC', 'user', [BASIC_USER], ['dissolution']),\n        ('user_active_ulc', Business.State.ACTIVE, 'ULC', 'user', [BASIC_USER], ['alteration', 'dissolution']),\n        ('user_active_llc', Business.State.ACTIVE, 'LLC', 'user', [BASIC_USER], ['dissolution']),\n        ('user_active_sp', Business.State.ACTIVE, 'SP', 'user', [BASIC_USER], ['changeOfRegistration', 'dissolution', 'registration']),\n        ('user_active_gp', Business.State.ACTIVE, 'GP', 'user', [BASIC_USER], ['changeOfRegistration', 'dissolution', 'registration']),\n\n        # historical business\n        ('staff_historical_cp', Business.State.HISTORICAL, 'CP', 'staff', [STAFF_ROLE],\n         ['courtOrder', 'registrarsNotation', 'registrarsOrder',\n         {'restoration': ['fullRestoration']}]),\n        ('staff_historical_bc', Business.State.HISTORICAL, 'BC', 'staff', [STAFF_ROLE],\n         ['courtOrder', 'registrarsNotation', 'registrarsOrder',\n         {'restoration': ['fullRestoration', 'limitedRestoration']}]),\n        ('staff_historical_ben', Business.State.HISTORICAL, 'BEN', 'staff', [STAFF_ROLE],\n         ['courtOrder', 'registrarsNotation', 'registrarsOrder',\n         {'restoration': ['fullRestoration', 'limitedRestoration']}]),\n        ('staff_historical_cc', Business.State.HISTORICAL, 'CC', 'staff', [STAFF_ROLE],\n         ['courtOrder', 'registrarsNotation', 'registrarsOrder',\n         {'restoration': ['fullRestoration', 'limitedRestoration']}]),\n        ('staff_historical_ulc', Business.State.HISTORICAL, 'ULC', 'staff', [STAFF_ROLE],\n         ['courtOrder', 'registrarsNotation', 'registrarsOrder',\n         {'restoration': ['fullRestoration', 'limitedRestoration']}]),\n        ('staff_historical_llc', Business.State.HISTORICAL, 'LLC', 'staff', [STAFF_ROLE],\n         ['courtOrder', 'registrarsNotation', 'registrarsOrder',\n         {'restoration': ['fullRestoration', 'limitedRestoration']}]),\n\n        ('user_historical_llc', Business.State.HISTORICAL, 'LLC', 'user', [BASIC_USER], []),\n    ]\n)\ndef test_get_allowed(monkeypatch, app, jwt, test_name, state, legal_type, username, roles, expected):\n    \"\"\"Assert that get allowed returns valid filings.\"\"\"\n    token = helper_create_jwt(jwt, roles=roles, username=username)\n    headers = {'Authorization': 'Bearer ' + token}\n\n    def mock_auth(one, two):  # pylint: disable=unused-argument; mocks of library methods\n        return headers[one]\n\n    with app.test_request_context():\n        monkeypatch.setattr('flask.request.headers.get', mock_auth)\n        filing_types = get_allowed(state, legal_type, jwt)\n        assert filing_types == expected\n\n\n@pytest.mark.parametrize(\n    'test_name,state,filing_type,sub_filing_type,legal_types,username,roles,expected',\n    [\n        # active business\n        ('staff_active_allowed', Business.State.ACTIVE, 'alteration', None,\n         ['BC', 'BEN', 'ULC'], 'staff', [STAFF_ROLE], True),\n        ('staff_active', Business.State.ACTIVE, 'alteration', None,\n         ['CP', 'CC', 'LLC'], 'staff', [STAFF_ROLE], False),\n\n        ('staff_active_allowed', Business.State.ACTIVE, 'annualReport', None,\n         ['CP', 'BEN'], 'staff', [STAFF_ROLE], True),\n        ('staff_active', Business.State.ACTIVE, 'annualReport', None,\n         ['BC', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], False),\n\n        ('staff_active_allowed', Business.State.ACTIVE, 'changeOfAddress', None,\n         ['CP', 'BEN'], 'staff', [STAFF_ROLE], True),\n        ('staff_active', Business.State.ACTIVE, 'changeOfAddress', None,\n         ['BC', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], False),\n\n        ('staff_active_allowed', Business.State.ACTIVE, 'changeOfDirectors', None,\n         ['CP', 'BEN'], 'staff', [STAFF_ROLE], True),\n        ('staff_active', Business.State.ACTIVE, 'changeOfDirectors', None,\n         ['BC', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], False),\n\n        ('staff_active_allowed', Business.State.ACTIVE, 'correction', None,\n         ['CP', 'BEN'], 'staff', [STAFF_ROLE], True),\n        ('staff_active', Business.State.ACTIVE, 'correction', None,\n         ['BC', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], False),\n\n        ('staff_active_allowed', Business.State.ACTIVE, 'courtOrder', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], True),\n\n        ('staff_active_allowed', Business.State.ACTIVE, 'dissolution', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC', 'SP', 'GP'], 'staff', [STAFF_ROLE], True),\n\n        ('staff_active_allowed', Business.State.ACTIVE, 'incorporationApplication', None,\n         ['CP', 'BC', 'BEN'], 'staff', [STAFF_ROLE], True),\n\n        ('staff_active', Business.State.ACTIVE, 'restoration', 'fullRestoration',\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], False),\n        ('staff_active', Business.State.ACTIVE, 'restoration', 'limitedRestoration',\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], False),\n\n        ('staff_active_allowed', Business.State.ACTIVE, 'specialResolution', None, ['CP'], 'staff', [STAFF_ROLE], True),\n        ('staff_active', Business.State.ACTIVE, 'specialResolution', None,\n         ['BC', 'BEN', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], False),\n\n        ('staff_active_allowed', Business.State.ACTIVE, 'transition', None,\n         ['BC', 'BEN'], 'staff', [STAFF_ROLE], True),\n        ('staff_active', Business.State.ACTIVE, 'transition', None,\n         ['CP', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], False),\n\n        ('staff_active_allowed', Business.State.ACTIVE, 'registrarsNotation', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], True),\n\n        ('staff_active_allowed', Business.State.ACTIVE, 'registrarsOrder', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], True),\n\n        ('staff_active_allowed', Business.State.ACTIVE, 'registration', None,\n         ['SP', 'GP'], 'staff', [STAFF_ROLE], True),\n\n        ('staff_active_allowed', Business.State.ACTIVE, 'changeOfRegistration', None,\n         ['SP', 'GP'], 'staff', [STAFF_ROLE], True),\n\n\n        ('user_active_allowed', Business.State.ACTIVE, 'alteration', None,\n         ['BC', 'BEN', 'ULC'], 'user', [BASIC_USER], True),\n        ('user_active', Business.State.ACTIVE, 'alteration', None,\n         ['CP', 'CC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_active_allowed', Business.State.ACTIVE, 'annualReport', None, ['CP', 'BEN'], 'user', [BASIC_USER], True),\n        ('user_active', Business.State.ACTIVE, 'annualReport', None,\n         ['BC', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_active_allowed', Business.State.ACTIVE, 'changeOfAddress', None,\n         ['CP', 'BEN'], 'user', [BASIC_USER], True),\n        ('user_active', Business.State.ACTIVE, 'changeOfAddress', None,\n         ['BC', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_active_allowed', Business.State.ACTIVE, 'changeOfDirectors', None,\n         ['CP', 'BEN'], 'user', [BASIC_USER], True),\n        ('user_active', Business.State.ACTIVE, 'changeOfDirectors', None,\n         ['BC', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_active', Business.State.ACTIVE, 'correction', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_active', Business.State.ACTIVE, 'courtOrder', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_active_allowed', Business.State.ACTIVE, 'dissolution', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC', 'SP', 'GP'], 'user', [BASIC_USER], True),\n\n        ('user_active_allowed', Business.State.ACTIVE, 'incorporationApplication', None,\n         ['CP', 'BC', 'BEN'], 'user', [BASIC_USER], True),\n\n        ('user_active_allowed', Business.State.ACTIVE, 'registration', None,\n         ['SP', 'GP'], 'user', [BASIC_USER], True),\n\n        ('user_active_allowed', Business.State.ACTIVE, 'changeOfRegistration', None,\n         ['SP', 'GP'], 'user', [BASIC_USER], True),\n\n        ('user_active', Business.State.ACTIVE, 'restoration', 'fullRestoration',\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_active', Business.State.ACTIVE, 'restoration', 'limitedRestoration',\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_active_allowed', Business.State.ACTIVE, 'specialResolution', None, ['CP'], 'user', [BASIC_USER], True),\n        ('user_active', Business.State.ACTIVE, 'specialResolution', None,\n         ['BC', 'BEN', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_active_allowed', Business.State.ACTIVE, 'transition', None, ['BC', 'BEN'], 'user', [BASIC_USER], True),\n        ('user_active', Business.State.ACTIVE, 'transition', None,\n         ['CP', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_active', Business.State.ACTIVE, 'registrarsNotation', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_active', Business.State.ACTIVE, 'registrarsOrder', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        # historical business\n        ('staff_historical', Business.State.HISTORICAL, 'alteration', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], False),\n\n        ('staff_historical', Business.State.HISTORICAL, 'annualReport', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], False),\n\n        ('staff_historical', Business.State.HISTORICAL, 'changeOfAddress', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], False),\n\n        ('staff_historical', Business.State.HISTORICAL, 'changeOfDirectors', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], False),\n\n        ('staff_historical', Business.State.HISTORICAL, 'correction', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], False),\n\n        ('staff_historical_allowed', Business.State.HISTORICAL, 'courtOrder', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], True),\n\n        ('staff_historical', Business.State.HISTORICAL, 'dissolution', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC', 'SP', 'GP'], 'staff', [STAFF_ROLE], False),\n\n        ('staff_historical', Business.State.HISTORICAL, 'incorporationApplication', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], False),\n\n        ('staff_historical_allowed', Business.State.HISTORICAL, 'restoration', 'fullRestoration',\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], True),\n\n        ('staff_historical_allowed', Business.State.HISTORICAL, 'restoration', 'limitedRestoration',\n         ['BC', 'BEN', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], True),\n        ('staff_historical', Business.State.HISTORICAL, 'restoration', 'limitedRestoration',\n         ['CP'], 'staff', [STAFF_ROLE], False),\n\n        ('staff_historical', Business.State.HISTORICAL, 'specialResolution', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], False),\n\n        ('staff_historical', Business.State.HISTORICAL, 'transition', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], False),\n\n        ('staff_historical_allowed', Business.State.HISTORICAL, 'registrarsNotation', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], True),\n\n        ('staff_historical_allowed', Business.State.HISTORICAL, 'registrarsOrder', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'staff', [STAFF_ROLE], True),\n\n        ('staff_historical', Business.State.HISTORICAL, 'registration', None,\n         ['SP', 'GP'], 'staff', [STAFF_ROLE], False),\n\n        ('staff_historical', Business.State.HISTORICAL, 'changeOfRegistration', None,\n         ['SP', 'GP'], 'staff', [STAFF_ROLE], False),\n\n\n        ('user_historical', Business.State.HISTORICAL, 'alteration', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_historical', Business.State.HISTORICAL, 'annualReport', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_historical', Business.State.HISTORICAL, 'changeOfAddress', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_historical', Business.State.HISTORICAL, 'changeOfDirectors', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_historical', Business.State.HISTORICAL, 'correction', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_historical', Business.State.HISTORICAL, 'courtOrder', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_historical', Business.State.HISTORICAL, 'dissolution', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC', 'SP', 'GP', 'SP', 'GP'], 'user', [BASIC_USER], False),\n\n        ('user_historical', Business.State.HISTORICAL, 'incorporationApplication', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_historical', Business.State.HISTORICAL, 'restoration', 'fullRestoration',\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_historical', Business.State.HISTORICAL, 'restoration', 'limitedRestoration',\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_historical', Business.State.HISTORICAL, 'specialResolution', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_historical', Business.State.HISTORICAL, 'transition', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_historical', Business.State.HISTORICAL, 'registrarsNotation', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_historical', Business.State.HISTORICAL, 'registrarsOrder', None,\n         ['CP', 'BC', 'BEN', 'CC', 'ULC', 'LLC'], 'user', [BASIC_USER], False),\n\n        ('user_historical', Business.State.HISTORICAL, 'registration', None,\n         ['SP', 'GP'], 'user', [BASIC_USER], False),\n\n        ('user_historical', Business.State.HISTORICAL, 'changeOfRegistration', None,\n         ['SP', 'GP'], 'user', [BASIC_USER], False),\n    ]\n)\ndef test_is_allowed(monkeypatch, app, jwt, test_name, state, filing_type, sub_filing_type,\n                    legal_types, username, roles, expected):\n    \"\"\"Assert that get allowed returns valid filings.\"\"\"\n    token = helper_create_jwt(jwt, roles=roles, username=username)\n    headers = {'Authorization': 'Bearer ' + token}\n\n    def mock_auth(one, two):  # pylint: disable=unused-argument; mocks of library methods\n        return headers[one]\n\n    with app.test_request_context():\n        monkeypatch.setattr('flask.request.headers.get', mock_auth)\n        for legal_type in legal_types:\n            filing_types = is_allowed(state, filing_type, legal_type, jwt, sub_filing_type)\n            assert filing_types == expected\n"
  },
  {
    "prompt": "fn create_future()\n\nfn unpartial(fn)\n\nfn _done_callback(fut, task)\n\nfn _cache_invalidate(wrapped, typed)\n\nfn _cache_clear(wrapped)\n\nfn _open(wrapped)\n\nfn _close(wrapped)\n\nfn _wait_closed(wrapped)\n\nfn _close_waited(wrapped, _)\n\nfn _cache_info(wrapped, maxsize)\n\nfn __cache_touch(wrapped, key)\n\nfn _cache_hit(wrapped, key)\n\nfn _cache_miss(wrapped, key)\n\nfn _get_loop(cls, kwargs, fn, fn_args, fn_kwargs)\n\nfn alru_cache(fn, maxsize, typed, expiration_time)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import asyncio\nimport time\n\nimport pytest\n\nfrom async_lru import alru_cache\n\npytestmark = pytest.mark.asyncio\n\n\nasync def test_expiration(check_lru, loop):\n    @alru_cache(maxsize=4, expiration_time=2, loop=loop)\n    async def coro(val):\n        return val\n\n    inputs = [1, 2, 3]\n    coros = [coro(v) for v in inputs]\n    ret = await asyncio.gather(*coros, loop=loop)\n    assert ret == inputs\n    check_lru(coro, hits=0, misses=3, cache=3, tasks=0, maxsize=4)\n\n    time.sleep(1)\n    inputs = 1\n    ret = await coro(inputs)\n    assert ret == inputs\n    check_lru(coro, hits=1, misses=3, cache=3, tasks=0, maxsize=4)\n\n    time.sleep(3)\n    inputs = 1\n    ret = await coro(inputs)\n    assert ret == inputs\n    check_lru(coro, hits=1, misses=4, cache=3, tasks=0, maxsize=4)\n"
  },
  {
    "prompt": "\"\"\"Perform string comparison with multiple levels of tolerance\n\nArgs:\n    word1: a string\n    word2: another string\n\nReturns:\n    a tuple with booleans specifying respectively whether the raw strings, their lower-case counterparts, their\n        unidecode counterparts and their lower-case unidecode counterparts match\"\"\"\nfn string_match(word1: str, word2: str) -> Tuple[(bool, bool, bool, bool)]\n\nclass TextMatch\n\n\"\"\"Compute the IoU between two sets of bounding boxes\n\nArgs:\n    boxes_1: bounding boxes of shape (N, 4) in format (xmin, ymin, xmax, ymax)\n    boxes_2: bounding boxes of shape (M, 4) in format (xmin, ymin, xmax, ymax)\nReturns:\n    the IoU matrix of shape (N, M)\"\"\"\nfn box_iou(boxes_1: np.ndarray, boxes_2: np.ndarray) -> np.ndarray\n\n\"\"\"Compute the IoA (intersection over area) between two sets of bounding boxes:\nioa(i, j) = inter(i, j) / area(i)\n\nArgs:\n    boxes_1: bounding boxes of shape (N, 4) in format (xmin, ymin, xmax, ymax)\n    boxes_2: bounding boxes of shape (M, 4) in format (xmin, ymin, xmax, ymax)\nReturns:\n    the IoA matrix of shape (N, M)\"\"\"\nfn box_ioa(boxes_1: np.ndarray, boxes_2: np.ndarray) -> np.ndarray\n\n\"\"\"Compute the IoU between two sets of boolean masks\n\nArgs:\n    masks_1: boolean masks of shape (N, H, W)\n    masks_2: boolean masks of shape (M, H, W)\n\nReturns:\n    the IoU matrix of shape (N, M)\"\"\"\nfn mask_iou(masks_1: np.ndarray, masks_2: np.ndarray) -> np.ndarray\n\n\"\"\"Convert boxes to masks\n\nArgs:\n    boxes: rotated bounding boxes of shape (N, 5) in format (x, y, w, h, alpha)\n    shape: spatial shapes of the output masks\n\nReturns:\n    the boolean masks of shape (N, H, W)\"\"\"\nfn rbox_to_mask(boxes: np.ndarray, shape: Tuple[(int, int)]) -> np.ndarray\n\n\"\"\"Perform non-max suppression, borrowed from <https://github.com/rbgirshick/fast-rcnn>`_.\n\nArgs:\n    boxes: np array of straight boxes: (*, 5), (xmin, ymin, xmax, ymax, score)\n    thresh: iou threshold to perform box suppression.\n\nReturns:\n    A list of box indexes to keep\"\"\"\nfn nms(boxes: np.ndarray, thresh: float) -> List[int]\n\nclass LocalizationConfusion\n\nclass OCRMetric\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright (C) 2021, Mindee.\n\n# This program is licensed under the Apache License version 2.\n# See LICENSE or go to <https://www.apache.org/licenses/LICENSE-2.0.txt> for full license details.\n\nimport pytest\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\nfrom doctr.utils.metrics import box_iou\n\n\n@pytest.mark.asyncio\nasync def test_text_detection(test_app_asyncio, mock_detection_image):\n\n    response = await test_app_asyncio.post(\"/detection\", files={'file': mock_detection_image})\n    assert response.status_code == 200\n    json_response = response.json()\n\n    gt_boxes = np.array([[1240, 430, 1355, 470], [1360, 430, 1495, 470]], dtype=np.float32)\n    gt_boxes[:, [0, 2]] = gt_boxes[:, [0, 2]] / 1654\n    gt_boxes[:, [1, 3]] = gt_boxes[:, [1, 3]] / 2339\n\n    # Check that IoU with GT if reasonable\n    assert isinstance(json_response, list) and len(json_response) == gt_boxes.shape[0]\n    pred_boxes = np.array([elt['box'] for elt in json_response])\n    iou_mat = box_iou(gt_boxes, pred_boxes)\n    gt_idxs, pred_idxs = linear_sum_assignment(-iou_mat)\n    is_kept = iou_mat[gt_idxs, pred_idxs] >= 0.8\n    assert gt_idxs[is_kept].shape[0] == gt_boxes.shape[0]\n"
  },
  {
    "prompt": "fn Error()\n\nclass TestError\n\nfn Boolean()\n\nclass TestBoolean\n\nfn Missing()\n\nclass TestMissing\n\nfn Number()\n\nclass TestNumber\n\nfn Text()\n\nclass TestText\n\nfn Index()\n\nclass TestIndex\n\nfn DateTimeField()\n\nclass TestDateTimeField\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright (c) 2010-2019 openpyxl\nimport pytest\n\nfrom io import BytesIO\nfrom zipfile import ZipFile\n\nfrom openpyxl.packaging.manifest import Manifest\nfrom openpyxl.xml.functions import fromstring, tostring\nfrom openpyxl.tests.helper import compare_xml\n\nfrom .test_fields import (\n    Index,\n    Number,\n    Text,\n)\n\n@pytest.fixture\ndef Record():\n    from ..record import Record\n    return Record\n\n\nclass TestRecord:\n\n    def test_ctor(self, Record, Number, Text, Index):\n        n = [Number(v=1), Number(v=25)]\n        s = [Text(v=\"2014-03-24\")]\n        x = [Index(), Index(), Index()]\n        fields = n + s + x\n        field = Record(_fields=fields)\n        xml = tostring(field.to_tree())\n        expected = \"\"\"\n        <r>\n          <n v=\"1\"/>\n          <n v=\"25\"/>\n          <s v=\"2014-03-24\"/>\n          <x v=\"0\"/>\n          <x v=\"0\"/>\n          <x v=\"0\"/>\n        </r>\n        \"\"\"\n        diff = compare_xml(xml, expected)\n        assert diff is None, diff\n\n\n    def test_from_xml(self, Record, Number, Text, Index):\n        src = \"\"\"\n        <r>\n          <n v=\"1\"/>\n          <x v=\"0\"/>\n          <s v=\"2014-03-24\"/>\n          <x v=\"0\"/>\n          <n v=\"25\"/>\n          <x v=\"0\"/>\n        </r>\n        \"\"\"\n        node = fromstring(src)\n        n = [Number(v=1), Number(v=25)]\n        s = [Text(v=\"2014-03-24\")]\n        x = [Index(), Index(), Index()]\n        fields = [\n            Number(v=1),\n            Index(),\n            Text(v=\"2014-03-24\"),\n            Index(),\n            Number(v=25),\n            Index(),\n        ]\n        field = Record.from_tree(node)\n        assert field == Record(_fields=fields)\n\n\n@pytest.fixture\ndef RecordList():\n    from ..record import RecordList\n    return RecordList\n\n\nclass TestRecordList:\n\n    def test_ctor(self, RecordList):\n        cache = RecordList()\n        xml = tostring(cache.to_tree())\n        expected = \"\"\"\n        <pivotCacheRecords xmlns=\"http://schemas.openxmlformats.org/spreadsheetml/2006/main\"\n           count=\"0\" />\n        \"\"\"\n        diff = compare_xml(xml, expected)\n        assert diff is None, diff\n\n\n    def test_from_xml(self, RecordList):\n        src = \"\"\"\n        <pivotCacheRecords count=\"0\" />\n        \"\"\"\n        node = fromstring(src)\n        cache = RecordList.from_tree(node)\n        assert cache == RecordList()\n\n\n    def test_write(self, RecordList):\n        out = BytesIO()\n        archive = ZipFile(out, mode=\"w\")\n        manifest = Manifest()\n\n        records = RecordList()\n        xml = tostring(records.to_tree())\n        records._write(archive, manifest)\n        manifest.append(records)\n\n        assert archive.namelist() == [records.path[1:]]\n        assert manifest.find(records.mime_type)\n"
  },
  {
    "prompt": "fn _generate_path_reserved_chars()\n\nfn _contains_quoted_reserved_chars(x)\n\n\"\"\"Normalize the href to be a path only relative to hostname and\nschema.\"\"\"\nfn _normalize_href(base, href)\n\nclass InvalidXMLResponse\n\nfn _clean_body(content, bad_chars)\n\nfn _parse_xml(content)\n\nfn _merge_xml(items)\n\nfn _fuzzy_matches_mimetype(strict, weak)\n\nclass Discover\n\nclass CalDiscover\n\nclass CardDiscover\n\nclass DAVSession\n\nclass DAVStorage\n\nclass CalDAVStorage\n\nclass CardDAVStorage\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom vdirsyncer.storage.dav import _BAD_XML_CHARS\nfrom vdirsyncer.storage.dav import _merge_xml\nfrom vdirsyncer.storage.dav import _parse_xml\n\n\ndef test_xml_utilities():\n    x = _parse_xml(\n        b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n        <multistatus xmlns=\"DAV:\">\n            <response>\n                <propstat>\n                    <status>HTTP/1.1 404 Not Found</status>\n                    <prop>\n                        <getcontenttype/>\n                    </prop>\n                </propstat>\n                <propstat>\n                    <prop>\n                        <resourcetype>\n                            <collection/>\n                        </resourcetype>\n                    </prop>\n                </propstat>\n            </response>\n        </multistatus>\n    \"\"\"\n    )\n\n    response = x.find(\"{DAV:}response\")\n    props = _merge_xml(response.findall(\"{DAV:}propstat/{DAV:}prop\"))\n    assert props.find(\"{DAV:}resourcetype/{DAV:}collection\") is not None\n    assert props.find(\"{DAV:}getcontenttype\") is not None\n\n\n@pytest.mark.parametrize(\"char\", range(32))\ndef test_xml_specialchars(char):\n    x = _parse_xml(\n        '<?xml version=\"1.0\" encoding=\"UTF-8\" ?>'\n        \"<foo>ye{}s\\r\\n\"\n        \"hello</foo>\".format(chr(char)).encode(\"ascii\")\n    )\n\n    if char in _BAD_XML_CHARS:\n        assert x.text == \"yes\\nhello\"\n"
  },
  {
    "prompt": "class ArbitraryAccessObj\n\nclass Mock\n\nclass ObjectIdHashComparator\n\nclass SubtypesInstanceManager\n\n\"\"\"Recursively get all subclasses of the given class.\"\"\"\nfn get_all_subclasses(clazz: Type) -> Set[Type]\n\nfn fully_qualified_class_name(klass: Type) -> str\n\n\"\"\"Return 'value' if it is not None, or 'alternative' otherwise.\"\"\"\nfn not_none_or(value: Optional[Any], alternative: Any) -> Any\n\n\"\"\"Recursively apply `func` to `obj` (might be a list, dict, or other object).\"\"\"\nfn recurse_object(obj: ComplexType, func: Callable, path: str) -> ComplexType\n\n\"\"\"Recursively changes all dict keys to first character lowercase. Skip children\nof any elements whose names are contained in skip_children_of (e.g., ['Tags'])\"\"\"\nfn keys_to_lower(obj: ComplexType, skip_children_of: List[str]) -> ComplexType\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom localstack.utils.objects import SubtypesInstanceManager\n\n\ndef test_subtypes_instance_manager():\n    class BaseClass(SubtypesInstanceManager):\n        def foo(self):\n            pass\n\n    class C1(BaseClass):\n        @staticmethod\n        def impl_name() -> str:\n            return \"c1\"\n\n        def foo(self):\n            return \"bar\"\n\n    instance1 = BaseClass.get(\"c1\")\n    assert instance1\n    assert BaseClass.get(\"c1\") == instance1\n    assert instance1.foo() == \"bar\"\n    with pytest.raises(Exception):\n        assert BaseClass.get(\"c2\")\n\n    class C2(BaseClass):\n        @staticmethod\n        def impl_name() -> str:\n            return \"c2\"\n\n        def foo(self):\n            return \"baz\"\n\n    instance2 = BaseClass.get(\"c2\")\n    assert BaseClass.get(\"c2\") == instance2\n    assert instance2.foo() == \"baz\"\n"
  },
  {
    "prompt": "fn T(x)\n\nclass FakeCalendar\n\nclass TestCalendarRegistration\n\n\"\"\"Test dispatcher and calendar default values.\"\"\"\nfn test_default_calendars()\n\nfn test_days_at_time(day, day_offset, time_offset, tz, expected)\n\n\"\"\"Get csv file as DataFrame for given calendar `name`.\"\"\"\nfn get_csv(name: str) -> pd.DataFrame\n\nclass Answers\n\n\"\"\"Wrap a method under test so that it skips input parsing.\"\"\"\nfn no_parsing(f: typing.Callable)\n\nclass ExchangeCalendarTestBase\n\nclass EuronextCalendarTestBase\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom exchange_calendars.exchange_calendar_xshg import XSHGExchangeCalendar\nfrom .test_exchange_calendar import ExchangeCalendarTestBase\nfrom .test_utils import T\n\n\nclass TestXSHGCalendar(ExchangeCalendarTestBase):\n    @pytest.fixture(scope=\"class\")\n    def calendar_cls(self):\n        yield XSHGExchangeCalendar\n\n    @pytest.fixture\n    def max_session_hours(self):\n        # Shanghai stock exchange is open from 9:30 am to 3pm\n        yield 5.5\n\n    @pytest.fixture\n    def start_bound(self):\n        yield T(\"1999-01-01\")\n\n    @pytest.fixture\n    def end_bound(self):\n        yield T(\"2025-12-31\")\n\n    @pytest.fixture\n    def regular_holidays_sample(self):\n        yield [\n            # 2017\n            \"2017-01-02\",\n            \"2017-01-27\",\n            \"2017-01-30\",\n            \"2017-01-31\",\n            \"2017-02-01\",\n            \"2017-02-02\",\n            \"2017-04-03\",\n            \"2017-04-04\",\n            \"2017-05-01\",\n            \"2017-05-29\",\n            \"2017-05-30\",\n            \"2017-10-02\",\n            \"2017-10-03\",\n            \"2017-10-04\",\n            \"2017-10-05\",\n            \"2017-10-06\",\n\n            # 2020\n            \"2020-01-31\"\n        ]\n"
  },
  {
    "prompt": "class MeasureIterator\n\nclass Measure\n\nclass PartIterator\n\nclass Part\n\nclass BeatIterator\n\nclass Beat\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\r\n\r\nfrom MidiCompose.logic.rhythm.beat import Beat\r\nfrom MidiCompose.logic.rhythm.measure import Measure\r\nfrom MidiCompose.logic.rhythm.part import Part\r\n\r\n@pytest.fixture\r\ndef part_1():\r\n\r\n    m1 = Measure([Beat([1,2,1,2]),\r\n                  Beat([1,0,0,1])])\r\n    m2 = Measure([Beat([2,2,1,1]),\r\n                  Beat([2,2,2,2])])\r\n\r\n    part = Part([m1,m2])\r\n\r\n    return part\r\n\r\ndef test_empty_constructor():\r\n    p = Part()\r\n    assert p.n_measures == 1\r\n    assert p.n_beats == 1\r\n    assert p.n_note_on == 0\r\n\r\ndef test_n_note_on(part_1):\r\n\r\n    assert part_1.n_note_on == 6\r\n\r\n\r\ndef test_iterator(part_1):\r\n    for m in part_1:\r\n        assert type(m) == Measure\r\n\r\n\r\n\r\n"
  },
  {
    "prompt": "class User\n\nclass UserChangeForm\n\nclass UserCreationForm\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"\nModule for all Form Tests.\n\"\"\"\nimport pytest\nfrom django.utils.translation import gettext_lazy as _\n\nfrom my_blog.users.forms import UserCreationForm\nfrom my_blog.users.models import User\n\npytestmark = pytest.mark.django_db\n\n\nclass TestUserCreationForm:\n    \"\"\"\n    Test class for all tests related to the UserCreationForm\n    \"\"\"\n\n    def test_username_validation_error_msg(self, user: User):\n        \"\"\"\n        Tests UserCreation Form's unique validator functions correctly by testing:\n            1) A new user with an existing username cannot be added.\n            2) Only 1 error is raised by the UserCreation Form\n            3) The desired error message is raised\n        \"\"\"\n\n        # The user already exists,\n        # hence cannot be created.\n        form = UserCreationForm(\n            {\n                \"username\": user.username,\n                \"password1\": user.password,\n                \"password2\": user.password,\n            }\n        )\n\n        assert not form.is_valid()\n        assert len(form.errors) == 1\n        assert \"username\" in form.errors\n        assert form.errors[\"username\"][0] == _(\"This username has already been taken.\")\n"
  },
  {
    "prompt": "\"\"\"Take a list of structures and write them to a json file.\n:param filename:\n:param structures:\"\"\"\nfn md_trajectory_to_file(filename: str, structures: List[Structure])\n\n\"\"\"Read a list of structures from a json file, formatted as in md_trajectory_to_file.\n:param filename:\"\"\"\nfn md_trajectory_from_file(filename: str)\n\nclass Structure\n\nclass StructureSource\n\nclass ForceSource\n\nclass Trajectory\n\n\"\"\"Returns a list of the unique species passed in, and a list of\nintegers indexing them.\n\n:param species: Species to index uniquely\n:return: List of the unique species, and integer indexes\"\"\"\nfn get_unique_species(species: List[Any]) -> (List, List[int])\n\n\"\"\"Helper utility to take a vasprun file name or Vasprun object\nand return a vasprun object.\n:param vasprun: Vasprun filename or object\"\"\"\nfn check_vasprun(vasprun: None, vasprun_kwargs: dict)\n\n\"\"\"Returns the positions, species, and cell of a POSCAR file.\nOutputs are specced for OTF module.\n\n:param input: POSCAR file input\n:return:\"\"\"\nfn parse_dft_input(input: str)\n\n\"\"\"Run a VASP calculation.\n:param calc_dir: Name of directory to perform calculation in\n:param dft_loc: Name of VASP command (i.e. executable location)\n:param structure: flare structure object\n:param en: whether to return the final energy along with the forces\n:param vasp_cmd: Command to run VASP (leave a formatter \"{}\" to insert dft_loc);\n    this can be used to specify mpirun, srun, etc.\nReturns:\n    forces on each atom (and energy if en=True)\"\"\"\nfn run_dft(calc_dir: str, dft_loc: str, structure: Structure, en: bool, vasp_cmd: None)\n\nfn run_dft_par(dft_input: str, structure: Structure, dft_command: str, n_cpus, dft_out, parallel_prefix, mpi, npool, screen_out)\n\n\"\"\"Parse the DFT input in a directory.\n:param vasp_input: directory of vasp input\"\"\"\nfn dft_input_to_structure(poscar: str)\n\n\"\"\"Writes a VASP POSCAR file from structure with the name poscar .\nWARNING: Destructively replaces the file with the name specified by poscar\n:param poscar: Name of file\n:param structure: structure to write to file\"\"\"\nfn edit_dft_input_positions(output_name: str, structure: Structure)\n\n\"\"\"Parses the DFT forces from the last ionic step of a VASP vasprun.xml file\n:param vasprun: pymatgen Vasprun object or vasprun filename\"\"\"\nfn parse_dft_forces(vasprun: None)\n\n\"\"\"Parses the DFT forces and energy from a VASP vasprun.xml file\n:param vasprun: pymatgen Vasprun object or vasprun filename\"\"\"\nfn parse_dft_forces_and_energy(vasprun: None)\n\n\"\"\"Returns a list of flare Structure objects decorated with forces, stress,\nand total energy from a MD trajectory performed in VASP.\n:param vasprun: pymatgen Vasprun object or vasprun filename\n:param ionic_step_skips: if True, only samples the configuration every\n    ionic_skip_steps steps.\"\"\"\nfn md_trajectory_from_vasprun(vasprun: None, ionic_step_skips: None, vasprun_kwargs: dict)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\npmgout = pytest.importorskip(\"pymatgen.io.vasp.outputs\")\nVasprun = pmgout.Vasprun\nimport os\nimport numpy as np\nfrom flare.struc import Structure, get_unique_species\nfrom flare.dft_interface.vasp_util import md_trajectory_from_vasprun\nfrom flare.utils.flare_io import md_trajectory_to_file, md_trajectory_from_file\n\npytestmark = pytest.mark.filterwarnings(\n    \"ignore::UserWarning\", \"ignore::pymatgen.io.vasp.outputs.UnconvergedVASPWarning\"\n)\n\n\ndef test_read_write_trajectory():\n    structures = md_trajectory_from_vasprun(\"test_files/test_vasprun.xml\")\n    fname = \"tst_traj.json\"\n    md_trajectory_to_file(fname, structures)\n    fstructures = md_trajectory_from_file(fname)\n    for s, f in zip(structures, fstructures):\n        assert np.isclose(s.forces, f.forces).all()\n        assert np.isclose(s.positions, f.positions).all()\n    os.system(\"rm tst_traj.json\")\n"
  },
  {
    "prompt": "fn getClient(name: str) -> GirderClient\n\nfn getTestFolder(client: GirderClient)\n\nfn admin_client() -> GirderClient\n\n\"\"\"Wait for all worker jobs to complete\"\"\"\nfn wait_for_jobs(client: GirderClient, max_wait_timeout, expected_status)\n\nfn match_user_server_data(user: Dict[(str, Any)], dataset) -> List[dict]\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import json\n\nfrom girder.constants import AccessType\nfrom girder_client import HttpError\nimport pytest\n\nfrom .conftest import getClient, getTestFolder, localDataRoot, users, wait_for_jobs\n\n\n@pytest.mark.integration\n@pytest.mark.parametrize(\"user\", users.values())\n@pytest.mark.run(order=3)\ndef test_reset_integration_env(user: dict):\n    client = getClient(user['login'])\n    privateFolder = getTestFolder(client)\n    client.delete(f\"folder/{privateFolder['_id']}\")\n\n\n@pytest.mark.integration\n@pytest.mark.parametrize(\"user\", users.values())\n@pytest.mark.run(order=4)\ndef test_upload_user_data(user: dict):\n    client = getClient(user['login'])\n    createdDatasets = []\n    for dataset in user['data']:\n        dsPath = localDataRoot / str(dataset['path'])\n        privateFolder = getTestFolder(client)\n        newDatasetFolder = client.createFolder(\n            privateFolder['_id'],\n            dataset['name'],\n            metadata={\n                'fps': dataset['fps'],\n                'type': dataset['type'],\n            },\n        )\n        createdDatasets.append(newDatasetFolder)\n        # Validate the fileset\n        filenames = [file.name for file in dsPath.iterdir()]\n        valid = client.post('dive_dataset/validate_files', json=filenames)\n        assert valid['ok'], 'File validation failed'\n        for file in dsPath.iterdir():\n            if file.is_file():\n                client.uploadFileToFolder(newDatasetFolder['_id'], str(file))\n        client.post(f'dive_rpc/postprocess/{newDatasetFolder[\"_id\"]}')\n        if dataset.get('sharedWith', False):\n            me = client.get('user/me')\n            otherClient = getClient(dataset['sharedWith'])\n            otherUser = otherClient.get('user/me')\n            with pytest.raises(HttpError):\n                otherClient.get(f'dive_dataset/{newDatasetFolder[\"_id\"]}')\n            client.put(\n                f'folder/{newDatasetFolder[\"_id\"]}/access',\n                data={\n                    'public': False,\n                    'recurse': False,\n                    'progress': False,\n                    'access': json.dumps(\n                        {\n                            'users': [\n                                {'id': me['_id'], 'level': AccessType.ADMIN, 'flags': []},\n                                {'id': otherUser['_id'], 'level': AccessType.READ, 'flags': []},\n                            ],\n                            'groups': [],\n                        }\n                    ),\n                },\n            )\n            assert (\n                otherClient.get(\n                    f'dive_dataset/{newDatasetFolder[\"_id\"]}', jsonResp=False\n                ).status_code\n                == 200\n            )\n\n    wait_for_jobs(client)\n    # Confirm that the new dataset looks like it should.\n    for created, expected in zip(createdDatasets, user['data']):\n        created = client.get(f'dive_dataset/{created[\"_id\"]}')\n        if expected['type'] == 'video':\n            assert created['fps'] == expected['originalFps'] or created['fps'] == expected['fps']\n            assert created['annotate']\n            assert created['originalFps'] == expected['originalFps']\n"
  },
  {
    "prompt": "\"\"\"Get list of file names from latest n ICESat-2 cycles\n\nParameters\n----------\nfiles : list\n    A list of file names.\ncycles: list\n    A list of available ICESat-2 cycles\nn: int, default 1\n    Number of latest ICESat-2 cycles to pick\n\nReturns\n-------\nviz_file_list : list\n    A list of file names from latest n ICESat-2 cycles\"\"\"\nfn files_in_latest_n_cycles(files: list, cycles: list, n: int, default 1) -> list\n\n\"\"\"Returns a list of granule information for file name string.\n\nParameters\n----------\nfilename: String\n    ICESat-2 file name\n\nReturns\n-------\ngran_paras_list : list\n    A list of parameters including RGT, cycle, and datetime of ICESat-2 data granule\"\"\"\nfn gran_paras(filename: String) -> list\n\n\"\"\"Check if user wants to proceed visualization when the API request number exceeds 200\n\nParameters\n----------\nmessage : string\n    Message to indicate users the options\"\"\"\nfn user_check(message: string)\n\nclass Visualize\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom icepyx.core.visualization import Visualize\nimport icepyx.core.visualization as vis\n\n\n@pytest.mark.parametrize(\n    \"n, exp\",\n    [\n        (\n            1,\n            [\n                \"ATL06_20200702014158_01020810_004_01.h5\",\n                \"ATL06_20200703011618_01170810_004_01.h5\",\n            ],\n        ),\n        (\n            2,\n            [\n                \"ATL06_20200612151119_11920712_004_01.h5\",\n                \"ATL06_20200616021517_12450710_004_01.h5\",\n                \"ATL06_20200702014158_01020810_004_01.h5\",\n                \"ATL06_20200703011618_01170810_004_01.h5\",\n            ],\n        ),\n        (\n            3,\n            [\n                \"ATL06_20200612151119_11920712_004_01.h5\",\n                \"ATL06_20200616021517_12450710_004_01.h5\",\n                \"ATL06_20200702014158_01020810_004_01.h5\",\n                \"ATL06_20200703011618_01170810_004_01.h5\",\n            ],\n        ),\n    ],\n)\ndef test_files_in_latest_cycles(n, exp):\n    files = [\n        \"ATL06_20190710071617_01860412_004_01.h5\",\n        \"ATL06_20190713182016_02390410_004_01.h5\",\n        \"ATL06_20200612151119_11920712_004_01.h5\",\n        \"ATL06_20200616021517_12450710_004_01.h5\",\n        \"ATL06_20200702014158_01020810_004_01.h5\",\n        \"ATL06_20200703011618_01170810_004_01.h5\",\n    ]\n    cycles = [8, 7, 4]\n    obs = vis.files_in_latest_n_cycles(files, cycles=cycles, n=n)\n    assert obs == exp\n\n\n@pytest.mark.parametrize(\n    \"filename, expect\",\n    [\n        ('ATL06_20190525202604_08790310_004_01.h5', [879, 3, '2019-05-25']),\n        ('ATL06_20190614194425_11840310_004_01.h5', [1184, 3, '2019-06-14']),\n        ('ATL07-02_20190624063616_13290301_004_01.h5', [1329, 3, '2019-06-24']),\n        ('ATL07-02_20190602190916_10010301_004_01.h5', [1001, 3, '2019-06-02']),\n        ('ATL10-02_20190611072656_11310301_004_01.h5', [1131, 3, '2019-06-11']),\n        ('ATL10-02_20190731045538_05060401_004_01.h5', [506, 4, '2019-07-31']),\n        ('ATL12_20190615023544_11890301_004_01.h5', [1189, 3, '2019-06-15']),\n        ('ATL12_20190721170332_03610401_004_01.h5', [361, 4, '2019-07-21']),\n    ],\n)\ndef test_gran_paras(filename, expect):\n\n    para_list = vis.gran_paras(filename)\n\n    assert para_list == expect\n\n\n@pytest.mark.parametrize(\n    \"product, date_range, bbox, expect\",\n    [\n        (\"ATL06\", [\"2019-6-15\", \"2019-7-1\"], [-64.5, -66, -63.5, -65], 3240),\n        (\"ATL07\", [\"2019-7-1\", \"2019-8-1\"], [-65, -66, -64.5, -65], 7160),\n        (\"ATL08\", [\"2019-6-15\", \"2019-7-1\"], [-18, 63, -17, 64], 852),\n        (\"ATL10\", [\"2019-8-1\", \"2019-9-1\"], [-64, -67, -60, -60], 7375),\n        (\"ATL12\", [\"2019-7-1\", \"2019-10-1\"], [-65.5, -65.5, -64.5, -65], 95),\n        (\"ATL13\", [\"2019-6-1\", \"2019-12-1\"], [-75, -51, -74, -50], 20),\n    ],\n)\ndef test_visualization_date_range(product, date_range, bbox, expect):\n\n    region_viz = Visualize(product=product, spatial_extent=bbox, date_range=date_range)\n    data_size = region_viz.parallel_request_OA().size\n\n    assert data_size == expect\n\n\n@pytest.mark.parametrize(\n    \"product, bbox, cycles, tracks, expect\",\n    [\n        (\"ATL06\", [-64.5, -66, -63.5, -65], [\"03\"], [\"1306\"], 3240),\n        (\"ATL07\", [-65, -66, -64.5, -65], [\"04\"], [\"0186\"], 7130),\n        (\"ATL08\", [-18, 63, -17, 64], [\"03\"], [\"1320\"], 852),\n        (\"ATL10\", [-64, -67, -60, -60], [\"04\"], [\"0681\"], 6015),\n        (\"ATL12\", [-65.5, -65.5, -64.5, -65], [\"05\"], [\"0041\"], 95),\n        (\"ATL13\", [-75, -51, -74, -50], [\"05\"], [\"0293\"], 20),\n    ],\n)\ndef test_visualization_orbits(product, bbox, cycles, tracks, expect):\n\n    region_viz = Visualize(\n        product=product, spatial_extent=bbox, cycles=cycles, tracks=tracks\n    )\n    data_size = region_viz.parallel_request_OA().size\n\n    assert data_size == expect\n"
  },
  {
    "prompt": "class PostgreSQLConnection\n\nclass PostgreSQLTransaction\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom migri.backends.postgresql import PostgreSQLConnection\nfrom test import QUERIES\n\n\n@pytest.mark.parametrize(\n    \"query_element,expected_query,expected_values\",\n    [\n        (QUERIES[0], \"INSERT INTO mytable (a) VALUES ($1), ($2)\", [150, 300]),\n        (QUERIES[1], \"UPDATE tbl SET info=$2 WHERE id=$1\", [39, \"ok\"]),\n        (QUERIES[2], \"SELECT * FROM school\", []),\n        (\n            QUERIES[3],\n            \"SELECT * FROM val WHERE (value < $1 AND status = $3) \"\n            \"OR (value > $2 AND status = $3)\",\n            [20, 100, \"ok\"],\n        ),\n    ],\n)\ndef test_compile(query_element, expected_query, expected_values):\n    backend = PostgreSQLConnection(\"postgres\")\n    assert backend._compile(query_element) == {\n        \"query\": expected_query,\n        \"values\": expected_values,\n    }\n"
  },
  {
    "prompt": "fn add_custom_commands()\n\nfn add_method(cls)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"Pytest plugin entry point. Used for any fixtures needed.\"\"\"\nimport pytest\nfrom .pytest_selenium_enhancer import add_custom_commands\n\n\n@pytest.fixture(scope='session')\ndef selenium_patcher():\n    \"\"\"Add custom .\"\"\"\n    add_custom_commands()\n"
  },
  {
    "prompt": "\"\"\"Assert valid RPCResponse.\"\"\"\nfn assert_valid_response(resp: RPCResponse)\n\n\"\"\"Compare RPC responses but ignore IDs.\"\"\"\nfn compare_responses_without_ids(left: RPCResponse, right: RPCResponse) -> None\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"Tests for the HTTP API Client.\"\"\"\nimport pytest\n\nimport solana.system_program as sp\nfrom solana.rpc.api import DataSliceOpt, Client\nfrom solana.keypair import Keypair\nfrom solana.rpc.core import RPCException\nfrom solana.rpc.types import RPCError\nfrom solana.transaction import Transaction\nfrom solana.rpc.commitment import Finalized\nfrom spl.token.constants import WRAPPED_SOL_MINT\n\nfrom .utils import AIRDROP_AMOUNT, assert_valid_response\n\n\n@pytest.mark.integration\ndef test_request_air_drop(stubbed_sender: Keypair, test_http_client: Client):\n    \"\"\"Test air drop to stubbed_sender.\"\"\"\n    resp = test_http_client.request_airdrop(stubbed_sender.public_key, AIRDROP_AMOUNT)\n    assert_valid_response(resp)\n    test_http_client.confirm_transaction(resp[\"result\"])\n    balance = test_http_client.get_balance(stubbed_sender.public_key)\n    assert balance[\"result\"][\"value\"] == AIRDROP_AMOUNT\n\n\n@pytest.mark.integration\ndef test_request_air_drop_prefetched_blockhash(stubbed_sender_prefetched_blockhash, test_http_client):\n    \"\"\"Test air drop to stubbed_sender.\"\"\"\n    resp = test_http_client.request_airdrop(stubbed_sender_prefetched_blockhash.public_key, AIRDROP_AMOUNT)\n    assert_valid_response(resp)\n    test_http_client.confirm_transaction(resp[\"result\"])\n    balance = test_http_client.get_balance(stubbed_sender_prefetched_blockhash.public_key)\n    assert balance[\"result\"][\"value\"] == AIRDROP_AMOUNT\n\n\n@pytest.mark.integration\ndef test_request_air_drop_cached_blockhash(stubbed_sender_cached_blockhash, test_http_client):\n    \"\"\"Test air drop to stubbed_sender.\"\"\"\n    resp = test_http_client.request_airdrop(stubbed_sender_cached_blockhash.public_key, AIRDROP_AMOUNT)\n    assert_valid_response(resp)\n    test_http_client.confirm_transaction(resp[\"result\"])\n    assert_valid_response(resp)\n    balance = test_http_client.get_balance(stubbed_sender_cached_blockhash.public_key)\n    assert balance[\"result\"][\"value\"] == AIRDROP_AMOUNT\n\n\n@pytest.mark.integration\ndef test_send_invalid_transaction(test_http_client):\n    \"\"\"Test sending an invalid transaction to localnet.\"\"\"\n    # Create transfer tx to transfer lamports from stubbed sender to stubbed_receiver\n    with pytest.raises(RPCException) as exc_info:\n        test_http_client.send_raw_transaction(b\"foo\")\n    assert exc_info.value.args[0].keys() == RPCError.__annotations__.keys()  # pylint: disable=no-member\n\n\n@pytest.mark.integration\ndef test_send_transaction_and_get_balance(stubbed_sender, stubbed_receiver, test_http_client):\n    \"\"\"Test sending a transaction to localnet.\"\"\"\n    # Create transfer tx to transfer lamports from stubbed sender to stubbed_receiver\n    transfer_tx = Transaction().add(\n        sp.transfer(sp.TransferParams(from_pubkey=stubbed_sender.public_key, to_pubkey=stubbed_receiver, lamports=1000))\n    )\n    resp = test_http_client.send_transaction(transfer_tx, stubbed_sender)\n    assert_valid_response(resp)\n    # Confirm transaction\n    test_http_client.confirm_transaction(resp[\"result\"])\n    # Check balances\n    resp = test_http_client.get_balance(stubbed_sender.public_key)\n    assert_valid_response(resp)\n    assert resp[\"result\"][\"value\"] == 9999994000\n    resp = test_http_client.get_balance(stubbed_receiver)\n    assert_valid_response(resp)\n    assert resp[\"result\"][\"value\"] == 954\n\n\n@pytest.mark.integration\ndef test_send_transaction_prefetched_blockhash(\n    stubbed_sender_prefetched_blockhash, stubbed_receiver_prefetched_blockhash, test_http_client\n):\n    \"\"\"Test sending a transaction to localnet.\"\"\"\n    # Create transfer tx to transfer lamports from stubbed sender to stubbed_receiver\n    transfer_tx = Transaction().add(\n        sp.transfer(\n            sp.TransferParams(\n                from_pubkey=stubbed_sender_prefetched_blockhash.public_key,\n                to_pubkey=stubbed_receiver_prefetched_blockhash,\n                lamports=1000,\n            )\n        )\n    )\n    recent_blockhash = test_http_client.parse_recent_blockhash(test_http_client.get_recent_blockhash(Finalized))\n    resp = test_http_client.send_transaction(\n        transfer_tx, stubbed_sender_prefetched_blockhash, recent_blockhash=recent_blockhash\n    )\n    assert_valid_response(resp)\n    # Confirm transaction\n    test_http_client.confirm_transaction(resp[\"result\"])\n    # Check balances\n    resp = test_http_client.get_balance(stubbed_sender_prefetched_blockhash.public_key)\n    assert_valid_response(resp)\n    assert resp[\"result\"][\"value\"] == 9999994000\n    resp = test_http_client.get_balance(stubbed_receiver_prefetched_blockhash)\n    assert_valid_response(resp)\n    assert resp[\"result\"][\"value\"] == 954\n\n\n@pytest.mark.integration\ndef test_send_transaction_cached_blockhash(\n    stubbed_sender_cached_blockhash, stubbed_receiver_cached_blockhash, test_http_client_cached_blockhash\n):\n    \"\"\"Test sending a transaction to localnet.\"\"\"\n    # Create transfer tx to transfer lamports from stubbed sender to stubbed_receiver\n    transfer_tx = Transaction().add(\n        sp.transfer(\n            sp.TransferParams(\n                from_pubkey=stubbed_sender_cached_blockhash.public_key,\n                to_pubkey=stubbed_receiver_cached_blockhash,\n                lamports=1000,\n            )\n        )\n    )\n    assert len(test_http_client_cached_blockhash.blockhash_cache.unused_blockhashes) == 0\n    assert len(test_http_client_cached_blockhash.blockhash_cache.used_blockhashes) == 0\n    resp = test_http_client_cached_blockhash.send_transaction(transfer_tx, stubbed_sender_cached_blockhash)\n    # we could have got a new blockhash or not depending on network latency and luck\n    assert len(test_http_client_cached_blockhash.blockhash_cache.unused_blockhashes) in (0, 1)\n    assert len(test_http_client_cached_blockhash.blockhash_cache.used_blockhashes) == 1\n    assert_valid_response(resp)\n    # Confirm transaction\n    test_http_client_cached_blockhash.confirm_transaction(resp[\"result\"])\n    # Check balances\n    resp = test_http_client_cached_blockhash.get_balance(stubbed_sender_cached_blockhash.public_key)\n    assert_valid_response(resp)\n    assert resp[\"result\"][\"value\"] == 9999994000\n\n    # Second transaction\n    transfer_tx = Transaction().add(\n        sp.transfer(\n            sp.TransferParams(\n                from_pubkey=stubbed_sender_cached_blockhash.public_key,\n                to_pubkey=stubbed_receiver_cached_blockhash,\n                lamports=2000,\n            )\n        )\n    )\n    resp = test_http_client_cached_blockhash.get_balance(stubbed_receiver_cached_blockhash)\n    assert_valid_response(resp)\n    assert resp[\"result\"][\"value\"] == 954\n    resp = test_http_client_cached_blockhash.send_transaction(transfer_tx, stubbed_sender_cached_blockhash)\n    # we could have got a new blockhash or not depending on network latency and luck\n    assert len(test_http_client_cached_blockhash.blockhash_cache.unused_blockhashes) in (0, 1)\n    assert len(test_http_client_cached_blockhash.blockhash_cache.used_blockhashes) in (1, 2)\n    assert_valid_response(resp)\n    # Confirm transaction\n    test_http_client_cached_blockhash.confirm_transaction(resp[\"result\"])\n    # Check balances\n    resp = test_http_client_cached_blockhash.get_balance(stubbed_sender_cached_blockhash.public_key)\n    assert_valid_response(resp)\n    assert resp[\"result\"][\"value\"] == 9999987000\n    assert len(test_http_client_cached_blockhash.blockhash_cache.unused_blockhashes) == 1\n    assert len(test_http_client_cached_blockhash.blockhash_cache.used_blockhashes) == 1\n\n\n@pytest.mark.integration\ndef test_send_raw_transaction_and_get_balance(stubbed_sender, stubbed_receiver, test_http_client):\n    \"\"\"Test sending a raw transaction to localnet.\"\"\"\n    # Get a recent blockhash\n    resp = test_http_client.get_recent_blockhash(Finalized)\n    assert_valid_response(resp)\n    recent_blockhash = resp[\"result\"][\"value\"][\"blockhash\"]\n    # Create transfer tx transfer lamports from stubbed sender to stubbed_receiver\n    transfer_tx = Transaction(recent_blockhash=recent_blockhash).add(\n        sp.transfer(sp.TransferParams(from_pubkey=stubbed_sender.public_key, to_pubkey=stubbed_receiver, lamports=1000))\n    )\n    # Sign transaction\n    transfer_tx.sign(stubbed_sender)\n    # Send raw transaction\n    resp = test_http_client.send_raw_transaction(transfer_tx.serialize())\n    assert_valid_response(resp)\n    # Confirm transaction\n    test_http_client.confirm_transaction(resp[\"result\"])\n    # Check balances\n    resp = test_http_client.get_balance(stubbed_sender.public_key)\n    assert_valid_response(resp)\n    assert resp[\"result\"][\"value\"] == 9999988000\n    resp = test_http_client.get_balance(stubbed_receiver)\n    assert_valid_response(resp)\n    assert resp[\"result\"][\"value\"] == 1954\n\n\n@pytest.mark.integration\ndef test_confirm_bad_signature(test_http_client: Client) -> None:\n    \"\"\"Test that RPCException is raised when trying to confirm an invalid signature.\"\"\"\n    with pytest.raises(RPCException) as exc_info:\n        test_http_client.confirm_transaction(\"foo\")\n    err_object = exc_info.value.args[0]\n    assert err_object == {\"code\": -32602, \"message\": \"Invalid param: WrongSize\"}\n\n\n@pytest.mark.integration\ndef test_get_block_commitment(test_http_client):\n    \"\"\"Test get block commitment.\"\"\"\n    resp = test_http_client.get_block_commitment(5)\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_block_time(test_http_client):\n    \"\"\"Test get block time.\"\"\"\n    resp = test_http_client.get_block_time(5)\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_cluster_nodes(test_http_client):\n    \"\"\"Test get cluster nodes.\"\"\"\n    resp = test_http_client.get_cluster_nodes()\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_confirmed_block(test_http_client):\n    \"\"\"Test get confirmed block.\"\"\"\n    resp = test_http_client.get_confirmed_block(1)\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_confirmed_block_with_encoding(test_http_client):\n    \"\"\"Test get confrimed block with encoding.\"\"\"\n    resp = test_http_client.get_confirmed_block(1, encoding=\"base64\")\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_block(test_http_client):\n    \"\"\"Test get block.\"\"\"\n    resp = test_http_client.get_block(1)\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_block_with_encoding(test_http_client):\n    \"\"\"Test get block with encoding.\"\"\"\n    resp = test_http_client.get_block(1, encoding=\"base64\")\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_block_height(test_http_client):\n    \"\"\"Test get height.\"\"\"\n    resp = test_http_client.get_block_height()\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_confirmed_blocks(test_http_client):\n    \"\"\"Test get confirmed blocks.\"\"\"\n    resp = test_http_client.get_confirmed_blocks(5, 10)\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_blocks(test_http_client):\n    \"\"\"Test get blocks.\"\"\"\n    resp = test_http_client.get_blocks(5, 10)\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_confirmed_signature_for_address2(test_http_client):\n    \"\"\"Test get confirmed signature for address2.\"\"\"\n    resp = test_http_client.get_confirmed_signature_for_address2(\"Vote111111111111111111111111111111111111111\", limit=1)\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_signatures_for_address(test_http_client):\n    \"\"\"Test get signatures for addresses.\"\"\"\n    resp = test_http_client.get_signatures_for_address(\"Vote111111111111111111111111111111111111111\", limit=1)\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_epoch_info(test_http_client):\n    \"\"\"Test get epoch info.\"\"\"\n    resp = test_http_client.get_epoch_info()\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_epoch_schedule(test_http_client):\n    \"\"\"Test get epoch schedule.\"\"\"\n    resp = test_http_client.get_epoch_schedule()\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_fee_calculator_for_blockhash(test_http_client):\n    \"\"\"Test get fee calculator for blockhash.\"\"\"\n    resp = test_http_client.get_recent_blockhash(Finalized)\n    assert_valid_response(resp)\n    resp = test_http_client.get_fee_calculator_for_blockhash(resp[\"result\"][\"value\"][\"blockhash\"])\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_slot(test_http_client):\n    \"\"\"Test get slot.\"\"\"\n    resp = test_http_client.get_slot()\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_fees(test_http_client):\n    \"\"\"Test get fees.\"\"\"\n    resp = test_http_client.get_fees()\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_first_available_block(test_http_client):\n    \"\"\"Test get first available block.\"\"\"\n    resp = test_http_client.get_first_available_block()\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_genesis_hash(test_http_client):\n    \"\"\"Test get genesis hash.\"\"\"\n    resp = test_http_client.get_genesis_hash()\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_identity(test_http_client):\n    \"\"\"Test get identity.\"\"\"\n    resp = test_http_client.get_genesis_hash()\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_inflation_governor(test_http_client):\n    \"\"\"Test get inflation governor.\"\"\"\n    resp = test_http_client.get_inflation_governor()\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_inflation_rate(test_http_client):\n    \"\"\"Test get inflation rate.\"\"\"\n    resp = test_http_client.get_inflation_rate()\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_largest_accounts(test_http_client):\n    \"\"\"Test get largest accounts.\"\"\"\n    resp = test_http_client.get_largest_accounts()\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_leader_schedule(test_http_client):\n    \"\"\"Test get leader schedule.\"\"\"\n    resp = test_http_client.get_leader_schedule()\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_minimum_balance_for_rent_exemption(test_http_client):\n    \"\"\"Test get minimum balance for rent exemption.\"\"\"\n    resp = test_http_client.get_minimum_balance_for_rent_exemption(50)\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_slot_leader(test_http_client):\n    \"\"\"Test get slot leader.\"\"\"\n    resp = test_http_client.get_slot_leader()\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_supply(test_http_client):\n    \"\"\"Test get slot leader.\"\"\"\n    resp = test_http_client.get_supply()\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_transaction_count(test_http_client):\n    \"\"\"Test get transactinon count.\"\"\"\n    resp = test_http_client.get_transaction_count()\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_version(test_http_client):\n    \"\"\"Test get version.\"\"\"\n    resp = test_http_client.get_version()\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_account_info(stubbed_sender, test_http_client):\n    \"\"\"Test get_account_info.\"\"\"\n    resp = test_http_client.get_account_info(stubbed_sender.public_key)\n    assert_valid_response(resp)\n    resp = test_http_client.get_account_info(stubbed_sender.public_key, encoding=\"jsonParsed\")\n    assert_valid_response(resp)\n    resp = test_http_client.get_account_info(stubbed_sender.public_key, data_slice=DataSliceOpt(1, 1))\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_multiple_accounts(stubbed_sender, test_http_client):\n    \"\"\"Test get_multiple_accounts.\"\"\"\n    pubkeys = [stubbed_sender.public_key] * 2\n    resp = test_http_client.get_multiple_accounts(pubkeys)\n    assert_valid_response(resp)\n    resp = test_http_client.get_multiple_accounts(pubkeys, encoding=\"jsonParsed\")\n    assert_valid_response(resp)\n    resp = test_http_client.get_multiple_accounts(pubkeys, data_slice=DataSliceOpt(1, 1))\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_token_largest_accounts(test_http_client):\n    \"\"\"Test get token largest accounts.\"\"\"\n    resp = test_http_client.get_token_largest_accounts(WRAPPED_SOL_MINT)\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_token_supply(test_http_client):\n    \"\"\"Test get token supply.\"\"\"\n    resp = test_http_client.get_token_supply(WRAPPED_SOL_MINT)\n    assert_valid_response(resp)\n\n\n@pytest.mark.integration\ndef test_get_vote_accounts(test_http_client):\n    \"\"\"Test get vote accounts.\"\"\"\n    resp = test_http_client.get_vote_accounts()\n    assert_valid_response(resp)\n"
  },
  {
    "prompt": "class State\n\nclass EigerStatus\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom tickit.devices.eiger.eiger_status import EigerStatus\n\n# # # # # EigerStatus Tests # # # # #\n\n\n@pytest.fixture\ndef eiger_status() -> EigerStatus:\n    return EigerStatus()\n\n\ndef test_eiger_status_constructor():\n    EigerStatus()\n\n\ndef test_eiger_status_getitem(eiger_status):\n    assert 24.5 == eiger_status[\"th0_temp\"]\n"
  },
  {
    "prompt": "class LinearReward\n\nclass ExpReward\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nimport sinergym.utils.rewards as R\n\n\n@pytest.mark.parametrize(\n    'power,temperatures,month,day,reward,reward_energy,reward_comfort',\n    [\n        # Input 1\n        (\n            186.5929171535975,\n            [22.16742570092868],\n            3,\n            31,\n            -0.009329645857679876,\n            -0.018659291715359752,\n            -0.0\n        ),\n        # Input 2\n        (\n            688.0477550424935,\n            [26.7881162590194],\n            3,\n            30,\n            -1.6784605172618248,\n            -0.06880477550424935,\n            -3.2881162590194\n        ),\n        # Input 3\n        (\n            23168.30752221127,\n            [20.37505026953311],\n            2,\n            25,\n            -1.1584153761105636,\n            -2.316830752221127,\n            -0.0\n        ),\n    ]\n)\ndef test_calculate(\n        simple_reward,\n        power,\n        temperatures,\n        month,\n        day,\n        reward,\n        reward_energy,\n        reward_comfort):\n    result = simple_reward.calculate(power, temperatures, month, day)\n    assert result[0] == reward\n    assert result[1]['reward_energy'] == reward_energy\n    assert result[1]['reward_comfort'] == reward_comfort\n"
  },
  {
    "prompt": "\"\"\"    \"\"\"\nfn verify_all(module, ALL)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "#-----------------------------------------------------------------------------\n# Copyright (c) 2012 - 2019, Anaconda, Inc., and Bokeh Contributors.\n# All rights reserved.\n#\n# The full license is in the file LICENSE.txt, distributed with this software.\n#-----------------------------------------------------------------------------\n\n#-----------------------------------------------------------------------------\n# Boilerplate\n#-----------------------------------------------------------------------------\nimport pytest ; pytest\n\n#-----------------------------------------------------------------------------\n# Imports\n#-----------------------------------------------------------------------------\n\n# Standard library imports\n\n# External imports\n\n# Bokeh imports\nfrom bokeh._testing.util.api import verify_all\n\n# Module under test\n#import bokeh.sampledata.glucose as bsg\n\n#-----------------------------------------------------------------------------\n# Setup\n#-----------------------------------------------------------------------------\n\nALL = (\n    'data',\n)\n\n#-----------------------------------------------------------------------------\n# General API\n#-----------------------------------------------------------------------------\n\nTest___all__ = pytest.mark.sampledata(verify_all(\"bokeh.sampledata.glucose\", ALL))\n\n@pytest.mark.sampledata\ndef test_data(pd):\n    import bokeh.sampledata.glucose as bsg\n    assert isinstance(bsg.data, pd.DataFrame)\n\n    # don't check detail for external data\n\n#-----------------------------------------------------------------------------\n# Dev API\n#-----------------------------------------------------------------------------\n\n#-----------------------------------------------------------------------------\n# Private API\n#-----------------------------------------------------------------------------\n\n#-----------------------------------------------------------------------------\n# Code\n#-----------------------------------------------------------------------------\n"
  },
  {
    "prompt": "class LabelCalibrator\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "#  Copyright 2021 ZBW  \u2013 Leibniz Information Centre for Economics\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport numpy as np\nimport pytest\nfrom sklearn.exceptions import NotFittedError\nfrom stwfsapy.text_features import mk_text_features\n\nfrom qualle.label_calibration.simple import LabelCalibrator\nfrom tests.common import DummyRegressor\n\n\n@pytest.fixture\ndef calibrator():\n    return LabelCalibrator(DummyRegressor())\n\n\ndef test_lc_predict(calibrator, X):\n    calibrator.fit(X, [3, 5])\n    assert np.array_equal(calibrator.predict(X), [0, 1])\n\n\ndef test_lc_predict_without_fit_raises_exc(calibrator, X):\n    with pytest.raises(NotFittedError):\n        calibrator.predict(X)\n\n\ndef test_lc_fit_fits_regressor_with_txt_features(calibrator, X, mocker):\n    y = [3, 5]\n    txt_features = mk_text_features().fit(X)\n    X_transformed = txt_features.transform(X)\n\n    spy = mocker.spy(calibrator.regressor, 'fit')\n    calibrator.fit(X, y)\n    spy.assert_called_once()\n    assert (spy.call_args[0][0].toarray() == X_transformed.toarray()).all()\n    assert spy.call_args[0][1] == y\n"
  },
  {
    "prompt": "class Backend\n\nclass NumpyBackend\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nimport numpy as np\nimport random\n\nfrom cem.backend import backend, NumpyBackend\n\ntry:\n    from cem.backend import CuPyBackend\n    import cupy as cp\n    skip_cupy_test = False\nexcept ImportError:\n    skip_cupy_test = True\n\n\ndef test_numpy_backend():\n    X = random.randint(0, 10) * 10\n    Y = random.randint(0, 10) * 10\n\n    zeros = backend.zeros((X, Y))\n    ones = backend.ones((X, Y))\n\n    assert isinstance(backend, NumpyBackend)\n    assert isinstance(zeros, np.ndarray)\n    assert isinstance(ones, np.ndarray)\n    assert backend.int == np.int64\n    assert backend.float == np.float64\n    assert zeros.shape == (X, Y)\n    assert ones.shape == (X, Y)\n    assert backend.sin(ones).any() == np.sin(ones).any()\n    assert backend.cos(ones).any() == np.cos(ones).any()\n\n\n@pytest.mark.skipif(skip_cupy_test, reason='CuPy is not installed.')\ndef test_cupy_backend():\n    backend.set_backend('cupy')\n    X = random.randint(0, 10) * 10\n    Y = random.randint(0, 10) * 10\n\n    zeros = backend.zeros((X, Y))\n    ones = backend.ones((X, Y))\n\n    assert isinstance(backend, CuPyBackend)\n    assert isinstance(zeros, cp.ndarray)\n    assert isinstance(ones, cp.ndarray)\n    assert backend.int == cp.int64\n    assert backend.float == cp.float64\n    assert zeros.shape == (X, Y)\n    assert ones.shape == (X, Y)\n    assert backend.sin(ones).all() == cp.sin(ones).all()\n    assert backend.cos(ones).all() == cp.cos(ones).all()\n\n\n@pytest.mark.skipif(skip_cupy_test, reason='CuPy is not installed.')\ndef test_set_backend():\n    backend.set_backend('numpy')\n    assert isinstance(backend, NumpyBackend)\n    backend.set_backend('cupy')\n    assert isinstance(backend, CuPyBackend)\n"
  },
  {
    "prompt": "class Bigchain\n\nclass BigchainDBError\n\nclass CriticalDoubleSpend\n\nclass CriticalDoubleInclusion\n\nclass CriticalDuplicateVote\n\nclass Voting\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom unittest.mock import patch\nfrom collections import Counter\n\nfrom bigchaindb.core import Bigchain\nfrom bigchaindb.exceptions import CriticalDuplicateVote\nfrom bigchaindb.voting import Voting, INVALID, VALID, UNDECIDED\n\n\n################################################################################\n# Tests for checking vote eligibility\n\n\ndef test_partition_eligible_votes():\n    class TestVoting(Voting):\n        @classmethod\n        def verify_vote_signature(cls, vote):\n            if vote['node_pubkey'] == 'invalid sig':\n                return False\n            if vote['node_pubkey'] == 'value error':\n                raise ValueError()\n            return True\n\n    voters = ['valid', 'invalid sig', 'value error', 'not in set']\n    votes = [{'node_pubkey': k} for k in voters]\n\n    el, inel = TestVoting.partition_eligible_votes(votes, voters[:-1])\n    assert el == [votes[0]]\n    assert inel == votes[1:]\n\n\n################################################################################\n# Test vote counting\n\n\ndef test_count_votes():\n    class TestVoting(Voting):\n        @classmethod\n        def verify_vote_schema(cls, vote):\n            return vote['node_pubkey'] != 'malformed'\n\n    voters = (['says invalid', 'malformed'] +\n              ['kosher' + str(i) for i in range(10)])\n\n    votes = [Bigchain(v).vote('block', 'a', True) for v in voters]\n    votes[0]['vote']['is_block_valid'] = False\n    # Incorrect previous block subtracts from n_valid and adds to n_invalid\n    votes[-1]['vote']['previous_block'] = 'z'\n\n    by_voter = dict(enumerate(votes))\n\n    assert TestVoting.count_votes(by_voter) == {\n        'counts': {\n            'n_valid': 9,    # 9 kosher votes\n            'n_invalid': 3,  # 1 invalid, 1 malformed, 1 rogue prev block\n        },\n        'malformed': [votes[1]],\n        'previous_block': 'a',\n        'other_previous_block': {'z': 1},\n    }\n\n\ndef test_must_agree_prev_block():\n    class TestVoting(Voting):\n        @classmethod\n        def verify_vote_schema(cls, vote):\n            return True\n\n    voters = 'abcd'\n    votes = [Bigchain(v).vote('block', 'a', True) for v in voters]\n    votes[0]['vote']['previous_block'] = 'b'\n    votes[1]['vote']['previous_block'] = 'c'\n    by_voter = dict(enumerate(votes))\n    assert TestVoting.count_votes(by_voter) == {\n        'counts': {\n            'n_valid': 2,\n            'n_invalid': 2,\n        },\n        'previous_block': 'a',\n        'other_previous_block': {'b': 1, 'c': 1},\n        'malformed': [],\n    }\n\n\n################################################################################\n# Tests for vote decision making\n\n\nDECISION_TESTS = [\n    {'n_voters': 1, 'n_valid': 1, 'n_invalid': 1},\n    {'n_voters': 2, 'n_valid': 2, 'n_invalid': 1},\n    {'n_voters': 3, 'n_valid': 2, 'n_invalid': 2},\n    {'n_voters': 4, 'n_valid': 3, 'n_invalid': 2},\n    {'n_voters': 5, 'n_valid': 3, 'n_invalid': 3},\n    {'n_voters': 6, 'n_valid': 4, 'n_invalid': 3},\n    {'n_voters': 7, 'n_valid': 4, 'n_invalid': 4},\n    {'n_voters': 8, 'n_valid': 5, 'n_invalid': 4}\n]\n\n\n@pytest.mark.parametrize('kwargs', DECISION_TESTS)\ndef test_decide_votes_valid(kwargs):\n    kwargs = kwargs.copy()\n    kwargs['n_invalid'] = 0\n    assert Voting.decide_votes(**kwargs) == VALID\n    kwargs['n_valid'] -= 1\n    assert Voting.decide_votes(**kwargs) == UNDECIDED\n\n\n@pytest.mark.parametrize('kwargs', DECISION_TESTS)\ndef test_decide_votes_invalid(kwargs):\n    kwargs = kwargs.copy()\n    kwargs['n_valid'] = 0\n    assert Voting.decide_votes(**kwargs) == INVALID\n    kwargs['n_invalid'] -= 1\n    assert Voting.decide_votes(**kwargs) == UNDECIDED\n\n\n################################################################################\n# Actions - test state transitions\n\n\n@pytest.mark.parametrize('n_voters', range(8))\ndef test_vote_actions(n_voters):\n    \"\"\"* Legal transitions are UNDECIDED -> [VALID|INVALID] only\n    * Block is never left UNDECIDED after voting\n    * Accomodates rogues on previous block / invalid schema\n    \"\"\"\n    class TestVoting(Voting):\n        @classmethod\n        def verify_vote_schema(cls, vote):\n            return type(vote['vote']['is_block_valid']) == bool\n\n        @classmethod\n        def verify_vote_signature(cls, vote):\n            return True\n\n    keyring = 'abcdefghijklmnopqrstuvwxyz'[:n_voters]\n    block = {'id': 'block', 'block': {'voters': keyring}}\n    state = UNDECIDED\n    todo = [(state, [], [])]\n\n    def branch(p, r):\n        todo.append((state, votes, votes + [{\n            'node_pubkey': keyring[len(votes)],\n            'vote': {'previous_block': p, 'is_block_valid': r}\n        }]))\n\n    while todo:\n        prev_state, prev_votes, votes = todo.pop(0)\n        results = Counter(v['vote']['is_block_valid'] for v in votes)\n        prev_blocks = Counter(v['vote']['previous_block'] for v in votes)\n        majority = n_voters // 2 + 1\n        honest = (len(votes) == majority and len(prev_blocks) == 1 and\n                  not results['lol'] and len(results) == 1)\n        closed = len(votes) == n_voters\n\n        # Test legal transition\n        if votes:\n            state = TestVoting.block_election(block, votes, keyring)['status']\n            assert prev_state in [state, UNDECIDED]\n\n        # Test that decision has been reached\n        if honest or closed:\n            assert state != UNDECIDED or n_voters == 0\n\n        if closed:\n            continue\n\n        # Can accomodate more votes, add them to the todo list.\n        # This vote is the good case\n        branch('A', True)\n        # This vote disagrees on previous block\n        branch('B', True)\n        # This vote says the block is invalid\n        branch('A', False)\n        # This vote is invalid\n        branch('A', 'lol')\n\n\n################################################################################\n# Tests for vote signature\n\n\ndef test_verify_vote_signature_passes(b):\n    vote = b.vote('block', 'a', True)\n    assert Voting.verify_vote_signature(vote)\n    vote['signature'] = ''\n    assert not Voting.verify_vote_signature(vote)\n\n\n################################################################################\n# Tests for vote schema\n\n\ndef test_verify_vote_schema(b):\n    vote = b.vote('b' * 64, 'a' * 64, True)\n    assert Voting.verify_vote_schema(vote)\n    vote = b.vote('b' * 64, 'a', True)\n    assert not Voting.verify_vote_schema(vote)\n    vote = b.vote('b', 'a' * 64, True)\n    assert not Voting.verify_vote_schema(vote)\n\n\n################################################################################\n# block_election tests\n\n\ndef test_block_election(b):\n\n    class TestVoting(Voting):\n        @classmethod\n        def verify_vote_signature(cls, vote):\n            return True\n\n        @classmethod\n        def verify_vote_schema(cls, vote):\n            return True\n\n    keyring = 'abc'\n    block = {'id': 'xyz', 'block': {'voters': 'ab'}}\n    votes = [{\n        'node_pubkey': c,\n        'vote': {'is_block_valid': True, 'previous_block': 'a'}\n    } for c in 'abc']\n\n    assert TestVoting.block_election(block, votes, keyring) == {\n        'status': VALID,\n        'block_id': 'xyz',\n        'counts': {'n_valid': 2, 'n_invalid': 0},\n        'ineligible': [votes[-1]],\n        'malformed': [],\n        'previous_block': 'a',\n        'other_previous_block': {},\n    }\n\n\n@patch('bigchaindb.voting.Voting.verify_vote_signature', return_value=True)\ndef test_duplicate_vote_throws_critical_error(b):\n    keyring = 'abc'\n    block = {'id': 'xyz', 'block': {'voters': 'ab'}}\n    votes = [{\n        'node_pubkey': c,\n        'vote': {'is_block_valid': True, 'previous_block': 'a'}\n    } for c in 'aabc']\n    with pytest.raises(CriticalDuplicateVote):\n        Voting.block_election(block, votes, keyring)\n"
  },
  {
    "prompt": "class anything_else_cls\n\n\"\"\"Ensure `fsm.anything_else` always sorts last\"\"\"\nfn key(symbol)\n\nclass OblivionError\n\nclass fsm\n\n\"\"\"An FSM accepting nothing (not even the empty string). This is\ndemonstrates that this is possible, and is also extremely useful\nin some situations\"\"\"\nfn null(alphabet)\n\n\"\"\"Return an FSM matching an empty string, \"\", only.\nThis is very useful in many situations\"\"\"\nfn epsilon(alphabet)\n\n\"\"\"Crawl several FSMs in parallel, mapping the states of a larger meta-FSM.\nTo determine whether a state in the larger FSM is final, pass all of the\nfinality statuses (e.g. [True, False, False] to `test`.\"\"\"\nfn parallel(fsms, test)\n\n\"\"\"Given the above conditions and instructions, crawl a new unknown FSM,\nmapping its states, final states and transitions. Return the new FSM.\nThis is a pretty powerful procedure which could potentially go on\nforever if you supply an evil version of follow().\"\"\"\nfn crawl(alphabet, initial, final, follow)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# -*- coding: utf-8 -*-\n\nif __name__ == \"__main__\":\n\traise Exception(\"Test files can't be run directly. Use `python -m pytest greenery`\")\n\nimport pytest\nfrom greenery.fsm import fsm, null, epsilon, anything_else\n\ndef test_addbug():\n\t# Odd bug with fsm.__add__(), exposed by \"[bc]*c\"\n\tint5A = fsm(\n\t\talphabet = {\"a\", \"b\", \"c\", anything_else},\n\t\tstates   = {0, 1},\n\t\tinitial  = 1,\n\t\tfinals   = {1},\n\t\tmap      = {\n\t\t\t0: {anything_else: 0, \"a\": 0, \"b\": 0, \"c\": 0},\n\t\t\t1: {anything_else: 0, \"a\": 0, \"b\": 1, \"c\": 1},\n\t\t}\n\t)\n\tassert int5A.accepts(\"\")\n\n\tint5B = fsm(\n\t\talphabet = {\"a\", \"b\", \"c\", anything_else},\n\t\tstates   = {0, 1, 2},\n\t\tinitial  = 1,\n\t\tfinals   = {0},\n\t\tmap      = {\n\t\t\t0: {anything_else: 2, \"a\": 2, \"b\": 2, \"c\": 2},\n\t\t\t1: {anything_else: 2, \"a\": 2, \"b\": 2, \"c\": 0},\n\t\t\t2: {anything_else: 2, \"a\": 2, \"b\": 2, \"c\": 2},\n\t\t}\n\t)\n\tassert int5B.accepts(\"c\")\n\n\tint5C = int5A + int5B\n\tassert int5C.accepts(\"c\")\n\t# assert int5C.initial == 0\n\ndef test_builtins():\n\tassert not null(\"a\").accepts(\"a\")\n\tassert epsilon(\"a\").accepts(\"\")\n\tassert not epsilon(\"a\").accepts(\"a\")\n\n@pytest.fixture\ndef a():\n\ta = fsm(\n\t\talphabet = {\"a\", \"b\"},\n\t\tstates   = {0, 1, \"ob\"},\n\t\tinitial  = 0,\n\t\tfinals   = {1},\n\t\tmap      = {\n\t\t\t0    : {\"a\" : 1   , \"b\" : \"ob\"},\n\t\t\t1    : {\"a\" : \"ob\", \"b\" : \"ob\"},\n\t\t\t\"ob\" : {\"a\" : \"ob\", \"b\" : \"ob\"},\n\t\t},\n\t)\n\treturn a\n\ndef test_a(a):\n\tassert not a.accepts(\"\")\n\tassert a.accepts(\"a\")\n\tassert not a.accepts(\"b\")\n\n@pytest.fixture\ndef b():\n\tb = fsm(\n\t\talphabet = {\"a\", \"b\"},\n\t\tstates   = {0, 1, \"ob\"},\n\t\tinitial  = 0,\n\t\tfinals   = {1},\n\t\tmap      = {\n\t\t\t0    : {\"a\" : \"ob\", \"b\" : 1   },\n\t\t\t1    : {\"a\" : \"ob\", \"b\" : \"ob\"},\n\t\t\t\"ob\" : {\"a\" : \"ob\", \"b\" : \"ob\"},\n\t\t},\n\t)\n\treturn b\n\ndef test_b(b):\n\tassert not b.accepts(\"\")\n\tassert not b.accepts(\"a\")\n\tassert b.accepts(\"b\")\n\ndef test_concatenation_aa(a):\n\tconcAA = a + a\n\tassert not concAA.accepts(\"\")\n\tassert not concAA.accepts(\"a\")\n\tassert concAA.accepts(\"aa\")\n\tassert not concAA.accepts(\"aaa\")\n\n\tconcAA = epsilon({\"a\", \"b\"}) + a + a\n\tassert not concAA.accepts(\"\")\n\tassert not concAA.accepts(\"a\")\n\tassert concAA.accepts(\"aa\")\n\tassert not concAA.accepts(\"aaa\")\n\ndef test_concatenation_ab(a, b):\n\tconcAB = a + b\n\tassert not concAB.accepts(\"\")\n\tassert not concAB.accepts(\"a\")\n\tassert not concAB.accepts(\"b\")\n\tassert not concAB.accepts(\"aa\")\n\tassert concAB.accepts(\"ab\")\n\tassert not concAB.accepts(\"ba\")\n\tassert not concAB.accepts(\"bb\")\n\ndef test_alternation_a(a):\n\taltA = a | null({\"a\", \"b\"})\n\tassert not altA.accepts(\"\")\n\tassert altA.accepts(\"a\")\n\ndef test_alternation_ab(a, b):\n\taltAB = a | b\n\tassert not altAB.accepts(\"\")\n\tassert altAB.accepts(\"a\")\n\tassert altAB.accepts(\"b\")\n\tassert not altAB.accepts(\"aa\")\n\tassert not altAB.accepts(\"ab\")\n\tassert not altAB.accepts(\"ba\")\n\tassert not altAB.accepts(\"bb\")\n\ndef test_star(a):\n\tstarA = a.star()\n\tassert starA.accepts(\"\")\n\tassert starA.accepts(\"a\")\n\tassert not starA.accepts(\"b\")\n\tassert starA.accepts(\"aaaaaaaaa\")\n\ndef test_multiply_0(a):\n\tzeroA = a * 0\n\tassert zeroA.accepts(\"\")\n\tassert not zeroA.accepts(\"a\")\n\ndef test_multiply_1(a):\n\toneA = a * 1\n\tassert not oneA.accepts(\"\")\n\tassert oneA.accepts(\"a\")\n\tassert not oneA.accepts(\"aa\")\n\ndef test_multiply_2(a):\n\ttwoA = a * 2\n\tassert not twoA.accepts(\"\")\n\tassert not twoA.accepts(\"a\")\n\tassert twoA.accepts(\"aa\")\n\tassert not twoA.accepts(\"aaa\")\n\ndef test_multiply_7(a):\n\tsevenA = a * 7\n\tassert not sevenA.accepts(\"aaaaaa\")\n\tassert sevenA.accepts(\"aaaaaaa\")\n\tassert not sevenA.accepts(\"aaaaaaaa\")\n\ndef test_optional_mul(a, b):\n\tunit = a + b\n\t# accepts \"ab\"\n\n\toptional = (epsilon(a.alphabet) | unit)\n\t# accepts \"(ab)?\n\tassert optional.accepts([])\n\tassert not optional.accepts([\"a\"])\n\tassert not optional.accepts([\"b\"])\n\tassert optional.accepts([\"a\", \"b\"])\n\tassert not optional.accepts([\"a\", \"a\"])\n\n\toptional = optional * 2\n\t# accepts \"(ab)?(ab)?\"\n\tassert optional.accepts([])\n\tassert not optional.accepts([\"a\"])\n\tassert not optional.accepts([\"b\"])\n\tassert not optional.accepts([\"a\", \"a\"])\n\tassert optional.accepts([\"a\", \"b\"])\n\tassert not optional.accepts([\"b\", \"a\"])\n\tassert not optional.accepts([\"b\", \"b\"])\n\tassert not optional.accepts([\"a\", \"a\", \"a\"])\n\tassert optional.accepts([\"a\", \"b\", \"a\", \"b\"])\n\ndef test_intersection_ab(a, b):\n\tintAB = a & b\n\tassert not intAB.accepts(\"\")\n\tassert not intAB.accepts(\"a\")\n\tassert not intAB.accepts(\"b\")\n\ndef test_negation(a):\n\teverythingbutA = a.everythingbut()\n\tassert everythingbutA.accepts(\"\")\n\tassert not everythingbutA.accepts(\"a\")\n\tassert everythingbutA.accepts(\"b\")\n\tassert everythingbutA.accepts(\"aa\")\n\tassert everythingbutA.accepts(\"ab\")\n\ndef test_crawl_reduction():\n\t# this is \"0*1\" in heavy disguise. crawl should resolve this duplication\n\t# Notice how states 2 and 3 behave identically. When resolved together,\n\t# states 1 and 2&3 also behave identically, so they, too should be resolved\n\t# (this is impossible to spot before 2 and 3 have been combined).\n\t# Finally, the oblivion state should be omitted.\n\tmerged = fsm(\n\t\talphabet = {\"0\", \"1\"},\n\t\tstates   = {1, 2, 3, 4, \"oblivion\"},\n\t\tinitial  = 1,\n\t\tfinals   = {4},\n\t\tmap      = {\n\t\t\t1          : {\"0\" : 2         , \"1\" : 4         },\n\t\t\t2          : {\"0\" : 3         , \"1\" : 4         },\n\t\t\t3          : {\"0\" : 3         , \"1\" : 4         },\n\t\t\t4          : {\"0\" : \"oblivion\", \"1\" : \"oblivion\"},\n\t\t\t\"oblivion\" : {\"0\" : \"oblivion\", \"1\" : \"oblivion\"},\n\t\t}\n\t).reduce()\n\tassert len(merged.states) == 2\n\ndef test_bug_28():\n\t# This is (ab*)* and it caused some defects.\n\tabstar = fsm(\n\t\talphabet = {'a', 'b'},\n\t\tstates   = {0, 1},\n\t\tinitial  = 0,\n\t\tfinals   = {1},\n\t\tmap = {\n\t\t\t0: {'a': 1},\n\t\t\t1: {'b': 1}\n\t\t}\n\t)\n\tassert abstar.accepts(\"a\")\n\tassert not abstar.accepts(\"b\")\n\tassert abstar.accepts(\"ab\")\n\tassert abstar.accepts(\"abb\")\n\tabstarstar = abstar.star()\n\tassert abstarstar.accepts(\"a\")\n\tassert not abstarstar.accepts(\"b\")\n\tassert abstarstar.accepts(\"ab\")\n\tassert not abstar.star().accepts(\"bb\")\n\ndef test_star_advanced():\n\t# This is (a*ba)*. Naively connecting the final states to the initial state\n\t# gives the incorrect result here.\n\tstarred = fsm(\n\t\talphabet = {\"a\", \"b\"},\n\t\tstates   = {0, 1, 2, \"oblivion\"},\n\t\tinitial  = 0,\n\t\tfinals   = {2},\n\t\tmap      = {\n\t\t\t0          : {\"a\" : 0         , \"b\" : 1         },\n\t\t\t1          : {\"a\" : 2         , \"b\" : \"oblivion\"},\n\t\t\t2          : {\"a\" : \"oblivion\", \"b\" : \"oblivion\"},\n\t\t\t\"oblivion\" : {\"a\" : \"oblivion\", \"b\" : \"oblivion\"},\n\t\t}\n\t).star()\n\tassert starred.alphabet == frozenset([\"a\", \"b\"])\n\tassert starred.accepts(\"\")\n\tassert not starred.accepts(\"a\")\n\tassert not starred.accepts(\"b\")\n\tassert not starred.accepts(\"aa\")\n\tassert starred.accepts(\"ba\")\n\tassert starred.accepts(\"aba\")\n\tassert starred.accepts(\"aaba\")\n\tassert not starred.accepts(\"aabb\")\n\tassert starred.accepts(\"abababa\")\n\ndef test_reduce():\n\t# FSM accepts no strings but has 3 states, needs only 1\n\tasdf = fsm(\n\t\talphabet = {None},\n\t\tstates   = {0, 1, 2},\n\t\tinitial  = 0,\n\t\tfinals   = {1},\n\t\tmap = {\n\t\t\t0 : {None : 2},\n\t\t\t1 : {None : 2},\n\t\t\t2 : {None : 2},\n\t\t},\n\t)\n\tasdf = asdf.reduce()\n\tassert len(asdf.states) == 1\n\ndef test_reverse_abc():\n\tabc = fsm(\n\t\talphabet = {\"a\", \"b\", \"c\"},\n\t\tstates   = {0, 1, 2, 3, None},\n\t\tinitial  = 0,\n\t\tfinals   = {3},\n\t\tmap = {\n\t\t\t0    : {\"a\" : 1   , \"b\" : None, \"c\" : None},\n\t\t\t1    : {\"a\" : None, \"b\" : 2   , \"c\" : None},\n\t\t\t2    : {\"a\" : None, \"b\" : None, \"c\" : 3   },\n\t\t\t3    : {\"a\" : None, \"b\" : None, \"c\" : None},\n\t\t\tNone : {\"a\" : None, \"b\" : None, \"c\" : None},\n\t\t},\n\t)\n\tcba = reversed(abc)\n\tassert cba.accepts(\"cba\")\n\ndef test_reverse_brzozowski():\n\t# This is (a|b)*a(a|b)\n\tbrzozowski = fsm(\n\t\talphabet = {\"a\", \"b\"},\n\t\tstates = {\"A\", \"B\", \"C\", \"D\", \"E\"},\n\t\tinitial = \"A\",\n\t\tfinals = {\"C\", \"E\"},\n\t\tmap = {\n\t\t\t\"A\" : {\"a\" : \"B\", \"b\" : \"D\"},\n\t\t\t\"B\" : {\"a\" : \"C\", \"b\" : \"E\"},\n\t\t\t\"C\" : {\"a\" : \"C\", \"b\" : \"E\"},\n\t\t\t\"D\" : {\"a\" : \"B\", \"b\" : \"D\"},\n\t\t\t\"E\" : {\"a\" : \"B\", \"b\" : \"D\"},\n\t\t},\n\t)\n\tassert brzozowski.accepts(\"aa\")\n\tassert brzozowski.accepts(\"ab\")\n\tassert brzozowski.accepts(\"aab\")\n\tassert brzozowski.accepts(\"bab\")\n\tassert brzozowski.accepts(\"abbbbbbbab\")\n\tassert not brzozowski.accepts(\"\")\n\tassert not brzozowski.accepts(\"a\")\n\tassert not brzozowski.accepts(\"b\")\n\tassert not brzozowski.accepts(\"ba\")\n\tassert not brzozowski.accepts(\"bb\")\n\tassert not brzozowski.accepts(\"bbbbbbbbbbbb\")\n\n\t# So this is (a|b)a(a|b)*\n\tb2 = reversed(brzozowski)\n\tassert b2.accepts(\"aa\")\n\tassert b2.accepts(\"ba\")\n\tassert b2.accepts(\"baa\")\n\tassert b2.accepts(\"bab\")\n\tassert b2.accepts(\"babbbbbbba\")\n\tassert not b2.accepts(\"\")\n\tassert not b2.accepts(\"a\")\n\tassert not b2.accepts(\"b\")\n\tassert not b2.accepts(\"ab\")\n\tassert not b2.accepts(\"bb\")\n\tassert not b2.accepts(\"bbbbbbbbbbbb\")\n\n\t# Test string generator functionality.\n\tgen = b2.strings()\n\tassert next(gen) == [\"a\", \"a\"]\n\tassert next(gen) == [\"b\", \"a\"]\n\tassert next(gen) == [\"a\", \"a\", \"a\"]\n\tassert next(gen) == [\"a\", \"a\", \"b\"]\n\tassert next(gen) == [\"b\", \"a\", \"a\"]\n\tassert next(gen) == [\"b\", \"a\", \"b\"]\n\tassert next(gen) == [\"a\", \"a\", \"a\", \"a\"]\n\ndef test_reverse_epsilon():\n\t# epsilon reversed is epsilon\n\tassert reversed(epsilon(\"a\")).accepts(\"\")\n\ndef test_binary_3():\n\t# Binary numbers divisible by 3.\n\t# Disallows the empty string\n\t# Allows \"0\" on its own, but not leading zeroes.\n\tdiv3 = fsm(\n\t\talphabet = {\"0\", \"1\"},\n\t\tstates = {\"initial\", \"zero\", 0, 1, 2, None},\n\t\tinitial = \"initial\",\n\t\tfinals = {\"zero\", 0},\n\t\tmap = {\n\t\t\t\"initial\" : {\"0\" : \"zero\", \"1\" : 1   },\n\t\t\t\"zero\"    : {\"0\" : None  , \"1\" : None},\n\t\t\t0         : {\"0\" : 0     , \"1\" : 1   },\n\t\t\t1         : {\"0\" : 2     , \"1\" : 0   },\n\t\t\t2         : {\"0\" : 1     , \"1\" : 2   },\n\t\t\tNone      : {\"0\" : None  , \"1\" : None},\n\t\t},\n\t)\n\tassert not div3.accepts(\"\")\n\tassert div3.accepts(\"0\")\n\tassert not div3.accepts(\"1\")\n\tassert not div3.accepts(\"00\")\n\tassert not div3.accepts(\"01\")\n\tassert not div3.accepts(\"10\")\n\tassert div3.accepts(\"11\")\n\tassert not div3.accepts(\"000\")\n\tassert not div3.accepts(\"001\")\n\tassert not div3.accepts(\"010\")\n\tassert not div3.accepts(\"011\")\n\tassert not div3.accepts(\"100\")\n\tassert not div3.accepts(\"101\")\n\tassert div3.accepts(\"110\")\n\tassert not div3.accepts(\"111\")\n\tassert not div3.accepts(\"0000\")\n\tassert not div3.accepts(\"0001\")\n\tassert not div3.accepts(\"0010\")\n\tassert not div3.accepts(\"0011\")\n\tassert not div3.accepts(\"0100\")\n\tassert not div3.accepts(\"0101\")\n\tassert not div3.accepts(\"0110\")\n\tassert not div3.accepts(\"0111\")\n\tassert not div3.accepts(\"1000\")\n\tassert div3.accepts(\"1001\")\n\ndef test_invalid_fsms():\n\t# initial state 1 is not a state\n\ttry:\n\t\tfsm(\n\t\t\talphabet = {},\n\t\t\tstates = {},\n\t\t\tinitial = 1,\n\t\t\tfinals = set(),\n\t\t\tmap = {}\n\t\t)\n\t\tassert False\n\texcept AssertionError:\n\t\tassert False\n\texcept Exception:\n\t\tpass\n\n\t# final state 2 not a state\n\ttry:\n\t\tfsm(\n\t\t\talphabet = {},\n\t\t\tstates = {1},\n\t\t\tinitial = 1,\n\t\t\tfinals = {2},\n\t\t\tmap = {}\n\t\t)\n\t\tassert False\n\texcept AssertionError:\n\t\tassert False\n\texcept Exception:\n\t\tpass\n\n\t# invalid transition for state 1, symbol \"a\"\n\ttry:\n\t\tfsm(\n\t\t\talphabet = {\"a\"},\n\t\t\tstates = {1},\n\t\t\tinitial = 1,\n\t\t\tfinals = set(),\n\t\t\tmap = {\n\t\t\t\t1 : {\"a\" : 2}\n\t\t\t}\n\t\t)\n\t\tassert False\n\texcept AssertionError:\n\t\tassert False\n\texcept Exception:\n\t\tpass\n\ndef test_bad_multiplier(a):\n\ttry:\n\t\tx = a * -1\n\t\tassert False\n\texcept AssertionError:\n\t\tassert False\n\texcept Exception:\n\t\tpass\n\ndef test_anything_else_acceptance():\n\ta = fsm(\n\t\talphabet = {\"a\", \"b\", \"c\", anything_else},\n\t\tstates = {1},\n\t\tinitial = 1,\n\t\tfinals = {1},\n\t\tmap = {\n\t\t\t1 : {\"a\" : 1, \"b\" : 1, \"c\" : 1, anything_else : 1}\n\t\t},\n\t)\n\tassert a.accepts(\"d\")\n\ndef test_difference(a, b):\n\taorb = fsm(\n\t\talphabet = {\"a\", \"b\"},\n\t\tstates = {0, 1, None},\n\t\tinitial = 0,\n\t\tfinals = {1},\n\t\tmap = { \n\t\t\t0    : {\"a\" : 1   , \"b\" : 1   },\n\t\t\t1    : {\"a\" : None, \"b\" : None},\n\t\t\tNone : {\"a\" : None, \"b\" : None},\n\t\t},\n\t)\n\n\tassert list((a ^ a).strings()) == []\n\tassert list((b ^ b).strings()) == []\n\tassert list((a ^ b).strings()) == [[\"a\"], [\"b\"]]\n\tassert list((aorb ^ a).strings()) == [[\"b\"]]\n\ndef test_empty(a, b):\n\tassert not a.empty()\n\tassert not b.empty()\n\n\tassert fsm(\n\t\talphabet = {},\n\t\tstates = {0, 1},\n\t\tinitial = 0,\n\t\tfinals = {1},\n\t\tmap = {0:{}, 1:{}},\n\t).empty()\n\n\tassert not fsm(\n\t\talphabet = {},\n\t\tstates = {0},\n\t\tinitial = 0,\n\t\tfinals = {0},\n\t\tmap = {0:{}},\n\t).empty()\n\n\tassert fsm(\n\t\talphabet = {\"a\", \"b\"},\n\t\tstates = {0, 1, None, 2},\n\t\tinitial = 0,\n\t\tfinals = {2},\n\t\tmap = { \n\t\t\t0    : {\"a\" : 1   , \"b\" : 1   },\n\t\t\t1    : {\"a\" : None, \"b\" : None},\n\t\t\tNone : {\"a\" : None, \"b\" : None},\n\t\t\t2    : {\"a\" : None, \"b\" : None},\n\t\t},\n\t).empty()\n\ndef test_equivalent(a, b):\n\tassert (a | b).equivalent(b | a)\n\ndef test_dead_default():\n\t'''\n\t\tYou may now omit a transition, or even an entire state, from the map. This\n\t\taffects every usage of `fsm.map`.\n\t'''\n\tblockquote = fsm(\n\t\talphabet = {\"/\", \"*\", anything_else},\n\t\tstates = {0, 1, 2, 3, 4, 5},\n\t\tinitial = 0,\n\t\tfinals = {4},\n\t\tmap = {\n\t\t\t\t0 : {\"/\" : 1},\n\t\t\t\t1 : {\"*\" : 2},\n\t\t\t\t2 : {\"/\" : 2, anything_else : 2, \"*\" : 3},\n\t\t\t\t3 : {\"/\" : 4, anything_else : 2, \"*\" : 3},\n\t\t}\n\t)\n\tassert blockquote.accepts([\"/\", \"*\", \"whatever\", \"*\", \"/\"])\n\tassert not blockquote.accepts([\"*\", \"*\", \"whatever\", \"*\", \"/\"])\n\tstr(blockquote) # test stringification\n\tblockquote | blockquote\n\tblockquote & blockquote\n\tblockquote ^ blockquote\n\treversed(blockquote)\n\tassert not blockquote.everythingbut().accepts([\"/\", \"*\", \"whatever\", \"*\", \"/\"])\n\tassert blockquote.everythingbut().accepts([\"*\"]) # deliberately seek oblivion\n\tassert blockquote.islive(3)\n\tassert blockquote.islive(4)\n\tassert not blockquote.islive(5)\n\tgen = blockquote.strings()\n\tassert next(gen) == [\"/\", \"*\", \"*\", \"/\"]\n\ndef test_alphabet_unions():\n\t# Thanks to sparse maps it should now be possible to compute the union of FSMs\n\t# with disagreeing alphabets!\n\ta = fsm(\n\t\talphabet = {\"a\"},\n\t\tstates   = {0, 1},\n\t\tinitial  = 0,\n\t\tfinals   = {1},\n\t\tmap      = {\n\t\t\t0    : {\"a\" : 1},\n\t\t},\n\t)\n\n\tb = fsm(\n\t\talphabet = {\"b\"},\n\t\tstates   = {0, 1},\n\t\tinitial  = 0,\n\t\tfinals   = {1},\n\t\tmap      = {\n\t\t\t0    : {\"b\" : 1},\n\t\t},\n\t)\n\n\tassert (a | b).accepts([\"a\"])\n\tassert (a | b).accepts([\"b\"])\n\tassert (a & b).empty()\n\tassert (a + b).accepts([\"a\", \"b\"])\n\tassert (a ^ b).accepts([\"a\"])\n\tassert (a ^ b).accepts([\"b\"])\n\ndef test_repr():\n\tassert repr(anything_else) == \"anything_else\"\n\tassert str(anything_else) == \"anything_else\"\n\ndef test_new_set_methods(a, b):\n\t# A whole bunch of new methods were added to the FSM module to enable FSMs to\n\t# function exactly as if they were sets of strings (symbol lists), see:\n\t# https://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset\n\t# But do they work?\n\tassert len(a) == 1\n\tassert len((a | b) * 4) == 16\n\ttry:\n\t\tlen(a.star())\n\t\tassert False\n\texcept OverflowError:\n\t\tpass\n\n\t# \"in\"\n\tassert \"a\" in a\n\tassert not \"a\" in b\n\tassert \"a\" not in b\n\n\t# List comprehension!\n\tfour = (a | b) * 2\n\tfor string in four:\n\t\tassert string == [\"a\", \"a\"]\n\t\tbreak\n\tassert [s for s in four] == [[\"a\", \"a\"], [\"a\", \"b\"], [\"b\", \"a\"], [\"b\", \"b\"]]\n\n\t# set.union() imitation\n\tassert fsm.union(a, b) == a.union(b)\n\tassert len(fsm.union()) == 0\n\tassert fsm.intersection(a, b) == a.intersection(b)\n\n\t# This takes a little explaining. In general, `a & b & c` is equivalent to\n\t# `EVERYTHING & a & b & c` where `EVERYTHING` is an FSM accepting every\n\t# possible string. Similarly `a` is equivalent to `EVERYTHING & a`, and the\n\t# intersection of no sets at all is... `EVERYTHING`.\n\t# However, since we compute the union of alphabets, and there are no\n\t# alphabets, the union is the empty set. So the only string which `EVERYTHING`\n\t# actually recognises is the empty string, [] (or \"\" if you prefer).\n\tint_none = fsm.intersection()\n\tassert len(int_none) == 1\n\tassert [] in int_none\n\n\tassert (a | b).difference(a) == fsm.difference((a | b), a) == (a | b) - a == b\n\tassert (a | b).difference(a, b) == fsm.difference((a | b), a, b) == (a | b) - a - b == null(\"ab\")\n\tassert a.symmetric_difference(b) == fsm.symmetric_difference(a, b) == a ^ b\n\tassert a.isdisjoint(b)\n\tassert a <= (a | b)\n\tassert a < (a | b)\n\tassert a != (a | b)\n\tassert (a | b) > a\n\tassert (a | b) >= a\n\n\tassert list(a.concatenate(a, a).strings()) == [[\"a\", \"a\", \"a\"]]\n\tassert list(a.concatenate().strings()) == [[\"a\"]]\n\tassert list(fsm.concatenate(b, a, b).strings()) == [[\"b\", \"a\", \"b\"]]\n\tassert list(fsm.concatenate().strings()) == []\n\tassert not a.copy() is a\n\ndef test_oblivion_crawl(a):\n\t# When crawling a new FSM, we should avoid generating an oblivion state.\n\t# `abc` has no oblivion state... all the results should not as well!\n\tabc = fsm(\n\t\talphabet = {\"a\", \"b\", \"c\"},\n\t\tstates = {0, 1, 2, 3},\n\t\tinitial = 0,\n\t\tfinals = {3},\n\t\tmap = {\n\t\t\t0 : {\"a\" : 1},\n\t\t\t1 : {\"b\" : 2},\n\t\t\t2 : {\"c\" : 3},\n\t\t}\n\t)\n\tassert len((abc + abc).states) == 7\n\tassert len(abc.star().states) == 3\n\tassert len((abc * 3).states) == 10\n\tassert len(reversed(abc).states) == 4\n\tassert len((abc | abc).states) == 4\n\tassert len((abc & abc).states) == 4\n\tassert len((abc ^ abc).states) == 1\n\tassert len((abc - abc).states) == 1\n\ndef test_concatenate_bug(a):\n\t# This exposes a defect in fsm.concatenate.\n\tassert fsm.concatenate(a, epsilon({\"a\"}), a).accepts(\"aa\")\n\tassert fsm.concatenate(a, epsilon({\"a\"}), epsilon({\"a\"}), a).accepts(\"aa\")\n\ndef test_derive(a, b):\n\t# Just some basic tests because this is mainly a regex thing.\n\tassert a.derive(\"a\") == epsilon({\"a\", \"b\"})\n\tassert a.derive(\"b\") == null({\"a\", \"b\"})\n\ttry:\n\t\ta.derive(\"c\")\n\t\tassert False\n\texcept KeyError:\n\t\tassert True\n\tassert (a * 3).derive(\"a\") == a * 2\n\tassert (a.star() - epsilon({\"a\", \"b\"})).derive(\"a\") == a.star()\n\ndef test_bug_36():\n\tetc1 = fsm(\n\t\talphabet = {anything_else},\n\t\tstates = {0},\n\t\tinitial = 0,\n\t\tfinals = {0},\n\t\tmap = {\n\t\t\t0: {\n\t\t\t\tanything_else: 0\n\t\t\t}\n\t\t}\n\t)\n\tetc2 = fsm(\n\t\talphabet = {'s', anything_else},\n\t\tstates = {0, 1},\n\t\tinitial = 0,\n\t\tfinals = {1},\n\t\tmap = {\n\t\t\t0: {\n\t\t\t\t's': 1\n\t\t\t},\n\t\t\t1: {\n\t\t\t\t's': 1,\n\t\t\t\tanything_else: 1\n\t\t\t}\n\t\t}\n\t)\n\tboth = etc1 & etc2\n\tassert etc1.accepts([\"s\"])\n\tassert etc2.accepts([\"s\"])\n\tassert both.alphabet == {anything_else, \"s\"}\n\tassert both.accepts([\"s\"])\n\ndef test_add_anything_else():\n\tfsm1=fsm( # [^a]\n\t\talphabet={\"a\",anything_else},\n\t\tstates={0,1},\n\t\tinitial=0,\n\t\tfinals={1},\n\t\tmap={0:{anything_else:1}}\n\t)\n\tfsm2=fsm( # [^b]\n\t\talphabet={\"b\",anything_else},\n\t\tstates={0,1},\n\t\tinitial=0,\n\t\tfinals={1},\n\t\tmap={0:{anything_else:1}}\n\t)\n\tassert (fsm1+fsm2).accepts(\"ba\")\n"
  },
  {
    "prompt": "class MoleculeGraphData\n\nclass BatchMoleculeGraphData\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import unittest\nimport pytest\nimport numpy as np\nfrom deepchem.utils.molecule_graph import MoleculeGraphData, BatchMoleculeGraphData\n\n\nclass TestMoleculeGraph(unittest.TestCase):\n\n  def test_molecule_graph_data(self):\n    num_nodes, num_node_features = 4, 32\n    num_edges, num_edge_features = 6, 32\n    node_features = np.random.random_sample((num_nodes, num_node_features))\n    edge_features = np.random.random_sample((num_edges, num_edge_features))\n    targets = np.random.random_sample(5)\n    edge_index = np.array([\n        [0, 1, 2, 2, 3, 4],\n        [1, 2, 0, 3, 4, 0],\n    ])\n    graph_features = None\n\n    mol_graph = MoleculeGraphData(\n        node_features=node_features,\n        edge_index=edge_index,\n        targets=targets,\n        edge_features=edge_features,\n        graph_features=graph_features)\n\n    assert mol_graph.num_nodes == num_nodes\n    assert mol_graph.num_node_features == num_node_features\n    assert mol_graph.num_edges == num_edges\n    assert mol_graph.num_edge_features == num_edge_features\n    assert mol_graph.targets.shape == (5,)\n\n  def test_invalid_molecule_graph_data(self):\n    with pytest.raises(ValueError):\n      invalid_node_features_type = list(np.random.random_sample((5, 5)))\n      edge_index = np.array([\n          [0, 1, 2, 2, 3, 4],\n          [1, 2, 0, 3, 4, 0],\n      ])\n      targets = np.random.random_sample(5)\n      mol_graph = MoleculeGraphData(\n          node_features=invalid_node_features_type,\n          edge_index=edge_index,\n          targets=targets,\n      )\n\n    with pytest.raises(ValueError):\n      node_features = np.random.random_sample((5, 5))\n      invalid_edge_index_shape = np.array([\n          [0, 1, 2, 2, 3, 4],\n          [1, 2, 0, 3, 4, 0],\n          [2, 2, 1, 4, 0, 3],\n      ])\n      targets = np.random.random_sample(5)\n      mol_graph = MoleculeGraphData(\n          node_features=node_features,\n          edge_index=invalid_edge_index_shape,\n          targets=targets,\n      )\n\n    with pytest.raises(TypeError):\n      node_features = np.random.random_sample((5, 5))\n      mol_graph = MoleculeGraphData(node_features=node_features)\n\n  def test_batch_molecule_graph_data(self):\n    num_nodes_list, num_edge_list = [3, 4, 5], [2, 4, 5]\n    num_node_features, num_edge_features = 32, 32\n    edge_index_list = [\n        np.array([[0, 1], [1, 2]]),\n        np.array([[0, 1, 2, 3], [1, 2, 0, 2]]),\n        np.array([[0, 1, 2, 3, 4], [1, 2, 3, 4, 5]])\n    ]\n    targets = np.random.random_sample(5)\n\n    molecule_graphs = [\n        MoleculeGraphData(\n            node_features=np.random.random_sample((num_nodes_list[i],\n                                                   num_node_features)),\n            edge_index=edge_index_list[i],\n            targets=targets,\n            edge_features=np.random.random_sample((num_edge_list[i],\n                                                   num_edge_features)),\n            graph_features=None) for i in range(len(num_edge_list))\n    ]\n    batch = BatchMoleculeGraphData(molecule_graphs)\n\n    assert batch.num_nodes == sum(num_nodes_list)\n    assert batch.num_node_features == num_node_features\n    assert batch.num_edges == sum(num_edge_list)\n    assert batch.num_edge_features == num_edge_features\n    assert batch.targets.shape == (3, 5)\n    assert batch.graph_index.shape == (sum(num_nodes_list),)\n"
  },
  {
    "prompt": "\"\"\"Execute search and return list of items ids\n\nArgs:\n    search (elasticsearch_dsl.Search): search instance\nReturns:\n    [int]: list of items ids\"\"\"\nfn get_search_ids(search: elasticsearch_dsl.Search) -> [int]\n\nclass DataDocType\n\n\"\"\"Initialize test index\"\"\"\nfn create_test_index()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nimport pytest\nfrom rest_framework.test import APIRequestFactory\nfrom elasticsearch_dsl import Q\n\nfrom rest_framework_elasticsearch.es_filters import (\n    ESFieldFilter, ElasticOrderingFilter, ElasticFieldsFilter,\n    ElasticFieldsRangeFilter, ElasticSearchFilter)\nfrom rest_framework_elasticsearch.es_views import ElasticAPIView\nfrom .test_data import DataDocType, DATA\nfrom .utils import get_search_ids\n\n\nrf = APIRequestFactory()\n\n\n@pytest.mark.parametrize('dataset, expected', [\n    (\n        ('label', 'name', 'description'),\n        ('label', 'name', 'description')\n    ),\n    (\n        ('label', None, 'description'),\n        ('label', 'label', 'description')\n    ),\n    (\n        ('label', None, None),\n        ('label', 'label', None)\n    )\n])\ndef test_es_field_filters(dataset, expected):\n    ffilter = ESFieldFilter(dataset[0],\n                            name=dataset[1],\n                            description=dataset[2])\n    assert expected == (ffilter.label, ffilter.name, ffilter.description)\n\n\nclass TestElasticOrderingFilter:\n\n    def setup_method(self):\n        self.backend = ElasticOrderingFilter()\n\n    def create_view(self, es_ordering_fields):\n        \"\"\"Create and return test view class instance\n\n        Args:\n            es_ordering_fields (tuple): ordering fields\n        Returns:\n            ElasticAPIView: test view instance\n        \"\"\"\n        view = ElasticAPIView()\n        view.es_ordering_fields = es_ordering_fields\n        return view\n\n    @pytest.mark.parametrize('es_ordering_fields, expected', [\n        (\n            (\"first_name\", (\"is_active\", \"active\")),\n            (\"first_name\", (\"is_active\", \"active\")),\n        ),\n        (\n            \"first_name\",\n            (\"first_name\",)\n        )\n    ])\n    def test_get_es_ordering_fields(self, es_ordering_fields, expected):\n        view = self.create_view(es_ordering_fields)\n        result = self.backend.get_es_ordering_fields(view)\n        assert expected == result\n\n    @pytest.mark.parametrize('es_ordering_fields, expected', [\n        (\n            (\"is_active\", \"active\"),\n            (\"is_active\", \"active\")\n        ),\n        (\n            \"first_name\",\n            (\"first_name\", \"first_name\")\n        )\n    ])\n    def test_validation(self, es_ordering_fields, expected):\n        result = ElasticOrderingFilter.validation(es_ordering_fields)\n        assert expected == result\n\n    def test_get_valid_fields_with_es_ordering_fields(self):\n        es_ordering_fields = (\n            \"first_name\",\n            \"last_name\",\n            (\"is_active\", \"active\")\n        )\n        view = self.create_view(es_ordering_fields)\n        expected = [\n            (\"first_name\", \"first_name\"),\n            (\"last_name\", \"last_name\"),\n            (\"is_active\", \"active\")\n        ]\n        result = self.backend.get_valid_fields([], view)\n        assert result == expected\n\n    def test_get_valid_fields_without_es_ordering_fields(self):\n        view = self.create_view(None)\n        valid_fields = []\n        self.backend.get_default_valid_fields = lambda q, v: valid_fields\n        result = self.backend.get_valid_fields([], view)\n        assert result == valid_fields\n\n    @pytest.mark.parametrize('fields, expected', [\n        (\n            ['first_name', 'last_name', '-active'],\n            ['first_name', '-is_active']\n        ),\n        (\n            ['+first_name', 'last_name', '#active'],\n            ['+first_name']\n        ),\n        (\n            ['+first_name', '-active'],\n            ['+first_name', '-is_active']\n        )\n    ])\n    def test_remove_invalid_fields(self, fields, expected):\n        es_ordering_fields = (\n            \"first_name\",\n            (\"is_active\", \"active\")\n        )\n        view = self.create_view(es_ordering_fields)\n        request = rf.get('/test/')\n        result = self.backend.remove_invalid_fields([], fields, view, request)\n        assert result == expected\n\n    def test_filter_search(self, search):\n\n        def get_expected():\n            \"\"\"Return expected data items sorted by id\"\"\"\n            items = [\n                (\n                    item['_id'],\n                    item['_source']['first_name'],\n                    item['_source']['is_active']\n                )\n                for item in DATA\n            ]\n            items = sorted(items, key=lambda tup: (tup[1], not tup[2]))\n            return items\n\n        es_ordering_fields = (\n            (\"first_name\", \"first_name\"),\n            (\"is_active\", \"-active\")\n        )\n        view = self.create_view(es_ordering_fields)\n\n        request = rf.get('/test/')\n        request.query_params = {'ordering': 'first_name,active'}\n\n        search = self.backend.filter_search(request, search, view)\n        result = [\n            (item.meta.id, item.first_name, item.is_active)\n            for item in search[:len(DATA)].execute()\n        ]\n        assert result == get_expected()\n\n\nclass TestElasticFieldsFilter:\n\n    def setup_method(self):\n        self.backend = ElasticFieldsFilter()\n\n    def create_view(self, es_filter_fields):\n        \"\"\"Create and return test view class instance\n\n        Args:\n            es_filter_fields ([ESFieldFilter]): filtering fields\n        Returns:\n            ElasticAPIView: test view instance\n        \"\"\"\n        view = ElasticAPIView()\n        view.es_model = DataDocType\n        view.es_filter_fields = es_filter_fields\n        return view\n\n    def test_get_es_filter_fields(self):\n        es_filter_fields = (\n            ESFieldFilter('skills'),\n            ESFieldFilter('active', 'is_active')\n        )\n        view = self.create_view(es_filter_fields)\n        result = self.backend.get_es_filter_fields(view)\n        assert result == es_filter_fields\n\n    @pytest.mark.parametrize('es_filter_fields, query_params, expected', [\n        (\n            [ESFieldFilter('active', 'is_active')],\n            {'active': 'False'},\n            [3, 6, 8, 10]\n        ),\n        (\n            [\n                ESFieldFilter('birthday')\n            ],\n            {'birthday': '1985-03-17T12:20:09'},\n            [1]\n        ),\n        (\n            [ESFieldFilter('skills')],\n            {'skills': 'python'},\n            [1, 4, 5, 10]\n        ),\n        (\n            [ESFieldFilter('skills')],\n            {'skills': 'python,ruby'},\n            [1, 4, 5, 6, 10]\n        ),\n        (\n            [ESFieldFilter('active', 'is_active')],\n            {'active': 'False', 'skills': 'python'},\n            [3, 6, 8, 10]\n        ),\n        (\n            [ESFieldFilter('active', 'is_active')],\n            {'active': 'False', 'skills': 'python'},\n            [3, 6, 8, 10]\n        ),\n        (\n            [ESFieldFilter('score')],\n            {'score': '200'},\n            [2, 13]\n        ),\n    ])\n    def test_filter_search(self, search, es_filter_fields,\n                           query_params, expected):\n        view = self.create_view(es_filter_fields)\n        request = rf.get('/test/')\n        request.query_params = query_params\n        search = self.backend.filter_search(request, search, view)\n        result = get_search_ids(search)\n        assert sorted(result) == sorted(expected)\n\n\nclass TestElasticFieldsRangeFilter:\n\n    def setup_method(self):\n        self.backend = ElasticFieldsRangeFilter()\n\n    def create_view(self, es_range_filter_fields):\n        \"\"\"Create and return test view class instance\n\n        Args:\n            es_range_filter_fields ([ESFieldFilter]): filtering range fields\n        Returns:\n            ElasticAPIView: test view instance\n        \"\"\"\n        view = ElasticAPIView()\n        view.es_model = DataDocType\n        view.es_range_filter_fields = es_range_filter_fields\n        return view\n\n    def test_get_es_filter_fields(self):\n        es_range_filter_fields = (\n            ESFieldFilter('skills'),\n            ESFieldFilter('active', 'is_active')\n        )\n        view = self.create_view(es_range_filter_fields)\n        result = self.backend.get_es_range_filter_fields(view)\n        assert result == es_range_filter_fields\n\n    @pytest.mark.parametrize('es_filter_fields, query_params, expected', [\n        (\n            [ESFieldFilter('score')],\n            {'from_score': '500'},\n            [6, 7, 8, 10, 11]\n        ),\n        (\n            [ESFieldFilter('score')],\n            {'to_score': '100'},\n            [1, 3, 5, 9, 12, 14]\n        ),\n        (\n            [ESFieldFilter('score')],\n            {'from_score': '500', 'to_score': '600'},\n            [7, 8, 10]\n        ),\n        (\n            [ESFieldFilter('score')], {}, [int(item['_id']) for item in DATA]\n        ),\n    ])\n    def test_filter_search(self, search, es_filter_fields,\n                           query_params, expected):\n        view = self.create_view(es_filter_fields)\n        request = rf.get('/test/')\n        request.query_params = query_params\n        search = self.backend.filter_search(request, search, view)\n        result = get_search_ids(search)\n        assert sorted(result) == sorted(expected)\n\n\nclass TestElasticSearchFilter:\n\n    def setup_method(self):\n        self.backend = ElasticSearchFilter()\n\n    def create_view(self, es_search_fields):\n        \"\"\"Create and return test view class instance\n\n        Args:\n            es_search_fields ([ESFieldFilter]): search fields\n        Returns:\n            ElasticAPIView: test view instance\n        \"\"\"\n        view = ElasticAPIView()\n        view.es_model = DataDocType\n        view.es_search_fields = es_search_fields\n        return view\n\n    @pytest.mark.parametrize('search_param, query_params, expected', [\n        (\n            None, {'search': 'test'}, 'test'\n        ),\n        (\n            'search', {'search': 'test'}, 'test'\n        ),\n        (\n            'q', {'q': 'test'}, 'test'\n        ),\n        (\n            'search', {'q': 'test'}, ''\n        ),\n    ])\n    def test_get_search_query(self, search_param, query_params, expected):\n        request = rf.get('/test/')\n        request.query_params = query_params\n        if search_param:\n            self.backend.search_param = search_param\n        result = self.backend.get_search_query(request)\n        assert result == expected\n\n    def test_get_es_query(self):\n\n        class TestElasticSearchFilter(ElasticSearchFilter):\n            def get_es_query(self, s_query, s_fields, **kwargs):\n                return Q(\"match\", query=s_query, field=s_fields)\n\n        s_query = \"test\"\n        s_fields = \"first_name\"\n        backend = TestElasticSearchFilter()\n        result = backend.get_es_query(s_query, s_fields)\n        expected = Q(\"match\", query=s_query, field=s_fields)\n        assert result == expected\n\n    @pytest.mark.parametrize('es_search_fields, query_params, expected', [\n        (\n            ('first_name',),\n            {'search': 'Zofia'},\n            [1]\n        ),\n        (\n            ('first_name', 'last_name', 'city'),\n            {'search': 'Zofia'},\n            [1]\n        ),\n        (\n            ('first_name', 'last_name', 'city'),\n            {'search': 'Zofia Rome'},\n            [4, 7]\n        ),\n        (\n            ('description'),\n            {'search': 'information'},\n            [2]\n        ),\n        (\n            ('description'),\n            {'search': 'Ford Prefect'},\n            [2, 8, 10, 5, 6, 12]\n        ),\n        (\n            ('description'),\n            {'search': 'Earth'},\n            [5, 3, 14]\n        ),\n        (\n            ('description'),\n            {'search': 'The Hitchhiker\u2019s Guide'},\n            [8]\n        ),\n    ])\n    def test_filter_search(self, search, es_search_fields,\n                           query_params, expected):\n        view = self.create_view(es_search_fields)\n        request = rf.get('/test/')\n        request.query_params = query_params\n        search = self.backend.filter_search(request, search, view)\n        result = get_search_ids(search)\n        assert result == expected\n"
  },
  {
    "prompt": "\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"Webhook tests for mobile_app.\"\"\"\nimport logging\n\nimport pytest\n\nfrom homeassistant.components.camera import SUPPORT_STREAM as CAMERA_SUPPORT_STREAM\nfrom homeassistant.components.mobile_app.const import CONF_SECRET\nfrom homeassistant.components.zone import DOMAIN as ZONE_DOMAIN\nfrom homeassistant.const import CONF_WEBHOOK_ID\nfrom homeassistant.core import callback\nfrom homeassistant.exceptions import HomeAssistantError\nfrom homeassistant.setup import async_setup_component\n\nfrom .const import CALL_SERVICE, FIRE_EVENT, REGISTER_CLEARTEXT, RENDER_TEMPLATE, UPDATE\n\nfrom tests.async_mock import patch\nfrom tests.common import async_mock_service\n\n_LOGGER = logging.getLogger(__name__)\n\n\ndef encrypt_payload(secret_key, payload):\n    \"\"\"Return a encrypted payload given a key and dictionary of data.\"\"\"\n    try:\n        from nacl.secret import SecretBox\n        from nacl.encoding import Base64Encoder\n    except (ImportError, OSError):\n        pytest.skip(\"libnacl/libsodium is not installed\")\n        return\n\n    import json\n\n    keylen = SecretBox.KEY_SIZE\n    prepped_key = secret_key.encode(\"utf-8\")\n    prepped_key = prepped_key[:keylen]\n    prepped_key = prepped_key.ljust(keylen, b\"\\0\")\n\n    payload = json.dumps(payload).encode(\"utf-8\")\n\n    return (\n        SecretBox(prepped_key).encrypt(payload, encoder=Base64Encoder).decode(\"utf-8\")\n    )\n\n\ndef decrypt_payload(secret_key, encrypted_data):\n    \"\"\"Return a decrypted payload given a key and a string of encrypted data.\"\"\"\n    try:\n        from nacl.secret import SecretBox\n        from nacl.encoding import Base64Encoder\n    except (ImportError, OSError):\n        pytest.skip(\"libnacl/libsodium is not installed\")\n        return\n\n    import json\n\n    keylen = SecretBox.KEY_SIZE\n    prepped_key = secret_key.encode(\"utf-8\")\n    prepped_key = prepped_key[:keylen]\n    prepped_key = prepped_key.ljust(keylen, b\"\\0\")\n\n    decrypted_data = SecretBox(prepped_key).decrypt(\n        encrypted_data, encoder=Base64Encoder\n    )\n    decrypted_data = decrypted_data.decode(\"utf-8\")\n\n    return json.loads(decrypted_data)\n\n\nasync def test_webhook_handle_render_template(create_registrations, webhook_client):\n    \"\"\"Test that we render templates properly.\"\"\"\n    resp = await webhook_client.post(\n        \"/api/webhook/{}\".format(create_registrations[1][\"webhook_id\"]),\n        json=RENDER_TEMPLATE,\n    )\n\n    assert resp.status == 200\n\n    json = await resp.json()\n    assert json == {\"one\": \"Hello world\"}\n\n\nasync def test_webhook_handle_call_services(hass, create_registrations, webhook_client):\n    \"\"\"Test that we call services properly.\"\"\"\n    calls = async_mock_service(hass, \"test\", \"mobile_app\")\n\n    resp = await webhook_client.post(\n        \"/api/webhook/{}\".format(create_registrations[1][\"webhook_id\"]),\n        json=CALL_SERVICE,\n    )\n\n    assert resp.status == 200\n\n    assert len(calls) == 1\n\n\nasync def test_webhook_handle_fire_event(hass, create_registrations, webhook_client):\n    \"\"\"Test that we can fire events.\"\"\"\n    events = []\n\n    @callback\n    def store_event(event):\n        \"\"\"Helepr to store events.\"\"\"\n        events.append(event)\n\n    hass.bus.async_listen(\"test_event\", store_event)\n\n    resp = await webhook_client.post(\n        \"/api/webhook/{}\".format(create_registrations[1][\"webhook_id\"]), json=FIRE_EVENT\n    )\n\n    assert resp.status == 200\n    json = await resp.json()\n    assert json == {}\n\n    assert len(events) == 1\n    assert events[0].data[\"hello\"] == \"yo world\"\n\n\nasync def test_webhook_update_registration(webhook_client, authed_api_client):\n    \"\"\"Test that a we can update an existing registration via webhook.\"\"\"\n    register_resp = await authed_api_client.post(\n        \"/api/mobile_app/registrations\", json=REGISTER_CLEARTEXT\n    )\n\n    assert register_resp.status == 201\n    register_json = await register_resp.json()\n\n    webhook_id = register_json[CONF_WEBHOOK_ID]\n\n    update_container = {\"type\": \"update_registration\", \"data\": UPDATE}\n\n    update_resp = await webhook_client.post(\n        f\"/api/webhook/{webhook_id}\", json=update_container\n    )\n\n    assert update_resp.status == 200\n    update_json = await update_resp.json()\n    assert update_json[\"app_version\"] == \"2.0.0\"\n    assert CONF_WEBHOOK_ID not in update_json\n    assert CONF_SECRET not in update_json\n\n\nasync def test_webhook_handle_get_zones(hass, create_registrations, webhook_client):\n    \"\"\"Test that we can get zones properly.\"\"\"\n    await async_setup_component(\n        hass, ZONE_DOMAIN, {ZONE_DOMAIN: {}},\n    )\n\n    resp = await webhook_client.post(\n        \"/api/webhook/{}\".format(create_registrations[1][\"webhook_id\"]),\n        json={\"type\": \"get_zones\"},\n    )\n\n    assert resp.status == 200\n\n    json = await resp.json()\n    assert len(json) == 1\n    zones = sorted(json, key=lambda entry: entry[\"entity_id\"])\n    assert zones[0][\"entity_id\"] == \"zone.home\"\n\n\nasync def test_webhook_handle_get_config(hass, create_registrations, webhook_client):\n    \"\"\"Test that we can get config properly.\"\"\"\n    resp = await webhook_client.post(\n        \"/api/webhook/{}\".format(create_registrations[1][\"webhook_id\"]),\n        json={\"type\": \"get_config\"},\n    )\n\n    assert resp.status == 200\n\n    json = await resp.json()\n    if \"components\" in json:\n        json[\"components\"] = set(json[\"components\"])\n    if \"whitelist_external_dirs\" in json:\n        json[\"whitelist_external_dirs\"] = set(json[\"whitelist_external_dirs\"])\n\n    hass_config = hass.config.as_dict()\n\n    expected_dict = {\n        \"latitude\": hass_config[\"latitude\"],\n        \"longitude\": hass_config[\"longitude\"],\n        \"elevation\": hass_config[\"elevation\"],\n        \"unit_system\": hass_config[\"unit_system\"],\n        \"location_name\": hass_config[\"location_name\"],\n        \"time_zone\": hass_config[\"time_zone\"],\n        \"components\": hass_config[\"components\"],\n        \"version\": hass_config[\"version\"],\n        \"theme_color\": \"#03A9F4\",  # Default frontend theme color\n    }\n\n    assert expected_dict == json\n\n\nasync def test_webhook_returns_error_incorrect_json(\n    webhook_client, create_registrations, caplog\n):\n    \"\"\"Test that an error is returned when JSON is invalid.\"\"\"\n    resp = await webhook_client.post(\n        \"/api/webhook/{}\".format(create_registrations[1][\"webhook_id\"]), data=\"not json\"\n    )\n\n    assert resp.status == 400\n    json = await resp.json()\n    assert json == {}\n    assert \"invalid JSON\" in caplog.text\n\n\nasync def test_webhook_handle_decryption(webhook_client, create_registrations):\n    \"\"\"Test that we can encrypt/decrypt properly.\"\"\"\n    key = create_registrations[0][\"secret\"]\n    data = encrypt_payload(key, RENDER_TEMPLATE[\"data\"])\n\n    container = {\"type\": \"render_template\", \"encrypted\": True, \"encrypted_data\": data}\n\n    resp = await webhook_client.post(\n        \"/api/webhook/{}\".format(create_registrations[0][\"webhook_id\"]), json=container\n    )\n\n    assert resp.status == 200\n\n    webhook_json = await resp.json()\n    assert \"encrypted_data\" in webhook_json\n\n    decrypted_data = decrypt_payload(key, webhook_json[\"encrypted_data\"])\n\n    assert decrypted_data == {\"one\": \"Hello world\"}\n\n\nasync def test_webhook_requires_encryption(webhook_client, create_registrations):\n    \"\"\"Test that encrypted registrations only accept encrypted data.\"\"\"\n    resp = await webhook_client.post(\n        \"/api/webhook/{}\".format(create_registrations[0][\"webhook_id\"]),\n        json=RENDER_TEMPLATE,\n    )\n\n    assert resp.status == 400\n\n    webhook_json = await resp.json()\n    assert \"error\" in webhook_json\n    assert webhook_json[\"success\"] is False\n    assert webhook_json[\"error\"][\"code\"] == \"encryption_required\"\n\n\nasync def test_webhook_update_location(hass, webhook_client, create_registrations):\n    \"\"\"Test that location can be updated.\"\"\"\n    resp = await webhook_client.post(\n        \"/api/webhook/{}\".format(create_registrations[1][\"webhook_id\"]),\n        json={\n            \"type\": \"update_location\",\n            \"data\": {\"gps\": [1, 2], \"gps_accuracy\": 10, \"altitude\": -10},\n        },\n    )\n\n    assert resp.status == 200\n\n    state = hass.states.get(\"device_tracker.test_1_2\")\n    assert state is not None\n    assert state.attributes[\"latitude\"] == 1.0\n    assert state.attributes[\"longitude\"] == 2.0\n    assert state.attributes[\"gps_accuracy\"] == 10\n    assert state.attributes[\"altitude\"] == -10\n\n\nasync def test_webhook_enable_encryption(hass, webhook_client, create_registrations):\n    \"\"\"Test that encryption can be added to a reg initially created without.\"\"\"\n    webhook_id = create_registrations[1][\"webhook_id\"]\n\n    enable_enc_resp = await webhook_client.post(\n        f\"/api/webhook/{webhook_id}\", json={\"type\": \"enable_encryption\"},\n    )\n\n    assert enable_enc_resp.status == 200\n\n    enable_enc_json = await enable_enc_resp.json()\n    assert len(enable_enc_json) == 1\n    assert CONF_SECRET in enable_enc_json\n\n    key = enable_enc_json[\"secret\"]\n\n    enc_required_resp = await webhook_client.post(\n        f\"/api/webhook/{webhook_id}\", json=RENDER_TEMPLATE,\n    )\n\n    assert enc_required_resp.status == 400\n\n    enc_required_json = await enc_required_resp.json()\n    assert \"error\" in enc_required_json\n    assert enc_required_json[\"success\"] is False\n    assert enc_required_json[\"error\"][\"code\"] == \"encryption_required\"\n\n    enc_data = encrypt_payload(key, RENDER_TEMPLATE[\"data\"])\n\n    container = {\n        \"type\": \"render_template\",\n        \"encrypted\": True,\n        \"encrypted_data\": enc_data,\n    }\n\n    enc_resp = await webhook_client.post(f\"/api/webhook/{webhook_id}\", json=container)\n\n    assert enc_resp.status == 200\n\n    enc_json = await enc_resp.json()\n    assert \"encrypted_data\" in enc_json\n\n    decrypted_data = decrypt_payload(key, enc_json[\"encrypted_data\"])\n\n    assert decrypted_data == {\"one\": \"Hello world\"}\n\n\nasync def test_webhook_camera_stream_non_existent(\n    hass, create_registrations, webhook_client\n):\n    \"\"\"Test fetching camera stream URLs for a non-existent camera.\"\"\"\n    webhook_id = create_registrations[1][\"webhook_id\"]\n\n    resp = await webhook_client.post(\n        f\"/api/webhook/{webhook_id}\",\n        json={\n            \"type\": \"stream_camera\",\n            \"data\": {\"camera_entity_id\": \"camera.doesnt_exist\"},\n        },\n    )\n\n    assert resp.status == 400\n    webhook_json = await resp.json()\n    assert webhook_json[\"success\"] is False\n\n\nasync def test_webhook_camera_stream_non_hls(\n    hass, create_registrations, webhook_client\n):\n    \"\"\"Test fetching camera stream URLs for a non-HLS/stream-supporting camera.\"\"\"\n    hass.states.async_set(\"camera.non_stream_camera\", \"idle\", {\"supported_features\": 0})\n\n    webhook_id = create_registrations[1][\"webhook_id\"]\n\n    resp = await webhook_client.post(\n        f\"/api/webhook/{webhook_id}\",\n        json={\n            \"type\": \"stream_camera\",\n            \"data\": {\"camera_entity_id\": \"camera.non_stream_camera\"},\n        },\n    )\n\n    assert resp.status == 200\n    webhook_json = await resp.json()\n    assert webhook_json[\"hls_path\"] is None\n    assert (\n        webhook_json[\"mjpeg_path\"]\n        == \"/api/camera_proxy_stream/camera.non_stream_camera\"\n    )\n\n\nasync def test_webhook_camera_stream_stream_available(\n    hass, create_registrations, webhook_client\n):\n    \"\"\"Test fetching camera stream URLs for an HLS/stream-supporting camera.\"\"\"\n    hass.states.async_set(\n        \"camera.stream_camera\", \"idle\", {\"supported_features\": CAMERA_SUPPORT_STREAM}\n    )\n\n    webhook_id = create_registrations[1][\"webhook_id\"]\n\n    with patch(\n        \"homeassistant.components.camera.async_request_stream\",\n        return_value=\"/api/streams/some_hls_stream\",\n    ):\n        resp = await webhook_client.post(\n            f\"/api/webhook/{webhook_id}\",\n            json={\n                \"type\": \"stream_camera\",\n                \"data\": {\"camera_entity_id\": \"camera.stream_camera\"},\n            },\n        )\n\n    assert resp.status == 200\n    webhook_json = await resp.json()\n    assert webhook_json[\"hls_path\"] == \"/api/streams/some_hls_stream\"\n    assert webhook_json[\"mjpeg_path\"] == \"/api/camera_proxy_stream/camera.stream_camera\"\n\n\nasync def test_webhook_camera_stream_stream_available_but_errors(\n    hass, create_registrations, webhook_client\n):\n    \"\"\"Test fetching camera stream URLs for an HLS/stream-supporting camera but that streaming errors.\"\"\"\n    hass.states.async_set(\n        \"camera.stream_camera\", \"idle\", {\"supported_features\": CAMERA_SUPPORT_STREAM}\n    )\n\n    webhook_id = create_registrations[1][\"webhook_id\"]\n\n    with patch(\n        \"homeassistant.components.camera.async_request_stream\",\n        side_effect=HomeAssistantError(),\n    ):\n        resp = await webhook_client.post(\n            f\"/api/webhook/{webhook_id}\",\n            json={\n                \"type\": \"stream_camera\",\n                \"data\": {\"camera_entity_id\": \"camera.stream_camera\"},\n            },\n        )\n\n    assert resp.status == 200\n    webhook_json = await resp.json()\n    assert webhook_json[\"hls_path\"] is None\n    assert webhook_json[\"mjpeg_path\"] == \"/api/camera_proxy_stream/camera.stream_camera\"\n"
  },
  {
    "prompt": "class MockMemcacheClient\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import six\nimport pytest\n\nfrom pymemcache.test.utils import MockMemcacheClient\n\n\n@pytest.mark.unit()\ndef test_get_set():\n    client = MockMemcacheClient()\n    assert client.get(b\"hello\") is None\n\n    client.set(b\"hello\", 12)\n    assert client.get(b\"hello\") == 12\n\n\n@pytest.mark.unit()\ndef test_get_set_unicide_key():\n    client = MockMemcacheClient()\n    assert client.get(u\"hello\") is None\n\n    client.set(b\"hello\", 12)\n    assert client.get(u\"hello\") == 12\n\n\n@pytest.mark.unit()\ndef test_get_set_non_ascii_value():\n    client = MockMemcacheClient()\n    assert client.get(b\"hello\") is None\n\n    # This is the value of msgpack.packb('non_ascii')\n    non_ascii_str = b'\\xa9non_ascii'\n    client.set(b\"hello\", non_ascii_str)\n    assert client.get(b\"hello\") == non_ascii_str\n\n\n@pytest.mark.unit()\ndef test_get_many_set_many():\n    client = MockMemcacheClient()\n    client.set(b\"h\", 1)\n\n    result = client.get_many([b\"h\", b\"e\", b\"l\", b\"o\"])\n    assert result == {b\"h\": 1}\n\n    # Convert keys into bytes\n    d = dict((k.encode('ascii'), v)\n             for k, v in six.iteritems(dict(h=1, e=2, z=3)))\n    client.set_many(d)\n    assert client.get_many([b\"h\", b\"e\", b\"z\", b\"o\"]) == d\n\n\n@pytest.mark.unit()\ndef test_get_many_set_many_non_ascii_values():\n    client = MockMemcacheClient()\n\n    # These are the values of calling msgpack.packb() on '1', '2', and '3'\n    non_ascii_1 = b'\\xa11'\n    non_ascii_2 = b'\\xa12'\n    non_ascii_3 = b'\\xa13'\n    client.set(b\"h\", non_ascii_1)\n\n    result = client.get_many([b\"h\", b\"e\", b\"l\", b\"o\"])\n    assert result == {b\"h\": non_ascii_1}\n\n    # Convert keys into bytes\n    d = dict((k.encode('ascii'), v)\n             for k, v in six.iteritems(\n                dict(h=non_ascii_1, e=non_ascii_2, z=non_ascii_3)\n             ))\n    client.set_many(d)\n    assert client.get_many([b\"h\", b\"e\", b\"z\", b\"o\"]) == d\n\n\n@pytest.mark.unit()\ndef test_add():\n    client = MockMemcacheClient()\n\n    client.add(b\"k\", 2)\n    assert client.get(b\"k\") == 2\n\n    client.add(b\"k\", 25)\n    assert client.get(b\"k\") == 2\n\n\n@pytest.mark.unit()\ndef test_delete():\n    client = MockMemcacheClient()\n\n    client.add(b\"k\", 2)\n    assert client.get(b\"k\") == 2\n\n    client.delete(b\"k\")\n    assert client.get(b\"k\") is None\n\n\n@pytest.mark.unit()\ndef test_incr_decr():\n    client = MockMemcacheClient()\n\n    client.add(b\"k\", 2)\n\n    client.incr(b\"k\", 4)\n    assert client.get(b\"k\") == 6\n\n    client.decr(b\"k\", 2)\n    assert client.get(b\"k\") == 4\n\n\n@pytest.mark.unit()\ndef test_prepand_append():\n    client = MockMemcacheClient()\n\n    client.set(b\"k\", '1')\n    client.append(b\"k\", 'a')\n    client.prepend(b\"k\", 'p')\n    assert client.get(b\"k\") == b'p1a'\n"
  },
  {
    "prompt": "class InvalidArgumentCombination\n\nclass ConditionType\n\nclass DerivedCondition\n\nclass Or\n\nclass And\n\nclass Xor\n\nclass Not\n\nclass argcomb\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"Tests for ArgComb. \"\"\"\n\n# pylint: disable=unused-argument, unused-variable\n\nfrom typing import Any, Callable\n\nimport pytest\n\nfrom argcomb import And, Else, InvalidArgumentCombination, Not, Or, Xor, argcomb\n\n\ndef test_default() -> None:\n    \"\"\"Test the ``default`` parameter of :function:``argcomb.__init__``.\n\n    This test also serves to check the basic functionality of\n    ``argcomb`` for different types of signature: normal arguments,\n    keyword only arguments, positional only arguments, or any\n    combination of the above.\n    \"\"\"\n\n    def test_func(\n        func: Callable[..., None], kw_only_count: int, pos_only_count: int\n    ) -> None:\n        \"\"\"Test a given function ``f``. \"\"\"\n\n        with pytest.raises(InvalidArgumentCombination):\n            func()\n        if pos_only_count == 0:\n            func(a=1)\n            func(a=1, b=1)\n            func(a=1, b=None)\n            with pytest.raises(InvalidArgumentCombination):\n                func(a=None, b=1)\n        if kw_only_count < 2:\n            func(1)\n\n        if kw_only_count < 2 and pos_only_count < 2:\n            func(1, b=1)\n            func(1, b=None)\n            with pytest.raises(InvalidArgumentCombination):\n                func(None, b=1)\n\n        if kw_only_count == 0:\n            func(1, 1)\n            func(1, None)\n            with pytest.raises(InvalidArgumentCombination):\n                func(None, 1)\n\n        if pos_only_count < 2:\n            with pytest.raises(InvalidArgumentCombination):\n                func(b=1)\n\n    @argcomb(\"a\")\n    def f(a: Any = None, b: Any = None) -> None:\n        ...\n\n    test_func(f, kw_only_count=0, pos_only_count=0)\n\n    @argcomb(\"a\")\n    def g(a: Any = None, *, b: Any = None) -> None:\n        ...\n\n    test_func(g, kw_only_count=1, pos_only_count=0)\n\n    @argcomb(\"a\")\n    def h(*, a: Any = None, b: Any = None) -> None:\n        ...\n\n    test_func(h, kw_only_count=2, pos_only_count=0)\n\n    @argcomb(\"a\")\n    def i(a: Any = None, /, b: Any = None) -> None:\n        ...\n\n    test_func(i, kw_only_count=0, pos_only_count=1)\n\n    @argcomb(\"a\")\n    def j(a: Any = None, b: Any = None, /) -> None:\n        ...\n\n    test_func(j, kw_only_count=0, pos_only_count=2)\n\n    @argcomb(\"a\")\n    def k(a: Any = None, /, *, b: Any = None) -> None:\n        ...\n\n    test_func(k, kw_only_count=1, pos_only_count=1)\n\n\ndef test_argument_specs() -> None:\n    \"\"\"Test providing specifications for arguments. \"\"\"\n\n    @argcomb(a=\"b\", c=\"d\")\n    def f(a: Any = None, b: Any = None, c: Any = None, d: Any = None) -> None:\n        ...\n\n    # 9 valid combinations\n    f()\n    f(d=1)\n    f(c=1, d=1)\n    f(b=1)\n    f(b=1, d=1)\n    f(b=1, c=1, d=1)\n    f(a=1, b=1)\n    f(a=1, b=1, d=1)\n    f(a=1, b=1, c=1, d=1)\n\n    # 7 invalid combinations\n    with pytest.raises(InvalidArgumentCombination):\n        f(c=1)\n    with pytest.raises(InvalidArgumentCombination):\n        f(b=1, c=1)\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=1)\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=1, d=1)\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=1, c=1)\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=1, c=1, d=1)\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=1, b=1, c=1)\n\n\ndef test_value_dependent_specs() -> None:\n    \"\"\"Test specifications which depend on argument value. \"\"\"\n\n    @argcomb(a={1: \"b\", 2: \"c\", 3: \"d\"})\n    def f(a: Any = None, b: Any = None, c: Any = None, d: Any = None) -> None:\n        ...\n\n    # valid\n    f()\n    f(a=1, b=4)\n    f(a=2, c=5)\n    f(a=3, d=6)\n    f(a=1, b=4, c=5)\n    f(a=1, b=4, c=5, d=6)\n    f(a=1, b=4, d=6)\n    f(a=2, c=5, d=6)\n    f(a=4)\n    f(b=4, c=5)\n    f(d=6)\n\n    # invalid\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=1, c=5)\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=1, c=5, d=6)\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=1)\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=2, b=3)\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=2, d=4)\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=3, b=3, c=4)\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=3)\n\n\ndef test_and() -> None:\n    \"\"\"Test ``And`` condition. \"\"\"\n\n    @argcomb(And(\"a\", \"b\"))\n    def f(a: Any = None, b: Any = None, c: Any = None) -> None:\n        ...\n\n    # \u00a0valid\n    f(a=1, b=2)\n    f(a=1, b=2, c=3)\n\n    # invalid\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=1)\n    with pytest.raises(InvalidArgumentCombination):\n        f(b=1)\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=1, b=None)\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=None, b=1)\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=1, c=1)\n    with pytest.raises(InvalidArgumentCombination):\n        f(b=1, c=1)\n    with pytest.raises(InvalidArgumentCombination):\n        f(c=1)\n    with pytest.raises(InvalidArgumentCombination):\n        f()\n\n\ndef test_or() -> None:\n    \"\"\"Test ``Or`` condition. \"\"\"\n\n    @argcomb(Or(\"a\", \"b\"))\n    def f(a: Any = None, b: Any = None) -> None:\n        ...\n\n    # valid\n    f(a=1)\n    f(b=2)\n    f(a=1, b=2)\n\n    # invalid\n    with pytest.raises(InvalidArgumentCombination):\n        f()\n\n\ndef test_not() -> None:\n    \"\"\"Test ``Not`` condition. \"\"\"\n\n    @argcomb(Not(\"a\"))\n    def f(a: Any = None) -> None:\n        ...\n\n    # valid\n    f()\n\n    # invalid\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=1)\n\n\ndef test_xor() -> None:\n    \"\"\"Test ``Xor`` condition. \"\"\"\n\n    @argcomb(Xor(\"a\", \"b\", \"c\"))\n    def f(a: Any = None, b: Any = None, c: Any = None) -> None:\n        ...\n\n    # valid\n    f(a=1)\n    f(b=1)\n    f(c=1)\n\n    # \u00a0invalid\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=1, b=1)\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=1, c=1)\n    with pytest.raises(InvalidArgumentCombination):\n        f(b=1, c=1)\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=1, b=1, c=1)\n    with pytest.raises(InvalidArgumentCombination):\n        f()\n\n\ndef test_else() -> None:\n    \"\"\"Test ``Else`` in value dependent specifications. \"\"\"\n\n    @argcomb(a={1: \"b\", Else: \"c\"})\n    def f(a: Any = None, b: Any = None, c: Any = None) -> None:\n        ...\n\n    # valid\n    f(a=2, c=1)\n\n    # \u00a0invalid\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=2, b=1)\n\n\ndef test_nested_condition() -> None:\n    \"\"\"Test a nested condition. \"\"\"\n\n    @argcomb(Or(And(\"a\", \"b\"), And(\"c\", \"d\")))\n    def f(a: Any = None, b: Any = None, c: Any = None, d: Any = None) -> None:\n        ...\n\n    # valid\n    f(a=1, b=1)\n    f(c=1, d=1)\n    f(a=1, b=1, c=1, d=1)\n\n    # invalid\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=1)\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=1, c=1)\n    with pytest.raises(InvalidArgumentCombination):\n        f()\n\n\ndef test_argument_named_default() -> None:\n    \"\"\"Test when an argument is named ``default``.\n\n    This collides with a positional only argument named ``default`` in\n    the ``argcomb`` signature, but as this is positional only this\n    should not matter.\n    \"\"\"\n\n    @argcomb(default=\"a\")\n    def f(default: Any = None, a: Any = None) -> None:\n        ...\n\n    f(a=1)\n    f(default=1, a=1)\n    with pytest.raises(InvalidArgumentCombination):\n        f(default=1)\n\n\ndef test_arguments_same_name() -> None:\n    \"\"\"Test that a warning is emitted when a function with two\n    identically named arguments. \"\"\"\n\n    @argcomb(a=\"b\")\n    def f(a: Any = None, /, b: Any = None, **kwargs: Any) -> None:\n        ...\n\n    with pytest.warns(UserWarning):\n        f(1, 2, a=3)  # pylint: disable=E1124\n\n\ndef test_default_arguments() -> None:\n    \"\"\"Test that default arguments are correctly recognised when they\n    are not ``None``. \"\"\"\n\n    @argcomb(a=\"b\")\n    def f(a: int = 1, b: int = 2) -> None:\n        ...\n\n    # valid since ``a`` is the default value\n    f(a=1)\n\n    with pytest.raises(InvalidArgumentCombination):\n        # invalid since ``b`` is the default value\n        f(a=2, b=2)\n\n\ndef test_kwargs() -> None:\n    \"\"\"Test functionality when signature uses ``**kwargs``. \"\"\"\n\n    @argcomb(a=\"b\")\n    def f(**kwargs: Any) -> None:\n        ...\n\n    f(a=1, b=1)\n    f(b=1, c=1)\n    with pytest.raises(InvalidArgumentCombination):\n        f(a=1)\n"
  },
  {
    "prompt": "fn setroot(engine, root)\n\nclass OctaveWithMatlabInterface\n\nfn octave_engine(root)\n\nfn matlab_engine(root)\n\nfn matrix(engine, obj)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright 2019 Alastair Pharo\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\nimport numpy\nimport os\nimport scipy\nfrom numpy.testing import assert_allclose\nfrom infsocsol.helpers import matrix\n\n@pytest.fixture(scope=\"module\", params=[\n    # engine    states time_step start         steps  steady steady_accuracy optim_accuracy\n    ( 'matlab', 10,    1,        (100.0, 0.5), 100,   True,  0.01,           0.009 ),\n    ( 'matlab', 20,    0.5,      (600.0, 0.6), 200,   True,  0.01,           0.015 ),\n    ( 'matlab', 40,    0.25,     (60.0, 0.1),  300,   True,  0.01,           0.018 ),\n    ( 'matlab', 10,    1,        (600.0, 1.0), 200,   False, 0.001,          None  ),\n    ( 'octave', 10,    1,        (100.0, 0.5), 100,   True,  0.001,          0.009 ),\n    ( 'octave', 20,    0.5,      (600.0, 0.6), 200,   True,  0.001,          0.015 )\n])\ndef fisheries_scenario(request):\n    return request.param\n\ndef test_fisheries_det_basic(engines, fisheries_scenario):\n    _engine, states, time_step, _start, steps, steady, steady_accuracy, optim_accuracy = fisheries_scenario\n    engine = engines[_engine]\n    start = matrix(engine, _start)\n    engine.cd(os.path.join(os.path.dirname(__file__), \"fisheries_det_basic\"))\n\n    engine.solve(float(states), float(time_step), nargout=0)\n    final = numpy.array(engine.sim_final(start, steps))\n    # This is determined by setting s\\dot = 0, which solves to 1 = x/L + q/r e\n    steady_one = numpy.dot(final, [1/600, 5/4])\n\n    if steady:\n        assert_allclose(steady_one, 1, atol=steady_accuracy)\n\n        # This is the most profitable steady state -- x = L/2 + c/2pq\n        profit_max_steady = numpy.array([[302.5, 0.39667]])\n        assert_allclose(final, profit_max_steady, rtol=optim_accuracy)\n    else:\n        assert steady_one > 1 + steady_accuracy\n"
  },
  {
    "prompt": "class CountryFactory\n\nclass StateFactory\n\nclass LocalityFactory\n\nclass AddressFactory\n\nclass PhoneContactMethodFactory\n\nclass EmailContactMethodFactory\n\nclass CoopTypeFactory\n\nclass CoopFactory\n\nclass PersonFactory\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom django.test import TestCase\nfrom .factories import CoopTypeFactory, CoopFactory, AddressFactory, PhoneContactMethodFactory \nfrom directory.models import Coop, CoopType\n\n\nclass ModelTests(TestCase):\n    @classmethod\n    def setUpTestData(cls):\n        print(\"setUpTestData: Run once to set up non-modified data for all class methods.\")\n        #management.call_command('loaddata', 'test_data.yaml', verbosity=0)\n        pass\n\n    @pytest.mark.django_db\n    def test_phone_create(self):\n        \"\"\" Test phone contact method \"\"\"    # create phone instance\n        phone_num = \"7739441467\"\n        phone = PhoneContactMethodFactory.create(phone=phone_num) \n        assert phone_num == phone.phone\n        assert phone.id is not None\n\n    @pytest.mark.django_db\n    def test_phone_create_invalid_num(self):\n        \"\"\" Test phone contact method \"\"\"    # create phone instance\n        phone_num = \"abcdefsfdsf\"\n        phone = PhoneContactMethodFactory.create(phone=phone_num) \n        assert phone_num == phone.phone  \n        assert phone.id is not None\n        print(\"\\n\\n\\n\\n-------------id is \", id) \n\n    @pytest.mark.django_db\n    def test_coop_type_create(self):\n        \"\"\" Test coop type model \"\"\"    # create customer model instance\n        coop_type = CoopTypeFactory(name=\"Test Coop Type Name\")\n        assert coop_type.name == \"Test Coop Type Name\"\n\n    @pytest.mark.django_db\n    def test_address_create(self):\n        \"\"\" Test address model \"\"\"    # create customer model instance\n        address = AddressFactory()\n        assert address is not None\n\n    @pytest.mark.django_db\n    def test_coop_create(self):\n        \"\"\" Test customer model \"\"\"    # create customer model instance\n        coop_from_factory = CoopFactory()\n        self.assertIsNotNone(coop_from_factory)\n\n        coop = Coop.objects.create(name='test')\n        coop.addresses.set(coop_from_factory.addresses.all()) \n        self.assertIsNotNone(coop)\n\n    @pytest.mark.django_db\n    def test_coop_create_with_existing_type(self):\n        \"\"\" Test customer model \"\"\"    # create customer model instance\n        coop_from_factory = CoopFactory()\n        self.assertIsNotNone(coop_from_factory)\n\n        coop_types = coop_from_factory.types\n        coop = CoopFactory.create(types=[coop_types.all().first()], addresses=coop_from_factory.addresses.all())\n        self.assertIsNotNone(coop)\n\n    @pytest.mark.django_db\n    def test_coop_create_with_no_types(self):\n        \"\"\" Test customer model \"\"\"    # create customer model instance\n        print(\"\\n\\n\\n\\n**********-------- starting test ....\\n\")\n        coop = CoopFactory.build(types=[])\n        print(\"phone:\",coop.phone.phone)\n        print(\"email:\",coop.email.email)\n        coop.full_clean()\n        self.assertIsNotNone(coop)\n        self.assertIsNone( coop.id )\n\n    def test_search_coops_wo_coords(self):\n        \"\"\"\n        Look for coops with addresses without latitude/longitude coords\n        \"\"\"\n        address = AddressFactory(latitude=None, longitude=None)\n        coop_from_factory = CoopFactory(addresses=[address])\n        # Verify coop appears when we search for those without a lat/lon\n        coops = Coop.objects.find_wo_coords()\n        results = list(coops)\n        assert len(results) > 0, \"Failed to find any matching results.\"\n        assert coop_from_factory in list(coops), \"Failed to find coop.\"\n \n\n"
  },
  {
    "prompt": "class User\n\nclass UserChangeForm\n\nclass UserCreationForm\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"\nModule for all Form Tests.\n\"\"\"\nimport pytest\nfrom django.utils.translation import gettext_lazy as _\n\nfrom djcutter.users.forms import UserCreationForm\nfrom djcutter.users.models import User\n\npytestmark = pytest.mark.django_db\n\n\nclass TestUserCreationForm:\n    \"\"\"\n    Test class for all tests related to the UserCreationForm\n    \"\"\"\n\n    def test_username_validation_error_msg(self, user: User):\n        \"\"\"\n        Tests UserCreation Form's unique validator functions correctly by testing:\n            1) A new user with an existing username cannot be added.\n            2) Only 1 error is raised by the UserCreation Form\n            3) The desired error message is raised\n        \"\"\"\n\n        # The user already exists,\n        # hence cannot be created.\n        form = UserCreationForm(\n            {\n                \"username\": user.username,\n                \"password1\": user.password,\n                \"password2\": user.password,\n            }\n        )\n\n        assert not form.is_valid()\n        assert len(form.errors) == 1\n        assert \"username\" in form.errors\n        assert form.errors[\"username\"][0] == _(\"This username has already been taken.\")\n"
  },
  {
    "prompt": "fn splunk_single(service, search)\n\nfn insert_char(string, char, integer)\n\nfn removeZero(tz)\n\nfn time_operations(dt)\n\nfn sendsingle(message, host, port)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright 2019 Splunk, Inc.\n#\n# Use of this source code is governed by a BSD-2-clause-style\n# license that can be found in the LICENSE-BSD2 file or at\n# https://opensource.org/licenses/BSD-2-Clause\n\nfrom jinja2 import Environment\n\nfrom .sendmessage import *\nfrom .splunkutils import *\nfrom .timeutils import *\n\nimport pytest\n\nenv = Environment()\n\n# time format for Apr  5 22:51:54 2021\n# <187>{{ arubadate }} {{ host }} authmgr[4130]: <124198> <4130> <ERRS> <{{ host }} 10.10.10.10>  {00:00:00:00:00:00-??} Missing server in attribute list, auth=VPN, utype=L3.\n# <187>{{ arubadate }} {{ host }} stm[4133]: <399803> <4133> <ERRS> <{{ host }} 10.10.10.10>  An internal system error has occurred at file sapm_ap_mgmt.c function sapm_get_img_build_version_str line 11853 error stat /mswitch/sap/mips64.ari failed: No such file or directory.\n# <188>{{ arubadate }} {{ host }} wms[4096]: <126005> <4096> <WARN> <{{ host }} 10.10.10.10> |ids| Interfering AP: The system classified an access point (BSSID 00:0e:8e:96:f4:32 and SSID  on CHANNEL 36) as interfering. Additional Info: Detector-AP-Name:00:0b:86:9e:6b:5f; Detector-AP-MAC:24:de:c6:70:2c:90; Detector-AP-Radio:1.\n# <191>{{ arubadate }} 10.10.10.10 dnsmasq: reading /etc/resolv.conf\n\ntestdata = [\n    \"<187>{{ arubadate }} {{ host }} authmgr[4130]: <124198> <4130> <ERRS> <{{ host }} 10.10.10.10>  {00:00:00:00:00:00-??} Missing server in attribute list, auth=VPN, utype=L3.\",\n    \"<187>{{ arubadate }} {{ host }} stm[4133]: <399803> <4133> <ERRS> <{{ host }} 10.10.10.10>  An internal system error has occurred at file sapm_ap_mgmt.c function sapm_get_img_build_version_str line 11853 error stat /mswitch/sap/mips64.ari failed: No such file or directory.\",\n    \"<188>{{ arubadate }} {{ host }} wms[4096]: <126005> <4096> <WARN> <{{ host }} 10.10.10.10> |ids| Interfering AP: The system classified an access point (BSSID 00:0e:8e:96:f4:32 and SSID  on CHANNEL 36) as interfering. Additional Info: Detector-AP-Name:00:0b:86:9e:6b:5f; Detector-AP-MAC:24:de:c6:70:2c:90; Detector-AP-Radio:1.\",\n    \"<188>{{ arubadate }} {{ host }} sapd[1362]: <127037> <WARN> |AP 00:0b:86:eb:4e:32@10.10.10.10 sapd| |ids-ap| AP(04:bd:88:8a:3a:60): Station Associated to Rogue AP: An AP detected a client a4:8d:3b:ae:68:68 associated to a rogue access point (BSSID 98:1e:19:31:63:b6 and SSID MySpectrumWiFib0-2G on CHANNEL 11).\",\n]\n\n\n@pytest.mark.parametrize(\"event\", testdata)\ndef test_aruba(\n    record_property, setup_wordlist, get_host_key, setup_splunk, setup_sc4s, event\n):\n    host = get_host_key\n\n    dt = datetime.datetime.now()\n    iso, bsd, time, date, tzoffset, tzname, epoch = time_operations(dt)\n    arubadate = dt.strftime(\"%b %d %H:%M:%S %Y\")\n\n    # Tune time functions\n    epoch = epoch[:-7]\n\n    mt = env.from_string(event + \"\\n\")\n    message = mt.render(mark=\"<188>\", bsd=bsd, host=host, arubadate=arubadate)\n\n    sendsingle(message, setup_sc4s[0], setup_sc4s[1][514])\n\n    st = env.from_string(\n        'search index=netops _time={{ epoch }} sourcetype=\"aruba:syslog\" host={{ host }}'\n    )\n    search = st.render(epoch=epoch, host=host)\n\n    resultCount, eventCount = splunk_single(setup_splunk, search)\n\n    record_property(\"host\", host)\n    record_property(\"resultCount\", resultCount)\n    record_property(\"message\", message)\n\n    assert resultCount == 1\n"
  },
  {
    "prompt": "fn normalize_privatekey_pem(pem)\n\n\"\"\"Create a new private key and start a certificate request (for a test\nto finish in one way or another).\"\"\"\nfn x509_data()\n\nclass TestX509Ext\n\nclass TestPKey\n\n\"\"\"Return a new X509Name with the given attributes.\"\"\"\nfn x509_name()\n\nclass TestX509Name\n\nclass _PKeyInteractionTestsMixin\n\nclass TestX509Req\n\nclass TestX509\n\nclass TestX509Store\n\nclass TestPKCS12\n\n\"\"\"Run the command line openssl tool with the given arguments and write\nthe given PEM to its stdin.  Not safe for quotes.\"\"\"\nfn _runopenssl(pem)\n\nclass TestLoadPublicKey\n\nclass TestFunction\n\nclass TestLoadCertificate\n\nclass TestPKCS7\n\nclass TestNetscapeSPKI\n\nclass TestRevoked\n\nclass TestCRL\n\nclass TestX509StoreContext\n\nclass TestSignVerify\n\nclass TestEllipticCurve\n\nclass EllipticCurveFactory\n\nclass TestEllipticCurveEquality\n\nclass TestEllipticCurveHash\n\n\"\"\"Perform various assertions about *theType* to ensure that it is a\nwell-defined type.  This is useful for extension types, where it's\npretty easy to do something wacky.  If something about the type is\nunusual, an exception will be raised.\n\n:param theType: The type object about which to make assertions.\n:param name: A string giving the name of the type.\n:param constructionArgs: Positional arguments to use with\n    *theType* to create an instance of it.\"\"\"\nfn is_consistent_type(theType: None, name: None)\n\nclass EqualityTestsMixin\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright (C) Jean-Paul Calderone\n# See LICENSE for details.\n\n\"\"\"\nUnit tests for :mod:`OpenSSL.SSL`.\n\"\"\"\n\nimport datetime\nimport sys\nimport uuid\n\nfrom gc import collect, get_referrers\nfrom errno import (\n    EAFNOSUPPORT, ECONNREFUSED, EINPROGRESS, EWOULDBLOCK, EPIPE, ESHUTDOWN)\nfrom sys import platform, getfilesystemencoding\nfrom socket import AF_INET, AF_INET6, MSG_PEEK, SHUT_RDWR, error, socket\nfrom os import makedirs\nfrom os.path import join\nfrom weakref import ref\nfrom warnings import simplefilter\n\nimport pytest\n\nfrom pretend import raiser\n\nfrom six import PY3, text_type\n\nfrom cryptography import x509\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives.asymmetric import rsa\nfrom cryptography.x509.oid import NameOID\n\n\nfrom OpenSSL.crypto import TYPE_RSA, FILETYPE_PEM\nfrom OpenSSL.crypto import PKey, X509, X509Extension, X509Store\nfrom OpenSSL.crypto import dump_privatekey, load_privatekey\nfrom OpenSSL.crypto import dump_certificate, load_certificate\nfrom OpenSSL.crypto import get_elliptic_curves\n\nfrom OpenSSL.SSL import OPENSSL_VERSION_NUMBER, SSLEAY_VERSION, SSLEAY_CFLAGS\nfrom OpenSSL.SSL import SSLEAY_PLATFORM, SSLEAY_DIR, SSLEAY_BUILT_ON\nfrom OpenSSL.SSL import SENT_SHUTDOWN, RECEIVED_SHUTDOWN\nfrom OpenSSL.SSL import (\n    SSLv2_METHOD, SSLv3_METHOD, SSLv23_METHOD, TLSv1_METHOD,\n    TLSv1_1_METHOD, TLSv1_2_METHOD)\nfrom OpenSSL.SSL import OP_SINGLE_DH_USE, OP_NO_SSLv2, OP_NO_SSLv3\nfrom OpenSSL.SSL import (\n    VERIFY_PEER, VERIFY_FAIL_IF_NO_PEER_CERT, VERIFY_CLIENT_ONCE, VERIFY_NONE)\n\nfrom OpenSSL import SSL\nfrom OpenSSL.SSL import (\n    SESS_CACHE_OFF, SESS_CACHE_CLIENT, SESS_CACHE_SERVER, SESS_CACHE_BOTH,\n    SESS_CACHE_NO_AUTO_CLEAR, SESS_CACHE_NO_INTERNAL_LOOKUP,\n    SESS_CACHE_NO_INTERNAL_STORE, SESS_CACHE_NO_INTERNAL)\n\nfrom OpenSSL.SSL import (\n    Error, SysCallError, WantReadError, WantWriteError, ZeroReturnError)\nfrom OpenSSL.SSL import (\n    Context, Session, Connection, SSLeay_version)\nfrom OpenSSL.SSL import _make_requires\n\nfrom OpenSSL._util import ffi as _ffi, lib as _lib\n\nfrom OpenSSL.SSL import (\n    OP_NO_QUERY_MTU, OP_COOKIE_EXCHANGE, OP_NO_TICKET, OP_NO_COMPRESSION,\n    MODE_RELEASE_BUFFERS)\n\nfrom OpenSSL.SSL import (\n    SSL_ST_CONNECT, SSL_ST_ACCEPT, SSL_ST_MASK,\n    SSL_CB_LOOP, SSL_CB_EXIT, SSL_CB_READ, SSL_CB_WRITE, SSL_CB_ALERT,\n    SSL_CB_READ_ALERT, SSL_CB_WRITE_ALERT, SSL_CB_ACCEPT_LOOP,\n    SSL_CB_ACCEPT_EXIT, SSL_CB_CONNECT_LOOP, SSL_CB_CONNECT_EXIT,\n    SSL_CB_HANDSHAKE_START, SSL_CB_HANDSHAKE_DONE)\n\ntry:\n    from OpenSSL.SSL import (\n        SSL_ST_INIT, SSL_ST_BEFORE, SSL_ST_OK, SSL_ST_RENEGOTIATE\n    )\nexcept ImportError:\n    SSL_ST_INIT = SSL_ST_BEFORE = SSL_ST_OK = SSL_ST_RENEGOTIATE = None\n\nfrom .util import WARNING_TYPE_EXPECTED, NON_ASCII, is_consistent_type\nfrom .test_crypto import (\n    cleartextCertificatePEM, cleartextPrivateKeyPEM,\n    client_cert_pem, client_key_pem, server_cert_pem, server_key_pem,\n    root_cert_pem)\n\n\n# openssl dhparam 1024 -out dh-1024.pem (note that 1024 is a small number of\n# bits to use)\ndhparam = \"\"\"\\\n-----BEGIN DH PARAMETERS-----\nMIGHAoGBALdUMvn+C9MM+y5BWZs11mSeH6HHoEq0UVbzVq7UojC1hbsZUuGukQ3a\nQh2/pwqb18BZFykrWB0zv/OkLa0kx4cuUgNrUVq1EFheBiX6YqryJ7t2sO09NQiO\nV7H54LmltOT/hEh6QWsJqb6BQgH65bswvV/XkYGja8/T0GzvbaVzAgEC\n-----END DH PARAMETERS-----\n\"\"\"\n\n\nskip_if_py3 = pytest.mark.skipif(PY3, reason=\"Python 2 only\")\n\n\ndef socket_any_family():\n    try:\n        return socket(AF_INET)\n    except error as e:\n        if e.errno == EAFNOSUPPORT:\n            return socket(AF_INET6)\n        raise\n\n\ndef loopback_address(socket):\n    if socket.family == AF_INET:\n        return \"127.0.0.1\"\n    else:\n        assert socket.family == AF_INET6\n        return \"::1\"\n\n\ndef join_bytes_or_unicode(prefix, suffix):\n    \"\"\"\n    Join two path components of either ``bytes`` or ``unicode``.\n\n    The return type is the same as the type of ``prefix``.\n    \"\"\"\n    # If the types are the same, nothing special is necessary.\n    if type(prefix) == type(suffix):\n        return join(prefix, suffix)\n\n    # Otherwise, coerce suffix to the type of prefix.\n    if isinstance(prefix, text_type):\n        return join(prefix, suffix.decode(getfilesystemencoding()))\n    else:\n        return join(prefix, suffix.encode(getfilesystemencoding()))\n\n\ndef verify_cb(conn, cert, errnum, depth, ok):\n    return ok\n\n\ndef socket_pair():\n    \"\"\"\n    Establish and return a pair of network sockets connected to each other.\n    \"\"\"\n    # Connect a pair of sockets\n    port = socket_any_family()\n    port.bind(('', 0))\n    port.listen(1)\n    client = socket(port.family)\n    client.setblocking(False)\n    client.connect_ex((loopback_address(port), port.getsockname()[1]))\n    client.setblocking(True)\n    server = port.accept()[0]\n\n    # Let's pass some unencrypted data to make sure our socket connection is\n    # fine.  Just one byte, so we don't have to worry about buffers getting\n    # filled up or fragmentation.\n    server.send(b\"x\")\n    assert client.recv(1024) == b\"x\"\n    client.send(b\"y\")\n    assert server.recv(1024) == b\"y\"\n\n    # Most of our callers want non-blocking sockets, make it easy for them.\n    server.setblocking(False)\n    client.setblocking(False)\n\n    return (server, client)\n\n\ndef handshake(client, server):\n    conns = [client, server]\n    while conns:\n        for conn in conns:\n            try:\n                conn.do_handshake()\n            except WantReadError:\n                pass\n            else:\n                conns.remove(conn)\n\n\ndef _create_certificate_chain():\n    \"\"\"\n    Construct and return a chain of certificates.\n\n        1. A new self-signed certificate authority certificate (cacert)\n        2. A new intermediate certificate signed by cacert (icert)\n        3. A new server certificate signed by icert (scert)\n    \"\"\"\n    caext = X509Extension(b'basicConstraints', False, b'CA:true')\n\n    # Step 1\n    cakey = PKey()\n    cakey.generate_key(TYPE_RSA, 1024)\n    cacert = X509()\n    cacert.get_subject().commonName = \"Authority Certificate\"\n    cacert.set_issuer(cacert.get_subject())\n    cacert.set_pubkey(cakey)\n    cacert.set_notBefore(b\"20000101000000Z\")\n    cacert.set_notAfter(b\"20200101000000Z\")\n    cacert.add_extensions([caext])\n    cacert.set_serial_number(0)\n    cacert.sign(cakey, \"sha1\")\n\n    # Step 2\n    ikey = PKey()\n    ikey.generate_key(TYPE_RSA, 1024)\n    icert = X509()\n    icert.get_subject().commonName = \"Intermediate Certificate\"\n    icert.set_issuer(cacert.get_subject())\n    icert.set_pubkey(ikey)\n    icert.set_notBefore(b\"20000101000000Z\")\n    icert.set_notAfter(b\"20200101000000Z\")\n    icert.add_extensions([caext])\n    icert.set_serial_number(0)\n    icert.sign(cakey, \"sha1\")\n\n    # Step 3\n    skey = PKey()\n    skey.generate_key(TYPE_RSA, 1024)\n    scert = X509()\n    scert.get_subject().commonName = \"Server Certificate\"\n    scert.set_issuer(icert.get_subject())\n    scert.set_pubkey(skey)\n    scert.set_notBefore(b\"20000101000000Z\")\n    scert.set_notAfter(b\"20200101000000Z\")\n    scert.add_extensions([\n        X509Extension(b'basicConstraints', True, b'CA:false')])\n    scert.set_serial_number(0)\n    scert.sign(ikey, \"sha1\")\n\n    return [(cakey, cacert), (ikey, icert), (skey, scert)]\n\n\ndef loopback_client_factory(socket, version=SSLv23_METHOD):\n    client = Connection(Context(version), socket)\n    client.set_connect_state()\n    return client\n\n\ndef loopback_server_factory(socket, version=SSLv23_METHOD):\n    ctx = Context(version)\n    ctx.use_privatekey(load_privatekey(FILETYPE_PEM, server_key_pem))\n    ctx.use_certificate(load_certificate(FILETYPE_PEM, server_cert_pem))\n    server = Connection(ctx, socket)\n    server.set_accept_state()\n    return server\n\n\ndef loopback(server_factory=None, client_factory=None):\n    \"\"\"\n    Create a connected socket pair and force two connected SSL sockets\n    to talk to each other via memory BIOs.\n    \"\"\"\n    if server_factory is None:\n        server_factory = loopback_server_factory\n    if client_factory is None:\n        client_factory = loopback_client_factory\n\n    (server, client) = socket_pair()\n    server = server_factory(server)\n    client = client_factory(client)\n\n    handshake(client, server)\n\n    server.setblocking(True)\n    client.setblocking(True)\n    return server, client\n\n\ndef interact_in_memory(client_conn, server_conn):\n    \"\"\"\n    Try to read application bytes from each of the two `Connection` objects.\n    Copy bytes back and forth between their send/receive buffers for as long\n    as there is anything to copy.  When there is nothing more to copy,\n    return `None`.  If one of them actually manages to deliver some application\n    bytes, return a two-tuple of the connection from which the bytes were read\n    and the bytes themselves.\n    \"\"\"\n    wrote = True\n    while wrote:\n        # Loop until neither side has anything to say\n        wrote = False\n\n        # Copy stuff from each side's send buffer to the other side's\n        # receive buffer.\n        for (read, write) in [(client_conn, server_conn),\n                              (server_conn, client_conn)]:\n\n            # Give the side a chance to generate some more bytes, or succeed.\n            try:\n                data = read.recv(2 ** 16)\n            except WantReadError:\n                # It didn't succeed, so we'll hope it generated some output.\n                pass\n            else:\n                # It did succeed, so we'll stop now and let the caller deal\n                # with it.\n                return (read, data)\n\n            while True:\n                # Keep copying as long as there's more stuff there.\n                try:\n                    dirty = read.bio_read(4096)\n                except WantReadError:\n                    # Okay, nothing more waiting to be sent.  Stop\n                    # processing this send buffer.\n                    break\n                else:\n                    # Keep track of the fact that someone generated some\n                    # output.\n                    wrote = True\n                    write.bio_write(dirty)\n\n\ndef handshake_in_memory(client_conn, server_conn):\n    \"\"\"\n    Perform the TLS handshake between two `Connection` instances connected to\n    each other via memory BIOs.\n    \"\"\"\n    client_conn.set_connect_state()\n    server_conn.set_accept_state()\n\n    for conn in [client_conn, server_conn]:\n        try:\n            conn.do_handshake()\n        except WantReadError:\n            pass\n\n    interact_in_memory(client_conn, server_conn)\n\n\nclass TestVersion(object):\n    \"\"\"\n    Tests for version information exposed by `OpenSSL.SSL.SSLeay_version` and\n    `OpenSSL.SSL.OPENSSL_VERSION_NUMBER`.\n    \"\"\"\n    def test_OPENSSL_VERSION_NUMBER(self):\n        \"\"\"\n        `OPENSSL_VERSION_NUMBER` is an integer with status in the low byte and\n        the patch, fix, minor, and major versions in the nibbles above that.\n        \"\"\"\n        assert isinstance(OPENSSL_VERSION_NUMBER, int)\n\n    def test_SSLeay_version(self):\n        \"\"\"\n        `SSLeay_version` takes a version type indicator and returns one of a\n        number of version strings based on that indicator.\n        \"\"\"\n        versions = {}\n        for t in [SSLEAY_VERSION, SSLEAY_CFLAGS, SSLEAY_BUILT_ON,\n                  SSLEAY_PLATFORM, SSLEAY_DIR]:\n            version = SSLeay_version(t)\n            versions[version] = t\n            assert isinstance(version, bytes)\n        assert len(versions) == 5\n\n\n@pytest.fixture\ndef ca_file(tmpdir):\n    \"\"\"\n    Create a valid PEM file with CA certificates and return the path.\n    \"\"\"\n    key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048,\n        backend=default_backend()\n    )\n    public_key = key.public_key()\n\n    builder = x509.CertificateBuilder()\n    builder = builder.subject_name(x509.Name([\n        x509.NameAttribute(NameOID.COMMON_NAME, u\"pyopenssl.org\"),\n    ]))\n    builder = builder.issuer_name(x509.Name([\n        x509.NameAttribute(NameOID.COMMON_NAME, u\"pyopenssl.org\"),\n    ]))\n    one_day = datetime.timedelta(1, 0, 0)\n    builder = builder.not_valid_before(datetime.datetime.today() - one_day)\n    builder = builder.not_valid_after(datetime.datetime.today() + one_day)\n    builder = builder.serial_number(int(uuid.uuid4()))\n    builder = builder.public_key(public_key)\n    builder = builder.add_extension(\n        x509.BasicConstraints(ca=True, path_length=None), critical=True,\n    )\n\n    certificate = builder.sign(\n        private_key=key, algorithm=hashes.SHA256(),\n        backend=default_backend()\n    )\n\n    ca_file = tmpdir.join(\"test.pem\")\n    ca_file.write_binary(\n        certificate.public_bytes(\n            encoding=serialization.Encoding.PEM,\n        )\n    )\n\n    return str(ca_file).encode(\"ascii\")\n\n\n@pytest.fixture\ndef context():\n    \"\"\"\n    A simple TLS 1.0 context.\n    \"\"\"\n    return Context(TLSv1_METHOD)\n\n\nclass TestContext(object):\n    \"\"\"\n    Unit tests for `OpenSSL.SSL.Context`.\n    \"\"\"\n    @pytest.mark.parametrize(\"cipher_string\", [\n        b\"hello world:AES128-SHA\",\n        u\"hello world:AES128-SHA\",\n    ])\n    def test_set_cipher_list(self, context, cipher_string):\n        \"\"\"\n        `Context.set_cipher_list` accepts both byte and unicode strings\n        for naming the ciphers which connections created with the context\n        object will be able to choose from.\n        \"\"\"\n        context.set_cipher_list(cipher_string)\n        conn = Connection(context, None)\n\n        assert \"AES128-SHA\" in conn.get_cipher_list()\n\n    def test_set_cipher_list_wrong_type(self, context):\n        \"\"\"\n        `Context.set_cipher_list` raises `TypeError` when passed a non-string\n        argument.\n        \"\"\"\n        with pytest.raises(TypeError):\n            context.set_cipher_list(object())\n\n    def test_set_cipher_list_no_cipher_match(self, context):\n        \"\"\"\n        `Context.set_cipher_list` raises `OpenSSL.SSL.Error` with a\n        `\"no cipher match\"` reason string regardless of the TLS\n        version.\n        \"\"\"\n        with pytest.raises(Error) as excinfo:\n            context.set_cipher_list(b\"imaginary-cipher\")\n        assert excinfo.value.args == (\n                [\n                    (\n                        'SSL routines',\n                        'SSL_CTX_set_cipher_list',\n                        'no cipher match',\n                    ),\n                ],\n            )\n\n    def test_load_client_ca(self, context, ca_file):\n        \"\"\"\n        `Context.load_client_ca` works as far as we can tell.\n        \"\"\"\n        context.load_client_ca(ca_file)\n\n    def test_load_client_ca_invalid(self, context, tmpdir):\n        \"\"\"\n        `Context.load_client_ca` raises an Error if the ca file is invalid.\n        \"\"\"\n        ca_file = tmpdir.join(\"test.pem\")\n        ca_file.write(\"\")\n\n        with pytest.raises(Error) as e:\n            context.load_client_ca(str(ca_file).encode(\"ascii\"))\n\n        assert \"PEM routines\" == e.value.args[0][0][0]\n\n    def test_load_client_ca_unicode(self, context, ca_file):\n        \"\"\"\n        Passing the path as unicode raises a warning but works.\n        \"\"\"\n        pytest.deprecated_call(\n            context.load_client_ca, ca_file.decode(\"ascii\")\n        )\n\n    def test_set_session_id(self, context):\n        \"\"\"\n        `Context.set_session_id` works as far as we can tell.\n        \"\"\"\n        context.set_session_id(b\"abc\")\n\n    def test_set_session_id_fail(self, context):\n        \"\"\"\n        `Context.set_session_id` errors are propagated.\n        \"\"\"\n        with pytest.raises(Error) as e:\n            context.set_session_id(b\"abc\" * 1000)\n\n        assert [\n            (\"SSL routines\",\n             \"SSL_CTX_set_session_id_context\",\n             \"ssl session id context too long\")\n        ] == e.value.args[0]\n\n    def test_set_session_id_unicode(self, context):\n        \"\"\"\n        `Context.set_session_id` raises a warning if a unicode string is\n        passed.\n        \"\"\"\n        pytest.deprecated_call(context.set_session_id, u\"abc\")\n\n    def test_method(self):\n        \"\"\"\n        `Context` can be instantiated with one of `SSLv2_METHOD`,\n        `SSLv3_METHOD`, `SSLv23_METHOD`, `TLSv1_METHOD`, `TLSv1_1_METHOD`,\n        or `TLSv1_2_METHOD`.\n        \"\"\"\n        methods = [SSLv23_METHOD, TLSv1_METHOD]\n        for meth in methods:\n            Context(meth)\n\n        maybe = [SSLv2_METHOD, SSLv3_METHOD, TLSv1_1_METHOD, TLSv1_2_METHOD]\n        for meth in maybe:\n            try:\n                Context(meth)\n            except (Error, ValueError):\n                # Some versions of OpenSSL have SSLv2 / TLSv1.1 / TLSv1.2, some\n                # don't.  Difficult to say in advance.\n                pass\n\n        with pytest.raises(TypeError):\n            Context(\"\")\n        with pytest.raises(ValueError):\n            Context(10)\n\n    def test_type(self):\n        \"\"\"\n        `Context` can be used to create instances of that type.\n        \"\"\"\n        assert is_consistent_type(Context, 'Context', TLSv1_METHOD)\n\n    def test_use_privatekey(self):\n        \"\"\"\n        `Context.use_privatekey` takes an `OpenSSL.crypto.PKey` instance.\n        \"\"\"\n        key = PKey()\n        key.generate_key(TYPE_RSA, 512)\n        ctx = Context(TLSv1_METHOD)\n        ctx.use_privatekey(key)\n        with pytest.raises(TypeError):\n            ctx.use_privatekey(\"\")\n\n    def test_use_privatekey_file_missing(self, tmpfile):\n        \"\"\"\n        `Context.use_privatekey_file` raises `OpenSSL.SSL.Error` when passed\n        the name of a file which does not exist.\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        with pytest.raises(Error):\n            ctx.use_privatekey_file(tmpfile)\n\n    def _use_privatekey_file_test(self, pemfile, filetype):\n        \"\"\"\n        Verify that calling ``Context.use_privatekey_file`` with the given\n        arguments does not raise an exception.\n        \"\"\"\n        key = PKey()\n        key.generate_key(TYPE_RSA, 512)\n\n        with open(pemfile, \"wt\") as pem:\n            pem.write(\n                dump_privatekey(FILETYPE_PEM, key).decode(\"ascii\")\n            )\n\n        ctx = Context(TLSv1_METHOD)\n        ctx.use_privatekey_file(pemfile, filetype)\n\n    @pytest.mark.parametrize('filetype', [object(), \"\", None, 1.0])\n    def test_wrong_privatekey_file_wrong_args(self, tmpfile, filetype):\n        \"\"\"\n        `Context.use_privatekey_file` raises `TypeError` when called with\n        a `filetype` which is not a valid file encoding.\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        with pytest.raises(TypeError):\n            ctx.use_privatekey_file(tmpfile, filetype)\n\n    def test_use_privatekey_file_bytes(self, tmpfile):\n        \"\"\"\n        A private key can be specified from a file by passing a ``bytes``\n        instance giving the file name to ``Context.use_privatekey_file``.\n        \"\"\"\n        self._use_privatekey_file_test(\n            tmpfile + NON_ASCII.encode(getfilesystemencoding()),\n            FILETYPE_PEM,\n        )\n\n    def test_use_privatekey_file_unicode(self, tmpfile):\n        \"\"\"\n        A private key can be specified from a file by passing a ``unicode``\n        instance giving the file name to ``Context.use_privatekey_file``.\n        \"\"\"\n        self._use_privatekey_file_test(\n            tmpfile.decode(getfilesystemencoding()) + NON_ASCII,\n            FILETYPE_PEM,\n        )\n\n    def test_use_certificate_wrong_args(self):\n        \"\"\"\n        `Context.use_certificate_wrong_args` raises `TypeError` when not passed\n        exactly one `OpenSSL.crypto.X509` instance as an argument.\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        with pytest.raises(TypeError):\n            ctx.use_certificate(\"hello, world\")\n\n    def test_use_certificate_uninitialized(self):\n        \"\"\"\n        `Context.use_certificate` raises `OpenSSL.SSL.Error` when passed a\n        `OpenSSL.crypto.X509` instance which has not been initialized\n        (ie, which does not actually have any certificate data).\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        with pytest.raises(Error):\n            ctx.use_certificate(X509())\n\n    def test_use_certificate(self):\n        \"\"\"\n        `Context.use_certificate` sets the certificate which will be\n        used to identify connections created using the context.\n        \"\"\"\n        # TODO\n        # Hard to assert anything.  But we could set a privatekey then ask\n        # OpenSSL if the cert and key agree using check_privatekey.  Then as\n        # long as check_privatekey works right we're good...\n        ctx = Context(TLSv1_METHOD)\n        ctx.use_certificate(\n            load_certificate(FILETYPE_PEM, cleartextCertificatePEM)\n        )\n\n    def test_use_certificate_file_wrong_args(self):\n        \"\"\"\n        `Context.use_certificate_file` raises `TypeError` if the first\n        argument is not a byte string or the second argument is not an integer.\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        with pytest.raises(TypeError):\n            ctx.use_certificate_file(object(), FILETYPE_PEM)\n        with pytest.raises(TypeError):\n            ctx.use_certificate_file(b\"somefile\", object())\n        with pytest.raises(TypeError):\n            ctx.use_certificate_file(object(), FILETYPE_PEM)\n\n    def test_use_certificate_file_missing(self, tmpfile):\n        \"\"\"\n        `Context.use_certificate_file` raises `OpenSSL.SSL.Error` if passed\n        the name of a file which does not exist.\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        with pytest.raises(Error):\n            ctx.use_certificate_file(tmpfile)\n\n    def _use_certificate_file_test(self, certificate_file):\n        \"\"\"\n        Verify that calling ``Context.use_certificate_file`` with the given\n        filename doesn't raise an exception.\n        \"\"\"\n        # TODO\n        # Hard to assert anything.  But we could set a privatekey then ask\n        # OpenSSL if the cert and key agree using check_privatekey.  Then as\n        # long as check_privatekey works right we're good...\n        with open(certificate_file, \"wb\") as pem_file:\n            pem_file.write(cleartextCertificatePEM)\n\n        ctx = Context(TLSv1_METHOD)\n        ctx.use_certificate_file(certificate_file)\n\n    def test_use_certificate_file_bytes(self, tmpfile):\n        \"\"\"\n        `Context.use_certificate_file` sets the certificate (given as a\n        `bytes` filename) which will be used to identify connections created\n        using the context.\n        \"\"\"\n        filename = tmpfile + NON_ASCII.encode(getfilesystemencoding())\n        self._use_certificate_file_test(filename)\n\n    def test_use_certificate_file_unicode(self, tmpfile):\n        \"\"\"\n        `Context.use_certificate_file` sets the certificate (given as a\n        `bytes` filename) which will be used to identify connections created\n        using the context.\n        \"\"\"\n        filename = tmpfile.decode(getfilesystemencoding()) + NON_ASCII\n        self._use_certificate_file_test(filename)\n\n    def test_check_privatekey_valid(self):\n        \"\"\"\n        `Context.check_privatekey` returns `None` if the `Context` instance\n        has been configured to use a matched key and certificate pair.\n        \"\"\"\n        key = load_privatekey(FILETYPE_PEM, client_key_pem)\n        cert = load_certificate(FILETYPE_PEM, client_cert_pem)\n        context = Context(TLSv1_METHOD)\n        context.use_privatekey(key)\n        context.use_certificate(cert)\n        assert None is context.check_privatekey()\n\n    def test_check_privatekey_invalid(self):\n        \"\"\"\n        `Context.check_privatekey` raises `Error` if the `Context` instance\n        has been configured to use a key and certificate pair which don't\n        relate to each other.\n        \"\"\"\n        key = load_privatekey(FILETYPE_PEM, client_key_pem)\n        cert = load_certificate(FILETYPE_PEM, server_cert_pem)\n        context = Context(TLSv1_METHOD)\n        context.use_privatekey(key)\n        context.use_certificate(cert)\n        with pytest.raises(Error):\n            context.check_privatekey()\n\n    def test_app_data(self):\n        \"\"\"\n        `Context.set_app_data` stores an object for later retrieval\n        using `Context.get_app_data`.\n        \"\"\"\n        app_data = object()\n        context = Context(TLSv1_METHOD)\n        context.set_app_data(app_data)\n        assert context.get_app_data() is app_data\n\n    def test_set_options_wrong_args(self):\n        \"\"\"\n        `Context.set_options` raises `TypeError` if called with\n        a non-`int` argument.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        with pytest.raises(TypeError):\n            context.set_options(None)\n\n    def test_set_options(self):\n        \"\"\"\n        `Context.set_options` returns the new options value.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        options = context.set_options(OP_NO_SSLv2)\n        assert options & OP_NO_SSLv2 == OP_NO_SSLv2\n\n    def test_set_mode_wrong_args(self):\n        \"\"\"\n        `Context.set_mode` raises `TypeError` if called with\n        a non-`int` argument.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        with pytest.raises(TypeError):\n            context.set_mode(None)\n\n    def test_set_mode(self):\n        \"\"\"\n        `Context.set_mode` accepts a mode bitvector and returns the\n        newly set mode.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        assert MODE_RELEASE_BUFFERS & context.set_mode(MODE_RELEASE_BUFFERS)\n\n    def test_set_timeout_wrong_args(self):\n        \"\"\"\n        `Context.set_timeout` raises `TypeError` if called with\n        a non-`int` argument.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        with pytest.raises(TypeError):\n            context.set_timeout(None)\n\n    def test_timeout(self):\n        \"\"\"\n        `Context.set_timeout` sets the session timeout for all connections\n        created using the context object. `Context.get_timeout` retrieves\n        this value.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        context.set_timeout(1234)\n        assert context.get_timeout() == 1234\n\n    def test_set_verify_depth_wrong_args(self):\n        \"\"\"\n        `Context.set_verify_depth` raises `TypeError` if called with a\n        non-`int` argument.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        with pytest.raises(TypeError):\n            context.set_verify_depth(None)\n\n    def test_verify_depth(self):\n        \"\"\"\n        `Context.set_verify_depth` sets the number of certificates in\n        a chain to follow before giving up.  The value can be retrieved with\n        `Context.get_verify_depth`.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        context.set_verify_depth(11)\n        assert context.get_verify_depth() == 11\n\n    def _write_encrypted_pem(self, passphrase, tmpfile):\n        \"\"\"\n        Write a new private key out to a new file, encrypted using the given\n        passphrase.  Return the path to the new file.\n        \"\"\"\n        key = PKey()\n        key.generate_key(TYPE_RSA, 512)\n        pem = dump_privatekey(FILETYPE_PEM, key, \"blowfish\", passphrase)\n        with open(tmpfile, 'w') as fObj:\n            fObj.write(pem.decode('ascii'))\n        return tmpfile\n\n    def test_set_passwd_cb_wrong_args(self):\n        \"\"\"\n        `Context.set_passwd_cb` raises `TypeError` if called with a\n        non-callable first argument.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        with pytest.raises(TypeError):\n            context.set_passwd_cb(None)\n\n    def test_set_passwd_cb(self, tmpfile):\n        \"\"\"\n        `Context.set_passwd_cb` accepts a callable which will be invoked when\n        a private key is loaded from an encrypted PEM.\n        \"\"\"\n        passphrase = b\"foobar\"\n        pemFile = self._write_encrypted_pem(passphrase, tmpfile)\n        calledWith = []\n\n        def passphraseCallback(maxlen, verify, extra):\n            calledWith.append((maxlen, verify, extra))\n            return passphrase\n        context = Context(TLSv1_METHOD)\n        context.set_passwd_cb(passphraseCallback)\n        context.use_privatekey_file(pemFile)\n        assert len(calledWith) == 1\n        assert isinstance(calledWith[0][0], int)\n        assert isinstance(calledWith[0][1], int)\n        assert calledWith[0][2] is None\n\n    def test_passwd_callback_exception(self, tmpfile):\n        \"\"\"\n        `Context.use_privatekey_file` propagates any exception raised\n        by the passphrase callback.\n        \"\"\"\n        pemFile = self._write_encrypted_pem(b\"monkeys are nice\", tmpfile)\n\n        def passphraseCallback(maxlen, verify, extra):\n            raise RuntimeError(\"Sorry, I am a fail.\")\n\n        context = Context(TLSv1_METHOD)\n        context.set_passwd_cb(passphraseCallback)\n        with pytest.raises(RuntimeError):\n            context.use_privatekey_file(pemFile)\n\n    def test_passwd_callback_false(self, tmpfile):\n        \"\"\"\n        `Context.use_privatekey_file` raises `OpenSSL.SSL.Error` if the\n        passphrase callback returns a false value.\n        \"\"\"\n        pemFile = self._write_encrypted_pem(b\"monkeys are nice\", tmpfile)\n\n        def passphraseCallback(maxlen, verify, extra):\n            return b\"\"\n\n        context = Context(TLSv1_METHOD)\n        context.set_passwd_cb(passphraseCallback)\n        with pytest.raises(Error):\n            context.use_privatekey_file(pemFile)\n\n    def test_passwd_callback_non_string(self, tmpfile):\n        \"\"\"\n        `Context.use_privatekey_file` raises `OpenSSL.SSL.Error` if the\n        passphrase callback returns a true non-string value.\n        \"\"\"\n        pemFile = self._write_encrypted_pem(b\"monkeys are nice\", tmpfile)\n\n        def passphraseCallback(maxlen, verify, extra):\n            return 10\n\n        context = Context(TLSv1_METHOD)\n        context.set_passwd_cb(passphraseCallback)\n        # TODO: Surely this is the wrong error?\n        with pytest.raises(ValueError):\n            context.use_privatekey_file(pemFile)\n\n    def test_passwd_callback_too_long(self, tmpfile):\n        \"\"\"\n        If the passphrase returned by the passphrase callback returns a string\n        longer than the indicated maximum length, it is truncated.\n        \"\"\"\n        # A priori knowledge!\n        passphrase = b\"x\" * 1024\n        pemFile = self._write_encrypted_pem(passphrase, tmpfile)\n\n        def passphraseCallback(maxlen, verify, extra):\n            assert maxlen == 1024\n            return passphrase + b\"y\"\n\n        context = Context(TLSv1_METHOD)\n        context.set_passwd_cb(passphraseCallback)\n        # This shall succeed because the truncated result is the correct\n        # passphrase.\n        context.use_privatekey_file(pemFile)\n\n    def test_set_info_callback(self):\n        \"\"\"\n        `Context.set_info_callback` accepts a callable which will be\n        invoked when certain information about an SSL connection is available.\n        \"\"\"\n        (server, client) = socket_pair()\n\n        clientSSL = Connection(Context(TLSv1_METHOD), client)\n        clientSSL.set_connect_state()\n\n        called = []\n\n        def info(conn, where, ret):\n            called.append((conn, where, ret))\n        context = Context(TLSv1_METHOD)\n        context.set_info_callback(info)\n        context.use_certificate(\n            load_certificate(FILETYPE_PEM, cleartextCertificatePEM))\n        context.use_privatekey(\n            load_privatekey(FILETYPE_PEM, cleartextPrivateKeyPEM))\n\n        serverSSL = Connection(context, server)\n        serverSSL.set_accept_state()\n\n        handshake(clientSSL, serverSSL)\n\n        # The callback must always be called with a Connection instance as the\n        # first argument.  It would probably be better to split this into\n        # separate tests for client and server side info callbacks so we could\n        # assert it is called with the right Connection instance.  It would\n        # also be good to assert *something* about `where` and `ret`.\n        notConnections = [\n            conn for (conn, where, ret) in called\n            if not isinstance(conn, Connection)]\n        assert [] == notConnections, (\n            \"Some info callback arguments were not Connection instances.\")\n\n    def _load_verify_locations_test(self, *args):\n        \"\"\"\n        Create a client context which will verify the peer certificate and call\n        its `load_verify_locations` method with the given arguments.\n        Then connect it to a server and ensure that the handshake succeeds.\n        \"\"\"\n        (server, client) = socket_pair()\n\n        clientContext = Context(TLSv1_METHOD)\n        clientContext.load_verify_locations(*args)\n        # Require that the server certificate verify properly or the\n        # connection will fail.\n        clientContext.set_verify(\n            VERIFY_PEER,\n            lambda conn, cert, errno, depth, preverify_ok: preverify_ok)\n\n        clientSSL = Connection(clientContext, client)\n        clientSSL.set_connect_state()\n\n        serverContext = Context(TLSv1_METHOD)\n        serverContext.use_certificate(\n            load_certificate(FILETYPE_PEM, cleartextCertificatePEM))\n        serverContext.use_privatekey(\n            load_privatekey(FILETYPE_PEM, cleartextPrivateKeyPEM))\n\n        serverSSL = Connection(serverContext, server)\n        serverSSL.set_accept_state()\n\n        # Without load_verify_locations above, the handshake\n        # will fail:\n        # Error: [('SSL routines', 'SSL3_GET_SERVER_CERTIFICATE',\n        #          'certificate verify failed')]\n        handshake(clientSSL, serverSSL)\n\n        cert = clientSSL.get_peer_certificate()\n        assert cert.get_subject().CN == 'Testing Root CA'\n\n    def _load_verify_cafile(self, cafile):\n        \"\"\"\n        Verify that if path to a file containing a certificate is passed to\n        `Context.load_verify_locations` for the ``cafile`` parameter, that\n        certificate is used as a trust root for the purposes of verifying\n        connections created using that `Context`.\n        \"\"\"\n        with open(cafile, 'w') as fObj:\n            fObj.write(cleartextCertificatePEM.decode('ascii'))\n\n        self._load_verify_locations_test(cafile)\n\n    def test_load_verify_bytes_cafile(self, tmpfile):\n        \"\"\"\n        `Context.load_verify_locations` accepts a file name as a `bytes`\n        instance and uses the certificates within for verification purposes.\n        \"\"\"\n        cafile = tmpfile + NON_ASCII.encode(getfilesystemencoding())\n        self._load_verify_cafile(cafile)\n\n    def test_load_verify_unicode_cafile(self, tmpfile):\n        \"\"\"\n        `Context.load_verify_locations` accepts a file name as a `unicode`\n        instance and uses the certificates within for verification purposes.\n        \"\"\"\n        self._load_verify_cafile(\n            tmpfile.decode(getfilesystemencoding()) + NON_ASCII\n        )\n\n    def test_load_verify_invalid_file(self, tmpfile):\n        \"\"\"\n        `Context.load_verify_locations` raises `Error` when passed a\n        non-existent cafile.\n        \"\"\"\n        clientContext = Context(TLSv1_METHOD)\n        with pytest.raises(Error):\n            clientContext.load_verify_locations(tmpfile)\n\n    def _load_verify_directory_locations_capath(self, capath):\n        \"\"\"\n        Verify that if path to a directory containing certificate files is\n        passed to ``Context.load_verify_locations`` for the ``capath``\n        parameter, those certificates are used as trust roots for the purposes\n        of verifying connections created using that ``Context``.\n        \"\"\"\n        makedirs(capath)\n        # Hash values computed manually with c_rehash to avoid depending on\n        # c_rehash in the test suite.  One is from OpenSSL 0.9.8, the other\n        # from OpenSSL 1.0.0.\n        for name in [b'c7adac82.0', b'c3705638.0']:\n            cafile = join_bytes_or_unicode(capath, name)\n            with open(cafile, 'w') as fObj:\n                fObj.write(cleartextCertificatePEM.decode('ascii'))\n\n        self._load_verify_locations_test(None, capath)\n\n    def test_load_verify_directory_bytes_capath(self, tmpfile):\n        \"\"\"\n        `Context.load_verify_locations` accepts a directory name as a `bytes`\n        instance and uses the certificates within for verification purposes.\n        \"\"\"\n        self._load_verify_directory_locations_capath(\n            tmpfile + NON_ASCII.encode(getfilesystemencoding())\n        )\n\n    def test_load_verify_directory_unicode_capath(self, tmpfile):\n        \"\"\"\n        `Context.load_verify_locations` accepts a directory name as a `unicode`\n        instance and uses the certificates within for verification purposes.\n        \"\"\"\n        self._load_verify_directory_locations_capath(\n            tmpfile.decode(getfilesystemencoding()) + NON_ASCII\n        )\n\n    def test_load_verify_locations_wrong_args(self):\n        \"\"\"\n        `Context.load_verify_locations` raises `TypeError` if with non-`str`\n        arguments.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        with pytest.raises(TypeError):\n            context.load_verify_locations(object())\n        with pytest.raises(TypeError):\n            context.load_verify_locations(object(), object())\n\n    @pytest.mark.skipif(\n        not platform.startswith(\"linux\"),\n        reason=\"Loading fallback paths is a linux-specific behavior to \"\n        \"accommodate pyca/cryptography manylinux1 wheels\"\n    )\n    def test_fallback_default_verify_paths(self, monkeypatch):\n        \"\"\"\n        Test that we load certificates successfully on linux from the fallback\n        path. To do this we set the _CRYPTOGRAPHY_MANYLINUX1_CA_FILE and\n        _CRYPTOGRAPHY_MANYLINUX1_CA_DIR vars to be equal to whatever the\n        current OpenSSL default is and we disable\n        SSL_CTX_SET_default_verify_paths so that it can't find certs unless\n        it loads via fallback.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        monkeypatch.setattr(\n            _lib, \"SSL_CTX_set_default_verify_paths\", lambda x: 1\n        )\n        monkeypatch.setattr(\n            SSL,\n            \"_CRYPTOGRAPHY_MANYLINUX1_CA_FILE\",\n            _ffi.string(_lib.X509_get_default_cert_file())\n        )\n        monkeypatch.setattr(\n            SSL,\n            \"_CRYPTOGRAPHY_MANYLINUX1_CA_DIR\",\n            _ffi.string(_lib.X509_get_default_cert_dir())\n        )\n        context.set_default_verify_paths()\n        store = context.get_cert_store()\n        sk_obj = _lib.X509_STORE_get0_objects(store._store)\n        assert sk_obj != _ffi.NULL\n        num = _lib.sk_X509_OBJECT_num(sk_obj)\n        assert num != 0\n\n    def test_check_env_vars(self, monkeypatch):\n        \"\"\"\n        Test that we return True/False appropriately if the env vars are set.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        dir_var = \"CUSTOM_DIR_VAR\"\n        file_var = \"CUSTOM_FILE_VAR\"\n        assert context._check_env_vars_set(dir_var, file_var) is False\n        monkeypatch.setenv(dir_var, \"value\")\n        monkeypatch.setenv(file_var, \"value\")\n        assert context._check_env_vars_set(dir_var, file_var) is True\n        assert context._check_env_vars_set(dir_var, file_var) is True\n\n    def test_verify_no_fallback_if_env_vars_set(self, monkeypatch):\n        \"\"\"\n        Test that we don't use the fallback path if env vars are set.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        monkeypatch.setattr(\n            _lib, \"SSL_CTX_set_default_verify_paths\", lambda x: 1\n        )\n        dir_env_var = _ffi.string(\n            _lib.X509_get_default_cert_dir_env()\n        ).decode(\"ascii\")\n        file_env_var = _ffi.string(\n            _lib.X509_get_default_cert_file_env()\n        ).decode(\"ascii\")\n        monkeypatch.setenv(dir_env_var, \"value\")\n        monkeypatch.setenv(file_env_var, \"value\")\n        context.set_default_verify_paths()\n\n        monkeypatch.setattr(\n            context,\n            \"_fallback_default_verify_paths\",\n            raiser(SystemError)\n        )\n        context.set_default_verify_paths()\n\n    @pytest.mark.skipif(\n        platform == \"win32\",\n        reason=\"set_default_verify_paths appears not to work on Windows.  \"\n        \"See LP#404343 and LP#404344.\"\n    )\n    def test_set_default_verify_paths(self):\n        \"\"\"\n        `Context.set_default_verify_paths` causes the platform-specific CA\n        certificate locations to be used for verification purposes.\n        \"\"\"\n        # Testing this requires a server with a certificate signed by one\n        # of the CAs in the platform CA location.  Getting one of those\n        # costs money.  Fortunately (or unfortunately, depending on your\n        # perspective), it's easy to think of a public server on the\n        # internet which has such a certificate.  Connecting to the network\n        # in a unit test is bad, but it's the only way I can think of to\n        # really test this. -exarkun\n        context = Context(SSLv23_METHOD)\n        context.set_default_verify_paths()\n        context.set_verify(\n            VERIFY_PEER,\n            lambda conn, cert, errno, depth, preverify_ok: preverify_ok)\n\n        client = socket_any_family()\n        client.connect((\"encrypted.google.com\", 443))\n        clientSSL = Connection(context, client)\n        clientSSL.set_connect_state()\n        clientSSL.set_tlsext_host_name(b\"encrypted.google.com\")\n        clientSSL.do_handshake()\n        clientSSL.send(b\"GET / HTTP/1.0\\r\\n\\r\\n\")\n        assert clientSSL.recv(1024)\n\n    def test_fallback_path_is_not_file_or_dir(self):\n        \"\"\"\n        Test that when passed empty arrays or paths that do not exist no\n        errors are raised.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        context._fallback_default_verify_paths([], [])\n        context._fallback_default_verify_paths(\n            [\"/not/a/file\"], [\"/not/a/dir\"]\n        )\n\n    def test_add_extra_chain_cert_invalid_cert(self):\n        \"\"\"\n        `Context.add_extra_chain_cert` raises `TypeError` if called with an\n        object which is not an instance of `X509`.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        with pytest.raises(TypeError):\n            context.add_extra_chain_cert(object())\n\n    def _handshake_test(self, serverContext, clientContext):\n        \"\"\"\n        Verify that a client and server created with the given contexts can\n        successfully handshake and communicate.\n        \"\"\"\n        serverSocket, clientSocket = socket_pair()\n\n        server = Connection(serverContext, serverSocket)\n        server.set_accept_state()\n\n        client = Connection(clientContext, clientSocket)\n        client.set_connect_state()\n\n        # Make them talk to each other.\n        # interact_in_memory(client, server)\n        for _ in range(3):\n            for s in [client, server]:\n                try:\n                    s.do_handshake()\n                except WantReadError:\n                    pass\n\n    def test_set_verify_callback_connection_argument(self):\n        \"\"\"\n        The first argument passed to the verify callback is the\n        `Connection` instance for which verification is taking place.\n        \"\"\"\n        serverContext = Context(TLSv1_METHOD)\n        serverContext.use_privatekey(\n            load_privatekey(FILETYPE_PEM, cleartextPrivateKeyPEM))\n        serverContext.use_certificate(\n            load_certificate(FILETYPE_PEM, cleartextCertificatePEM))\n        serverConnection = Connection(serverContext, None)\n\n        class VerifyCallback(object):\n            def callback(self, connection, *args):\n                self.connection = connection\n                return 1\n\n        verify = VerifyCallback()\n        clientContext = Context(TLSv1_METHOD)\n        clientContext.set_verify(VERIFY_PEER, verify.callback)\n        clientConnection = Connection(clientContext, None)\n        clientConnection.set_connect_state()\n\n        handshake_in_memory(clientConnection, serverConnection)\n\n        assert verify.connection is clientConnection\n\n    def test_x509_in_verify_works(self):\n        \"\"\"\n        We had a bug where the X509 cert instantiated in the callback wrapper\n        didn't __init__ so it was missing objects needed when calling\n        get_subject. This test sets up a handshake where we call get_subject\n        on the cert provided to the verify callback.\n        \"\"\"\n        serverContext = Context(TLSv1_METHOD)\n        serverContext.use_privatekey(\n            load_privatekey(FILETYPE_PEM, cleartextPrivateKeyPEM))\n        serverContext.use_certificate(\n            load_certificate(FILETYPE_PEM, cleartextCertificatePEM))\n        serverConnection = Connection(serverContext, None)\n\n        def verify_cb_get_subject(conn, cert, errnum, depth, ok):\n            assert cert.get_subject()\n            return 1\n\n        clientContext = Context(TLSv1_METHOD)\n        clientContext.set_verify(VERIFY_PEER, verify_cb_get_subject)\n        clientConnection = Connection(clientContext, None)\n        clientConnection.set_connect_state()\n\n        handshake_in_memory(clientConnection, serverConnection)\n\n    def test_set_verify_callback_exception(self):\n        \"\"\"\n        If the verify callback passed to `Context.set_verify` raises an\n        exception, verification fails and the exception is propagated to the\n        caller of `Connection.do_handshake`.\n        \"\"\"\n        serverContext = Context(TLSv1_2_METHOD)\n        serverContext.use_privatekey(\n            load_privatekey(FILETYPE_PEM, cleartextPrivateKeyPEM))\n        serverContext.use_certificate(\n            load_certificate(FILETYPE_PEM, cleartextCertificatePEM))\n\n        clientContext = Context(TLSv1_2_METHOD)\n\n        def verify_callback(*args):\n            raise Exception(\"silly verify failure\")\n        clientContext.set_verify(VERIFY_PEER, verify_callback)\n\n        with pytest.raises(Exception) as exc:\n            self._handshake_test(serverContext, clientContext)\n\n        assert \"silly verify failure\" == str(exc.value)\n\n    def test_add_extra_chain_cert(self, tmpdir):\n        \"\"\"\n        `Context.add_extra_chain_cert` accepts an `X509`\n        instance to add to the certificate chain.\n\n        See `_create_certificate_chain` for the details of the\n        certificate chain tested.\n\n        The chain is tested by starting a server with scert and connecting\n        to it with a client which trusts cacert and requires verification to\n        succeed.\n        \"\"\"\n        chain = _create_certificate_chain()\n        [(cakey, cacert), (ikey, icert), (skey, scert)] = chain\n\n        # Dump the CA certificate to a file because that's the only way to load\n        # it as a trusted CA in the client context.\n        for cert, name in [(cacert, 'ca.pem'),\n                           (icert, 'i.pem'),\n                           (scert, 's.pem')]:\n            with tmpdir.join(name).open('w') as f:\n                f.write(dump_certificate(FILETYPE_PEM, cert).decode('ascii'))\n\n        for key, name in [(cakey, 'ca.key'),\n                          (ikey, 'i.key'),\n                          (skey, 's.key')]:\n            with tmpdir.join(name).open('w') as f:\n                f.write(dump_privatekey(FILETYPE_PEM, key).decode('ascii'))\n\n        # Create the server context\n        serverContext = Context(TLSv1_METHOD)\n        serverContext.use_privatekey(skey)\n        serverContext.use_certificate(scert)\n        # The client already has cacert, we only need to give them icert.\n        serverContext.add_extra_chain_cert(icert)\n\n        # Create the client\n        clientContext = Context(TLSv1_METHOD)\n        clientContext.set_verify(\n            VERIFY_PEER | VERIFY_FAIL_IF_NO_PEER_CERT, verify_cb)\n        clientContext.load_verify_locations(str(tmpdir.join(\"ca.pem\")))\n\n        # Try it out.\n        self._handshake_test(serverContext, clientContext)\n\n    def _use_certificate_chain_file_test(self, certdir):\n        \"\"\"\n        Verify that `Context.use_certificate_chain_file` reads a\n        certificate chain from a specified file.\n\n        The chain is tested by starting a server with scert and connecting to\n        it with a client which trusts cacert and requires verification to\n        succeed.\n        \"\"\"\n        chain = _create_certificate_chain()\n        [(cakey, cacert), (ikey, icert), (skey, scert)] = chain\n\n        makedirs(certdir)\n\n        chainFile = join_bytes_or_unicode(certdir, \"chain.pem\")\n        caFile = join_bytes_or_unicode(certdir, \"ca.pem\")\n\n        # Write out the chain file.\n        with open(chainFile, 'wb') as fObj:\n            # Most specific to least general.\n            fObj.write(dump_certificate(FILETYPE_PEM, scert))\n            fObj.write(dump_certificate(FILETYPE_PEM, icert))\n            fObj.write(dump_certificate(FILETYPE_PEM, cacert))\n\n        with open(caFile, 'w') as fObj:\n            fObj.write(dump_certificate(FILETYPE_PEM, cacert).decode('ascii'))\n\n        serverContext = Context(TLSv1_METHOD)\n        serverContext.use_certificate_chain_file(chainFile)\n        serverContext.use_privatekey(skey)\n\n        clientContext = Context(TLSv1_METHOD)\n        clientContext.set_verify(\n            VERIFY_PEER | VERIFY_FAIL_IF_NO_PEER_CERT, verify_cb)\n        clientContext.load_verify_locations(caFile)\n\n        self._handshake_test(serverContext, clientContext)\n\n    def test_use_certificate_chain_file_bytes(self, tmpfile):\n        \"\"\"\n        ``Context.use_certificate_chain_file`` accepts the name of a file (as\n        an instance of ``bytes``) to specify additional certificates to use to\n        construct and verify a trust chain.\n        \"\"\"\n        self._use_certificate_chain_file_test(\n            tmpfile + NON_ASCII.encode(getfilesystemencoding())\n        )\n\n    def test_use_certificate_chain_file_unicode(self, tmpfile):\n        \"\"\"\n        ``Context.use_certificate_chain_file`` accepts the name of a file (as\n        an instance of ``unicode``) to specify additional certificates to use\n        to construct and verify a trust chain.\n        \"\"\"\n        self._use_certificate_chain_file_test(\n            tmpfile.decode(getfilesystemencoding()) + NON_ASCII\n        )\n\n    def test_use_certificate_chain_file_wrong_args(self):\n        \"\"\"\n        `Context.use_certificate_chain_file` raises `TypeError` if passed a\n        non-byte string single argument.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        with pytest.raises(TypeError):\n            context.use_certificate_chain_file(object())\n\n    def test_use_certificate_chain_file_missing_file(self, tmpfile):\n        \"\"\"\n        `Context.use_certificate_chain_file` raises `OpenSSL.SSL.Error` when\n        passed a bad chain file name (for example, the name of a file which\n        does not exist).\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        with pytest.raises(Error):\n            context.use_certificate_chain_file(tmpfile)\n\n    def test_set_verify_mode(self):\n        \"\"\"\n        `Context.get_verify_mode` returns the verify mode flags previously\n        passed to `Context.set_verify`.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        assert context.get_verify_mode() == 0\n        context.set_verify(\n            VERIFY_PEER | VERIFY_CLIENT_ONCE, lambda *args: None)\n        assert context.get_verify_mode() == (VERIFY_PEER | VERIFY_CLIENT_ONCE)\n\n    @pytest.mark.parametrize('mode', [None, 1.0, object(), 'mode'])\n    def test_set_verify_wrong_mode_arg(self, mode):\n        \"\"\"\n        `Context.set_verify` raises `TypeError` if the first argument is\n        not an integer.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        with pytest.raises(TypeError):\n            context.set_verify(mode=mode, callback=lambda *args: None)\n\n    @pytest.mark.parametrize('callback', [None, 1.0, 'mode', ('foo', 'bar')])\n    def test_set_verify_wrong_callable_arg(self, callback):\n        \"\"\"\n        `Context.set_verify` raises `TypeError` if the second argument\n        is not callable.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        with pytest.raises(TypeError):\n            context.set_verify(mode=VERIFY_PEER, callback=callback)\n\n    def test_load_tmp_dh_wrong_args(self):\n        \"\"\"\n        `Context.load_tmp_dh` raises `TypeError` if called with a\n        non-`str` argument.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        with pytest.raises(TypeError):\n            context.load_tmp_dh(object())\n\n    def test_load_tmp_dh_missing_file(self):\n        \"\"\"\n        `Context.load_tmp_dh` raises `OpenSSL.SSL.Error` if the\n        specified file does not exist.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        with pytest.raises(Error):\n            context.load_tmp_dh(b\"hello\")\n\n    def _load_tmp_dh_test(self, dhfilename):\n        \"\"\"\n        Verify that calling ``Context.load_tmp_dh`` with the given filename\n        does not raise an exception.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        with open(dhfilename, \"w\") as dhfile:\n            dhfile.write(dhparam)\n\n        context.load_tmp_dh(dhfilename)\n        # XXX What should I assert here? -exarkun\n\n    def test_load_tmp_dh_bytes(self, tmpfile):\n        \"\"\"\n        `Context.load_tmp_dh` loads Diffie-Hellman parameters from the\n        specified file (given as ``bytes``).\n        \"\"\"\n        self._load_tmp_dh_test(\n            tmpfile + NON_ASCII.encode(getfilesystemencoding()),\n        )\n\n    def test_load_tmp_dh_unicode(self, tmpfile):\n        \"\"\"\n        `Context.load_tmp_dh` loads Diffie-Hellman parameters from the\n        specified file (given as ``unicode``).\n        \"\"\"\n        self._load_tmp_dh_test(\n            tmpfile.decode(getfilesystemencoding()) + NON_ASCII,\n        )\n\n    def test_set_tmp_ecdh(self):\n        \"\"\"\n        `Context.set_tmp_ecdh` sets the elliptic curve for Diffie-Hellman to\n        the specified curve.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        for curve in get_elliptic_curves():\n            if curve.name.startswith(u\"Oakley-\"):\n                # Setting Oakley-EC2N-4 and Oakley-EC2N-3 adds\n                # ('bignum routines', 'BN_mod_inverse', 'no inverse') to the\n                # error queue on OpenSSL 1.0.2.\n                continue\n            # The only easily \"assertable\" thing is that it does not raise an\n            # exception.\n            context.set_tmp_ecdh(curve)\n\n    def test_set_session_cache_mode_wrong_args(self):\n        \"\"\"\n        `Context.set_session_cache_mode` raises `TypeError` if called with\n        a non-integer argument.\n        called with other than one integer argument.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        with pytest.raises(TypeError):\n            context.set_session_cache_mode(object())\n\n    def test_session_cache_mode(self):\n        \"\"\"\n        `Context.set_session_cache_mode` specifies how sessions are cached.\n        The setting can be retrieved via `Context.get_session_cache_mode`.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        context.set_session_cache_mode(SESS_CACHE_OFF)\n        off = context.set_session_cache_mode(SESS_CACHE_BOTH)\n        assert SESS_CACHE_OFF == off\n        assert SESS_CACHE_BOTH == context.get_session_cache_mode()\n\n    def test_get_cert_store(self):\n        \"\"\"\n        `Context.get_cert_store` returns a `X509Store` instance.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        store = context.get_cert_store()\n        assert isinstance(store, X509Store)\n\n    def test_set_tlsext_use_srtp_not_bytes(self):\n        \"\"\"\n        `Context.set_tlsext_use_srtp' enables negotiating SRTP keying material.\n\n        It raises a TypeError if the list of profiles is not a byte string.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        with pytest.raises(TypeError):\n            context.set_tlsext_use_srtp(text_type('SRTP_AES128_CM_SHA1_80'))\n\n    def test_set_tlsext_use_srtp_invalid_profile(self):\n        \"\"\"\n        `Context.set_tlsext_use_srtp' enables negotiating SRTP keying material.\n\n        It raises an Error if the call to OpenSSL fails.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        with pytest.raises(Error):\n            context.set_tlsext_use_srtp(b'SRTP_BOGUS')\n\n    def test_set_tlsext_use_srtp_valid(self):\n        \"\"\"\n        `Context.set_tlsext_use_srtp' enables negotiating SRTP keying material.\n\n        It does not return anything.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        assert context.set_tlsext_use_srtp(b'SRTP_AES128_CM_SHA1_80') is None\n\n\nclass TestServerNameCallback(object):\n    \"\"\"\n    Tests for `Context.set_tlsext_servername_callback` and its\n    interaction with `Connection`.\n    \"\"\"\n    def test_old_callback_forgotten(self):\n        \"\"\"\n        If `Context.set_tlsext_servername_callback` is used to specify\n        a new callback, the one it replaces is dereferenced.\n        \"\"\"\n        def callback(connection):  # pragma: no cover\n            pass\n\n        def replacement(connection):  # pragma: no cover\n            pass\n\n        context = Context(TLSv1_METHOD)\n        context.set_tlsext_servername_callback(callback)\n\n        tracker = ref(callback)\n        del callback\n\n        context.set_tlsext_servername_callback(replacement)\n\n        # One run of the garbage collector happens to work on CPython.  PyPy\n        # doesn't collect the underlying object until a second run for whatever\n        # reason.  That's fine, it still demonstrates our code has properly\n        # dropped the reference.\n        collect()\n        collect()\n\n        callback = tracker()\n        if callback is not None:\n            referrers = get_referrers(callback)\n            if len(referrers) > 1:  # pragma: nocover\n                pytest.fail(\"Some references remain: %r\" % (referrers,))\n\n    def test_no_servername(self):\n        \"\"\"\n        When a client specifies no server name, the callback passed to\n        `Context.set_tlsext_servername_callback` is invoked and the\n        result of `Connection.get_servername` is `None`.\n        \"\"\"\n        args = []\n\n        def servername(conn):\n            args.append((conn, conn.get_servername()))\n        context = Context(TLSv1_METHOD)\n        context.set_tlsext_servername_callback(servername)\n\n        # Lose our reference to it.  The Context is responsible for keeping it\n        # alive now.\n        del servername\n        collect()\n\n        # Necessary to actually accept the connection\n        context.use_privatekey(load_privatekey(FILETYPE_PEM, server_key_pem))\n        context.use_certificate(\n            load_certificate(FILETYPE_PEM, server_cert_pem))\n\n        # Do a little connection to trigger the logic\n        server = Connection(context, None)\n        server.set_accept_state()\n\n        client = Connection(Context(TLSv1_METHOD), None)\n        client.set_connect_state()\n\n        interact_in_memory(server, client)\n\n        assert args == [(server, None)]\n\n    def test_servername(self):\n        \"\"\"\n        When a client specifies a server name in its hello message, the\n        callback passed to `Contexts.set_tlsext_servername_callback` is\n        invoked and the result of `Connection.get_servername` is that\n        server name.\n        \"\"\"\n        args = []\n\n        def servername(conn):\n            args.append((conn, conn.get_servername()))\n        context = Context(TLSv1_METHOD)\n        context.set_tlsext_servername_callback(servername)\n\n        # Necessary to actually accept the connection\n        context.use_privatekey(load_privatekey(FILETYPE_PEM, server_key_pem))\n        context.use_certificate(\n            load_certificate(FILETYPE_PEM, server_cert_pem))\n\n        # Do a little connection to trigger the logic\n        server = Connection(context, None)\n        server.set_accept_state()\n\n        client = Connection(Context(TLSv1_METHOD), None)\n        client.set_connect_state()\n        client.set_tlsext_host_name(b\"foo1.example.com\")\n\n        interact_in_memory(server, client)\n\n        assert args == [(server, b\"foo1.example.com\")]\n\n\n@pytest.mark.skipif(\n    not _lib.Cryptography_HAS_NEXTPROTONEG, reason=\"NPN is not available\"\n)\nclass TestNextProtoNegotiation(object):\n    \"\"\"\n    Test for Next Protocol Negotiation in PyOpenSSL.\n    \"\"\"\n    def test_npn_success(self):\n        \"\"\"\n        Tests that clients and servers that agree on the negotiated next\n        protocol can correct establish a connection, and that the agreed\n        protocol is reported by the connections.\n        \"\"\"\n        advertise_args = []\n        select_args = []\n\n        def advertise(conn):\n            advertise_args.append((conn,))\n            return [b'http/1.1', b'spdy/2']\n\n        def select(conn, options):\n            select_args.append((conn, options))\n            return b'spdy/2'\n\n        server_context = Context(TLSv1_METHOD)\n        server_context.set_npn_advertise_callback(advertise)\n\n        client_context = Context(TLSv1_METHOD)\n        client_context.set_npn_select_callback(select)\n\n        # Necessary to actually accept the connection\n        server_context.use_privatekey(\n            load_privatekey(FILETYPE_PEM, server_key_pem))\n        server_context.use_certificate(\n            load_certificate(FILETYPE_PEM, server_cert_pem))\n\n        # Do a little connection to trigger the logic\n        server = Connection(server_context, None)\n        server.set_accept_state()\n\n        client = Connection(client_context, None)\n        client.set_connect_state()\n\n        interact_in_memory(server, client)\n\n        assert advertise_args == [(server,)]\n        assert select_args == [(client, [b'http/1.1', b'spdy/2'])]\n\n        assert server.get_next_proto_negotiated() == b'spdy/2'\n        assert client.get_next_proto_negotiated() == b'spdy/2'\n\n    def test_npn_client_fail(self):\n        \"\"\"\n        Tests that when clients and servers cannot agree on what protocol\n        to use next that the TLS connection does not get established.\n        \"\"\"\n        advertise_args = []\n        select_args = []\n\n        def advertise(conn):\n            advertise_args.append((conn,))\n            return [b'http/1.1', b'spdy/2']\n\n        def select(conn, options):\n            select_args.append((conn, options))\n            return b''\n\n        server_context = Context(TLSv1_METHOD)\n        server_context.set_npn_advertise_callback(advertise)\n\n        client_context = Context(TLSv1_METHOD)\n        client_context.set_npn_select_callback(select)\n\n        # Necessary to actually accept the connection\n        server_context.use_privatekey(\n            load_privatekey(FILETYPE_PEM, server_key_pem))\n        server_context.use_certificate(\n            load_certificate(FILETYPE_PEM, server_cert_pem))\n\n        # Do a little connection to trigger the logic\n        server = Connection(server_context, None)\n        server.set_accept_state()\n\n        client = Connection(client_context, None)\n        client.set_connect_state()\n\n        # If the client doesn't return anything, the connection will fail.\n        with pytest.raises(Error):\n            interact_in_memory(server, client)\n\n        assert advertise_args == [(server,)]\n        assert select_args == [(client, [b'http/1.1', b'spdy/2'])]\n\n    def test_npn_select_error(self):\n        \"\"\"\n        Test that we can handle exceptions in the select callback. If\n        select fails it should be fatal to the connection.\n        \"\"\"\n        advertise_args = []\n\n        def advertise(conn):\n            advertise_args.append((conn,))\n            return [b'http/1.1', b'spdy/2']\n\n        def select(conn, options):\n            raise TypeError\n\n        server_context = Context(TLSv1_METHOD)\n        server_context.set_npn_advertise_callback(advertise)\n\n        client_context = Context(TLSv1_METHOD)\n        client_context.set_npn_select_callback(select)\n\n        # Necessary to actually accept the connection\n        server_context.use_privatekey(\n            load_privatekey(FILETYPE_PEM, server_key_pem))\n        server_context.use_certificate(\n            load_certificate(FILETYPE_PEM, server_cert_pem))\n\n        # Do a little connection to trigger the logic\n        server = Connection(server_context, None)\n        server.set_accept_state()\n\n        client = Connection(client_context, None)\n        client.set_connect_state()\n\n        # If the callback throws an exception it should be raised here.\n        with pytest.raises(TypeError):\n            interact_in_memory(server, client)\n        assert advertise_args == [(server,), ]\n\n    def test_npn_advertise_error(self):\n        \"\"\"\n        Test that we can handle exceptions in the advertise callback. If\n        advertise fails no NPN is advertised to the client.\n        \"\"\"\n        select_args = []\n\n        def advertise(conn):\n            raise TypeError\n\n        def select(conn, options):  # pragma: nocover\n            \"\"\"\n            Assert later that no args are actually appended.\n            \"\"\"\n            select_args.append((conn, options))\n            return b''\n\n        server_context = Context(TLSv1_METHOD)\n        server_context.set_npn_advertise_callback(advertise)\n\n        client_context = Context(TLSv1_METHOD)\n        client_context.set_npn_select_callback(select)\n\n        # Necessary to actually accept the connection\n        server_context.use_privatekey(\n            load_privatekey(FILETYPE_PEM, server_key_pem))\n        server_context.use_certificate(\n            load_certificate(FILETYPE_PEM, server_cert_pem))\n\n        # Do a little connection to trigger the logic\n        server = Connection(server_context, None)\n        server.set_accept_state()\n\n        client = Connection(client_context, None)\n        client.set_connect_state()\n\n        # If the client doesn't return anything, the connection will fail.\n        with pytest.raises(TypeError):\n            interact_in_memory(server, client)\n        assert select_args == []\n\n\nclass TestApplicationLayerProtoNegotiation(object):\n    \"\"\"\n    Tests for ALPN in PyOpenSSL.\n    \"\"\"\n    # Skip tests on versions that don't support ALPN.\n    if _lib.Cryptography_HAS_ALPN:\n\n        def test_alpn_success(self):\n            \"\"\"\n            Clients and servers that agree on the negotiated ALPN protocol can\n            correct establish a connection, and the agreed protocol is reported\n            by the connections.\n            \"\"\"\n            select_args = []\n\n            def select(conn, options):\n                select_args.append((conn, options))\n                return b'spdy/2'\n\n            client_context = Context(TLSv1_METHOD)\n            client_context.set_alpn_protos([b'http/1.1', b'spdy/2'])\n\n            server_context = Context(TLSv1_METHOD)\n            server_context.set_alpn_select_callback(select)\n\n            # Necessary to actually accept the connection\n            server_context.use_privatekey(\n                load_privatekey(FILETYPE_PEM, server_key_pem))\n            server_context.use_certificate(\n                load_certificate(FILETYPE_PEM, server_cert_pem))\n\n            # Do a little connection to trigger the logic\n            server = Connection(server_context, None)\n            server.set_accept_state()\n\n            client = Connection(client_context, None)\n            client.set_connect_state()\n\n            interact_in_memory(server, client)\n\n            assert select_args == [(server, [b'http/1.1', b'spdy/2'])]\n\n            assert server.get_alpn_proto_negotiated() == b'spdy/2'\n            assert client.get_alpn_proto_negotiated() == b'spdy/2'\n\n        def test_alpn_set_on_connection(self):\n            \"\"\"\n            The same as test_alpn_success, but setting the ALPN protocols on\n            the connection rather than the context.\n            \"\"\"\n            select_args = []\n\n            def select(conn, options):\n                select_args.append((conn, options))\n                return b'spdy/2'\n\n            # Setup the client context but don't set any ALPN protocols.\n            client_context = Context(TLSv1_METHOD)\n\n            server_context = Context(TLSv1_METHOD)\n            server_context.set_alpn_select_callback(select)\n\n            # Necessary to actually accept the connection\n            server_context.use_privatekey(\n                load_privatekey(FILETYPE_PEM, server_key_pem))\n            server_context.use_certificate(\n                load_certificate(FILETYPE_PEM, server_cert_pem))\n\n            # Do a little connection to trigger the logic\n            server = Connection(server_context, None)\n            server.set_accept_state()\n\n            # Set the ALPN protocols on the client connection.\n            client = Connection(client_context, None)\n            client.set_alpn_protos([b'http/1.1', b'spdy/2'])\n            client.set_connect_state()\n\n            interact_in_memory(server, client)\n\n            assert select_args == [(server, [b'http/1.1', b'spdy/2'])]\n\n            assert server.get_alpn_proto_negotiated() == b'spdy/2'\n            assert client.get_alpn_proto_negotiated() == b'spdy/2'\n\n        def test_alpn_server_fail(self):\n            \"\"\"\n            When clients and servers cannot agree on what protocol to use next\n            the TLS connection does not get established.\n            \"\"\"\n            select_args = []\n\n            def select(conn, options):\n                select_args.append((conn, options))\n                return b''\n\n            client_context = Context(TLSv1_METHOD)\n            client_context.set_alpn_protos([b'http/1.1', b'spdy/2'])\n\n            server_context = Context(TLSv1_METHOD)\n            server_context.set_alpn_select_callback(select)\n\n            # Necessary to actually accept the connection\n            server_context.use_privatekey(\n                load_privatekey(FILETYPE_PEM, server_key_pem))\n            server_context.use_certificate(\n                load_certificate(FILETYPE_PEM, server_cert_pem))\n\n            # Do a little connection to trigger the logic\n            server = Connection(server_context, None)\n            server.set_accept_state()\n\n            client = Connection(client_context, None)\n            client.set_connect_state()\n\n            # If the client doesn't return anything, the connection will fail.\n            with pytest.raises(Error):\n                interact_in_memory(server, client)\n\n            assert select_args == [(server, [b'http/1.1', b'spdy/2'])]\n\n        def test_alpn_no_server(self):\n            \"\"\"\n            When clients and servers cannot agree on what protocol to use next\n            because the server doesn't offer ALPN, no protocol is negotiated.\n            \"\"\"\n            client_context = Context(TLSv1_METHOD)\n            client_context.set_alpn_protos([b'http/1.1', b'spdy/2'])\n\n            server_context = Context(TLSv1_METHOD)\n\n            # Necessary to actually accept the connection\n            server_context.use_privatekey(\n                load_privatekey(FILETYPE_PEM, server_key_pem))\n            server_context.use_certificate(\n                load_certificate(FILETYPE_PEM, server_cert_pem))\n\n            # Do a little connection to trigger the logic\n            server = Connection(server_context, None)\n            server.set_accept_state()\n\n            client = Connection(client_context, None)\n            client.set_connect_state()\n\n            # Do the dance.\n            interact_in_memory(server, client)\n\n            assert client.get_alpn_proto_negotiated() == b''\n\n        def test_alpn_callback_exception(self):\n            \"\"\"\n            We can handle exceptions in the ALPN select callback.\n            \"\"\"\n            select_args = []\n\n            def select(conn, options):\n                select_args.append((conn, options))\n                raise TypeError()\n\n            client_context = Context(TLSv1_METHOD)\n            client_context.set_alpn_protos([b'http/1.1', b'spdy/2'])\n\n            server_context = Context(TLSv1_METHOD)\n            server_context.set_alpn_select_callback(select)\n\n            # Necessary to actually accept the connection\n            server_context.use_privatekey(\n                load_privatekey(FILETYPE_PEM, server_key_pem))\n            server_context.use_certificate(\n                load_certificate(FILETYPE_PEM, server_cert_pem))\n\n            # Do a little connection to trigger the logic\n            server = Connection(server_context, None)\n            server.set_accept_state()\n\n            client = Connection(client_context, None)\n            client.set_connect_state()\n\n            with pytest.raises(TypeError):\n                interact_in_memory(server, client)\n            assert select_args == [(server, [b'http/1.1', b'spdy/2'])]\n\n    else:\n        # No ALPN.\n        def test_alpn_not_implemented(self):\n            \"\"\"\n            If ALPN is not in OpenSSL, we should raise NotImplementedError.\n            \"\"\"\n            # Test the context methods first.\n            context = Context(TLSv1_METHOD)\n            with pytest.raises(NotImplementedError):\n                context.set_alpn_protos(None)\n            with pytest.raises(NotImplementedError):\n                context.set_alpn_select_callback(None)\n\n            # Now test a connection.\n            conn = Connection(context)\n            with pytest.raises(NotImplementedError):\n                conn.set_alpn_protos(None)\n\n\nclass TestSession(object):\n    \"\"\"\n    Unit tests for :py:obj:`OpenSSL.SSL.Session`.\n    \"\"\"\n    def test_construction(self):\n        \"\"\"\n        :py:class:`Session` can be constructed with no arguments, creating\n        a new instance of that type.\n        \"\"\"\n        new_session = Session()\n        assert isinstance(new_session, Session)\n\n\nclass TestConnection(object):\n    \"\"\"\n    Unit tests for `OpenSSL.SSL.Connection`.\n    \"\"\"\n    # XXX get_peer_certificate -> None\n    # XXX sock_shutdown\n    # XXX master_key -> TypeError\n    # XXX server_random -> TypeError\n    # XXX connect -> TypeError\n    # XXX connect_ex -> TypeError\n    # XXX set_connect_state -> TypeError\n    # XXX set_accept_state -> TypeError\n    # XXX do_handshake -> TypeError\n    # XXX bio_read -> TypeError\n    # XXX recv -> TypeError\n    # XXX send -> TypeError\n    # XXX bio_write -> TypeError\n\n    def test_type(self):\n        \"\"\"\n        `Connection` can be used to create instances of that type.\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        assert is_consistent_type(Connection, 'Connection', ctx, None)\n\n    @pytest.mark.parametrize('bad_context', [object(), 'context', None, 1])\n    def test_wrong_args(self, bad_context):\n        \"\"\"\n        `Connection.__init__` raises `TypeError` if called with a non-`Context`\n        instance argument.\n        \"\"\"\n        with pytest.raises(TypeError):\n            Connection(bad_context)\n\n    def test_get_context(self):\n        \"\"\"\n        `Connection.get_context` returns the `Context` instance used to\n        construct the `Connection` instance.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        connection = Connection(context, None)\n        assert connection.get_context() is context\n\n    def test_set_context_wrong_args(self):\n        \"\"\"\n        `Connection.set_context` raises `TypeError` if called with a\n        non-`Context` instance argument.\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        connection = Connection(ctx, None)\n        with pytest.raises(TypeError):\n            connection.set_context(object())\n        with pytest.raises(TypeError):\n            connection.set_context(\"hello\")\n        with pytest.raises(TypeError):\n            connection.set_context(1)\n        assert ctx is connection.get_context()\n\n    def test_set_context(self):\n        \"\"\"\n        `Connection.set_context` specifies a new `Context` instance to be\n        used for the connection.\n        \"\"\"\n        original = Context(SSLv23_METHOD)\n        replacement = Context(TLSv1_METHOD)\n        connection = Connection(original, None)\n        connection.set_context(replacement)\n        assert replacement is connection.get_context()\n        # Lose our references to the contexts, just in case the Connection\n        # isn't properly managing its own contributions to their reference\n        # counts.\n        del original, replacement\n        collect()\n\n    def test_set_tlsext_host_name_wrong_args(self):\n        \"\"\"\n        If `Connection.set_tlsext_host_name` is called with a non-byte string\n        argument or a byte string with an embedded NUL, `TypeError` is raised.\n        \"\"\"\n        conn = Connection(Context(TLSv1_METHOD), None)\n        with pytest.raises(TypeError):\n            conn.set_tlsext_host_name(object())\n        with pytest.raises(TypeError):\n            conn.set_tlsext_host_name(b\"with\\0null\")\n\n        if PY3:\n            # On Python 3.x, don't accidentally implicitly convert from text.\n            with pytest.raises(TypeError):\n                conn.set_tlsext_host_name(b\"example.com\".decode(\"ascii\"))\n\n    def test_pending(self):\n        \"\"\"\n        `Connection.pending` returns the number of bytes available for\n        immediate read.\n        \"\"\"\n        connection = Connection(Context(TLSv1_METHOD), None)\n        assert connection.pending() == 0\n\n    def test_peek(self):\n        \"\"\"\n        `Connection.recv` peeks into the connection if `socket.MSG_PEEK` is\n        passed.\n        \"\"\"\n        server, client = loopback()\n        server.send(b'xy')\n        assert client.recv(2, MSG_PEEK) == b'xy'\n        assert client.recv(2, MSG_PEEK) == b'xy'\n        assert client.recv(2) == b'xy'\n\n    def test_connect_wrong_args(self):\n        \"\"\"\n        `Connection.connect` raises `TypeError` if called with a non-address\n        argument.\n        \"\"\"\n        connection = Connection(Context(TLSv1_METHOD), socket_any_family())\n        with pytest.raises(TypeError):\n            connection.connect(None)\n\n    def test_connect_refused(self):\n        \"\"\"\n        `Connection.connect` raises `socket.error` if the underlying socket\n        connect method raises it.\n        \"\"\"\n        client = socket_any_family()\n        context = Context(TLSv1_METHOD)\n        clientSSL = Connection(context, client)\n        # pytest.raises here doesn't work because of a bug in py.test on Python\n        # 2.6: https://github.com/pytest-dev/pytest/issues/988\n        try:\n            clientSSL.connect((loopback_address(client), 1))\n        except error as e:\n            exc = e\n        assert exc.args[0] == ECONNREFUSED\n\n    def test_connect(self):\n        \"\"\"\n        `Connection.connect` establishes a connection to the specified address.\n        \"\"\"\n        port = socket_any_family()\n        port.bind(('', 0))\n        port.listen(3)\n\n        clientSSL = Connection(Context(TLSv1_METHOD), socket(port.family))\n        clientSSL.connect((loopback_address(port), port.getsockname()[1]))\n        # XXX An assertion?  Or something?\n\n    @pytest.mark.skipif(\n        platform == \"darwin\",\n        reason=\"connect_ex sometimes causes a kernel panic on OS X 10.6.4\"\n    )\n    def test_connect_ex(self):\n        \"\"\"\n        If there is a connection error, `Connection.connect_ex` returns the\n        errno instead of raising an exception.\n        \"\"\"\n        port = socket_any_family()\n        port.bind(('', 0))\n        port.listen(3)\n\n        clientSSL = Connection(Context(TLSv1_METHOD), socket(port.family))\n        clientSSL.setblocking(False)\n        result = clientSSL.connect_ex(port.getsockname())\n        expected = (EINPROGRESS, EWOULDBLOCK)\n        assert result in expected\n\n    def test_accept(self):\n        \"\"\"\n        `Connection.accept` accepts a pending connection attempt and returns a\n        tuple of a new `Connection` (the accepted client) and the address the\n        connection originated from.\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        ctx.use_privatekey(load_privatekey(FILETYPE_PEM, server_key_pem))\n        ctx.use_certificate(load_certificate(FILETYPE_PEM, server_cert_pem))\n        port = socket_any_family()\n        portSSL = Connection(ctx, port)\n        portSSL.bind(('', 0))\n        portSSL.listen(3)\n\n        clientSSL = Connection(Context(TLSv1_METHOD), socket(port.family))\n\n        # Calling portSSL.getsockname() here to get the server IP address\n        # sounds great, but frequently fails on Windows.\n        clientSSL.connect((loopback_address(port), portSSL.getsockname()[1]))\n\n        serverSSL, address = portSSL.accept()\n\n        assert isinstance(serverSSL, Connection)\n        assert serverSSL.get_context() is ctx\n        assert address == clientSSL.getsockname()\n\n    def test_shutdown_wrong_args(self):\n        \"\"\"\n        `Connection.set_shutdown` raises `TypeError` if called with arguments\n        other than integers.\n        \"\"\"\n        connection = Connection(Context(TLSv1_METHOD), None)\n        with pytest.raises(TypeError):\n            connection.set_shutdown(None)\n\n    def test_shutdown(self):\n        \"\"\"\n        `Connection.shutdown` performs an SSL-level connection shutdown.\n        \"\"\"\n        server, client = loopback()\n        assert not server.shutdown()\n        assert server.get_shutdown() == SENT_SHUTDOWN\n        with pytest.raises(ZeroReturnError):\n            client.recv(1024)\n        assert client.get_shutdown() == RECEIVED_SHUTDOWN\n        client.shutdown()\n        assert client.get_shutdown() == (SENT_SHUTDOWN | RECEIVED_SHUTDOWN)\n        with pytest.raises(ZeroReturnError):\n            server.recv(1024)\n        assert server.get_shutdown() == (SENT_SHUTDOWN | RECEIVED_SHUTDOWN)\n\n    def test_shutdown_closed(self):\n        \"\"\"\n        If the underlying socket is closed, `Connection.shutdown` propagates\n        the write error from the low level write call.\n        \"\"\"\n        server, client = loopback()\n        server.sock_shutdown(2)\n        with pytest.raises(SysCallError) as exc:\n            server.shutdown()\n        if platform == \"win32\":\n            assert exc.value.args[0] == ESHUTDOWN\n        else:\n            assert exc.value.args[0] == EPIPE\n\n    def test_shutdown_truncated(self):\n        \"\"\"\n        If the underlying connection is truncated, `Connection.shutdown`\n        raises an `Error`.\n        \"\"\"\n        server_ctx = Context(TLSv1_METHOD)\n        client_ctx = Context(TLSv1_METHOD)\n        server_ctx.use_privatekey(\n            load_privatekey(FILETYPE_PEM, server_key_pem))\n        server_ctx.use_certificate(\n            load_certificate(FILETYPE_PEM, server_cert_pem))\n        server = Connection(server_ctx, None)\n        client = Connection(client_ctx, None)\n        handshake_in_memory(client, server)\n        assert not server.shutdown()\n        with pytest.raises(WantReadError):\n            server.shutdown()\n        server.bio_shutdown()\n        with pytest.raises(Error):\n            server.shutdown()\n\n    def test_set_shutdown(self):\n        \"\"\"\n        `Connection.set_shutdown` sets the state of the SSL connection\n        shutdown process.\n        \"\"\"\n        connection = Connection(Context(TLSv1_METHOD), socket_any_family())\n        connection.set_shutdown(RECEIVED_SHUTDOWN)\n        assert connection.get_shutdown() == RECEIVED_SHUTDOWN\n\n    def test_state_string(self):\n        \"\"\"\n        `Connection.state_string` verbosely describes the current state of\n        the `Connection`.\n        \"\"\"\n        server, client = socket_pair()\n        server = loopback_server_factory(server)\n        client = loopback_client_factory(client)\n\n        assert server.get_state_string() in [\n            b\"before/accept initialization\", b\"before SSL initialization\"\n        ]\n        assert client.get_state_string() in [\n            b\"before/connect initialization\", b\"before SSL initialization\"\n        ]\n\n    def test_app_data(self):\n        \"\"\"\n        Any object can be set as app data by passing it to\n        `Connection.set_app_data` and later retrieved with\n        `Connection.get_app_data`.\n        \"\"\"\n        conn = Connection(Context(TLSv1_METHOD), None)\n        assert None is conn.get_app_data()\n        app_data = object()\n        conn.set_app_data(app_data)\n        assert conn.get_app_data() is app_data\n\n    def test_makefile(self):\n        \"\"\"\n        `Connection.makefile` is not implemented and calling that\n        method raises `NotImplementedError`.\n        \"\"\"\n        conn = Connection(Context(TLSv1_METHOD), None)\n        with pytest.raises(NotImplementedError):\n            conn.makefile()\n\n    def test_get_certificate(self):\n        \"\"\"\n        `Connection.get_certificate` returns the local certificate.\n        \"\"\"\n        chain = _create_certificate_chain()\n        [(cakey, cacert), (ikey, icert), (skey, scert)] = chain\n\n        context = Context(TLSv1_METHOD)\n        context.use_certificate(scert)\n        client = Connection(context, None)\n        cert = client.get_certificate()\n        assert cert is not None\n        assert \"Server Certificate\" == cert.get_subject().CN\n\n    def test_get_certificate_none(self):\n        \"\"\"\n        `Connection.get_certificate` returns the local certificate.\n\n        If there is no certificate, it returns None.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        client = Connection(context, None)\n        cert = client.get_certificate()\n        assert cert is None\n\n    def test_get_peer_cert_chain(self):\n        \"\"\"\n        `Connection.get_peer_cert_chain` returns a list of certificates\n        which the connected server returned for the certification verification.\n        \"\"\"\n        chain = _create_certificate_chain()\n        [(cakey, cacert), (ikey, icert), (skey, scert)] = chain\n\n        serverContext = Context(TLSv1_METHOD)\n        serverContext.use_privatekey(skey)\n        serverContext.use_certificate(scert)\n        serverContext.add_extra_chain_cert(icert)\n        serverContext.add_extra_chain_cert(cacert)\n        server = Connection(serverContext, None)\n        server.set_accept_state()\n\n        # Create the client\n        clientContext = Context(TLSv1_METHOD)\n        clientContext.set_verify(VERIFY_NONE, verify_cb)\n        client = Connection(clientContext, None)\n        client.set_connect_state()\n\n        interact_in_memory(client, server)\n\n        chain = client.get_peer_cert_chain()\n        assert len(chain) == 3\n        assert \"Server Certificate\" == chain[0].get_subject().CN\n        assert \"Intermediate Certificate\" == chain[1].get_subject().CN\n        assert \"Authority Certificate\" == chain[2].get_subject().CN\n\n    def test_get_peer_cert_chain_none(self):\n        \"\"\"\n        `Connection.get_peer_cert_chain` returns `None` if the peer sends\n        no certificate chain.\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        ctx.use_privatekey(load_privatekey(FILETYPE_PEM, server_key_pem))\n        ctx.use_certificate(load_certificate(FILETYPE_PEM, server_cert_pem))\n        server = Connection(ctx, None)\n        server.set_accept_state()\n        client = Connection(Context(TLSv1_METHOD), None)\n        client.set_connect_state()\n        interact_in_memory(client, server)\n        assert None is server.get_peer_cert_chain()\n\n    def test_get_session_unconnected(self):\n        \"\"\"\n        `Connection.get_session` returns `None` when used with an object\n        which has not been connected.\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        server = Connection(ctx, None)\n        session = server.get_session()\n        assert None is session\n\n    def test_server_get_session(self):\n        \"\"\"\n        On the server side of a connection, `Connection.get_session` returns a\n        `Session` instance representing the SSL session for that connection.\n        \"\"\"\n        server, client = loopback()\n        session = server.get_session()\n        assert isinstance(session, Session)\n\n    def test_client_get_session(self):\n        \"\"\"\n        On the client side of a connection, `Connection.get_session`\n        returns a `Session` instance representing the SSL session for\n        that connection.\n        \"\"\"\n        server, client = loopback()\n        session = client.get_session()\n        assert isinstance(session, Session)\n\n    def test_set_session_wrong_args(self):\n        \"\"\"\n        `Connection.set_session` raises `TypeError` if called with an object\n        that is not an instance of `Session`.\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        connection = Connection(ctx, None)\n        with pytest.raises(TypeError):\n            connection.set_session(123)\n        with pytest.raises(TypeError):\n            connection.set_session(\"hello\")\n        with pytest.raises(TypeError):\n            connection.set_session(object())\n\n    def test_client_set_session(self):\n        \"\"\"\n        `Connection.set_session`, when used prior to a connection being\n        established, accepts a `Session` instance and causes an attempt to\n        re-use the session it represents when the SSL handshake is performed.\n        \"\"\"\n        key = load_privatekey(FILETYPE_PEM, server_key_pem)\n        cert = load_certificate(FILETYPE_PEM, server_cert_pem)\n        ctx = Context(TLSv1_2_METHOD)\n        ctx.use_privatekey(key)\n        ctx.use_certificate(cert)\n        ctx.set_session_id(\"unity-test\")\n\n        def makeServer(socket):\n            server = Connection(ctx, socket)\n            server.set_accept_state()\n            return server\n\n        originalServer, originalClient = loopback(\n            server_factory=makeServer)\n        originalSession = originalClient.get_session()\n\n        def makeClient(socket):\n            client = loopback_client_factory(socket)\n            client.set_session(originalSession)\n            return client\n        resumedServer, resumedClient = loopback(\n            server_factory=makeServer,\n            client_factory=makeClient)\n\n        # This is a proxy: in general, we have no access to any unique\n        # identifier for the session (new enough versions of OpenSSL expose\n        # a hash which could be usable, but \"new enough\" is very, very new).\n        # Instead, exploit the fact that the master key is re-used if the\n        # session is re-used.  As long as the master key for the two\n        # connections is the same, the session was re-used!\n        assert originalServer.master_key() == resumedServer.master_key()\n\n    def test_set_session_wrong_method(self):\n        \"\"\"\n        If `Connection.set_session` is passed a `Session` instance associated\n        with a context using a different SSL method than the `Connection`\n        is using, a `OpenSSL.SSL.Error` is raised.\n        \"\"\"\n        # Make this work on both OpenSSL 1.0.0, which doesn't support TLSv1.2\n        # and also on OpenSSL 1.1.0 which doesn't support SSLv3. (SSL_ST_INIT\n        # is a way to check for 1.1.0)\n        if SSL_ST_INIT is None:\n            v1 = TLSv1_2_METHOD\n            v2 = TLSv1_METHOD\n        elif hasattr(_lib, \"SSLv3_method\"):\n            v1 = TLSv1_METHOD\n            v2 = SSLv3_METHOD\n        else:\n            pytest.skip(\"Test requires either OpenSSL 1.1.0 or SSLv3\")\n\n        key = load_privatekey(FILETYPE_PEM, server_key_pem)\n        cert = load_certificate(FILETYPE_PEM, server_cert_pem)\n        ctx = Context(v1)\n        ctx.use_privatekey(key)\n        ctx.use_certificate(cert)\n        ctx.set_session_id(\"unity-test\")\n\n        def makeServer(socket):\n            server = Connection(ctx, socket)\n            server.set_accept_state()\n            return server\n\n        def makeOriginalClient(socket):\n            client = Connection(Context(v1), socket)\n            client.set_connect_state()\n            return client\n\n        originalServer, originalClient = loopback(\n            server_factory=makeServer, client_factory=makeOriginalClient)\n        originalSession = originalClient.get_session()\n\n        def makeClient(socket):\n            # Intentionally use a different, incompatible method here.\n            client = Connection(Context(v2), socket)\n            client.set_connect_state()\n            client.set_session(originalSession)\n            return client\n\n        with pytest.raises(Error):\n            loopback(client_factory=makeClient, server_factory=makeServer)\n\n    def test_wantWriteError(self):\n        \"\"\"\n        `Connection` methods which generate output raise\n        `OpenSSL.SSL.WantWriteError` if writing to the connection's BIO\n        fail indicating a should-write state.\n        \"\"\"\n        client_socket, server_socket = socket_pair()\n        # Fill up the client's send buffer so Connection won't be able to write\n        # anything.  Only write a single byte at a time so we can be sure we\n        # completely fill the buffer.  Even though the socket API is allowed to\n        # signal a short write via its return value it seems this doesn't\n        # always happen on all platforms (FreeBSD and OS X particular) for the\n        # very last bit of available buffer space.\n        msg = b\"x\"\n        for i in range(1024 * 1024 * 64):\n            try:\n                client_socket.send(msg)\n            except error as e:\n                if e.errno == EWOULDBLOCK:\n                    break\n                raise\n        else:\n            pytest.fail(\n                \"Failed to fill socket buffer, cannot test BIO want write\")\n\n        ctx = Context(TLSv1_METHOD)\n        conn = Connection(ctx, client_socket)\n        # Client's speak first, so make it an SSL client\n        conn.set_connect_state()\n        with pytest.raises(WantWriteError):\n            conn.do_handshake()\n\n    # XXX want_read\n\n    def test_get_finished_before_connect(self):\n        \"\"\"\n        `Connection.get_finished` returns `None` before TLS handshake\n        is completed.\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        connection = Connection(ctx, None)\n        assert connection.get_finished() is None\n\n    def test_get_peer_finished_before_connect(self):\n        \"\"\"\n        `Connection.get_peer_finished` returns `None` before TLS handshake\n        is completed.\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        connection = Connection(ctx, None)\n        assert connection.get_peer_finished() is None\n\n    def test_get_finished(self):\n        \"\"\"\n        `Connection.get_finished` method returns the TLS Finished message send\n        from client, or server. Finished messages are send during\n        TLS handshake.\n        \"\"\"\n        server, client = loopback()\n\n        assert server.get_finished() is not None\n        assert len(server.get_finished()) > 0\n\n    def test_get_peer_finished(self):\n        \"\"\"\n        `Connection.get_peer_finished` method returns the TLS Finished\n        message received from client, or server. Finished messages are send\n        during TLS handshake.\n        \"\"\"\n        server, client = loopback()\n\n        assert server.get_peer_finished() is not None\n        assert len(server.get_peer_finished()) > 0\n\n    def test_tls_finished_message_symmetry(self):\n        \"\"\"\n        The TLS Finished message send by server must be the TLS Finished\n        message received by client.\n\n        The TLS Finished message send by client must be the TLS Finished\n        message received by server.\n        \"\"\"\n        server, client = loopback()\n\n        assert server.get_finished() == client.get_peer_finished()\n        assert client.get_finished() == server.get_peer_finished()\n\n    def test_get_cipher_name_before_connect(self):\n        \"\"\"\n        `Connection.get_cipher_name` returns `None` if no connection\n        has been established.\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        conn = Connection(ctx, None)\n        assert conn.get_cipher_name() is None\n\n    def test_get_cipher_name(self):\n        \"\"\"\n        `Connection.get_cipher_name` returns a `unicode` string giving the\n        name of the currently used cipher.\n        \"\"\"\n        server, client = loopback()\n        server_cipher_name, client_cipher_name = \\\n            server.get_cipher_name(), client.get_cipher_name()\n\n        assert isinstance(server_cipher_name, text_type)\n        assert isinstance(client_cipher_name, text_type)\n\n        assert server_cipher_name == client_cipher_name\n\n    def test_get_cipher_version_before_connect(self):\n        \"\"\"\n        `Connection.get_cipher_version` returns `None` if no connection\n        has been established.\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        conn = Connection(ctx, None)\n        assert conn.get_cipher_version() is None\n\n    def test_get_cipher_version(self):\n        \"\"\"\n        `Connection.get_cipher_version` returns a `unicode` string giving\n        the protocol name of the currently used cipher.\n        \"\"\"\n        server, client = loopback()\n        server_cipher_version, client_cipher_version = \\\n            server.get_cipher_version(), client.get_cipher_version()\n\n        assert isinstance(server_cipher_version, text_type)\n        assert isinstance(client_cipher_version, text_type)\n\n        assert server_cipher_version == client_cipher_version\n\n    def test_get_cipher_bits_before_connect(self):\n        \"\"\"\n        `Connection.get_cipher_bits` returns `None` if no connection has\n        been established.\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        conn = Connection(ctx, None)\n        assert conn.get_cipher_bits() is None\n\n    def test_get_cipher_bits(self):\n        \"\"\"\n        `Connection.get_cipher_bits` returns the number of secret bits\n        of the currently used cipher.\n        \"\"\"\n        server, client = loopback()\n        server_cipher_bits, client_cipher_bits = \\\n            server.get_cipher_bits(), client.get_cipher_bits()\n\n        assert isinstance(server_cipher_bits, int)\n        assert isinstance(client_cipher_bits, int)\n\n        assert server_cipher_bits == client_cipher_bits\n\n    def test_get_protocol_version_name(self):\n        \"\"\"\n        `Connection.get_protocol_version_name()` returns a string giving the\n        protocol version of the current connection.\n        \"\"\"\n        server, client = loopback()\n        client_protocol_version_name = client.get_protocol_version_name()\n        server_protocol_version_name = server.get_protocol_version_name()\n\n        assert isinstance(server_protocol_version_name, text_type)\n        assert isinstance(client_protocol_version_name, text_type)\n\n        assert server_protocol_version_name == client_protocol_version_name\n\n    def test_get_protocol_version(self):\n        \"\"\"\n        `Connection.get_protocol_version()` returns an integer\n        giving the protocol version of the current connection.\n        \"\"\"\n        server, client = loopback()\n        client_protocol_version = client.get_protocol_version()\n        server_protocol_version = server.get_protocol_version()\n\n        assert isinstance(server_protocol_version, int)\n        assert isinstance(client_protocol_version, int)\n\n        assert server_protocol_version == client_protocol_version\n\n    def test_wantReadError(self):\n        \"\"\"\n        `Connection.bio_read` raises `OpenSSL.SSL.WantReadError` if there are\n        no bytes available to be read from the BIO.\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        conn = Connection(ctx, None)\n        with pytest.raises(WantReadError):\n            conn.bio_read(1024)\n\n    @pytest.mark.parametrize('bufsize', [1.0, None, object(), 'bufsize'])\n    def test_bio_read_wrong_args(self, bufsize):\n        \"\"\"\n        `Connection.bio_read` raises `TypeError` if passed a non-integer\n        argument.\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        conn = Connection(ctx, None)\n        with pytest.raises(TypeError):\n            conn.bio_read(bufsize)\n\n    def test_buffer_size(self):\n        \"\"\"\n        `Connection.bio_read` accepts an integer giving the maximum number\n        of bytes to read and return.\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        conn = Connection(ctx, None)\n        conn.set_connect_state()\n        try:\n            conn.do_handshake()\n        except WantReadError:\n            pass\n        data = conn.bio_read(2)\n        assert 2 == len(data)\n\n\nclass TestConnectionGetCipherList(object):\n    \"\"\"\n    Tests for `Connection.get_cipher_list`.\n    \"\"\"\n    def test_result(self):\n        \"\"\"\n        `Connection.get_cipher_list` returns a list of `bytes` giving the\n        names of the ciphers which might be used.\n        \"\"\"\n        connection = Connection(Context(TLSv1_METHOD), None)\n        ciphers = connection.get_cipher_list()\n        assert isinstance(ciphers, list)\n        for cipher in ciphers:\n            assert isinstance(cipher, str)\n\n\nclass VeryLarge(bytes):\n    \"\"\"\n    Mock object so that we don't have to allocate 2**31 bytes\n    \"\"\"\n    def __len__(self):\n        return 2**31\n\n\nclass TestConnectionSend(object):\n    \"\"\"\n    Tests for `Connection.send`.\n    \"\"\"\n    def test_wrong_args(self):\n        \"\"\"\n        When called with arguments other than string argument for its first\n        parameter, `Connection.send` raises `TypeError`.\n        \"\"\"\n        connection = Connection(Context(TLSv1_METHOD), None)\n        with pytest.raises(TypeError):\n            connection.send(object())\n\n    def test_short_bytes(self):\n        \"\"\"\n        When passed a short byte string, `Connection.send` transmits all of it\n        and returns the number of bytes sent.\n        \"\"\"\n        server, client = loopback()\n        count = server.send(b'xy')\n        assert count == 2\n        assert client.recv(2) == b'xy'\n\n    def test_text(self):\n        \"\"\"\n        When passed a text, `Connection.send` transmits all of it and\n        returns the number of bytes sent. It also raises a DeprecationWarning.\n        \"\"\"\n        server, client = loopback()\n        with pytest.warns(DeprecationWarning) as w:\n            simplefilter(\"always\")\n            count = server.send(b\"xy\".decode(\"ascii\"))\n            assert (\n                \"{0} for buf is no longer accepted, use bytes\".format(\n                    WARNING_TYPE_EXPECTED\n                ) == str(w[-1].message))\n        assert count == 2\n        assert client.recv(2) == b'xy'\n\n    def test_short_memoryview(self):\n        \"\"\"\n        When passed a memoryview onto a small number of bytes,\n        `Connection.send` transmits all of them and returns the number\n        of bytes sent.\n        \"\"\"\n        server, client = loopback()\n        count = server.send(memoryview(b'xy'))\n        assert count == 2\n        assert client.recv(2) == b'xy'\n\n    @skip_if_py3\n    def test_short_buffer(self):\n        \"\"\"\n        When passed a buffer containing a small number of bytes,\n        `Connection.send` transmits all of them and returns the number\n        of bytes sent.\n        \"\"\"\n        server, client = loopback()\n        count = server.send(buffer(b'xy'))\n        assert count == 2\n        assert client.recv(2) == b'xy'\n\n    @pytest.mark.skipif(\n        sys.maxsize < 2**31,\n        reason=\"sys.maxsize < 2**31 - test requires 64 bit\"\n    )\n    def test_buf_too_large(self):\n        \"\"\"\n        When passed a buffer containing >= 2**31 bytes,\n        `Connection.send` bails out as SSL_write only\n        accepts an int for the buffer length.\n        \"\"\"\n        connection = Connection(Context(TLSv1_METHOD), None)\n        with pytest.raises(ValueError) as exc_info:\n            connection.send(VeryLarge())\n        exc_info.match(r\"Cannot send more than .+ bytes at once\")\n\n\ndef _make_memoryview(size):\n    \"\"\"\n    Create a new ``memoryview`` wrapped around a ``bytearray`` of the given\n    size.\n    \"\"\"\n    return memoryview(bytearray(size))\n\n\nclass TestConnectionRecvInto(object):\n    \"\"\"\n    Tests for `Connection.recv_into`.\n    \"\"\"\n    def _no_length_test(self, factory):\n        \"\"\"\n        Assert that when the given buffer is passed to `Connection.recv_into`,\n        whatever bytes are available to be received that fit into that buffer\n        are written into that buffer.\n        \"\"\"\n        output_buffer = factory(5)\n\n        server, client = loopback()\n        server.send(b'xy')\n\n        assert client.recv_into(output_buffer) == 2\n        assert output_buffer == bytearray(b'xy\\x00\\x00\\x00')\n\n    def test_bytearray_no_length(self):\n        \"\"\"\n        `Connection.recv_into` can be passed a `bytearray` instance and data\n        in the receive buffer is written to it.\n        \"\"\"\n        self._no_length_test(bytearray)\n\n    def _respects_length_test(self, factory):\n        \"\"\"\n        Assert that when the given buffer is passed to `Connection.recv_into`\n        along with a value for `nbytes` that is less than the size of that\n        buffer, only `nbytes` bytes are written into the buffer.\n        \"\"\"\n        output_buffer = factory(10)\n\n        server, client = loopback()\n        server.send(b'abcdefghij')\n\n        assert client.recv_into(output_buffer, 5) == 5\n        assert output_buffer == bytearray(b'abcde\\x00\\x00\\x00\\x00\\x00')\n\n    def test_bytearray_respects_length(self):\n        \"\"\"\n        When called with a `bytearray` instance, `Connection.recv_into`\n        respects the `nbytes` parameter and doesn't copy in more than that\n        number of bytes.\n        \"\"\"\n        self._respects_length_test(bytearray)\n\n    def _doesnt_overfill_test(self, factory):\n        \"\"\"\n        Assert that if there are more bytes available to be read from the\n        receive buffer than would fit into the buffer passed to\n        `Connection.recv_into`, only as many as fit are written into it.\n        \"\"\"\n        output_buffer = factory(5)\n\n        server, client = loopback()\n        server.send(b'abcdefghij')\n\n        assert client.recv_into(output_buffer) == 5\n        assert output_buffer == bytearray(b'abcde')\n        rest = client.recv(5)\n        assert b'fghij' == rest\n\n    def test_bytearray_doesnt_overfill(self):\n        \"\"\"\n        When called with a `bytearray` instance, `Connection.recv_into`\n        respects the size of the array and doesn't write more bytes into it\n        than will fit.\n        \"\"\"\n        self._doesnt_overfill_test(bytearray)\n\n    def test_bytearray_really_doesnt_overfill(self):\n        \"\"\"\n        When called with a `bytearray` instance and an `nbytes` value that is\n        too large, `Connection.recv_into` respects the size of the array and\n        not the `nbytes` value and doesn't write more bytes into the buffer\n        than will fit.\n        \"\"\"\n        self._doesnt_overfill_test(bytearray)\n\n    def test_peek(self):\n        server, client = loopback()\n        server.send(b'xy')\n\n        for _ in range(2):\n            output_buffer = bytearray(5)\n            assert client.recv_into(output_buffer, flags=MSG_PEEK) == 2\n            assert output_buffer == bytearray(b'xy\\x00\\x00\\x00')\n\n    def test_memoryview_no_length(self):\n        \"\"\"\n        `Connection.recv_into` can be passed a `memoryview` instance and data\n        in the receive buffer is written to it.\n        \"\"\"\n        self._no_length_test(_make_memoryview)\n\n    def test_memoryview_respects_length(self):\n        \"\"\"\n        When called with a `memoryview` instance, `Connection.recv_into`\n        respects the ``nbytes`` parameter and doesn't copy more than that\n        number of bytes in.\n        \"\"\"\n        self._respects_length_test(_make_memoryview)\n\n    def test_memoryview_doesnt_overfill(self):\n        \"\"\"\n        When called with a `memoryview` instance, `Connection.recv_into`\n        respects the size of the array and doesn't write more bytes into it\n        than will fit.\n        \"\"\"\n        self._doesnt_overfill_test(_make_memoryview)\n\n    def test_memoryview_really_doesnt_overfill(self):\n        \"\"\"\n        When called with a `memoryview` instance and an `nbytes` value that is\n        too large, `Connection.recv_into` respects the size of the array and\n        not the `nbytes` value and doesn't write more bytes into the buffer\n        than will fit.\n        \"\"\"\n        self._doesnt_overfill_test(_make_memoryview)\n\n\nclass TestConnectionSendall(object):\n    \"\"\"\n    Tests for `Connection.sendall`.\n    \"\"\"\n    def test_wrong_args(self):\n        \"\"\"\n        When called with arguments other than a string argument for its first\n        parameter, `Connection.sendall` raises `TypeError`.\n        \"\"\"\n        connection = Connection(Context(TLSv1_METHOD), None)\n        with pytest.raises(TypeError):\n            connection.sendall(object())\n\n    def test_short(self):\n        \"\"\"\n        `Connection.sendall` transmits all of the bytes in the string\n        passed to it.\n        \"\"\"\n        server, client = loopback()\n        server.sendall(b'x')\n        assert client.recv(1) == b'x'\n\n    def test_text(self):\n        \"\"\"\n        `Connection.sendall` transmits all the content in the string passed\n        to it, raising a DeprecationWarning in case of this being a text.\n        \"\"\"\n        server, client = loopback()\n        with pytest.warns(DeprecationWarning) as w:\n            simplefilter(\"always\")\n            server.sendall(b\"x\".decode(\"ascii\"))\n            assert (\n                \"{0} for buf is no longer accepted, use bytes\".format(\n                    WARNING_TYPE_EXPECTED\n                ) == str(w[-1].message))\n        assert client.recv(1) == b\"x\"\n\n    def test_short_memoryview(self):\n        \"\"\"\n        When passed a memoryview onto a small number of bytes,\n        `Connection.sendall` transmits all of them.\n        \"\"\"\n        server, client = loopback()\n        server.sendall(memoryview(b'x'))\n        assert client.recv(1) == b'x'\n\n    @skip_if_py3\n    def test_short_buffers(self):\n        \"\"\"\n        When passed a buffer containing a small number of bytes,\n        `Connection.sendall` transmits all of them.\n        \"\"\"\n        server, client = loopback()\n        server.sendall(buffer(b'x'))\n        assert client.recv(1) == b'x'\n\n    def test_long(self):\n        \"\"\"\n        `Connection.sendall` transmits all the bytes in the string passed to it\n        even if this requires multiple calls of an underlying write function.\n        \"\"\"\n        server, client = loopback()\n        # Should be enough, underlying SSL_write should only do 16k at a time.\n        # On Windows, after 32k of bytes the write will block (forever\n        # - because no one is yet reading).\n        message = b'x' * (1024 * 32 - 1) + b'y'\n        server.sendall(message)\n        accum = []\n        received = 0\n        while received < len(message):\n            data = client.recv(1024)\n            accum.append(data)\n            received += len(data)\n        assert message == b''.join(accum)\n\n    def test_closed(self):\n        \"\"\"\n        If the underlying socket is closed, `Connection.sendall` propagates the\n        write error from the low level write call.\n        \"\"\"\n        server, client = loopback()\n        server.sock_shutdown(2)\n        with pytest.raises(SysCallError) as err:\n            server.sendall(b\"hello, world\")\n        if platform == \"win32\":\n            assert err.value.args[0] == ESHUTDOWN\n        else:\n            assert err.value.args[0] == EPIPE\n\n\nclass TestConnectionRenegotiate(object):\n    \"\"\"\n    Tests for SSL renegotiation APIs.\n    \"\"\"\n    def test_total_renegotiations(self):\n        \"\"\"\n        `Connection.total_renegotiations` returns `0` before any renegotiations\n        have happened.\n        \"\"\"\n        connection = Connection(Context(TLSv1_METHOD), None)\n        assert connection.total_renegotiations() == 0\n\n    def test_renegotiate(self):\n        \"\"\"\n        Go through a complete renegotiation cycle.\n        \"\"\"\n        server, client = loopback(\n            lambda s: loopback_server_factory(s, TLSv1_2_METHOD),\n            lambda s: loopback_client_factory(s, TLSv1_2_METHOD),\n        )\n\n        server.send(b\"hello world\")\n\n        assert b\"hello world\" == client.recv(len(b\"hello world\"))\n\n        assert 0 == server.total_renegotiations()\n        assert False is server.renegotiate_pending()\n\n        assert True is server.renegotiate()\n\n        assert True is server.renegotiate_pending()\n\n        server.setblocking(False)\n        client.setblocking(False)\n\n        client.do_handshake()\n        server.do_handshake()\n\n        assert 1 == server.total_renegotiations()\n        while False is server.renegotiate_pending():\n            pass\n\n\nclass TestError(object):\n    \"\"\"\n    Unit tests for `OpenSSL.SSL.Error`.\n    \"\"\"\n    def test_type(self):\n        \"\"\"\n        `Error` is an exception type.\n        \"\"\"\n        assert issubclass(Error, Exception)\n        assert Error.__name__ == 'Error'\n\n\nclass TestConstants(object):\n    \"\"\"\n    Tests for the values of constants exposed in `OpenSSL.SSL`.\n\n    These are values defined by OpenSSL intended only to be used as flags to\n    OpenSSL APIs.  The only assertions it seems can be made about them is\n    their values.\n    \"\"\"\n    @pytest.mark.skipif(\n        OP_NO_QUERY_MTU is None,\n        reason=\"OP_NO_QUERY_MTU unavailable - OpenSSL version may be too old\"\n    )\n    def test_op_no_query_mtu(self):\n        \"\"\"\n        The value of `OpenSSL.SSL.OP_NO_QUERY_MTU` is 0x1000, the value\n        of `SSL_OP_NO_QUERY_MTU` defined by `openssl/ssl.h`.\n        \"\"\"\n        assert OP_NO_QUERY_MTU == 0x1000\n\n    @pytest.mark.skipif(\n        OP_COOKIE_EXCHANGE is None,\n        reason=\"OP_COOKIE_EXCHANGE unavailable - \"\n        \"OpenSSL version may be too old\"\n    )\n    def test_op_cookie_exchange(self):\n        \"\"\"\n        The value of `OpenSSL.SSL.OP_COOKIE_EXCHANGE` is 0x2000, the\n        value of `SSL_OP_COOKIE_EXCHANGE` defined by `openssl/ssl.h`.\n        \"\"\"\n        assert OP_COOKIE_EXCHANGE == 0x2000\n\n    @pytest.mark.skipif(\n        OP_NO_TICKET is None,\n        reason=\"OP_NO_TICKET unavailable - OpenSSL version may be too old\"\n    )\n    def test_op_no_ticket(self):\n        \"\"\"\n        The value of `OpenSSL.SSL.OP_NO_TICKET` is 0x4000, the value of\n        `SSL_OP_NO_TICKET` defined by `openssl/ssl.h`.\n        \"\"\"\n        assert OP_NO_TICKET == 0x4000\n\n    @pytest.mark.skipif(\n        OP_NO_COMPRESSION is None,\n        reason=\"OP_NO_COMPRESSION unavailable - OpenSSL version may be too old\"\n    )\n    def test_op_no_compression(self):\n        \"\"\"\n        The value of `OpenSSL.SSL.OP_NO_COMPRESSION` is 0x20000, the\n        value of `SSL_OP_NO_COMPRESSION` defined by `openssl/ssl.h`.\n        \"\"\"\n        assert OP_NO_COMPRESSION == 0x20000\n\n    def test_sess_cache_off(self):\n        \"\"\"\n        The value of `OpenSSL.SSL.SESS_CACHE_OFF` 0x0, the value of\n        `SSL_SESS_CACHE_OFF` defined by `openssl/ssl.h`.\n        \"\"\"\n        assert 0x0 == SESS_CACHE_OFF\n\n    def test_sess_cache_client(self):\n        \"\"\"\n        The value of `OpenSSL.SSL.SESS_CACHE_CLIENT` 0x1, the value of\n        `SSL_SESS_CACHE_CLIENT` defined by `openssl/ssl.h`.\n        \"\"\"\n        assert 0x1 == SESS_CACHE_CLIENT\n\n    def test_sess_cache_server(self):\n        \"\"\"\n        The value of `OpenSSL.SSL.SESS_CACHE_SERVER` 0x2, the value of\n        `SSL_SESS_CACHE_SERVER` defined by `openssl/ssl.h`.\n        \"\"\"\n        assert 0x2 == SESS_CACHE_SERVER\n\n    def test_sess_cache_both(self):\n        \"\"\"\n        The value of `OpenSSL.SSL.SESS_CACHE_BOTH` 0x3, the value of\n        `SSL_SESS_CACHE_BOTH` defined by `openssl/ssl.h`.\n        \"\"\"\n        assert 0x3 == SESS_CACHE_BOTH\n\n    def test_sess_cache_no_auto_clear(self):\n        \"\"\"\n        The value of `OpenSSL.SSL.SESS_CACHE_NO_AUTO_CLEAR` 0x80, the\n        value of `SSL_SESS_CACHE_NO_AUTO_CLEAR` defined by\n        `openssl/ssl.h`.\n        \"\"\"\n        assert 0x80 == SESS_CACHE_NO_AUTO_CLEAR\n\n    def test_sess_cache_no_internal_lookup(self):\n        \"\"\"\n        The value of `OpenSSL.SSL.SESS_CACHE_NO_INTERNAL_LOOKUP` 0x100,\n        the value of `SSL_SESS_CACHE_NO_INTERNAL_LOOKUP` defined by\n        `openssl/ssl.h`.\n        \"\"\"\n        assert 0x100 == SESS_CACHE_NO_INTERNAL_LOOKUP\n\n    def test_sess_cache_no_internal_store(self):\n        \"\"\"\n        The value of `OpenSSL.SSL.SESS_CACHE_NO_INTERNAL_STORE` 0x200,\n        the value of `SSL_SESS_CACHE_NO_INTERNAL_STORE` defined by\n        `openssl/ssl.h`.\n        \"\"\"\n        assert 0x200 == SESS_CACHE_NO_INTERNAL_STORE\n\n    def test_sess_cache_no_internal(self):\n        \"\"\"\n        The value of `OpenSSL.SSL.SESS_CACHE_NO_INTERNAL` 0x300, the\n        value of `SSL_SESS_CACHE_NO_INTERNAL` defined by\n        `openssl/ssl.h`.\n        \"\"\"\n        assert 0x300 == SESS_CACHE_NO_INTERNAL\n\n\nclass TestMemoryBIO(object):\n    \"\"\"\n    Tests for `OpenSSL.SSL.Connection` using a memory BIO.\n    \"\"\"\n    def _server(self, sock):\n        \"\"\"\n        Create a new server-side SSL `Connection` object wrapped around `sock`.\n        \"\"\"\n        # Create the server side Connection.  This is mostly setup boilerplate\n        # - use TLSv1, use a particular certificate, etc.\n        server_ctx = Context(TLSv1_METHOD)\n        server_ctx.set_options(OP_NO_SSLv2 | OP_NO_SSLv3 | OP_SINGLE_DH_USE)\n        server_ctx.set_verify(\n            VERIFY_PEER | VERIFY_FAIL_IF_NO_PEER_CERT | VERIFY_CLIENT_ONCE,\n            verify_cb\n        )\n        server_store = server_ctx.get_cert_store()\n        server_ctx.use_privatekey(\n            load_privatekey(FILETYPE_PEM, server_key_pem))\n        server_ctx.use_certificate(\n            load_certificate(FILETYPE_PEM, server_cert_pem))\n        server_ctx.check_privatekey()\n        server_store.add_cert(load_certificate(FILETYPE_PEM, root_cert_pem))\n        # Here the Connection is actually created.  If None is passed as the\n        # 2nd parameter, it indicates a memory BIO should be created.\n        server_conn = Connection(server_ctx, sock)\n        server_conn.set_accept_state()\n        return server_conn\n\n    def _client(self, sock):\n        \"\"\"\n        Create a new client-side SSL `Connection` object wrapped around `sock`.\n        \"\"\"\n        # Now create the client side Connection.  Similar boilerplate to the\n        # above.\n        client_ctx = Context(TLSv1_METHOD)\n        client_ctx.set_options(OP_NO_SSLv2 | OP_NO_SSLv3 | OP_SINGLE_DH_USE)\n        client_ctx.set_verify(\n            VERIFY_PEER | VERIFY_FAIL_IF_NO_PEER_CERT | VERIFY_CLIENT_ONCE,\n            verify_cb\n        )\n        client_store = client_ctx.get_cert_store()\n        client_ctx.use_privatekey(\n            load_privatekey(FILETYPE_PEM, client_key_pem))\n        client_ctx.use_certificate(\n            load_certificate(FILETYPE_PEM, client_cert_pem))\n        client_ctx.check_privatekey()\n        client_store.add_cert(load_certificate(FILETYPE_PEM, root_cert_pem))\n        client_conn = Connection(client_ctx, sock)\n        client_conn.set_connect_state()\n        return client_conn\n\n    def test_memory_connect(self):\n        \"\"\"\n        Two `Connection`s which use memory BIOs can be manually connected by\n        reading from the output of each and writing those bytes to the input of\n        the other and in this way establish a connection and exchange\n        application-level bytes with each other.\n        \"\"\"\n        server_conn = self._server(None)\n        client_conn = self._client(None)\n\n        # There should be no key or nonces yet.\n        assert server_conn.master_key() is None\n        assert server_conn.client_random() is None\n        assert server_conn.server_random() is None\n\n        # First, the handshake needs to happen.  We'll deliver bytes back and\n        # forth between the client and server until neither of them feels like\n        # speaking any more.\n        assert interact_in_memory(client_conn, server_conn) is None\n\n        # Now that the handshake is done, there should be a key and nonces.\n        assert server_conn.master_key() is not None\n        assert server_conn.client_random() is not None\n        assert server_conn.server_random() is not None\n        assert server_conn.client_random() == client_conn.client_random()\n        assert server_conn.server_random() == client_conn.server_random()\n        assert server_conn.client_random() != server_conn.server_random()\n        assert client_conn.client_random() != client_conn.server_random()\n\n        # Export key material for other uses.\n        cekm = client_conn.export_keying_material(b'LABEL', 32)\n        sekm = server_conn.export_keying_material(b'LABEL', 32)\n        assert cekm is not None\n        assert sekm is not None\n        assert cekm == sekm\n        assert len(sekm) == 32\n\n        # Export key material for other uses with additional context.\n        cekmc = client_conn.export_keying_material(b'LABEL', 32, b'CONTEXT')\n        sekmc = server_conn.export_keying_material(b'LABEL', 32, b'CONTEXT')\n        assert cekmc is not None\n        assert sekmc is not None\n        assert cekmc == sekmc\n        assert cekmc != cekm\n        assert sekmc != sekm\n        # Export with alternate label\n        cekmt = client_conn.export_keying_material(b'test', 32, b'CONTEXT')\n        sekmt = server_conn.export_keying_material(b'test', 32, b'CONTEXT')\n        assert cekmc != cekmt\n        assert sekmc != sekmt\n\n        # Here are the bytes we'll try to send.\n        important_message = b'One if by land, two if by sea.'\n\n        server_conn.write(important_message)\n        assert (\n            interact_in_memory(client_conn, server_conn) ==\n            (client_conn, important_message))\n\n        client_conn.write(important_message[::-1])\n        assert (\n            interact_in_memory(client_conn, server_conn) ==\n            (server_conn, important_message[::-1]))\n\n    def test_socket_connect(self):\n        \"\"\"\n        Just like `test_memory_connect` but with an actual socket.\n\n        This is primarily to rule out the memory BIO code as the source of any\n        problems encountered while passing data over a `Connection` (if\n        this test fails, there must be a problem outside the memory BIO code,\n        as no memory BIO is involved here).  Even though this isn't a memory\n        BIO test, it's convenient to have it here.\n        \"\"\"\n        server_conn, client_conn = loopback()\n\n        important_message = b\"Help me Obi Wan Kenobi, you're my only hope.\"\n        client_conn.send(important_message)\n        msg = server_conn.recv(1024)\n        assert msg == important_message\n\n        # Again in the other direction, just for fun.\n        important_message = important_message[::-1]\n        server_conn.send(important_message)\n        msg = client_conn.recv(1024)\n        assert msg == important_message\n\n    def test_socket_overrides_memory(self):\n        \"\"\"\n        Test that `OpenSSL.SSL.bio_read` and `OpenSSL.SSL.bio_write` don't\n        work on `OpenSSL.SSL.Connection`() that use sockets.\n        \"\"\"\n        context = Context(TLSv1_METHOD)\n        client = socket_any_family()\n        clientSSL = Connection(context, client)\n        with pytest.raises(TypeError):\n            clientSSL.bio_read(100)\n        with pytest.raises(TypeError):\n            clientSSL.bio_write(\"foo\")\n        with pytest.raises(TypeError):\n            clientSSL.bio_shutdown()\n\n    def test_outgoing_overflow(self):\n        \"\"\"\n        If more bytes than can be written to the memory BIO are passed to\n        `Connection.send` at once, the number of bytes which were written is\n        returned and that many bytes from the beginning of the input can be\n        read from the other end of the connection.\n        \"\"\"\n        server = self._server(None)\n        client = self._client(None)\n\n        interact_in_memory(client, server)\n\n        size = 2 ** 15\n        sent = client.send(b\"x\" * size)\n        # Sanity check.  We're trying to test what happens when the entire\n        # input can't be sent.  If the entire input was sent, this test is\n        # meaningless.\n        assert sent < size\n\n        receiver, received = interact_in_memory(client, server)\n        assert receiver is server\n\n        # We can rely on all of these bytes being received at once because\n        # loopback passes 2 ** 16 to recv - more than 2 ** 15.\n        assert len(received) == sent\n\n    def test_shutdown(self):\n        \"\"\"\n        `Connection.bio_shutdown` signals the end of the data stream\n        from which the `Connection` reads.\n        \"\"\"\n        server = self._server(None)\n        server.bio_shutdown()\n        with pytest.raises(Error) as err:\n            server.recv(1024)\n        # We don't want WantReadError or ZeroReturnError or anything - it's a\n        # handshake failure.\n        assert type(err.value) in [Error, SysCallError]\n\n    def test_unexpected_EOF(self):\n        \"\"\"\n        If the connection is lost before an orderly SSL shutdown occurs,\n        `OpenSSL.SSL.SysCallError` is raised with a message of\n        \"Unexpected EOF\".\n        \"\"\"\n        server_conn, client_conn = loopback()\n        client_conn.sock_shutdown(SHUT_RDWR)\n        with pytest.raises(SysCallError) as err:\n            server_conn.recv(1024)\n        assert err.value.args == (-1, \"Unexpected EOF\")\n\n    def _check_client_ca_list(self, func):\n        \"\"\"\n        Verify the return value of the `get_client_ca_list` method for\n        server and client connections.\n\n        :param func: A function which will be called with the server context\n            before the client and server are connected to each other.  This\n            function should specify a list of CAs for the server to send to the\n            client and return that same list.  The list will be used to verify\n            that `get_client_ca_list` returns the proper value at\n            various times.\n        \"\"\"\n        server = self._server(None)\n        client = self._client(None)\n        assert client.get_client_ca_list() == []\n        assert server.get_client_ca_list() == []\n        ctx = server.get_context()\n        expected = func(ctx)\n        assert client.get_client_ca_list() == []\n        assert server.get_client_ca_list() == expected\n        interact_in_memory(client, server)\n        assert client.get_client_ca_list() == expected\n        assert server.get_client_ca_list() == expected\n\n    def test_set_client_ca_list_errors(self):\n        \"\"\"\n        `Context.set_client_ca_list` raises a `TypeError` if called with a\n        non-list or a list that contains objects other than X509Names.\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        with pytest.raises(TypeError):\n            ctx.set_client_ca_list(\"spam\")\n        with pytest.raises(TypeError):\n            ctx.set_client_ca_list([\"spam\"])\n\n    def test_set_empty_ca_list(self):\n        \"\"\"\n        If passed an empty list, `Context.set_client_ca_list` configures the\n        context to send no CA names to the client and, on both the server and\n        client sides, `Connection.get_client_ca_list` returns an empty list\n        after the connection is set up.\n        \"\"\"\n        def no_ca(ctx):\n            ctx.set_client_ca_list([])\n            return []\n        self._check_client_ca_list(no_ca)\n\n    def test_set_one_ca_list(self):\n        \"\"\"\n        If passed a list containing a single X509Name,\n        `Context.set_client_ca_list` configures the context to send\n        that CA name to the client and, on both the server and client sides,\n        `Connection.get_client_ca_list` returns a list containing that\n        X509Name after the connection is set up.\n        \"\"\"\n        cacert = load_certificate(FILETYPE_PEM, root_cert_pem)\n        cadesc = cacert.get_subject()\n\n        def single_ca(ctx):\n            ctx.set_client_ca_list([cadesc])\n            return [cadesc]\n        self._check_client_ca_list(single_ca)\n\n    def test_set_multiple_ca_list(self):\n        \"\"\"\n        If passed a list containing multiple X509Name objects,\n        `Context.set_client_ca_list` configures the context to send\n        those CA names to the client and, on both the server and client sides,\n        `Connection.get_client_ca_list` returns a list containing those\n        X509Names after the connection is set up.\n        \"\"\"\n        secert = load_certificate(FILETYPE_PEM, server_cert_pem)\n        clcert = load_certificate(FILETYPE_PEM, server_cert_pem)\n\n        sedesc = secert.get_subject()\n        cldesc = clcert.get_subject()\n\n        def multiple_ca(ctx):\n            L = [sedesc, cldesc]\n            ctx.set_client_ca_list(L)\n            return L\n        self._check_client_ca_list(multiple_ca)\n\n    def test_reset_ca_list(self):\n        \"\"\"\n        If called multiple times, only the X509Names passed to the final call\n        of `Context.set_client_ca_list` are used to configure the CA\n        names sent to the client.\n        \"\"\"\n        cacert = load_certificate(FILETYPE_PEM, root_cert_pem)\n        secert = load_certificate(FILETYPE_PEM, server_cert_pem)\n        clcert = load_certificate(FILETYPE_PEM, server_cert_pem)\n\n        cadesc = cacert.get_subject()\n        sedesc = secert.get_subject()\n        cldesc = clcert.get_subject()\n\n        def changed_ca(ctx):\n            ctx.set_client_ca_list([sedesc, cldesc])\n            ctx.set_client_ca_list([cadesc])\n            return [cadesc]\n        self._check_client_ca_list(changed_ca)\n\n    def test_mutated_ca_list(self):\n        \"\"\"\n        If the list passed to `Context.set_client_ca_list` is mutated\n        afterwards, this does not affect the list of CA names sent to the\n        client.\n        \"\"\"\n        cacert = load_certificate(FILETYPE_PEM, root_cert_pem)\n        secert = load_certificate(FILETYPE_PEM, server_cert_pem)\n\n        cadesc = cacert.get_subject()\n        sedesc = secert.get_subject()\n\n        def mutated_ca(ctx):\n            L = [cadesc]\n            ctx.set_client_ca_list([cadesc])\n            L.append(sedesc)\n            return [cadesc]\n        self._check_client_ca_list(mutated_ca)\n\n    def test_add_client_ca_wrong_args(self):\n        \"\"\"\n        `Context.add_client_ca` raises `TypeError` if called with\n        a non-X509 object.\n        \"\"\"\n        ctx = Context(TLSv1_METHOD)\n        with pytest.raises(TypeError):\n            ctx.add_client_ca(\"spam\")\n\n    def test_one_add_client_ca(self):\n        \"\"\"\n        A certificate's subject can be added as a CA to be sent to the client\n        with `Context.add_client_ca`.\n        \"\"\"\n        cacert = load_certificate(FILETYPE_PEM, root_cert_pem)\n        cadesc = cacert.get_subject()\n\n        def single_ca(ctx):\n            ctx.add_client_ca(cacert)\n            return [cadesc]\n        self._check_client_ca_list(single_ca)\n\n    def test_multiple_add_client_ca(self):\n        \"\"\"\n        Multiple CA names can be sent to the client by calling\n        `Context.add_client_ca` with multiple X509 objects.\n        \"\"\"\n        cacert = load_certificate(FILETYPE_PEM, root_cert_pem)\n        secert = load_certificate(FILETYPE_PEM, server_cert_pem)\n\n        cadesc = cacert.get_subject()\n        sedesc = secert.get_subject()\n\n        def multiple_ca(ctx):\n            ctx.add_client_ca(cacert)\n            ctx.add_client_ca(secert)\n            return [cadesc, sedesc]\n        self._check_client_ca_list(multiple_ca)\n\n    def test_set_and_add_client_ca(self):\n        \"\"\"\n        A call to `Context.set_client_ca_list` followed by a call to\n        `Context.add_client_ca` results in using the CA names from the\n        first call and the CA name from the second call.\n        \"\"\"\n        cacert = load_certificate(FILETYPE_PEM, root_cert_pem)\n        secert = load_certificate(FILETYPE_PEM, server_cert_pem)\n        clcert = load_certificate(FILETYPE_PEM, server_cert_pem)\n\n        cadesc = cacert.get_subject()\n        sedesc = secert.get_subject()\n        cldesc = clcert.get_subject()\n\n        def mixed_set_add_ca(ctx):\n            ctx.set_client_ca_list([cadesc, sedesc])\n            ctx.add_client_ca(clcert)\n            return [cadesc, sedesc, cldesc]\n        self._check_client_ca_list(mixed_set_add_ca)\n\n    def test_set_after_add_client_ca(self):\n        \"\"\"\n        A call to `Context.set_client_ca_list` after a call to\n        `Context.add_client_ca` replaces the CA name specified by the\n        former call with the names specified by the latter call.\n        \"\"\"\n        cacert = load_certificate(FILETYPE_PEM, root_cert_pem)\n        secert = load_certificate(FILETYPE_PEM, server_cert_pem)\n        clcert = load_certificate(FILETYPE_PEM, server_cert_pem)\n\n        cadesc = cacert.get_subject()\n        sedesc = secert.get_subject()\n\n        def set_replaces_add_ca(ctx):\n            ctx.add_client_ca(clcert)\n            ctx.set_client_ca_list([cadesc])\n            ctx.add_client_ca(secert)\n            return [cadesc, sedesc]\n        self._check_client_ca_list(set_replaces_add_ca)\n\n\nclass TestInfoConstants(object):\n    \"\"\"\n    Tests for assorted constants exposed for use in info callbacks.\n    \"\"\"\n    def test_integers(self):\n        \"\"\"\n        All of the info constants are integers.\n\n        This is a very weak test.  It would be nice to have one that actually\n        verifies that as certain info events happen, the value passed to the\n        info callback matches up with the constant exposed by OpenSSL.SSL.\n        \"\"\"\n        for const in [\n            SSL_ST_CONNECT, SSL_ST_ACCEPT, SSL_ST_MASK,\n            SSL_CB_LOOP, SSL_CB_EXIT, SSL_CB_READ, SSL_CB_WRITE, SSL_CB_ALERT,\n            SSL_CB_READ_ALERT, SSL_CB_WRITE_ALERT, SSL_CB_ACCEPT_LOOP,\n            SSL_CB_ACCEPT_EXIT, SSL_CB_CONNECT_LOOP, SSL_CB_CONNECT_EXIT,\n            SSL_CB_HANDSHAKE_START, SSL_CB_HANDSHAKE_DONE\n        ]:\n            assert isinstance(const, int)\n\n        # These constants don't exist on OpenSSL 1.1.0\n        for const in [\n            SSL_ST_INIT, SSL_ST_BEFORE, SSL_ST_OK, SSL_ST_RENEGOTIATE\n        ]:\n            assert const is None or isinstance(const, int)\n\n\nclass TestRequires(object):\n    \"\"\"\n    Tests for the decorator factory used to conditionally raise\n    NotImplementedError when older OpenSSLs are used.\n    \"\"\"\n    def test_available(self):\n        \"\"\"\n        When the OpenSSL functionality is available the decorated functions\n        work appropriately.\n        \"\"\"\n        feature_guard = _make_requires(True, \"Error text\")\n        results = []\n\n        @feature_guard\n        def inner():\n            results.append(True)\n            return True\n\n        assert inner() is True\n        assert [True] == results\n\n    def test_unavailable(self):\n        \"\"\"\n        When the OpenSSL functionality is not available the decorated function\n        does not execute and NotImplementedError is raised.\n        \"\"\"\n        feature_guard = _make_requires(False, \"Error text\")\n\n        @feature_guard\n        def inner():  # pragma: nocover\n            pytest.fail(\"Should not be called\")\n\n        with pytest.raises(NotImplementedError) as e:\n            inner()\n\n        assert \"Error text\" in str(e.value)\n\n\nclass TestOCSP(object):\n    \"\"\"\n    Tests for PyOpenSSL's OCSP stapling support.\n    \"\"\"\n    sample_ocsp_data = b\"this is totally ocsp data\"\n\n    def _client_connection(self, callback, data, request_ocsp=True):\n        \"\"\"\n        Builds a client connection suitable for using OCSP.\n\n        :param callback: The callback to register for OCSP.\n        :param data: The opaque data object that will be handed to the\n            OCSP callback.\n        :param request_ocsp: Whether the client will actually ask for OCSP\n            stapling. Useful for testing only.\n        \"\"\"\n        ctx = Context(SSLv23_METHOD)\n        ctx.set_ocsp_client_callback(callback, data)\n        client = Connection(ctx)\n\n        if request_ocsp:\n            client.request_ocsp()\n\n        client.set_connect_state()\n        return client\n\n    def _server_connection(self, callback, data):\n        \"\"\"\n        Builds a server connection suitable for using OCSP.\n\n        :param callback: The callback to register for OCSP.\n        :param data: The opaque data object that will be handed to the\n            OCSP callback.\n        \"\"\"\n        ctx = Context(SSLv23_METHOD)\n        ctx.use_privatekey(load_privatekey(FILETYPE_PEM, server_key_pem))\n        ctx.use_certificate(load_certificate(FILETYPE_PEM, server_cert_pem))\n        ctx.set_ocsp_server_callback(callback, data)\n        server = Connection(ctx)\n        server.set_accept_state()\n        return server\n\n    def test_callbacks_arent_called_by_default(self):\n        \"\"\"\n        If both the client and the server have registered OCSP callbacks, but\n        the client does not send the OCSP request, neither callback gets\n        called.\n        \"\"\"\n        def ocsp_callback(*args, **kwargs):  # pragma: nocover\n            pytest.fail(\"Should not be called\")\n\n        client = self._client_connection(\n            callback=ocsp_callback, data=None, request_ocsp=False\n        )\n        server = self._server_connection(callback=ocsp_callback, data=None)\n        handshake_in_memory(client, server)\n\n    def test_client_negotiates_without_server(self):\n        \"\"\"\n        If the client wants to do OCSP but the server does not, the handshake\n        succeeds, and the client callback fires with an empty byte string.\n        \"\"\"\n        called = []\n\n        def ocsp_callback(conn, ocsp_data, ignored):\n            called.append(ocsp_data)\n            return True\n\n        client = self._client_connection(callback=ocsp_callback, data=None)\n        server = loopback_server_factory(socket=None)\n        handshake_in_memory(client, server)\n\n        assert len(called) == 1\n        assert called[0] == b''\n\n    def test_client_receives_servers_data(self):\n        \"\"\"\n        The data the server sends in its callback is received by the client.\n        \"\"\"\n        calls = []\n\n        def server_callback(*args, **kwargs):\n            return self.sample_ocsp_data\n\n        def client_callback(conn, ocsp_data, ignored):\n            calls.append(ocsp_data)\n            return True\n\n        client = self._client_connection(callback=client_callback, data=None)\n        server = self._server_connection(callback=server_callback, data=None)\n        handshake_in_memory(client, server)\n\n        assert len(calls) == 1\n        assert calls[0] == self.sample_ocsp_data\n\n    def test_callbacks_are_invoked_with_connections(self):\n        \"\"\"\n        The first arguments to both callbacks are their respective connections.\n        \"\"\"\n        client_calls = []\n        server_calls = []\n\n        def client_callback(conn, *args, **kwargs):\n            client_calls.append(conn)\n            return True\n\n        def server_callback(conn, *args, **kwargs):\n            server_calls.append(conn)\n            return self.sample_ocsp_data\n\n        client = self._client_connection(callback=client_callback, data=None)\n        server = self._server_connection(callback=server_callback, data=None)\n        handshake_in_memory(client, server)\n\n        assert len(client_calls) == 1\n        assert len(server_calls) == 1\n        assert client_calls[0] is client\n        assert server_calls[0] is server\n\n    def test_opaque_data_is_passed_through(self):\n        \"\"\"\n        Both callbacks receive an opaque, user-provided piece of data in their\n        callbacks as the final argument.\n        \"\"\"\n        calls = []\n\n        def server_callback(*args):\n            calls.append(args)\n            return self.sample_ocsp_data\n\n        def client_callback(*args):\n            calls.append(args)\n            return True\n\n        sentinel = object()\n\n        client = self._client_connection(\n            callback=client_callback, data=sentinel\n        )\n        server = self._server_connection(\n            callback=server_callback, data=sentinel\n        )\n        handshake_in_memory(client, server)\n\n        assert len(calls) == 2\n        assert calls[0][-1] is sentinel\n        assert calls[1][-1] is sentinel\n\n    def test_server_returns_empty_string(self):\n        \"\"\"\n        If the server returns an empty bytestring from its callback, the\n        client callback is called with the empty bytestring.\n        \"\"\"\n        client_calls = []\n\n        def server_callback(*args):\n            return b''\n\n        def client_callback(conn, ocsp_data, ignored):\n            client_calls.append(ocsp_data)\n            return True\n\n        client = self._client_connection(callback=client_callback, data=None)\n        server = self._server_connection(callback=server_callback, data=None)\n        handshake_in_memory(client, server)\n\n        assert len(client_calls) == 1\n        assert client_calls[0] == b''\n\n    def test_client_returns_false_terminates_handshake(self):\n        \"\"\"\n        If the client returns False from its callback, the handshake fails.\n        \"\"\"\n        def server_callback(*args):\n            return self.sample_ocsp_data\n\n        def client_callback(*args):\n            return False\n\n        client = self._client_connection(callback=client_callback, data=None)\n        server = self._server_connection(callback=server_callback, data=None)\n\n        with pytest.raises(Error):\n            handshake_in_memory(client, server)\n\n    def test_exceptions_in_client_bubble_up(self):\n        \"\"\"\n        The callbacks thrown in the client callback bubble up to the caller.\n        \"\"\"\n        class SentinelException(Exception):\n            pass\n\n        def server_callback(*args):\n            return self.sample_ocsp_data\n\n        def client_callback(*args):\n            raise SentinelException()\n\n        client = self._client_connection(callback=client_callback, data=None)\n        server = self._server_connection(callback=server_callback, data=None)\n\n        with pytest.raises(SentinelException):\n            handshake_in_memory(client, server)\n\n    def test_exceptions_in_server_bubble_up(self):\n        \"\"\"\n        The callbacks thrown in the server callback bubble up to the caller.\n        \"\"\"\n        class SentinelException(Exception):\n            pass\n\n        def server_callback(*args):\n            raise SentinelException()\n\n        def client_callback(*args):  # pragma: nocover\n            pytest.fail(\"Should not be called\")\n\n        client = self._client_connection(callback=client_callback, data=None)\n        server = self._server_connection(callback=server_callback, data=None)\n\n        with pytest.raises(SentinelException):\n            handshake_in_memory(client, server)\n\n    def test_server_must_return_bytes(self):\n        \"\"\"\n        The server callback must return a bytestring, or a TypeError is thrown.\n        \"\"\"\n        def server_callback(*args):\n            return self.sample_ocsp_data.decode('ascii')\n\n        def client_callback(*args):  # pragma: nocover\n            pytest.fail(\"Should not be called\")\n\n        client = self._client_connection(callback=client_callback, data=None)\n        server = self._server_connection(callback=server_callback, data=None)\n\n        with pytest.raises(TypeError):\n            handshake_in_memory(client, server)\n"
  },
  {
    "prompt": "class MemoizationPolicy\n\nclass AugmentedMemoizationPolicy\n\n\"\"\"Load the NLU interpreter at `nlu_path`.\n\nArgs:\n    agent: Instance of `Agent` to inspect for an interpreter if `nlu_path` is\n        `None`.\n    nlu_path: NLU model path.\n\nReturns:\n    The NLU interpreter.\"\"\"\nfn _load_interpreter(agent: Agent, nlu_path: Optional[Text]) -> NaturalLanguageInterpreter\n\n\"\"\"Load the domain and policy ensemble from the model at `core_path`.\n\nArgs:\n    core_path: Core model path.\n\nReturns:\n    An instance of `Domain` and `PolicyEnsemble` if `core_path` is not `None`.\"\"\"\nfn _load_domain_and_policy_ensemble(core_path: Optional[Text]) -> Tuple[(Optional[Domain], Optional[PolicyEnsemble])]\n\n\"\"\"Load the persisted model into memory and set the model on the agent.\n\nArgs:\n    agent: Instance of `Agent` to update with the new model.\n    model_directory: Rasa model directory.\n    fingerprint: Fingerprint of the supplied model at `model_directory`.\"\"\"\nfn _load_and_set_updated_model(agent: Agent, model_directory: Text, fingerprint: Text) -> None\n\nclass Agent\n\n\"\"\"Log the sklearn evaluation metrics.\"\"\"\nfn log_evaluation_table(report: Text, precision: float, f1: float, accuracy: float) -> None\n\n\"\"\"Remove those examples without an intent.\n\nArgs:\n    intent_results: intent evaluation results\n\nReturns: intent evaluation results\"\"\"\nfn remove_empty_intent_examples(intent_results: List[IntentEvaluationResult]) -> List[IntentEvaluationResult]\n\n\"\"\"Remove those examples without a response.\n\nArgs:\n    response_results: response selection evaluation results\n\nReturns: response selection evaluation results\"\"\"\nfn remove_empty_response_examples(response_results: List[ResponseSelectionEvaluationResult]) -> List[ResponseSelectionEvaluationResult]\n\n\"\"\"Remove intent groups with less than cutoff instances.\n\nArgs:\n    training_data: training data\n    cutoff: threshold\n\nReturns: updated training data\"\"\"\nfn drop_intents_below_freq(training_data: TrainingData, cutoff: int) -> TrainingData\n\n\"\"\"Write successful intent predictions to a file.\n\nArgs:\n    intent_results: intent evaluation result\n    successes_filename: filename of file to save successful predictions to\"\"\"\nfn write_intent_successes(intent_results: List[IntentEvaluationResult], successes_filename: Text) -> None\n\n\"\"\"Write incorrect intent predictions to a file.\n\nArgs:\n    intent_results: intent evaluation result\n    errors_filename: filename of file to save incorrect predictions to\"\"\"\nfn write_intent_errors(intent_results: List[IntentEvaluationResult], errors_filename: Text) -> None\n\n\"\"\"Write successful response selection predictions to a file.\n\nArgs:\n    response_results: response selection evaluation result\n    successes_filename: filename of file to save successful predictions to\"\"\"\nfn write_response_successes(response_results: List[ResponseSelectionEvaluationResult], successes_filename: Text) -> None\n\n\"\"\"Write incorrect response selection predictions to a file.\n\nArgs:\n    response_results: response selection evaluation result\n    errors_filename: filename of file to save incorrect predictions to\"\"\"\nfn write_response_errors(response_results: List[ResponseSelectionEvaluationResult], errors_filename: Text) -> None\n\n\"\"\"Create histogram of confidence distribution.\n\nArgs:\n    results: evaluation results\n    hist_filename: filename to save plot to\n    target_key: key of target in results\n    prediction_key: key of predictions in results\n    title: title of plot\"\"\"\nfn plot_attribute_confidences(results: Union[(List[IntentEvaluationResult], List[ResponseSelectionEvaluationResult])], hist_filename: Optional[Text], target_key: Text, prediction_key: Text, title: Text) -> None\n\n\"\"\"Create histogram of confidence distribution.\n\nArgs:\n    results: evaluation results\n    hist_filename: filename to save plot to\n    target_key: key of target in results\n    prediction_key: key of predictions in results\n    title: title of plot\"\"\"\nfn plot_entity_confidences(merged_targets: List[Text], merged_predictions: List[Text], merged_confidences: List[float], hist_filename: Text, title: Text) -> None\n\n\"\"\"Creates summary statistics for response selection.\n\nOnly considers those examples with a set response.\nOthers are filtered out. Returns a dictionary of containing the\nevaluation result.\n\nArgs:\n    response_selection_results: response selection evaluation results\n    output_directory: directory to store files to\n    successes: if True success are written down to disk\n    errors: if True errors are written down to disk\n    disable_plotting: if True no plots are created\n\nReturns: dictionary with evaluation results\"\"\"\nfn evaluate_response_selections(response_selection_results: List[ResponseSelectionEvaluationResult], output_directory: Optional[Text], successes: bool, errors: bool, disable_plotting: bool) -> Dict\n\n\"\"\"Adds a field \"confused_with\" to the evaluation report.\n\nThe value is a dict of {\"false_positive_label\": false_positive_count} pairs.\nIf there are no false positives in the confusion matrix,\nthe dict will be empty. Typically we include the two most\ncommonly false positive labels, three in the rare case that\nthe diagonal element in the confusion matrix is not one of the\nthree highest values in the row.\n\nArgs:\n    report: the evaluation report\n    confusion_matrix: confusion matrix\n    labels: list of labels\n\nReturns: updated evaluation report\"\"\"\nfn _add_confused_labels_to_report(report: Dict[(Text, Dict[(Text, Any)])], confusion_matrix: np.ndarray, labels: List[Text], exclude_labels: Optional[List[Text]]) -> Dict[(Text, Dict[(Text, Union[(Dict, Any)])])]\n\n\"\"\"Creates summary statistics for intents.\n\nOnly considers those examples with a set intent. Others are filtered out.\nReturns a dictionary of containing the evaluation result.\n\nArgs:\n    intent_results: intent evaluation results\n    output_directory: directory to store files to\n    successes: if True correct predictions are written to disk\n    errors: if True incorrect predictions are written to disk\n    disable_plotting: if True no plots are created\n\nReturns: dictionary with evaluation results\"\"\"\nfn evaluate_intents(intent_results: List[IntentEvaluationResult], output_directory: Optional[Text], successes: bool, errors: bool, disable_plotting: bool) -> Dict\n\n\"\"\"Concatenates all labels of the aligned predictions.\n\nTakes the aligned prediction labels which are grouped for each message\nand concatenates them.\n\nArgs:\n    aligned_predictions: aligned predictions\n    extractor: entity extractor name\n\nReturns: concatenated predictions\"\"\"\nfn merge_labels(aligned_predictions: List[Dict], extractor: Optional[Text]) -> List[Text]\n\n\"\"\"Concatenates all confidences of the aligned predictions.\n\nTakes the aligned prediction confidences which are grouped for each message\nand concatenates them.\n\nArgs:\n    aligned_predictions: aligned predictions\n    extractor: entity extractor name\n\nReturns: concatenated confidences\"\"\"\nfn merge_confidences(aligned_predictions: List[Dict], extractor: Optional[Text]) -> List[float]\n\n\"\"\"Replaces label names in a list of labels.\n\nArgs:\n    labels: list of labels\n    old: old label name that should be replaced\n    new: new label name\n\nReturns: updated labels\"\"\"\nfn substitute_labels(labels: List[Text], old: Text, new: Text) -> List[Text]\n\n\"\"\"Write incorrect entity predictions to a file.\n\nArgs:\n    entity_results: response selection evaluation result\n    merged_predictions: list of predicted entity labels\n    merged_targets: list of true entity labels\n    error_filename: filename of file to save incorrect predictions to\"\"\"\nfn write_incorrect_entity_predictions(entity_results: List[EntityEvaluationResult], merged_targets: List[Text], merged_predictions: List[Text], error_filename: Text) -> None\n\n\"\"\"Get incorrect entity predictions.\n\nArgs:\n    entity_results: entity evaluation results\n    merged_predictions: list of predicted entity labels\n    merged_targets: list of true entity labels\n\nReturns: list of incorrect predictions\"\"\"\nfn collect_incorrect_entity_predictions(entity_results: List[EntityEvaluationResult], merged_predictions: List[Text], merged_targets: List[Text])\n\n\"\"\"Write correct entity predictions to a file.\n\nArgs:\n    entity_results: response selection evaluation result\n    merged_predictions: list of predicted entity labels\n    merged_targets: list of true entity labels\n    successes_filename: filename of file to save correct predictions to\"\"\"\nfn write_successful_entity_predictions(entity_results: List[EntityEvaluationResult], merged_targets: List[Text], merged_predictions: List[Text], successes_filename: Text) -> None\n\n\"\"\"Get correct entity predictions.\n\nArgs:\n    entity_results: entity evaluation results\n    merged_predictions: list of predicted entity labels\n    merged_targets: list of true entity labels\n\nReturns: list of correct predictions\"\"\"\nfn collect_successful_entity_predictions(entity_results: List[EntityEvaluationResult], merged_predictions: List[Text], merged_targets: List[Text])\n\n\"\"\"Creates summary statistics for each entity extractor.\n\nLogs precision, recall, and F1 per entity type for each extractor.\n\nArgs:\n    entity_results: entity evaluation results\n    extractors: entity extractors to consider\n    output_directory: directory to store files to\n    successes: if True correct predictions are written to disk\n    errors: if True incorrect predictions are written to disk\n    disable_plotting: if True no plots are created\n\nReturns: dictionary with evaluation results\"\"\"\nfn evaluate_entities(entity_results: List[EntityEvaluationResult], extractors: Set[Text], output_directory: Optional[Text], successes: bool, errors: bool, disable_plotting: bool) -> Dict\n\n\"\"\"Checks if a token is within the boundaries of an entity.\"\"\"\nfn is_token_within_entity(token: Token, entity: Dict) -> bool\n\n\"\"\"Checks if a token crosses the boundaries of an entity.\"\"\"\nfn does_token_cross_borders(token: Token, entity: Dict) -> bool\n\n\"\"\"Calculates how many characters a given token and entity share.\"\"\"\nfn determine_intersection(token: Token, entity: Dict) -> int\n\n\"\"\"Checks if entities overlap.\n\nI.e. cross each others start and end boundaries.\n\nArgs:\n    entities: list of entities\n\nReturns: true if entities overlap, false otherwise.\"\"\"\nfn do_entities_overlap(entities: List[Dict]) -> bool\n\n\"\"\"Finds the entities that intersect with a token.\n\nArgs:\n    token: a single token\n    entities: entities found by a single extractor\n\nReturns: list of entities\"\"\"\nfn find_intersecting_entities(token: Token, entities: List[Dict]) -> List[Dict]\n\n\"\"\"Determines the best fitting entity given intersecting entities.\n\nArgs:\n    token: a single token\n    candidates: entities found by a single extractor\n    attribute_key: the attribute key of interest\n\nReturns:\n    the value of the attribute key of the best fitting entity\"\"\"\nfn pick_best_entity_fit(token: Token, candidates: List[Dict[(Text, Any)]]) -> Optional[Dict[(Text, Any)]]\n\n\"\"\"Determines the token label for the provided attribute key given entities that do\nnot overlap.\n\nArgs:\n    token: a single token\n    entities: entities found by a single extractor\n    extractors: list of extractors\n    attribute_key: the attribute key for which the entity type should be returned\nReturns:\n    entity type\"\"\"\nfn determine_token_labels(token: Token, entities: List[Dict], extractors: Optional[Set[Text]], attribute_key: Text) -> Text\n\n\"\"\"Determines the best fitting entity for the given token, given entities that do\nnot overlap.\n\nArgs:\n    token: a single token\n    entities: entities found by a single extractor\n    extractors: list of extractors\n\nReturns:\n    entity type\"\"\"\nfn determine_entity_for_token(token: Token, entities: List[Dict[(Text, Any)]], extractors: Optional[Set[Text]]) -> Optional[Dict[(Text, Any)]]\n\n\"\"\"Checks if extractors support overlapping entities\"\"\"\nfn do_extractors_support_overlap(extractors: Optional[Set[Text]]) -> bool\n\n\"\"\"Aligns entity predictions to the message tokens.\n\nDetermines for every token the true label based on the\nprediction targets and the label assigned by each\nsingle extractor.\n\nArgs:\n    result: entity evaluation result\n    extractors: the entity extractors that should be considered\n\nReturns: dictionary containing the true token labels and token labels\n         from the extractors\"\"\"\nfn align_entity_predictions(result: EntityEvaluationResult, extractors: Set[Text]) -> Dict\n\n\"\"\"Concatenate labels for entity type, role, and group for evaluation.\n\nIn order to calculate metrics also for entity type, role, and group we need to\nconcatenate their labels. For example, 'location.destination'. This allows\nus to report metrics for every combination of entity type, role, and group.\n\nArgs:\n    token: the token we are looking at\n    entities: the available entities\n    extractors: the extractor of interest\n\nReturns:\n    the entity label of the provided token\"\"\"\nfn _concat_entity_labels(token: Token, entities: List[Dict], extractors: Optional[Set[Text]]) -> Text\n\n\"\"\"Get the confidence value of the best fitting entity.\n\nIf multiple confidence values are present, e.g. for type, role, group, we\npick the lowest confidence value.\n\nArgs:\n    token: the token we are looking at\n    entities: the available entities\n    extractors: the extractor of interest\n\nReturns:\n    the confidence value\"\"\"\nfn _get_entity_confidences(token: Token, entities: List[Dict], extractors: Optional[Set[Text]]) -> float\n\n\"\"\"Aligns entity predictions to the message tokens for the whole dataset\nusing align_entity_predictions.\n\nArgs:\n    entity_results: list of entity prediction results\n    extractors: the entity extractors that should be considered\n\nReturns: list of dictionaries containing the true token labels and token\nlabels from the extractors\"\"\"\nfn align_all_entity_predictions(entity_results: List[EntityEvaluationResult], extractors: Set[Text]) -> List[Dict]\n\n\"\"\"Runs the model for the test set and extracts targets and predictions.\n\nReturns intent results (intent targets and predictions, the original\nmessages and the confidences of the predictions), response results (\nresponse targets and predictions) as well as entity results\n(entity_targets, entity_predictions, and tokens).\n\nArgs:\n    interpreter: the interpreter\n    test_data: test data\n\nReturns: intent, response, and entity evaluation results\"\"\"\nfn get_eval_data(interpreter: Interpreter, test_data: TrainingData) -> Tuple[(List[IntentEvaluationResult], List[ResponseSelectionEvaluationResult], List[EntityEvaluationResult])]\n\n\"\"\"Finds the names of entity extractors used by the interpreter.\n\nProcessors are removed since they do not detect the boundaries themselves.\n\nArgs:\n    interpreter: the interpreter\n\nReturns: entity extractor names\"\"\"\nfn get_entity_extractors(interpreter: Interpreter) -> Set[Text]\n\n\"\"\"Checks whether entity extractor is present.\"\"\"\nfn is_entity_extractor_present(interpreter: Interpreter) -> bool\n\n\"\"\"Checks whether intent classifier is present.\"\"\"\nfn is_intent_classifier_present(interpreter: Interpreter) -> bool\n\n\"\"\"Checks whether response selector is present.\"\"\"\nfn is_response_selector_present(interpreter: Interpreter) -> bool\n\n\"\"\"Gets all available response selector types.\"\"\"\nfn get_available_response_selector_types(interpreter: Interpreter) -> List[Text]\n\n\"\"\"Remove pre-trained extractors from the pipeline.\n\nRemove pre-trained extractors so that entities from pre-trained extractors\nare not predicted upon parsing.\n\nArgs:\n    pipeline: the pipeline\n\nReturns:\n    Updated pipeline\"\"\"\nfn remove_pretrained_extractors(pipeline: List[Component]) -> List[Component]\n\n\"\"\"Evaluate intent classification, response selection and entity extraction.\n\nArgs:\n    data_path: path to the test data\n    model_path: path to the model\n    output_directory: path to folder where all output will be stored\n    successes: if true successful predictions are written to a file\n    errors: if true incorrect predictions are written to a file\n    component_builder: component builder\n    disable_plotting: if true confusion matrix and histogram will not be rendered\n\nReturns: dictionary containing evaluation results\"\"\"\nfn run_evaluation(data_path: Text, model_path: Text, output_directory: Optional[Text], successes: bool, errors: bool, component_builder: Optional[ComponentBuilder], disable_plotting: bool) -> Dict\n\n\"\"\"Generates n cross validation folds for given training data.\"\"\"\nfn generate_folds(n: int, training_data: TrainingData) -> Iterator[Tuple[(TrainingData, TrainingData)]]\n\n\"\"\"Collects intent, response selection and entity metrics for cross validation\nfolds.\n\nIf `intent_results`, `response_selection_results` or `entity_results` is provided\nas a list, prediction results are also collected.\n\nArgs:\n    intent_metrics: intent metrics\n    entity_metrics: entity metrics\n    response_selection_metrics: response selection metrics\n    interpreter: the interpreter\n    data: training data\n    intent_results: intent evaluation results\n    entity_results: entity evaluation results\n    response_selection_results: reponse selection evaluation results\n\nReturns: intent, entity, and response selection metrics\"\"\"\nfn combine_result(intent_metrics: IntentMetrics, entity_metrics: EntityMetrics, response_selection_metrics: ResponseSelectionMetrics, interpreter: Interpreter, data: TrainingData, intent_results: Optional[List[IntentEvaluationResult]], entity_results: Optional[List[EntityEvaluationResult]], response_selection_results: Optional[List[ResponseSelectionEvaluationResult]]) -> Tuple[(IntentMetrics, EntityMetrics, ResponseSelectionMetrics)]\n\nfn _contains_entity_labels(entity_results: List[EntityEvaluationResult]) -> bool\n\n\"\"\"Stratified cross validation on data.\n\nArgs:\n    data: Training Data\n    n_folds: integer, number of cv folds\n    nlu_config: nlu config file\n    output: path to folder where reports are stored\n    successes: if true successful predictions are written to a file\n    errors: if true incorrect predictions are written to a file\n    disable_plotting: if true no confusion matrix and historgram plates are created\n\nReturns:\n    dictionary with key, list structure, where each entry in list\n          corresponds to the relevant result for one fold\"\"\"\nfn cross_validate(data: TrainingData, n_folds: int, nlu_config: Union[(RasaNLUModelConfig, Text)], output: Optional[Text], successes: bool, errors: bool, disable_plotting: bool) -> Tuple[(CVEvaluationResult, CVEvaluationResult, CVEvaluationResult)]\n\nfn _targets_predictions_from(results: Union[(List[IntentEvaluationResult], List[ResponseSelectionEvaluationResult])], target_key: Text, prediction_key: Text) -> Iterator[Iterable[Optional[Text]]]\n\n\"\"\"Computes metrics for intent classification, response selection and entity\nextraction.\n\nArgs:\n    interpreter: the interpreter\n    training_data: training data\n\nReturns: intent, response selection and entity metrics, and prediction results.\"\"\"\nfn compute_metrics(interpreter: Interpreter, training_data: TrainingData) -> Tuple[(IntentMetrics, EntityMetrics, ResponseSelectionMetrics, List[IntentEvaluationResult], List[EntityEvaluationResult], List[ResponseSelectionEvaluationResult])]\n\n\"\"\"Trains and compares multiple NLU models.\nFor each run and exclusion percentage a model per config file is trained.\nThereby, the model is trained only on the current percentage of training data.\nAfterwards, the model is tested on the complete test data of that run.\nAll results are stored in the provided output directory.\n\nArgs:\n    configs: config files needed for training\n    data: training data\n    exclusion_percentages: percentages of training data to exclude during comparison\n    f_score_results: dictionary of model name to f-score results per run\n    model_names: names of the models to train\n    output: the output directory\n    runs: number of comparison runs\n\nReturns: training examples per run\"\"\"\nfn compare_nlu(configs: List[Text], data: TrainingData, exclusion_percentages: List[int], f_score_results: Dict[(Text, Any)], model_names: List[Text], output: Text, runs: int) -> List[int]\n\n\"\"\"Computes evaluation metrics for a given corpus and returns the results.\n\nArgs:\n    results: evaluation results\n    target_key: target key name\n    prediction_key: prediction key name\n\nReturns: metrics\"\"\"\nfn _compute_metrics(results: Union[(List[IntentEvaluationResult], List[ResponseSelectionEvaluationResult])], target_key: Text, prediction_key: Text) -> Union[(IntentMetrics, ResponseSelectionMetrics)]\n\n\"\"\"Computes entity evaluation metrics and returns the results.\n\nArgs:\n    entity_results: entity evaluation results\n    interpreter: the interpreter\n\nReturns: entity metrics\"\"\"\nfn _compute_entity_metrics(entity_results: List[EntityEvaluationResult], interpreter: Interpreter) -> EntityMetrics\n\n\"\"\"Logs results of cross validation.\n\nArgs:\n    results: dictionary of results returned from cross validation\n    dataset_name: string of which dataset the results are from, e.g. test/train\"\"\"\nfn log_results(results: IntentMetrics, dataset_name: Text) -> None\n\n\"\"\"Logs entity results of cross validation.\n\nArgs:\n    results: dictionary of dictionaries of results returned from cross validation\n    dataset_name: string of which dataset the results are from, e.g. test/train\"\"\"\nfn log_entity_results(results: EntityMetrics, dataset_name: Text) -> None\n\nfn configure_colored_logging(loglevel: Text) -> None\n\nfn enable_async_loop_debugging(event_loop: AbstractEventLoop, slow_callback_duration: float) -> AbstractEventLoop\n\n\"\"\"Saves object to file.\n\nArgs:\n    filename: the filename to save the object to\n    obj: the object to store\"\"\"\nfn pickle_dump(filename: Union[(Text, Path)], obj: Any) -> None\n\n\"\"\"Loads an object from a file.\n\nArgs:\n    filename: the filename to load the object from\n\nReturns: the loaded object\"\"\"\nfn pickle_load(filename: Union[(Text, Path)]) -> Any\n\n\"\"\"Tries to unpack a byte array interpreting it as an archive.\n\nTries to use tar first to unpack, if that fails, zip will be used.\"\"\"\nfn unarchive(byte_array: bytes, directory: Text) -> Text\n\n\"\"\"Creates a tempfile.NamedTemporaryFile object for data.\n\nmode defines NamedTemporaryFile's  mode parameter in py3.\"\"\"\nfn create_temporary_file(data: Any, suffix: Text, mode: Text) -> Text\n\n\"\"\"Creates a tempfile.TemporaryDirectory.\"\"\"\nfn create_temporary_directory() -> Text\n\n\"\"\"Makes sure all directories in the 'file_path' exists.\"\"\"\nfn create_path(file_path: Text) -> None\n\n\"\"\"Creates a `Validator` class which can be used with `questionary` to validate\nfile paths.\"\"\"\nfn file_type_validator(valid_file_types: List[Text], error_message: Text) -> Type[Validator]\n\n\"\"\"Creates a `Validator` class which can be used with `questionary` to validate\nthat the user entered something other than whitespace.\"\"\"\nfn not_empty_validator(error_message: Text) -> Type[Validator]\n\n\"\"\"Helper method to create `Validator` classes from callable functions. Should be\nremoved when questionary supports `Validator` objects.\"\"\"\nfn create_validator(function: Callable[([Text], bool)], error_message: Text) -> Type[Validator]\n\n\"\"\"Create an archive from a folder.\"\"\"\nfn zip_folder(folder: Text) -> Text\n\n\"\"\"Unpickle an object from file using json.\n\nArgs:\n    file_name: the file to load the object from\n\nReturns: the object\"\"\"\nfn json_unpickle(file_name: Union[(Text, Path)]) -> Any\n\n\"\"\"Pickle an object to a file using json.\n\nArgs:\n    file_name: the file to store the object to\n    obj: the object to store\"\"\"\nfn json_pickle(file_name: Union[(Text, Path)], obj: Any) -> None\n\nclass WrongPredictionException\n\nclass EvaluationStore\n\nclass WronglyPredictedAction\n\nclass EndToEndUserUtterance\n\nclass WronglyClassifiedUserUtterance\n\n\"\"\"Extract only the token variables from an entity dict.\"\"\"\nfn _clean_entity_results(text: Text, entity_results: List[Dict[(Text, Any)]]) -> List[Dict[(Text, Any)]]\n\n\"\"\"Return full retrieval intent, if it's present, or normal intent otherwise.\n\nArgs:\n    parsed: Predicted parsed data.\n\nReturns:\n    The extracted intent.\"\"\"\nfn _get_full_retrieval_intent(parsed: Dict[(Text, Any)]) -> Text\n\nfn _collect_user_uttered_predictions(event: UserUttered, predicted: Dict[(Text, Any)], partial_tracker: DialogueStateTracker, fail_on_prediction_errors: bool) -> EvaluationStore\n\n\"\"\"Add `ActionExecutionRejected` event to the tracker.\n\nDuring evaluation, we don't run action server, therefore in order to correctly\ntest unhappy paths of the loops, we need to emulate loop rejection.\n\nArgs:\n    partial_tracker: a :class:`rasa.core.trackers.DialogueStateTracker`\"\"\"\nfn emulate_loop_rejection(partial_tracker: DialogueStateTracker) -> None\n\nfn _collect_action_executed_predictions(processor: MessageProcessor, partial_tracker: DialogueStateTracker, event: ActionExecuted, fail_on_prediction_errors: bool, circuit_breaker_tripped: bool) -> Tuple[(EvaluationStore, PolicyPrediction)]\n\nfn _form_might_have_been_rejected(domain: Domain, tracker: DialogueStateTracker, predicted_action_name: Text) -> bool\n\n\"\"\"Given a list of action items, returns the fraction of actions\n\nthat were predicted using one of the Memoization policies.\"\"\"\nfn _in_training_data_fraction(action_list: List[Dict[(Text, Any)]]) -> float\n\n\"\"\"Write given stories to the given file.\"\"\"\nfn _log_stories(trackers: List[DialogueStateTracker], file_path: Text) -> None\n\n\"\"\"Log the sklearn evaluation metrics.\"\"\"\nfn _log_evaluation_table(golds: List[Any], name: Text, report: Dict[(Text, Any)], precision: float, f1: float, accuracy: float, in_training_data_fraction: float, include_report: bool) -> None\n\n\"\"\"Plot a confusion matrix of story evaluation.\"\"\"\nfn _plot_story_evaluation(targets: List[Text], predictions: List[Text], output_directory: Optional[Text]) -> None\n\nclass bcolors\n\nfn wrap_with_color() -> Text\n\n\"\"\"Emit a `warnings.warn` with sensible defaults and a colored warning msg.\"\"\"\nfn raise_warning(message: Text, category: Optional[Type[Warning]], docs: Optional[Text]) -> None\n\n\"\"\"Writes text to a file.\n\nArgs:\n    content: The content to write.\n    file_path: The path to which the content should be written.\n    encoding: The encoding which should be used.\n    append: Whether to append to the file or to truncate the file.\"\"\"\nfn write_text_file(content: Text, file_path: Union[(Text, Path)], encoding: Text, append: bool) -> None\n\n\"\"\"Read text from a file.\"\"\"\nfn read_file(filename: Union[(Text, Path)], encoding: Text) -> Any\n\n\"\"\"Read json from a file.\"\"\"\nfn read_json_file(filename: Union[(Text, Path)]) -> Any\n\n\"\"\"Returns all files and folders excluding hidden files.\n\nIf the path points to a file, returns the file. This is a recursive\nimplementation returning files in any depth of the path.\"\"\"\nfn list_directory(path: Text) -> List[Text]\n\n\"\"\"Returns all files excluding hidden files.\n\nIf the path points to a file, returns the file.\"\"\"\nfn list_files(path: Text) -> List[Text]\n\n\"\"\"Splits of a filenames prefix until after the first ``_``.\"\"\"\nfn _filename_without_prefix(file: Text) -> Text\n\n\"\"\"Returns all folders excluding hidden files.\n\nIf the path points to a file, returns an empty list.\"\"\"\nfn list_subdirectories(path: Text) -> List[Text]\n\n\"\"\"Calculate a hash which is stable, independent of a containers key order.\n\nWorks for lists and dictionaries. For keys and values, we recursively call\n`hash(...)` on them. Keep in mind that a list with keys in a different order\nwill create the same hash!\n\nArgs:\n    obj: dictionary or list to be hashed.\n    encoding: encoding used for dumping objects as strings\n\nReturns:\n    hash of the container.\"\"\"\nfn deep_container_fingerprint(obj: Union[(List[Any], Dict[(Any, Any)])], encoding: Text) -> Text\n\n\"\"\"Calculate the fingerprint for a dictionary.\n\nThe dictionary can contain any keys and values which are either a dict,\na list or a elements which can be dumped as a string.\n\nArgs:\n    dictionary: dictionary to be hashed\n    encoding: encoding used for dumping objects as strings\n\nReturns:\n    The hash of the dictionary\"\"\"\nfn get_dictionary_fingerprint(dictionary: Dict[(Any, Any)], encoding: Text) -> Text\n\n\"\"\"Calculate a fingerprint for an unordered list.\n\nArgs:\n    elements: unordered list\n    encoding: encoding used for dumping objects as strings\n\nReturns:\n    the fingerprint of the list\"\"\"\nfn get_list_fingerprint(elements: List[Any], encoding: Text) -> Text\n\n\"\"\"Calculate the md5 hash for a text.\"\"\"\nfn get_text_hash(text: Text, encoding: Text) -> Text\n\nfn json_to_string(obj: Any) -> Text\n\n\"\"\"Ensure that any string read by yaml is represented as unicode.\"\"\"\nfn fix_yaml_loader() -> None\n\n\"\"\"Enable yaml loader to process the environment variables in the yaml.\"\"\"\nfn replace_environment_variables() -> None\n\n\"\"\"Parses yaml from a text.\n\nArgs:\n    content: A text containing yaml content.\n    reader_type: Reader type to use. By default \"safe\" will be used\n\nRaises:\n    ruamel.yaml.parser.ParserError: If there was an error when parsing the YAML.\"\"\"\nfn read_yaml(content: Text, reader_type: Union[(Text, List[Text])]) -> Any\n\nfn _is_ascii(text: Text) -> bool\n\n\"\"\"Parses a yaml file.\n\nRaises an exception if the content of the file can not be parsed as YAML.\n\nArgs:\n    filename: The path to the file which should be read.\n\nReturns:\n    Parsed content of the file.\"\"\"\nfn read_yaml_file(filename: Union[(Text, Path)]) -> Union[(List[Any], Dict[(Text, Any)])]\n\n\"\"\"Writes a yaml to the file or to the stream\n\nArgs:\n    data: The data to write.\n    target: The path to the file which should be written or a stream object\n    should_preserve_key_order: Whether to force preserve key order in `data`.\"\"\"\nfn write_yaml(data: Any, target: Union[(Text, Path, StringIO)], should_preserve_key_order: bool) -> None\n\n\"\"\"Convert object to an `OrderedDict`.\n\nArgs:\n    obj: Object to convert.\n\nReturns:\n    An `OrderedDict` with all nested dictionaries converted if `obj` is a\n    dictionary, otherwise the object itself.\"\"\"\nfn convert_to_ordered_dict(obj: Any) -> Any\n\n\"\"\"Ensure that `OrderedDict`s are dumped so that the order of keys is respected.\"\"\"\nfn _enable_ordered_dict_yaml_dumping() -> None\n\n\"\"\"Returns `True` if log level is set to WARNING or ERROR, `False` otherwise.\"\"\"\nfn is_logging_disabled() -> bool\n\n\"\"\"Creates any missing parent directories of this file path.\"\"\"\nfn create_directory_for_file(file_path: Union[(Text, Path)]) -> None\n\n\"\"\"Dump an object as a json string to a file.\"\"\"\nfn dump_obj_as_json_to_file(filename: Union[(Text, Path)], obj: Any) -> None\n\n\"\"\"Writes data (python dict) to a yaml string.\n\nArgs:\n    obj: The object to dump. Has to be serializable.\n    should_preserve_key_order: Whether to force preserve key order in `data`.\n\nReturns:\n    The object converted to a YAML string.\"\"\"\nfn dump_obj_as_yaml_to_string(obj: Any, should_preserve_key_order: bool) -> Text\n\n\"\"\"Creates a directory and its super paths.\n\nSucceeds even if the path already exists.\"\"\"\nfn create_directory(directory_path: Text) -> None\n\n\"\"\"Thin wrapper around `raise_warning()` to raise a deprecation warning. It requires\na version until which we'll warn, and after which the support for the feature will\nbe removed.\"\"\"\nfn raise_deprecation_warning(message: Text, warn_until_version: Text, docs: Optional[Text]) -> None\n\n\"\"\"Parses a yaml configuration file. Content needs to be a dictionary\n\nArgs:\n    filename: The path to the file which should be read.\"\"\"\nfn read_config_file(filename: Union[(Path, Text)]) -> Dict[(Text, Any)]\n\nfn is_subdirectory(path: Text, potential_parent_directory: Text) -> bool\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import os\nfrom pathlib import Path\nfrom typing import Any, Text, Dict\n\nimport pytest\n\nimport rasa.shared.utils.io\nimport rasa.utils.io\nfrom rasa.core.test import (\n    _create_data_generator,\n    _collect_story_predictions,\n    test as evaluate_stories,\n    FAILED_STORIES_FILE,\n    CONFUSION_MATRIX_STORIES_FILE,\n    REPORT_STORIES_FILE,\n    SUCCESSFUL_STORIES_FILE,\n    _clean_entity_results,\n)\nfrom rasa.core.policies.memoization import MemoizationPolicy\n\n# we need this import to ignore the warning...\n# noinspection PyUnresolvedReferences\nfrom rasa.nlu.test import run_evaluation\nfrom rasa.core.agent import Agent\nfrom tests.core.conftest import (\n    DEFAULT_STORIES_FILE,\n    E2E_STORY_FILE_UNKNOWN_ENTITY,\n    END_TO_END_STORY_FILE,\n    E2E_STORY_FILE_TRIPS_CIRCUIT_BREAKER,\n    STORY_FILE_TRIPS_CIRCUIT_BREAKER,\n)\n\n\nasync def test_evaluation_file_creation(tmpdir: Path, default_agent: Agent):\n    failed_stories_path = str(tmpdir / FAILED_STORIES_FILE)\n    success_stories_path = str(tmpdir / SUCCESSFUL_STORIES_FILE)\n    report_path = str(tmpdir / REPORT_STORIES_FILE)\n    confusion_matrix_path = str(tmpdir / CONFUSION_MATRIX_STORIES_FILE)\n\n    await evaluate_stories(\n        stories=DEFAULT_STORIES_FILE,\n        agent=default_agent,\n        out_directory=str(tmpdir),\n        max_stories=None,\n        e2e=False,\n        errors=True,\n        successes=True,\n    )\n\n    assert os.path.isfile(failed_stories_path)\n    assert os.path.isfile(success_stories_path)\n    assert os.path.isfile(report_path)\n    assert os.path.isfile(confusion_matrix_path)\n\n\n@pytest.mark.parametrize(\n    \"test_file\", [END_TO_END_STORY_FILE, \"data/test_evaluations/end_to_end_story.yml\"]\n)\nasync def test_end_to_end_evaluation_script(default_agent: Agent, test_file: Text):\n    generator = await _create_data_generator(test_file, default_agent, use_e2e=True)\n    completed_trackers = generator.generate_story_trackers()\n\n    story_evaluation, num_stories = await _collect_story_predictions(\n        completed_trackers, default_agent, use_e2e=True\n    )\n\n    serialised_store = [\n        \"utter_greet\",\n        \"action_listen\",\n        \"utter_greet\",\n        \"action_listen\",\n        \"utter_default\",\n        \"action_listen\",\n        \"utter_goodbye\",\n        \"action_listen\",\n        \"utter_greet\",\n        \"action_listen\",\n        \"utter_default\",\n        \"action_listen\",\n        \"greet\",\n        \"greet\",\n        \"default\",\n        \"goodbye\",\n        \"greet\",\n        \"default\",\n        '[{\"name\": \"Max\"}]{\"entity\": \"name\", \"value\": \"Max\"}',\n    ]\n\n    assert story_evaluation.evaluation_store.serialise()[0] == serialised_store\n    assert not story_evaluation.evaluation_store.has_prediction_target_mismatch()\n    assert len(story_evaluation.failed_stories) == 0\n    assert num_stories == 3\n\n\nasync def test_end_to_end_evaluation_script_unknown_entity(default_agent: Agent):\n    generator = await _create_data_generator(\n        E2E_STORY_FILE_UNKNOWN_ENTITY, default_agent, use_e2e=True\n    )\n    completed_trackers = generator.generate_story_trackers()\n\n    story_evaluation, num_stories = await _collect_story_predictions(\n        completed_trackers, default_agent, use_e2e=True\n    )\n\n    assert story_evaluation.evaluation_store.has_prediction_target_mismatch()\n    assert len(story_evaluation.failed_stories) == 1\n    assert num_stories == 1\n\n\nasync def test_end_to_evaluation_with_forms(form_bot_agent: Agent):\n    generator = await _create_data_generator(\n        \"data/test_evaluations/form-end-to-end-stories.md\", form_bot_agent, use_e2e=True\n    )\n    test_stories = generator.generate_story_trackers()\n\n    story_evaluation, num_stories = await _collect_story_predictions(\n        test_stories, form_bot_agent, use_e2e=True\n    )\n\n    assert not story_evaluation.evaluation_store.has_prediction_target_mismatch()\n\n\nasync def test_source_in_failed_stories(tmpdir: Path, default_agent: Agent):\n    stories_path = str(tmpdir / FAILED_STORIES_FILE)\n\n    await evaluate_stories(\n        stories=E2E_STORY_FILE_UNKNOWN_ENTITY,\n        agent=default_agent,\n        out_directory=str(tmpdir),\n        max_stories=None,\n        e2e=False,\n    )\n\n    failed_stories = rasa.shared.utils.io.read_file(stories_path)\n\n    assert (\n        f\"story: simple_story_with_unknown_entity ({E2E_STORY_FILE_UNKNOWN_ENTITY})\"\n        in failed_stories\n    )\n\n\nasync def test_end_to_evaluation_trips_circuit_breaker():\n    agent = Agent(\n        domain=\"data/test_domains/default.yml\",\n        policies=[MemoizationPolicy(max_history=11)],\n    )\n    training_data = await agent.load_data(STORY_FILE_TRIPS_CIRCUIT_BREAKER)\n    agent.train(training_data)\n\n    generator = await _create_data_generator(\n        E2E_STORY_FILE_TRIPS_CIRCUIT_BREAKER, agent, use_e2e=True\n    )\n    test_stories = generator.generate_story_trackers()\n\n    story_evaluation, num_stories = await _collect_story_predictions(\n        test_stories, agent, use_e2e=True\n    )\n\n    circuit_trip_predicted = [\n        \"utter_greet\",\n        \"utter_greet\",\n        \"utter_greet\",\n        \"utter_greet\",\n        \"utter_greet\",\n        \"utter_greet\",\n        \"utter_greet\",\n        \"utter_greet\",\n        \"utter_greet\",\n        \"utter_greet\",\n        \"circuit breaker tripped\",\n        \"circuit breaker tripped\",\n    ]\n\n    assert (\n        story_evaluation.evaluation_store.action_predictions == circuit_trip_predicted\n    )\n    assert num_stories == 1\n\n\n@pytest.mark.parametrize(\n    \"text, entity, expected_entity\",\n    [\n        (\n            \"The first one please.\",\n            {\n                \"extractor\": \"DucklingEntityExtractor\",\n                \"entity\": \"ordinal\",\n                \"confidence\": 0.87,\n                \"start\": 4,\n                \"end\": 9,\n                \"value\": 1,\n            },\n            {\n                \"text\": \"The first one please.\",\n                \"entity\": \"ordinal\",\n                \"start\": 4,\n                \"end\": 9,\n                \"value\": \"1\",\n            },\n        ),\n        (\n            \"The first one please.\",\n            {\n                \"extractor\": \"CRFEntityExtractor\",\n                \"entity\": \"ordinal\",\n                \"confidence\": 0.87,\n                \"start\": 4,\n                \"end\": 9,\n                \"value\": \"1\",\n            },\n            {\n                \"text\": \"The first one please.\",\n                \"entity\": \"ordinal\",\n                \"start\": 4,\n                \"end\": 9,\n                \"value\": \"1\",\n            },\n        ),\n        (\n            \"Italian food\",\n            {\n                \"extractor\": \"DIETClassifier\",\n                \"entity\": \"cuisine\",\n                \"confidence\": 0.99,\n                \"start\": 0,\n                \"end\": 7,\n                \"value\": \"Italian\",\n            },\n            {\n                \"text\": \"Italian food\",\n                \"entity\": \"cuisine\",\n                \"start\": 0,\n                \"end\": 7,\n                \"value\": \"Italian\",\n            },\n        ),\n    ],\n)\ndef test_event_has_proper_implementation(\n    text: Text, entity: Dict[Text, Any], expected_entity: Dict[Text, Any]\n):\n    actual_entities = _clean_entity_results(text, [entity])\n\n    assert actual_entities[0] == expected_entity\n\n\n@pytest.mark.parametrize(\n    \"test_file\",\n    [\n        (\"data/test_yaml_stories/test_full_retrieval_intent_story.yml\"),\n        (\"data/test_yaml_stories/test_base_retrieval_intent_story.yml\"),\n    ],\n)\nasync def test_retrieval_intent(response_selector_agent: Agent, test_file: Text):\n    generator = await _create_data_generator(\n        test_file, response_selector_agent, use_e2e=True,\n    )\n    test_stories = generator.generate_story_trackers()\n\n    story_evaluation, num_stories = await _collect_story_predictions(\n        test_stories, response_selector_agent, use_e2e=True\n    )\n    # check that test story can either specify base intent or full retrieval intent\n    assert not story_evaluation.evaluation_store.has_prediction_target_mismatch()\n\n\n@pytest.mark.parametrize(\n    \"test_file\",\n    [\n        (\"data/test_yaml_stories/test_full_retrieval_intent_wrong_prediction.yml\"),\n        (\"data/test_yaml_stories/test_base_retrieval_intent_wrong_prediction.yml\"),\n    ],\n)\nasync def test_retrieval_intent_wrong_prediction(\n    tmpdir: Path, response_selector_agent: Agent, test_file: Text\n):\n    stories_path = str(tmpdir / FAILED_STORIES_FILE)\n\n    await evaluate_stories(\n        stories=test_file,\n        agent=response_selector_agent,\n        out_directory=str(tmpdir),\n        max_stories=None,\n        e2e=True,\n    )\n\n    failed_stories = rasa.shared.utils.io.read_file(stories_path)\n\n    # check if the predicted entry contains full retrieval intent\n    assert \"# predicted: chitchat/ask_name\" in failed_stories\n"
  },
  {
    "prompt": "class Result\n\nclass Failure\n\nclass Success\n\n\"\"\"Decorator to convert exception-throwing function to ``Result`` container.\n\nShould be used with care, since it only catches ``Exception`` subclasses.\nIt does not catch ``BaseException`` subclasses.\n\nIf you need to mark ``async`` function as ``safe``,\nuse :func:`returns.future.future_safe` instead.\nThis decorator only works with sync functions. Example:\n\n.. code:: python\n\n  >>> from returns.result import Result, Success, safe\n\n  >>> @safe\n  ... def might_raise(arg: int) -> float:\n  ...     return 1 / arg\n\n  >>> assert might_raise(1) == Success(1.0)\n  >>> assert isinstance(might_raise(0), Result.failure_type)\n\nSimilar to :func:`returns.io.impure_safe`\nand :func:`returns.future.future_safe` decorators.\n\nRequires our :ref:`mypy plugin <mypy-plugins>`.\"\"\"\nfn safe(function: Callable[(..., _ValueType)]) -> Callable[(..., ResultE[_ValueType])]\n\n\"\"\"Determines if a container was successful or not.\n\n.. code:: python\n\n  >>> from returns.maybe import Some, Nothing\n  >>> from returns.result import Failure, Success\n  >>> from returns.io import IOSuccess, IOFailure\n\n  >>> assert is_successful(Some(1))\n  >>> assert not is_successful(Nothing)\n\n  >>> assert is_successful(Success(1))\n  >>> assert not is_successful(Failure(1))\n\n  >>> assert is_successful(IOSuccess(1))\n  >>> assert not is_successful(IOFailure(1))\n\nThis function can work with containers\nthat are instance of :class:`returns.interfaces.unwrappable.Unwrappable`.\"\"\"\nfn is_successful(container: Unwrappable) -> bool\n\nclass IO\n\n\"\"\"Decorator to mark function that it returns :class:`~IO` container.\n\nIf you need to mark ``async`` function as impure,\nuse :func:`returns.future.future` instead.\nThis decorator only works with sync functions. Example:\n\n.. code:: python\n\n  >>> from returns.io import IO, impure\n\n  >>> @impure\n  ... def function(arg: int) -> int:\n  ...     return arg + 1  # this action is pure, just an example\n  ...\n\n  >>> assert function(1) == IO(2)\n\nRequires our :ref:`mypy plugin <mypy-plugins>`.\"\"\"\nfn impure(function: Callable[(..., _NewValueType)]) -> Callable[(..., IO[_NewValueType])]\n\nclass IOResult\n\nclass IOFailure\n\nclass IOSuccess\n\n\"\"\"Decorator to mark function that it returns :class:`~IOResult` container.\n\nShould be used with care, since it only catches ``Exception`` subclasses.\nIt does not catch ``BaseException`` subclasses.\n\nIf you need to mark ``async`` function as impure,\nuse :func:`returns.future.future_safe` instead.\nThis decorator only works with sync functions. Example:\n\n.. code:: python\n\n  >>> from returns.io import IOSuccess, impure_safe\n\n  >>> @impure_safe\n  ... def function(arg: int) -> float:\n  ...     return 1 / arg\n  ...\n\n  >>> assert function(1) == IOSuccess(1.0)\n  >>> assert function(0).failure()\n\nSimilar to :func:`returns.future.future_safe`\nand :func:`returns.result.safe` decorators.\n\nRequires our :ref:`mypy plugin <mypy-plugins>`.\"\"\"\nfn impure_safe(function: Callable[(..., _NewValueType)]) -> Callable[(..., IOResultE[_NewValueType])]\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from typing import List, Tuple\n\nimport pytest\n\nfrom returns.io import IOFailure, IOResult, IOSuccess\nfrom returns.pipeline import managed\nfrom returns.result import Failure, Result, Success\n\n_acquire_success = IOSuccess('acquire success')\n_acquire_failure = IOFailure('acquire failure')\n\n\ndef _use_success(inner_value: str) -> IOResult[str, str]:\n    return IOSuccess('use success')\n\n\ndef _use_failure(inner_value: str) -> IOResult[str, str]:\n    return IOFailure('use failure')\n\n\nclass _ReleaseSuccess(object):\n    def __init__(self, logs: List[Tuple[str, Result[str, str]]]) -> None:\n        self._logs = logs\n\n    def __call__(\n        self,\n        inner_value: str,\n        use_result: Result[str, str],\n    ) -> IOResult[None, str]:\n        self._logs.append((inner_value, use_result))\n        return IOSuccess(None)\n\n\nclass _ReleaseFailure(object):\n    def __init__(self, logs: List[Tuple[str, Result[str, str]]]) -> None:\n        self._logs = logs\n\n    def __call__(\n        self,\n        inner_value: str,\n        use_result: Result[str, str],\n    ) -> IOResult[None, str]:\n        return IOFailure('release failure')\n\n\n@pytest.mark.parametrize(('acquire', 'use', 'release', 'final_result', 'log'), [\n    # Acquire success:\n    (\n        _acquire_success,\n        _use_success,\n        _ReleaseSuccess,\n        IOSuccess('use success'),\n        [('acquire success', Success('use success'))],\n    ),\n    (\n        _acquire_success,\n        _use_success,\n        _ReleaseFailure,\n        IOFailure('release failure'),\n        [],\n    ),\n    (\n        _acquire_success,\n        _use_failure,\n        _ReleaseSuccess,\n        IOFailure('use failure'),\n        [('acquire success', Failure('use failure'))],\n    ),\n    (\n        _acquire_success,\n        _use_failure,\n        _ReleaseFailure,\n        IOFailure('release failure'),\n        [],\n    ),\n\n    # Acquire failure:\n    (\n        _acquire_failure,\n        _use_success,\n        _ReleaseSuccess,\n        IOFailure('acquire failure'),\n        [],\n    ),\n    (\n        _acquire_failure,\n        _use_failure,\n        _ReleaseSuccess,\n        IOFailure('acquire failure'),\n        [],\n    ),\n    (\n        _acquire_failure,\n        _use_success,\n        _ReleaseFailure,\n        IOFailure('acquire failure'),\n        [],\n    ),\n    (\n        _acquire_failure,\n        _use_failure,\n        _ReleaseFailure,\n        IOFailure('acquire failure'),\n        [],\n    ),\n])\ndef test_all_success(acquire, use, release, final_result, log):\n    \"\"\"Ensures that managed works as intended.\"\"\"\n    pipeline_logs: List[Tuple[str, Result[str, str]]] = []\n    pipeline_result = managed(\n        use,\n        release(pipeline_logs),\n    )(acquire)\n\n    assert pipeline_result == final_result\n    assert pipeline_logs == log\n\n\ndef test_full_typing():\n    \"\"\"This test is here to be a case for typing.\"\"\"\n    logs: List[Tuple[str, Result[str, str]]] = []\n    pipeline_result = managed(\n        _use_success,\n        _ReleaseSuccess(logs),\n    )(_acquire_success)\n\n    assert pipeline_result == IOSuccess('use success')\n    assert logs == [('acquire success', Success('use success'))]\n"
  },
  {
    "prompt": "fn check_common_metrics(aggregator, expected_tags, count)\n\nfn check_bgw_metrics(aggregator, expected_tags)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# (C) Datadog, Inc. 2021-present\n# All rights reserved\n# Licensed under Simplified BSD License (see LICENSE)\nimport pytest\n\nfrom datadog_checks.postgres.relationsmanager import ALL_SCHEMAS, IDX_METRICS, LOCK_METRICS, RelationsManager\n\nfrom .common import SCHEMA_NAME\n\npytestmark = pytest.mark.unit\n\n\n@pytest.mark.parametrize(\n    'relations_config,expected_filter',\n    [\n        (\n            [\n                {'relation_regex': 'ix.*', 'schemas': ['public', 's1', 's2']},\n                {'relation_regex': 'ibx.*', 'schemas': ['public']},\n                {'relation_regex': 'icx.*', 'schemas': ['public']},\n            ],\n            \"( relname ~ 'ix.*' AND schemaname = ANY(array['public','s1','s2']::text[]) ) \"\n            \"OR ( relname ~ 'ibx.*' AND schemaname = ANY(array['public']::text[]) ) \"\n            \"OR ( relname ~ 'icx.*' AND schemaname = ANY(array['public']::text[]) )\",\n        ),\n        (\n            [\n                {'relation_regex': '.+_archive'},\n            ],\n            \"( relname ~ '.+_archive' )\",\n        ),\n        (\n            [\n                {'relation_name': 'my_table', 'schemas': ['public', 'app'], 'relkind': ['r']},  # relkind ignored\n                {'relation_name': 'my_table2', 'relkind': ['p', 'r']},  # relkind ignored\n                {'relation_regex': 'table.*'},\n            ],\n            \"( relname = 'my_table' AND schemaname = ANY(array['public','app']::text[]) ) \"\n            \"OR ( relname = 'my_table2' ) \"\n            \"OR ( relname ~ 'table.*' )\",\n        ),\n        (\n            ['table1', 'table2'],\n            \"( relname = 'table1' ) OR ( relname = 'table2' )\",\n        ),\n    ],\n)\ndef test_relations_cases(relations_config, expected_filter):\n    query = '{relations}'\n    relations = RelationsManager(relations_config)\n    query_filter = relations.filter_relation_query(query, SCHEMA_NAME)\n    assert query_filter == expected_filter\n\n\ndef test_relation_filter():\n    query = \"Select foo from bar where {relations}\"\n    relations_config = [{'relation_name': 'breed', 'schemas': ['public']}]\n    relations = RelationsManager(relations_config)\n\n    query_filter = relations.filter_relation_query(query, SCHEMA_NAME)\n    assert (\n        query_filter == \"Select foo from bar where ( relname = 'breed' AND schemaname = ANY(array['public']::text[]) )\"\n    )\n\n\ndef test_relation_filter_no_schemas():\n    query = \"Select foo from bar where {relations}\"\n    relations_config = [{'relation_name': 'persons', 'schemas': [ALL_SCHEMAS]}]\n    relations = RelationsManager(relations_config)\n\n    query_filter = relations.filter_relation_query(query, SCHEMA_NAME)\n    assert query_filter == \"Select foo from bar where ( relname = 'persons' )\"\n\n\ndef test_relation_filter_regex():\n    query = \"Select foo from bar where {relations}\"\n    relations_config = [{'relation_regex': 'b.*', 'schemas': [ALL_SCHEMAS]}]\n    relations = RelationsManager(relations_config)\n\n    query_filter = relations.filter_relation_query(query, SCHEMA_NAME)\n    assert query_filter == \"Select foo from bar where ( relname ~ 'b.*' )\"\n\n\ndef test_relation_filter_relkind():\n    query = LOCK_METRICS['query'].replace('{metrics_columns}', 'foo')\n    relations_config = [{'relation_regex': 'b.*', 'schemas': [ALL_SCHEMAS], 'relkind': ['r', 't']}]\n    relations = RelationsManager(relations_config)\n\n    query_filter = relations.filter_relation_query(query, SCHEMA_NAME)\n    assert \"AND relkind = ANY(array['r','t'])\" in query_filter\n\n\ndef test_relkind_does_not_apply_to_index_metrics():\n    query = IDX_METRICS['query'].replace('{metrics_columns}', 'foo')\n    relations_config = [{'relation_regex': 'b.*', 'schemas': [ALL_SCHEMAS], 'relkind': ['r']}]\n    relations = RelationsManager(relations_config)\n\n    query_filter = relations.filter_relation_query(query, SCHEMA_NAME)\n    assert 'relkind' not in query_filter\n"
  }
]
