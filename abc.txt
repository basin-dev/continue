{
"prompt": "

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from __future__ import division, absolute_import, print_function

__copyright__ = "Copyright (C) 2015 Andreas Kloeckner"

__license__ = """
Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
"""

import sys
import numpy as np  # noqa
import numpy.linalg as la
import loopy as lp
import pyopencl as cl
import pyopencl.clrandom  # noqa

import logging
logger = logging.getLogger(__name__)

try:
    import faulthandler
except ImportError:
    pass
else:
    faulthandler.enable()

from pyopencl.tools import pytest_generate_tests_for_pyopencl \
        as pytest_generate_tests

__all__ = [
        "pytest_generate_tests",
        "cl"  # 'cl.create_some_context'
        ]


from loopy.version import LOOPY_USE_LANGUAGE_VERSION_2018_2  # noqa


def test_diff(ctx_factory):
    ctx = ctx_factory()
    queue = cl.CommandQueue(ctx)

    knl = lp.make_function(
         """{ [i,j]: 0<=i,j<n }""",
         """
         <> a = 1/(1+sinh(x[i] + y[j])**2)
         z[i] = sum(j, exp(a * x[j]))
         """)

    knl = lp.fix_parameters(knl, n=50)

    from loopy.transform.diff import diff_kernel
    dknl, diff_map = diff_kernel(knl, "z", "x")
    dknl = lp.make_program(dknl)
    dknl = lp.remove_unused_arguments(dknl)

    dknl = lp.add_inames_to_insn(dknl, "diff_i0", "writes:a_dx or writes:a")

    print(dknl)

    n = 50
    x = np.random.randn(n)
    y = np.random.randn(n)

    dx = np.random.randn(n)

    fac = 1e-1
    h1 = 1e-4
    h2 = h1 * fac

    evt, (z0,) = knl(queue, x=x, y=y)
    evt, (z1,) = knl(queue, x=(x + h1*dx), y=y)
    evt, (z2,) = knl(queue, x=(x + h2*dx), y=y)

    dknl = lp.set_options(dknl, write_cl=True)
    evt, (df,) = dknl(queue, x=x, y=y)

    diff1 = (z1-z0)
    diff2 = (z2-z0)

    diff1_predicted = df.dot(h1*dx)
    diff2_predicted = df.dot(h2*dx)

    err1 = la.norm(diff1 - diff1_predicted) / la.norm(diff1)
    err2 = la.norm(diff2 - diff2_predicted) / la.norm(diff2)
    print(err1, err2)

    assert (err2 < err1 * fac * 1.1).all()


if __name__ == "__main__":
    if len(sys.argv) > 1:
        exec(sys.argv[1])
    else:
        from pytest import main
        main([__file__])

# vim: foldmethod=marker
<|endoftext|>"
},
{
"prompt": "fn get_fg_position(id, building_block)

fn get_centroid(building_block)

fn get_closest_point(points, point)

fn get_edges(vertex)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

import stk

from ....case_data import CaseData
from .utilities import get_centroid, get_closest_point, get_edges


@pytest.fixture
def tail_1(position, flip, building_block_1):
    point1, point2 = points = (
        position + [-10, 0, 0],
        position + [10, 0, 0],
    )

    def get_centroid_point(building_block):
        return get_closest_point(
            points=points,
            point=get_centroid(building_block),
        )

    vertex = stk.polymer.linear.TailVertex(0, position, flip)
    return CaseData(
        vertex=vertex,
        edges=(tuple(get_edges(vertex))[0], ),
        building_block=building_block_1,
        position=position,
        alignment_tests={get_centroid_point: point2},
        functional_group_edges={0: 0},
    )
<|endoftext|>"
},
{
"prompt": """"Retrieve exemplary the MNIST model to test te overall MIA Framework"""
fn get_data()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
import os
import numpy as np

from .utils import get_data
from ..wb_attack_data_generator import WBAttackGenerator


def test_data():
    """
    The Data Generator should not crash
    """
    model, x_train, x_test, y_train, y_test = get_data()
    X = np.concatenate((x_train, x_test))
    Y = np.concatenate((y_train, y_test))
    wb_generator_train = WBAttackGenerator(model, X, Y,
                                           range(0, len(x_train) // 2), range(len(x_train) // 2, len(x_train)),
                                           10, 10, last_layer_only=True)
    wb_generator_train.write_attack_info(f'{os.getcwd()}/libs/MIA/tests/fixtures/', "mnist_train")

    assert os.path.exists(f'{os.getcwd()}/libs/MIA/tests/fixtures/mnist_train_data_inf.json')
    assert os.path.exists(f'{os.getcwd()}/libs/MIA/tests/fixtures/mnist_train_target_train_attack_data.h5')
    assert os.path.exists(f'{os.getcwd()}/libs/MIA/tests/fixtures/mnist_train_target_test_attack_data.h5')
<|endoftext|>"
},
{
"prompt": "fn parse_request(request)

fn conduit_connect(request, context)

fn unexpected(request, context)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# encoding: utf-8
import json

import pytest
import requests_mock

import configfetcher
import phabricator
import wikibugs
from tests.common import root
from tests.wikibugs_network.common import parse_request, conduit_connect, unexpected


class WikibugsFixture:
    def __init__(self):
        self.events = []
        self.wikibugs = wikibugs.Wikibugs2(
            configfetcher.ConfigFetcher(str(root / "config.json.example"))
        )
        self.wikibugs.process_event = lambda event: self.events.append(event)

    def poll(self):
        self.wikibugs.poll()


@pytest.fixture()
def bugs():
    return WikibugsFixture()


def feed_query_initial(request, context):
    content = parse_request(request)
    assert int(content['limit']) == 1
    assert 'before' not in content
    return json.loads(r"""{"result":{"PHID-STRY-cdxv7sji5d7wnjmiuqgv":{"class":"PhabricatorApplicationTransactionFeedStory","epoch":1577802875,"authorPHID":"PHID-USER-pzp7mdlx7otgdlggnyhh","chronologicalKey":"6776611750075855743","data":{"objectPHID":"PHID-TASK-rnay3rzefpqhoaqm3guo","transactionPHIDs":{"PHID-XACT-TASK-5esu7y3d7evlsi2":"PHID-XACT-TASK-5esu7y3d7evlsi2"}}}},"error_code":null,"error_info":null}""")  # noqa


def feed_query_second(request, context):
    content = parse_request(request)
    assert content['before'] == 6776611750075855743
    assert content['view'] == 'data'
    return json.loads(r"""{"result":[],"error_code":null,"error_info":null}""")


def feed_query_third(request, context):
    content = parse_request(request)
    assert content['before'] == 6776611750075855743
    assert content['view'] == 'data'
    return json.loads(r"""{"result":{"PHID-STRY-etrbfg7qqflcsoexaxqr":{"class":"PhabricatorApplicationTransactionFeedStory","epoch":1577804347,"authorPHID":"PHID-USER-idceizaw6elwiwm5xshb","chronologicalKey":"6776618070283272953","data":{"objectPHID":"PHID-TASK-he2h6hqmwrdrav3cxqew","transactionPHIDs":{"PHID-XACT-TASK-k6asmqpfv2t37tp":"PHID-XACT-TASK-k6asmqpfv2t37tp"}}},"PHID-STRY-x6pr64eeimmcjl3jbsay":{"class":"PhabricatorApplicationTransactionFeedStory","epoch":1577804344,"authorPHID":"PHID-USER-idceizaw6elwiwm5xshb","chronologicalKey":"6776618060350723377","data":{"objectPHID":"PHID-TASK-he2h6hqmwrdrav3cxqew","transactionPHIDs":{"PHID-XACT-TASK-ix5urhvrpvn22e2":"PHID-XACT-TASK-ix5urhvrpvn22e2"}}},"PHID-STRY-cpcsc3r3444i3vaw66bo":{"class":"PhabricatorApplicationTransactionFeedStory","epoch":1577804267,"authorPHID":"PHID-USER-muirnivxp5hzppn2a3z7","chronologicalKey":"6776617727166200626","data":{"objectPHID":"PHID-TASK-dgq26etiz4wecd24gkmb","transactionPHIDs":{"PHID-XACT-TASK-zd6b2kmmj5pnfwm":"PHID-XACT-TASK-zd6b2kmmj5pnfwm"}}}},"error_code":null,"error_info":null}""")  # noqa


def feed_query_error_response(request, context):
    return json.loads(r"""{"result":null,"error_code":"ERR-CONDUIT-CORE","error_info":"Cursor \"6771969043218032437\" does not identify a valid object in query \"PhabricatorFeedQuery\"."}""")  # noqa


def test_polling(bugs):
    with requests_mock.mock() as m:
        m.post('/api/conduit.connect', [{'json': conduit_connect}, {'json': unexpected}])
        m.post('/api/feed.query', [{'json': feed_query_initial}, {'json': feed_query_second}, {'json': unexpected}])
        bugs.poll()
        assert bugs.events == []

        m.post('/api/feed.query', [{'json': feed_query_third}, {'json': unexpected}])
        bugs.poll()

        assert len(bugs.events) == 3

        # TODO: add more extensive tests


def test_error_response(bugs):
    with requests_mock.mock() as m:
        m.post('/api/conduit.connect', [{'json': conduit_connect}, {'json': unexpected}])
        m.post('/api/feed.query', [{'json': feed_query_initial}, {'json': feed_query_second}])
        bugs.poll()

        m.post('/api/feed.query', [{'json': feed_query_error_response}, {'json': unexpected}])
        with pytest.raises(phabricator.PhabricatorException):
            bugs.poll()
<|endoftext|>"
},
{
"prompt": "fn add_alpine_subscription(request)

fn add_subscription(api_conf: callable)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from tests.functional.services.api.conftest import USER_API_CONFS
from tests.functional import get_logger
from tests.functional.services.utils.http_utils import (
    http_post,
    http_get,
    RequestFailedError,
    http_del,
)

ALPINE_LATEST_SUBSCRIPTION = {
    "subscription_key": "docker.io/alpine:latest",
    "subscription_type": "tag_update",
}
_logger = get_logger(__name__)


@pytest.fixture(scope="class", params=USER_API_CONFS)
def add_alpine_subscription(request):
    subscription = add_subscription(request.param)

    def remove_subscription():
        resp = http_del(
            ["subscriptions", subscription.get("subscription_id")], config=request.param
        )
        if resp.code != 200:
            raise RequestFailedError(resp.url, resp.code, resp.body)

    request.addfinalizer(remove_subscription)
    return subscription, request.param


def add_subscription(api_conf: callable):
    added_subscription = None
    resp = http_post(["subscriptions"], ALPINE_LATEST_SUBSCRIPTION, config=api_conf)
    if (
        resp.code == 500
        and resp.body.get("message") == "subscription already exists in DB"
    ):
        # Already exists
        resp = http_get(["subscriptions"], config=api_conf)
        subscription_list = resp.body
        for subscription in subscription_list:
            if (
                subscription.get("subscription_type") == "tag_update"
                and subscription.get("subscription_key") == "docker.io/alpine:latest"
            ):
                added_subscription = subscription
                break
    elif resp.code != 200:
        raise RequestFailedError(resp.url, resp.code, resp.body)
    else:
        added_subscription = resp.body[0]
    return added_subscription
<|endoftext|>"
},
{
"prompt": "fn clock(datetime)

class MemoryRepositoryAdapter(RepositoryAdapter):
	type_tag = 'inmemory'
	fn setup(self, backups, bucket, force_clear)
	fn fetch_backups(self) -> List[Backup]
	fn cleanup_backups(self, backups: List[Backup])
	fn parse(cls, name: str) -> dict
	fn backups(self)
	fn backups(self, values)


class MemoryRepositoryLink(RepositoryLink):
	type_tag_source = 'inmemory'
	type_tag_target = 'inmemory'
	fn copy_backup(self, backup)


class MemoryBackupCreator(BackupCreator):
	type_tag = 'inmemory'
	RepositoryAdapter = MemoryRepositoryAdapter
	fn setup(self, bucket, source_bucket, target_bucket)
	fn source_adapter(self)
	fn target_adapter(self)
	fn build_backup(self, backup)
	fn source_backups(self)
	fn target_backups(self)


class MockLocalRepository(RepositoryAdapter):
	fn __init__(self, directory)
	fn setup(self)
	fn fetch_backups(self)
	fn cleanup_backups(self, backups)


class MockFtpRepository(RepositoryAdapter):
	fn __init__(self, server)
	fn setup(self)
	fn fetch_backups(self)
	fn cleanup_backups(self, backups)


class MockMysqlToLocal(BackupCreator):
	fn setup(self, directory)
	fn target_adapter(self)
	fn build_backup(self)


class MockLocalToFtp(RepositoryLink):
	fn setup(self, directory, server)
	fn source_adapter(self)
	fn target_adapter(self)
	fn build_backup(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from easybackup.core.hook import Hook
from easybackup.core.repository import Repository
from easybackup.core.backup_supervisor import BackupSupervisor
from .mock import MemoryBackupCreator, MemoryRepositoryAdapter, clock

mockbackups = [
    'easybackup-myproject-db-20200420_130000.tar',
    'easybackup-myproject-db-20200420_130100.tar',
    'easybackup-myproject-db-20200421_130000.tar',
    'easybackup-myproject-db-20200422_130000.tar',
]
memory_backup_creator = MemoryBackupCreator()


@pytest.fixture
def memory_adapter():
    return MemoryRepositoryAdapter(backups=mockbackups)


def test_register_hook():

    obj = {'done': False}

    @Hook.register('test_hook')
    def do_something(obj):
        obj['done'] = True

    Hook.plays('test_hook', obj)
    assert obj['done'] is True


@clock('20200422_130000')
def test_emit_event_on_backup_build(memory_adapter):

    rep = Repository(
        adapter=memory_adapter
    )
    backup_supervisor = BackupSupervisor(
        project='myproject',
        volume='db',
        creator=memory_backup_creator,
        repository=rep
    )

    backup_list = []
    @Hook.register('before_build_backup')
    @Hook.register('after_build_backup')
    def put_backup_on_list(creator, backup, repository):
        backup_list.append(backup)

    backup_supervisor.build_backup()
    backups = rep.fetch()
    assert backups[-1].formated_name == backup_list[0].formated_name
    assert backups[-1].formated_name == backup_list[1].formated_name
<|endoftext|>"
},
{
"prompt": "class Map:
	fn __init__(self, pattern: List[str])
	fn check_position(self, right: int, down: int) -> str
	fn is_end_of_slope_reached(self, down: int) -> bool


class Trajectory:
	fn __init__(self, right: int, down: int)


fn count_trees_on_the_slope(map: Map, trajectory: Trajectory) -> int

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from day_3_tobogan_trajectory.check_slope import count_trees_on_the_slope, \
    Trajectory
from day_3_tobogan_trajectory.map import Map


class TestCheckSlock:
    def test_should_count_no_tree_on_the_slope(self):
        # given
        trajectory = Trajectory(right=3, down=1)
        map = Map(pattern=[
            '..',
            '..'
        ])

        # when
        assert count_trees_on_the_slope(map, trajectory) == 0

    def test_should_count_one_tree_on_the_slope(self):
        # given
        trajectory = Trajectory(right=3, down=1)
        map = Map(pattern=[
            '....',
            '...#'
        ])

        # when
        assert count_trees_on_the_slope(map, trajectory) == 1

    def test_should_count_1_tree_on_the_slope_with_example(self):
        # given
        trajectory = Trajectory(right=3, down=1)
        map = Map(pattern=[
            '..##.......',
            '#...#...#..',
            '.#....#..#.',
        ])

        # when
        assert count_trees_on_the_slope(map, trajectory) == 1

    def test_should_count_2_tree_on_the_slope_with_example(self):
        # given
        trajectory = Trajectory(right=3, down=1)
        map = Map(pattern=[
            '..##.......',
            '#...#...#..',
            '.#....#..#.',
            '..#.#...#.#',
            '.#...##..#.'
        ])

        # when
        assert count_trees_on_the_slope(map, trajectory) == 2

    def test_should_count_7_tree_on_the_slope_with_full_example(self):
        # given
        trajectory = Trajectory(right=3, down=1)
        map = Map(pattern=[
            '..##.......',
            '#...#...#..',
            '.#....#..#.',
            '..#.#...#.#',
            '.#...##..#.',
            '..#.##.....',
            '.#.#.#....#',
            '.#........#',
            '#.##...#...',
            '#...##....#',
            '.#..#...#.#',
        ])

        # when
        assert count_trees_on_the_slope(map, trajectory) == 7

    @pytest.mark.parametrize("trajectory,tree_count", [
        (Trajectory(right=1, down=1), 2),
        (Trajectory(right=3, down=1), 7),
        (Trajectory(right=5, down=1), 3),
        (Trajectory(right=7, down=1), 4),
        (Trajectory(right=1, down=2), 2)
    ])
    def test_should_count_2_tree_on_different_trajectory(self, trajectory, tree_count):
        # given
        map = Map(pattern=[
            '..##.......',
            '#...#...#..',
            '.#....#..#.',
            '..#.#...#.#',
            '.#...##..#.',
            '..#.##.....',
            '.#.#.#....#',
            '.#........#',
            '#.##...#...',
            '#...##....#',
            '.#..#...#.#',
        ])

        # when
        assert count_trees_on_the_slope(map, trajectory) == tree_count
<|endoftext|>"
},
{
"prompt": """"A unique, named, sentinel object."""
class _SentinelObject:
	__slots__ = ('name',)
	fn __init__(self, name)
	fn __repr__(self)
	fn __reduce__(self)


"""Access items to return a named object, usable as a sentinel."""
class _Sentinel:
	fn __getitem__(self, name)
	fn __reduce__(self)
	fn batch(self, names)


class NamedMock(MagicMock):
	fn __name__(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import re
import pytest
import compose as cp

from unittest.mock import Mock

from .base import sentinel


def test_flip():
    @cp.flip
    def func(a, b):
        return [a, b]

    x, y = sentinel['x'], sentinel['y']
    assert func(x, y) == [y, x]


def test_safe_apply():
    func = Mock(name='func')
    arg = sentinel['arg']
    cp.safe_apply(func, arg)
    func.assert_called_once_with(arg)


def test_safe_apply_error():
    exc = RuntimeError('some runtime error')
    func = Mock(name='func', side_effect=exc)
    arg = sentinel['arg']

    message = f"{func!r}({arg!r}) caused error: {exc}"
    with pytest.raises(cp.ComposeError, match=re.escape(message)) as excinfo:
        cp.safe_apply(func, arg)

    assert excinfo.value.origin is exc


def test_compose_error():
    func, arg = Mock(name='func'), Mock(name='arg')
    exc = cp.ComposeError(func, arg)
    exc.__cause__ = origin = ImportError('origin exception')

    assert exc.func is func
    assert exc.arg is arg
    assert exc.origin is origin

    # __str__
    assert str(exc) == f"{func!r}({arg!r}) caused error: {origin}"

    # __repr__
    assert repr(exc) == f"ComposeError(func={func!r}, arg={arg!r})"
<|endoftext|>"
},
{
"prompt": "

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from django.core.exceptions import ImproperlyConfigured

import pytest

from metroid.subscribe import match_handler_subject


def test_valid_pattern() -> None:
    """
    Tests if a valid pattern matches  the provided subject.
    """
    subject = r'^something\/tests\/haha.*$'
    subject_in_message = 'something/tests/haha/asd-123'
    is_match = match_handler_subject(subject=subject, message_subject=subject_in_message, is_regex=True)
    assert is_match is True


def test_wrong_subject_match_on_pattern() -> None:
    """
    Tests if the validation fails if not a matching reggex is provided.
    """
    subject = r'^something\/tests\/haha.*$'
    subject_in_message = 'tests/haha/asd-123'
    is_match = match_handler_subject(subject=subject, message_subject=subject_in_message, is_regex=True)
    assert is_match is False


def test_match_on_string() -> None:
    """
    Tests if the pattern matches the subject provided in string format.
    """
    subject = 'tests/haha.Create'
    subject_in_message = 'tests/haha.Create'
    is_match = match_handler_subject(subject=subject, message_subject=subject_in_message, is_regex=False)
    assert is_match is True


def test_bogus_string() -> None:
    """
    Tests  if a very weird string with special characters matches .
    """
    subject = 'tests/haha$somethi#ngth"atshoul,-,.dn/otbehere'
    subject_in_message = 'tests/haha$somethi#ngth"atshoul,-,.dn/otbehere'
    is_match = match_handler_subject(subject=subject, message_subject=subject_in_message, is_regex=False)
    assert is_match is True


def test_if_exception_is_thrown() -> None:
    """
    Tests if the correct exception is thrown upon providing an invalid regex.
    """
    with pytest.raises(ImproperlyConfigured) as e:
        subject = 'tests/invalid['
        subject_in_message = 'tests/invalid['
        is_match = match_handler_subject(subject=subject, message_subject=subject_in_message, is_regex=True)
    assert str(e.value) == f'Provided regex pattern: {subject} is invalid.'
<|endoftext|>"
},
{
"prompt": "fn _path_to_tests()

fn path_to_tests()

fn tmp_directory()

fn add_current_to_sys_path()

fn grid_search_3_params()

fn grid_search_4_params()

fn grid_search_2_params()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from copy import copy
import sys
import os
import shutil
import tempfile
from pathlib import Path

import pytest

# These are fixtures to get the same configuration that matplotlib uses
# to run tests with pytest. Note that importing other fixtures from that
# module leads to weird error messages (e.g. "pd", which patches pandas)
# https://github.com/matplotlib/matplotlib/blob/master/lib/matplotlib/testing/conftest.py
from matplotlib.testing.conftest import (mpl_test_settings,
                                         mpl_image_comparison_parameters)
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn import datasets


def _path_to_tests():
    return Path(__file__).absolute().parent


@pytest.fixture(scope='session')
def path_to_tests():
    return _path_to_tests()


@pytest.fixture()
def tmp_directory():
    old = os.getcwd()
    tmp = tempfile.mkdtemp()
    os.chdir(str(tmp))

    yield tmp

    shutil.rmtree(str(tmp))
    os.chdir(old)


@pytest.fixture
def add_current_to_sys_path():
    old = copy(sys.path)
    sys.path.insert(0, os.path.abspath('.'))
    yield sys.path
    sys.path = old


@pytest.fixture(scope='session')
def grid_search_3_params():
    parameters = {
        'n_estimators': [1, 2, 5, 10],
        'criterion': ['gini', 'entropy'],
        'max_features': ['sqrt', 'log2'],
    }

    est = RandomForestClassifier(random_state=42)
    clf = GridSearchCV(est, parameters, cv=5)

    X, y = datasets.make_classification(200,
                                        10,
                                        n_informative=5,
                                        class_sep=0.7,
                                        random_state=42)
    clf.fit(X, y)

    return clf


@pytest.fixture(scope='session')
def grid_search_4_params():
    parameters = {
        'n_estimators': [1, 2, 5, 10],
        'criterion': ['gini', 'entropy'],
        'max_features': ['sqrt', 'log2'],
        'min_samples_split': [2],
    }

    est = RandomForestClassifier(random_state=42)
    clf = GridSearchCV(est, parameters, cv=5)

    X, y = datasets.make_classification(200,
                                        10,
                                        n_informative=5,
                                        class_sep=0.7,
                                        random_state=42)
    clf.fit(X, y)

    return clf


@pytest.fixture(scope='session')
def grid_search_2_params():
    parameters = {
        'n_estimators': [1, 2, 5, 10],
        'criterion': ['gini', 'entropy'],
    }

    est = RandomForestClassifier(random_state=42)
    clf = GridSearchCV(est, parameters, cv=5)

    X, y = datasets.make_classification(200,
                                        10,
                                        n_informative=5,
                                        class_sep=0.7,
                                        random_state=42)
    clf.fit(X, y)

    return clf
<|endoftext|>"
},
{
"prompt": """"Parse a form"""
fn batch_parse_form(project_id, input_uri, destination_uri)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific ladnguage governing permissions and
# limitations under the License.

import os
import uuid

from google.cloud import storage
import pytest

import batch_parse_form_beta


BUCKET = 'document-ai-{}'.format(uuid.uuid4())
OUTPUT_PREFIX = 'TEST_OUTPUT_{}'.format(uuid.uuid4())
PROJECT_ID = os.environ['GCLOUD_PROJECT']
INPUT_URI = 'gs://cloud-samples-data/documentai/invoice.pdf'
BATCH_OUTPUT_URI = 'gs://{}/{}/'.format(BUCKET, OUTPUT_PREFIX)


@pytest.fixture(autouse=True)
def setup_teardown():
    """Create a temporary bucket to store annotation output."""
    storage_client = storage.Client()
    bucket = storage_client.create_bucket(BUCKET)

    yield

    bucket.delete(force=True)


def test_batch_parse_form(capsys):
    batch_parse_form_beta.batch_parse_form(PROJECT_ID, INPUT_URI, BATCH_OUTPUT_URI)
    out, _ = capsys.readouterr()
    assert 'Output files' in out
<|endoftext|>"
},
{
"prompt": "class CodeReportWriter:
	fn __init__(self, code_queryset)
	fn get_fields_iterator(self)
	fn get_report(self, format, as_response)
	fn format_xls_report(self, filename, as_response)
	fn _format_delimited_report(self, filename, as_response, field_delimiter, record_delimiter)
	fn format_csv_report(self, filename, as_response)
	fn format_tsv_report(self, filename, as_response)
	fn format_pdf_report(self, filename, as_response)


fn get_code_report(format, as_response, order_by_literate_code)

fn get_setting(name, default)

fn get_integer_setting(name, default)

fn validate_settings()

fn _validate_code()

fn _validate_prefixes()

fn _validate_print()

fn _create_test_order()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from io import BytesIO

import pytest
from django.conf import settings

from lippukala.models import Code, Order
from lippukala.reports import CodeReportWriter, get_code_report
from lippukala.settings import PREFIXES

from .utils import _create_test_order

pytestmark = pytest.mark.django_db


def test_creating_order():
    order = _create_test_order()
    assert order.code_set.count() == 25, "orders don't hold their codes"
    for code in order.code_set.all():
        assert code.literate_code.startswith(PREFIXES[code.prefix]), "prefixes don't work"


def test_cant_create_invalid_prefix():
    order = Order.objects.create(comment="Dummy")

    with pytest.raises(ValueError):
        Code.objects.create(order=order, prefix="HQ")

    with pytest.raises(ValueError):
        Code.objects.create(order=order, prefix="69")

    order.delete()


def test_double_use_code():
    order = _create_test_order()
    code = order.code_set.all()[:1][0]
    assert not code.is_used, "code is not used"
    code.set_used(save=True)
    assert code.is_used, "code is used"
    code = Code.objects.get(pk=code.pk)  # reload it to see if saving worked
    assert code.is_used, "code is used (even from the db)"

    with pytest.raises(ValueError):
        code.set_used()


def test_csv_reports_have_good_stuff():
    order = _create_test_order()
    csv_report_data = get_code_report("csv", False).decode(settings.DEFAULT_CHARSET)
    # We don't particularly care if we have extra orders/codes at this point, just as long
    # as the ones we just created are found
    for code in order.code_set.all():
        assert code.literate_code in csv_report_data, f"code {code!r} was missing"


def test_all_report_formats_seem_to_work():
    _create_test_order()
    formats = [n.split("_")[1] for n in dir(CodeReportWriter) if n.startswith("format_")]
    for format in formats:
        assert get_code_report(format, False, True)
        assert get_code_report(format, True, False)


@pytest.mark.parametrize("one_per_page", (False, True))
def test_printing(one_per_page):
    from lippukala.printing import OrderPrinter

    printer = OrderPrinter()
    printer.ONE_TICKET_PER_PAGE = one_per_page
    for x in range(3):
        order = _create_test_order()
        printer.process_order(order)

    outf = BytesIO()
    outf.write(printer.finish())
    assert outf.getvalue().startswith(b"%PDF")
<|endoftext|>"
},
{
"prompt": """"A Bokeh-specific ``DeprecationWarning`` subclass.

Used to selectively filter Bokeh deprecations for unconditional display."""
class BokehDeprecationWarning(DeprecationWarning):


"""A Bokeh-specific ``UserWarning`` subclass.

Used to selectively filter Bokeh warnings for unconditional display."""
class BokehUserWarning(UserWarning):


"""    """
fn verify_all(module, ALL)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "#-----------------------------------------------------------------------------
# Copyright (c) 2012 - 2020, Anaconda, Inc., and Bokeh Contributors.
# All rights reserved.
#
# The full license is in the file LICENSE.txt, distributed with this software.
#-----------------------------------------------------------------------------

#-----------------------------------------------------------------------------
# Boilerplate
#-----------------------------------------------------------------------------
import pytest ; pytest

#-----------------------------------------------------------------------------
# Imports
#-----------------------------------------------------------------------------

# Standard library imports
import warnings

# Bokeh imports
from bokeh._testing.util.api import verify_all
from bokeh.util.warnings import BokehDeprecationWarning, BokehUserWarning

# Module under test
import bokeh as b # isort:skip

#-----------------------------------------------------------------------------
# Setup
#-----------------------------------------------------------------------------

ALL =  (
    '__version__',
    'license',
    'sampledata',
)

_LICENSE = """\
Copyright (c) 2012 - 2020, Anaconda, Inc., and Bokeh Contributors
All rights reserved.

Redistribution and use in source and binary forms, with or without modification,
are permitted provided that the following conditions are met:

Redistributions of source code must retain the above copyright notice,
this list of conditions and the following disclaimer.

Redistributions in binary form must reproduce the above copyright notice,
this list of conditions and the following disclaimer in the documentation
and/or other materials provided with the distribution.

Neither the name of Anaconda nor the names of any contributors
may be used to endorse or promote products derived from this software
without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
THE POSSIBILITY OF SUCH DAMAGE.

"""

#-----------------------------------------------------------------------------
# General API
#-----------------------------------------------------------------------------

Test___all__ = verify_all(b, ALL)

def test___version___type() -> None:
    assert isinstance(b.__version__, str)

def test___version___defined() -> None:
    assert b.__version__ != 'unknown'

def test_license(capsys) -> None:
    b.license()
    out, err = capsys.readouterr()
    assert out == _LICENSE

class TestWarnings:
    @pytest.mark.parametrize('cat', (BokehDeprecationWarning, BokehUserWarning))
    def test_bokeh_custom(self, cat) -> None:
        r = warnings.formatwarning("message", cat, "line", "lineno")
        assert r == "%s: %s\n" %(cat.__name__, "message")

    def test_general_default(self) -> None:
        r = warnings.formatwarning("message", RuntimeWarning, "line", "lineno")
        assert r == "line:lineno: RuntimeWarning: message\n"

    # TODO (bev) issue with this one test and 3.9 support PR
    @pytest.mark.skip
    def test_filters(self) -> None:
        assert ('always', None, BokehUserWarning, None, 0) in warnings.filters
        assert ('always', None, BokehDeprecationWarning, None, 0) in warnings.filters

#-----------------------------------------------------------------------------
# Dev API
#-----------------------------------------------------------------------------

#-----------------------------------------------------------------------------
# Private API
#-----------------------------------------------------------------------------

#-----------------------------------------------------------------------------
# Code
#-----------------------------------------------------------------------------
<|endoftext|>"
},
{
"prompt": "class TextAnalyticsClientPreparer(AzureMgmtPreparer):
	fn __init__(self, client_cls, client_kwargs)
	fn create_resource(self, name)


class TextAnalyticsTest(AzureRecordedTestCase):
	fn assertOpinionsEqual(self, opinion_one, opinion_two)
	fn validateConfidenceScores(self, confidence_scores)
	fn assert_healthcare_data_sources_equal(self, data_sources_a, data_sources_b)
	fn assert_healthcare_entities_equal(self, entity_a, entity_b)
	fn document_result_to_action_type(self, document_result)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# coding=utf-8
# ------------------------------------
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.
# ------------------------------------

import pytest
from azure.ai.textanalytics.aio import TextAnalyticsClient
from azure.core.credentials import AzureKeyCredential
from testcase import TextAnalyticsPreparer
from testcase import TextAnalyticsTest


class TestAuth(TextAnalyticsTest):

    @pytest.mark.live_test_only
    @TextAnalyticsPreparer()
    async def test_active_directory_auth(self, textanalytics_test_endpoint):
        token = self.get_credential(TextAnalyticsClient, is_async=True)
        text_analytics = TextAnalyticsClient(textanalytics_test_endpoint, token)

        docs = [{"id": "1", "text": "I should take my cat to the veterinarian."},
                {"id": "2", "text": "Este es un document escrito en Español."},
                {"id": "3", "text": "猫は幸せ"},
                {"id": "4", "text": "Fahrt nach Stuttgart und dann zum Hotel zu Fu."}]

        response = await text_analytics.detect_language(docs)

    @TextAnalyticsPreparer()
    async def test_empty_credentials(self, textanalytics_test_endpoint, textanalytics_test_api_key):
        with pytest.raises(TypeError):
            text_analytics = TextAnalyticsClient(textanalytics_test_endpoint, "")

    @TextAnalyticsPreparer()
    def test_bad_type_for_credentials(self, textanalytics_test_endpoint, textanalytics_test_api_key):
        with pytest.raises(TypeError):
            text_analytics = TextAnalyticsClient(textanalytics_test_endpoint, [])

    @TextAnalyticsPreparer()
    def test_none_credentials(self, textanalytics_test_endpoint, textanalytics_test_api_key):
        with pytest.raises(ValueError):
            text_analytics = TextAnalyticsClient(textanalytics_test_endpoint, None)

    @TextAnalyticsPreparer()
    def test_none_endpoint(self, textanalytics_test_endpoint, textanalytics_test_api_key):
        with pytest.raises(ValueError):
            text_analytics = TextAnalyticsClient(None, AzureKeyCredential(textanalytics_test_api_key))
<|endoftext|>"
},
{
"prompt": "class Migrate:
	upgrade_operators: List[str] = []
	downgrade_operators: List[str] = []
	_upgrade_fk_m2m_index_operators: List[str] = []
	_downgrade_fk_m2m_index_operators: List[str] = []
	_upgrade_m2m: List[str] = []
	_downgrade_m2m: List[str] = []
	_aerich = Aerich.__name__
	_rename_old = []
	_rename_new = []
	ddl: BaseDDL
	_last_version_content: Optional[dict] = None
	app: str
	migrate_location: str
	dialect: str
	_db_version: Optional[str] = None
	fn get_all_version_files(cls) -> List[str]
	fn _get_model(cls, model: str) -> Type[Model]
	"""add operator,differentiate fk because fk is order limit
	:param operator:
	:param upgrade:
	:param fk_m2m:
	:return:"""
	fn _add_operator(cls, operator: str, upgrade: None, fk_m2m: None)
	"""diff models and add operators
	:param old_models:
	:param new_models:
	:param upgrade:
	:return:"""
	fn diff_models(cls, old_models: Dict[(str, dict)], new_models: Dict[(str, dict)], upgrade: None)
	fn add_model(cls, model: Type[Model])
	fn drop_model(cls, table_name: str)
	fn create_m2m(cls, model: Type[Model], field_describe: dict, reference_table_describe: dict)
	fn drop_m2m(cls, table_name: str)
	fn _resolve_fk_fields_name(cls, model: Type[Model], fields_name: Tuple[str])
	fn _drop_index(cls, model: Type[Model], fields_name: Tuple[str], unique)
	fn _add_index(cls, model: Type[Model], fields_name: Tuple[str], unique)
	fn _add_field(cls, model: Type[Model], field_describe: dict, is_pk: bool)
	fn _alter_default(cls, model: Type[Model], field_describe: dict)
	fn _alter_null(cls, model: Type[Model], field_describe: dict)
	fn _set_comment(cls, model: Type[Model], field_describe: dict)
	fn _modify_field(cls, model: Type[Model], field_describe: dict)
	fn _drop_fk(cls, model: Type[Model], field_describe: dict, reference_table_describe: dict)
	fn _remove_field(cls, model: Type[Model], column_name: str)
	fn _rename_field(cls, model: Type[Model], old_field_name: str, new_field_name: str)
	fn _change_field(cls, model: Type[Model], old_field_describe: dict, new_field_describe: dict)
	"""add fk
	:param model:
	:param field_describe:
	:param reference_table_describe:
	:return:"""
	fn _add_fk(cls, model: Type[Model], field_describe: dict, reference_table_describe: dict)
	"""fk/m2m/index must be last when add,first when drop
	:return:"""
	fn _merge_operators(cls)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import asyncio
import os

import pytest
from tortoise import Tortoise, expand_db_url, generate_schema_for_client
from tortoise.backends.asyncpg.schema_generator import AsyncpgSchemaGenerator
from tortoise.backends.mysql.schema_generator import MySQLSchemaGenerator
from tortoise.backends.sqlite.schema_generator import SqliteSchemaGenerator

from aerich.ddl.mysql import MysqlDDL
from aerich.ddl.postgres import PostgresDDL
from aerich.ddl.sqlite import SqliteDDL
from aerich.migrate import Migrate

db_url = os.getenv("TEST_DB", "sqlite://:memory:")
db_url_second = os.getenv("TEST_DB_SECOND", "sqlite://:memory:")
tortoise_orm = {
    "connections": {
        "default": expand_db_url(db_url, True),
        "second": expand_db_url(db_url_second, True),
    },
    "apps": {
        "models": {
            "models": ["tests.models", "aerich.models"],
            "default_connection": "default",
        },
        "models_second": {"models": ["tests.models_second"], "default_connection": "second"},
    },
}


@pytest.fixture(scope="function", autouse=True)
def reset_migrate():
    Migrate.upgrade_operators = []
    Migrate.downgrade_operators = []
    Migrate._upgrade_fk_m2m_index_operators = []
    Migrate._downgrade_fk_m2m_index_operators = []
    Migrate._upgrade_m2m = []
    Migrate._downgrade_m2m = []


@pytest.fixture(scope="session")
def event_loop():
    policy = asyncio.get_event_loop_policy()
    res = policy.new_event_loop()
    asyncio.set_event_loop(res)
    res._close = res.close
    res.close = lambda: None

    yield res

    res._close()


@pytest.fixture(scope="session", autouse=True)
async def initialize_tests(event_loop, request):
    await Tortoise.init(config=tortoise_orm, _create_db=True)
    await generate_schema_for_client(Tortoise.get_connection("default"), safe=True)

    client = Tortoise.get_connection("default")
    if client.schema_generator is MySQLSchemaGenerator:
        Migrate.ddl = MysqlDDL(client)
    elif client.schema_generator is SqliteSchemaGenerator:
        Migrate.ddl = SqliteDDL(client)
    elif client.schema_generator is AsyncpgSchemaGenerator:
        Migrate.ddl = PostgresDDL(client)
    Migrate.dialect = Migrate.ddl.DIALECT
    request.addfinalizer(lambda: event_loop.run_until_complete(Tortoise._drop_databases()))
<|endoftext|>"
},
{
"prompt": "fn parity(count)

class CountRobotdocsconf(RobotdocsconfBase):
	fn __init__(self, count)
	fn robotdocsconf(self)
	fn _metadata_str(self)


class Counts:
	fn __init__(self, counts)
	fn __hash__(self)
	fn __repr__(self)
	fn __eq__(self, other)
	fn __len__(self)
	fn counts(self)


class NameRobotdocsconfBase:
	"""Return robotdocsconf dictionary.
	        """
	fn robotdocsconf(self)
	"""Return name of the robotdocsconf (section)
	        """
	fn name(self)
	fn __eq__(self, other)
	fn __repr__(self)
	fn __len__(self)
	fn __hash__(self)


class ExpectedRobotdocsconf(NameRobotdocsconfBase):
	fn __init__(self, counts)
	fn robotdocsconf(self)
	fn name(self)


class ActualRobotdocsconf(NameRobotdocsconfBase):
	fn __init__(self, name, robotdocsconf)
	fn robotdocsconf(self)
	fn name(self)


class RobotDoc:
	fn __init__(self, tmpdir, rootpaths)
	fn ctx(self)
	fn rootpath(self)
	fn expected_robotdocsconfs(self)
	fn _robotdocsconfs_gen(self)
	fn _get_every_second(self, start)
	fn _create_robotdocsconf_files(self)
	fn _create_to_root(self, rootpath)
	fn _mkdir_rootpath(self)
	fn _content_gen(self)
	fn _mkdir_args(self)


class CliVerifier:
	fn __init__(self)
	fn set_script_runner(self, script_runner)
	fn set_docspec(self, docspec)
	fn set_mock_crldocbuilder(self, mock_crldocbuilder)
	fn set_robotdoc(self, robotdoc)
	fn set_resource(self, resource)
	fn verify(self)
	fn _verify_valid(self)
	fn _assert_robotdocsconfs(self)
	fn _verify_invalid(self)
	fn _run(self)
	fn _args(self)
	fn _actual_docspec(self)
	fn _actual_robotdocsconfs(self)
	fn _actual_resource(self)
	fn _actual_counts(robotdocsconfs)
	fn _actual_arg(self)
	fn _expected_docspec(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import itertools
import logging
import mock
import pytest
from .cliverifier import CliVerifier
from .robotdoc import RobotDoc


__copyright__ = "Copyright (C) 2019, Nokia"


def pytest_configure(config):  # pylint: disable=unused-argument
    logging.basicConfig(
        format='%(processName)s %(asctime)s.%(msecs)03d %(levelname)s %(message)s',
        level=logging.DEBUG)
    # TODO: remove workaround for too verbose flakes after
    # https://github.com/tholo/pytest-flake8/issues/42 corrected
    logging.getLogger('flake8').setLevel(logging.WARN)


@pytest.fixture
def cliverifier(script_runner, docspec, mock_crldocbuilder, robotdoc, resource):
    c = CliVerifier()
    c.set_script_runner(script_runner)
    c.set_docspec(docspec)
    c.set_mock_crldocbuilder(mock_crldocbuilder)
    c.set_robotdoc(robotdoc)
    c.set_resource(resource)
    return c


DOCSPECS = ['all', 'builtin', 'api', 'crl']
DOCSPEC_COMB = [set(c) for r in range(len(DOCSPECS))
                for c in itertools.combinations(DOCSPECS, r)]


@pytest.fixture(params=DOCSPEC_COMB)
def docspec(request):
    return request.param


@pytest.fixture()
def mock_crldocbuilder():
    with mock.patch('crl.doc.cmdline.CRLDocBuilder') as p:
        yield p


ROOT_PATHS = [[], [['root']], [['root0'], ['root1']], [['root'], ['parent', 'child']]]


@pytest.fixture(params=ROOT_PATHS)
def robotdoc(request, tmpdir):
    with RobotDoc(tmpdir=tmpdir, rootpaths=request.param).ctx() as r:
        yield r


RESOURCES = [['resource{}'.format(i) for i in range(j)] for j in range(3)]


@pytest.fixture(params=RESOURCES)
def resource(request):
    return request.param
<|endoftext|>"
},
{
"prompt": """"A requisite that annotates the requisitor."""
class Annotate(Requisite):
	priority = -1
	"""Args:
	    key (String)
	    value (Any)
	
	See also:
	    $GraphNode"""
	fn __init__(self, key, value)
	fn satisfy(self, requisitor, workflow)


"""A requisite that alters the requisitor (assigns some requisite to another task).

Includes the assignee into the workflow automatically."""
class Assign(Requisite):
	priority = -1
	"""Args:
	    task (Task) - an assignee
	    requisite (Requisite) - a requisite to assign"""
	fn __init__(self, task, requisite)
	fn satisfy(self, requisitor, workflow)


"""A requisite that extends an annotation of the requisitor.

Assumes the annotation values form a set."""
class ExtendAnnotation(Requisite):
	priority = -2
	"""Args:
	    key (String)
	    values (Set[Any])"""
	fn __init__(self, key, values)
	fn satisfy(self, requisitor, workflow)


"""A requisite that adds a link from some task to the requisitor.

Includes the task into the workflow automatically.

The requisitor must not be $None."""
class Follow(Requisite):
	priority = -2
	"""Args:
	    task (Task)"""
	fn __init__(self, task)
	fn satisfy(self, requisitor, workflow)


"""A requisite that includes some task into the workflow.

Recursively satisfies the requisite of the new task.
Collects all requisites along the MRO."""
class Include(Requisite):
	"""Args:
	    task (Task)"""
	fn __init__(self, task)
	fn satisfy(self, requisitor, workflow)


"""A requisite that attempts to satisfy other requisites.

This is a helper class that allows to treat requisite collections as requisites.
It preserves the requisitor."""
class SatisfyAll(Requisite):
	"""Args:
	    requisites (Iterable[Requisite])"""
	fn __init__(self, requisites)
	fn satisfy(self, requisitor, workflow)


"""Get the canonical form of the requisite from a shortcut.

Converts:
  - a mapping to a series of $Assign's
        Keys - assignees, values - assignments.
  - an iterable object to a corresponding $SatisfyAll
  - a task to a corresponding $Follow
        This is the most convenient way to define dependencies.

Args:
    requisite (Any) - an object to convert to a requisite

Returns:
    Requisite

Raises:
    RequisiteConformationError if $requisite cannot be converted to a requisite"""
fn conform(requisite)

"""Create an auto-conforming requisite property.

Allows you to write shorter requisite declarations.

Args:
    evaluator (Callable[[Any], Any]) - a requisite property evaluator

Returns:
    Property - auto-conforming requisite property"""
fn shortcut(evaluator)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from edera import Condition
from edera import Task
from edera.requisites import shortcut
from edera.storages import InMemoryStorage
from edera.testing import DefaultScenario
from edera.testing import Stub
from edera.testing import Test
from edera.testing import TestableTask
from edera.workflow import WorkflowBuilder


def test_testable_task_annotates_itself():

    class S(DefaultScenario):
        pass

    class A(TestableTask):
        pass

    class B(TestableTask):

        def execute(self):
            pass

    class C(TestableTask):

        @shortcut
        def requisite(self):
            return [A(), B()]

        @property
        def tests(self):
            yield S()

    workflow = WorkflowBuilder().build(C())
    assert workflow[A()]["tests"] == set()
    assert workflow[B()]["tests"] == {DefaultScenario()}
    assert workflow[C()]["tests"] == {S()}


def test_testing_task_has_correct_target():

    class A(Task):
        pass

    class T(Test):

        registry = InMemoryStorage()

    test = T(scenario=DefaultScenario(), subject=A())
    assert not test.target.check()
    test.execute()
    assert test.target.check()


def test_testing_task_runs_scenario():

    class A(Task):

        def execute(self):
            raise RuntimeError

    class T(Test):

        registry = InMemoryStorage()

    test = T(scenario=DefaultScenario(), subject=A())
    assert not set(test.target.invariants)
    assert not test.target.check()
    with pytest.raises(RuntimeError):
        test.execute()
    assert not test.target.check()


def test_testing_task_postchecks_target():

    class E(Condition):

        def check(self):
            raise RuntimeError

    class A(Task):

        target = E()

    class T(Test):

        registry = InMemoryStorage()

    test = T(scenario=DefaultScenario(), subject=A())
    assert (test.target >> E()) in set(test.target.invariants)
    assert not test.target.check()
    with pytest.raises(RuntimeError):
        test.execute()
    assert not test.target.check()


def test_stubbing_task_runs_scenario():

    class F(Condition):

        def check(self):
            return False

    class A(Task):

        target = F()

        def execute(self):
            raise RuntimeError

    stub = Stub(scenario=DefaultScenario(), subject=A())
    assert stub.target == F()
    with pytest.raises(RuntimeError):
        stub.execute()
<|endoftext|>"
},
{
"prompt": """"Adapts tunable parameters, namely step size and mass matrix, during the
warmup phase. This class provides lookup properties to read the latest
values of ``step_size`` and ``inverse_mass_matrix``. These values are
periodically updated when adaptation is engaged."""
class WarmupAdapter(object):
	fn __init__(self, step_size, adapt_step_size, target_accept_prob, adapt_mass_matrix, is_diag_mass)
	fn _build_adaptation_schedule(self)
	"""Finds a reasonable step size and resets step size adaptation scheme."""
	fn reset_step_size_adaptation(self, z)
	fn _update_step_size(self, accept_prob)
	fn _update_r_dist(self)
	fn _end_adaptation(self)
	"""Model specific properties that are specified when the HMC kernel is setup.
	
	:param warmup_steps: Number of warmup steps that the sampler is initialized with.
	:param initial_step_size: Step size to use to initialize the Dual Averaging scheme.
	:param inv_mass_matrix: Initial value of the inverse mass matrix.
	:param find_reasonable_step_size_fn: A callable to find reasonable step size when
	    mass matrix is changed."""
	fn configure(self, warmup_steps: None, initial_step_size: None, inv_mass_matrix: None, find_reasonable_step_size_fn: None)
	"""Called at each step during the warmup phase to learn tunable
	parameters.
	
	:param int t: time step, beginning at 0.
	:param dict z: latent variables.
	:param float accept_prob: acceptance probability of the proposal."""
	fn step(self, t: int, z: dict, accept_prob: float)
	fn adaptation_schedule(self)
	fn inverse_mass_matrix(self)
	fn inverse_mass_matrix(self, value)
	fn r_dist(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Copyright (c) 2017-2019 Uber Technologies, Inc.
# SPDX-License-Identifier: Apache-2.0

import pytest
import torch

from pyro.infer.mcmc.adaptation import (
    ArrowheadMassMatrix,
    BlockMassMatrix,
    WarmupAdapter,
    adapt_window,
)
from tests.common import assert_close, assert_equal, skipif_rocm


@pytest.mark.parametrize("adapt_step_size, adapt_mass, warmup_steps, expected", [
    (False, False, 100, []),
    (False, True, 50, [(0, 6), (7, 44), (45, 49)]),
    (True, False, 150, [(0, 74), (75, 99), (100, 149)]),
    (True, True, 200, [(0, 74), (75, 99), (100, 149), (150, 199)]),
    (True, True, 280, [(0, 74), (75, 99), (100, 229), (230, 279)]),
    (True, True, 18, [(0, 17)]),
])
@skipif_rocm
def test_adaptation_schedule(adapt_step_size, adapt_mass, warmup_steps, expected):
    adapter = WarmupAdapter(0.1,
                            adapt_step_size=adapt_step_size,
                            adapt_mass_matrix=adapt_mass)
    adapter.configure(warmup_steps, mass_matrix_shape={"z": (5, 5)})
    expected_schedule = [adapt_window(i, j) for i, j in expected]
    assert_equal(adapter.adaptation_schedule, expected_schedule, prec=0)


@pytest.mark.parametrize("diagonal", [True, False])
@skipif_rocm
def test_arrowhead_mass_matrix(diagonal):
    shape = (2, 3)
    num_samples = 1000

    size = shape[0] * shape[1]
    block_adapter = BlockMassMatrix()
    arrowhead_adapter = ArrowheadMassMatrix()
    mass_matrix_shape = (size,) if diagonal else (size, size)
    block_adapter.configure({("z",): mass_matrix_shape})
    arrowhead_adapter.configure({("z",): mass_matrix_shape})

    cov = torch.randn(size, size)
    cov = torch.mm(cov, cov.t())
    if diagonal:
        cov = cov.diag().diag()
    z_dist = torch.distributions.MultivariateNormal(torch.zeros(size), covariance_matrix=cov)
    g_dist = torch.distributions.MultivariateNormal(torch.zeros(size), precision_matrix=cov)
    z_samples = z_dist.sample((num_samples,)).reshape((num_samples,) + shape)
    g_samples = g_dist.sample((num_samples,)).reshape((num_samples,) + shape)

    for i in range(num_samples):
        block_adapter.update({"z": z_samples[i]}, {"z": g_samples[i]})
        arrowhead_adapter.update({"z": z_samples[i]}, {"z": g_samples[i]})
    block_adapter.end_adaptation()
    arrowhead_adapter.end_adaptation()

    assert_close(arrowhead_adapter.inverse_mass_matrix[('z',)],
                 block_adapter.inverse_mass_matrix[('z',)],
                 atol=0.3, rtol=0.3)
<|endoftext|>"
},
{
"prompt": "fn get_first_leaf_with_value(code, value)

fn _read(nb_str)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import jupytext
import pytest

from testutils import exploratory, mixed, _read
from soorgeon import proto

# TODO: do we need roundtrip conversion? we'l only use this for static analysis
# so i think we're fine
mixed_expected = '1 + 1 # Cell 1\n2 + 2 # Cell 3'


@pytest.mark.parametrize('code, expected', [
    [mixed, mixed_expected],
])
def test_prototask_str(code, expected):
    assert str(
        proto.ProtoTask('name', _read(code).cells, df_format=None,
                        py=True)) == expected


@pytest.mark.parametrize('cells_idx, expected', [
    [(0, 3), 'from sklearn.datasets import load_iris'],
])
def test_prototask_add_imports_cell(cells_idx, expected):
    cells = jupytext.reads(exploratory,
                           fmt='py:light').cells[cells_idx[0]:cells_idx[1]]
    pt = proto.ProtoTask('task', cells, df_format=None, py=True)
    cell = pt._add_imports_cell(exploratory,
                                add_pathlib_and_pickle=False,
                                definitions=None,
                                df_format=None)
    assert cell['source'] == expected
<|endoftext|>"
},
{
"prompt": "fn mock_connection(fn)

class MockObject(object):
	fn __init__(self)


class MockConnection(Connection):
	fn __init__(self)
	fn write(self, data)
	fn last(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import datetime
import re

import pytest

from pyromancer.decorators import command
from pyromancer.exceptions import CommandException
from pyromancer.objects import Match, Line, Timer
from pyromancer.test.decorators import mock_connection
from pyromancer.test.mock_objects import MockObject

MESSAGES = [
    ('Hello world', 'Hello world'),
    (('Hello {}', 'world'), 'Hello world'),
    (('Hello {} and {}', 'world', 'Mars'), 'Hello world and Mars'),
    (('Hello {} and {}', ['world', 'Mars']), 'Hello world and Mars'),
    (('Hello {sphere}', {'sphere': 'world'}), 'Hello world'),
    (('Hello {} and {red_one}', 'world', {'red_one': 'Mars'}),
     'Hello world and Mars'),
    (('Hello {}, {} and {red_one}', 'world', 'moon', {'red_one': 'Mars'}),
     'Hello world, moon and Mars'),
    (('Hello {}, {} and {red_one}', ['world', 'moon'], {'red_one': 'Mars'}),
     'Hello world, moon and Mars'),
    (('Hello {}', ['world', 'moon']), "Hello ['world', 'moon']"),
]


def test_command_decorator_set_function():
    instance = command(r'')
    function = lambda m: m
    instance(function)

    assert isinstance(function.command, command)
    assert function.command.function is function


@mock_connection
def test_command_messaging_return_tuple(c):
    line = Line(':John!JDoe@some.host PRIVMSG #Chan :Some cool message', c)
    instance = command(r'')
    match = Match(None, line, c)

    for msg, expected in MESSAGES:
        instance.send_messages(msg, match, [])
        assert c.last == 'PRIVMSG #Chan :{}'.format(expected)


@mock_connection
def test_command_messaging_return_list(c):
    line = Line(':John!JDoe@some.host PRIVMSG #Chan :Some cool message', c)
    instance = command(r'')
    match = Match(None, line, c)

    instance.send_messages([msg for msg, expected in MESSAGES], match, [])

    for index, (msg, expected) in enumerate(MESSAGES):
        assert c.outbox[index] == 'PRIVMSG #Chan :{}'.format(expected)


@mock_connection
def test_command_messaging_yielding(c):
    def mock_command():
        for msg, expected in MESSAGES:
            yield msg

    line = Line(':John!JDoe@some.host PRIVMSG #Chan :Some cool message', c)
    instance = command(r'')
    match = Match(None, line, c)

    instance.send_messages(mock_command(), match, [])

    for index, (msg, expected) in enumerate(MESSAGES):
        assert c.outbox[index] == 'PRIVMSG #Chan :{}'.format(expected)


def test_command_appends_timers():
    instance = command(r'')
    match = Match(None, None, None)
    timers = []

    instance.send_messages((datetime.timedelta(seconds=3), 'User', 'Hi'),
                           match, timers)
    assert len(timers) == 1
    assert isinstance(timers[0], Timer)
    assert timers[0].scheduled == datetime.timedelta(seconds=3)
    assert timers[0].msg_tuple == ('User', 'Hi', (), {},)


@mock_connection
def test_command_matches_patterns(c):
    line = Line(':John!JDoe@some.host PRIVMSG #Chan :Some cool message', c)
    settings = MockObject(command_prefix='!')

    instance = command(r'^Some', prefix=False)
    assert bool(instance.matches(line, settings)) is True

    instance = command(r'message$', prefix=False)
    assert bool(instance.matches(line, settings)) is True

    instance = command(r'^Some cool message$', prefix=False)
    assert bool(instance.matches(line, settings)) is True

    instance = command(r'mESsagE', prefix=False, flags=re.IGNORECASE)
    assert bool(instance.matches(line, settings)) is True

    instance = command(r'Some')
    assert bool(instance.matches(line, settings)) is False

    settings = MockObject(command_prefix='S')

    instance = command(r'^ome')
    assert bool(instance.matches(line, settings)) is True

    instance = command(r'cool')
    assert bool(instance.matches(line, settings)) is True

    line = Line(':irc.example.net 376 A :End of MOTD command', c)

    instance = command(r'example', prefix=False)
    assert bool(instance.matches(line, settings)) is False

    instance = command(r'example', raw=True, prefix=False)
    assert bool(instance.matches(line, settings)) is True


@mock_connection
def test_command_matches_code(c):
    with pytest.raises(CommandException):
        command()

    with pytest.raises(CommandException):
        command(code='Foo')

    settings = MockObject(command_prefix='!')
    instance = command(code=376)

    line = Line(':irc.example.net 376 A :End of MOTD command', c)
    assert line.code == 376
    assert bool(instance.matches(line, settings)) is True

    line = Line(':irc.example.net 375 A :- irc.example.net message of the day',
                c)
    assert line.code == 375
    assert bool(instance.matches(line, settings)) is False


@mock_connection
def test_command_matches_command(c):
    # A command is a 4 to 5 character all-capital string received from the
    # server. Examples: JOIN, QUIT, NICK, etc.
    settings = MockObject(command_prefix='!')
    instance = command(command='PART')

    line = Line(':John!JDoe@some.host PART #Chan :"Bye !"', c)
    assert line.command == 'PART'
    assert bool(instance.matches(line, settings)) is True

    line = Line(':John!JDoe@some.host NICK Paul"', c)
    assert line.command == 'NICK'
    assert bool(instance.matches(line, settings)) is False
<|endoftext|>"
},
{
"prompt": """"Setup for test run."""
fn env_setup()

"""Cleanup post test run."""
fn env_teardown()

"""File or folder, it is deleted.

Args:
    path: path to a file or dir"""
fn delete_it(path: None)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """"
Used for pytest plugins & session scoped fixtures.
"""
from __future__ import absolute_import
import sys
import mock
import pytest
import tests.common as tc


@pytest.fixture(scope='session', autouse=True)
def setup_test_bed(request):
    """
    Fixture sets up the testing environment for pakit as a whole.

    Session scope, executes before all tests.
    """
    request.addfinalizer(tc.env_teardown)
    tc.env_setup()


@pytest.yield_fixture()
def mock_print():
    """
    A fixture that mocks python's print function during test.
    """
    if sys.version_info < (3, 0):
        print_mod = '__builtin__.print'
    else:
        print_mod = 'builtins.print'
    with mock.patch(print_mod) as mock_obj:
        yield mock_obj


@pytest.yield_fixture()
def mock_input():
    """
    A fixture that mocks python's print function during test.
    """
    if sys.version_info < (3, 0):
        input_mod = '__builtin__.raw_input'
    else:
        input_mod = 'builtins.input'
    with mock.patch(input_mod) as mock_obj:
        yield mock_obj


@pytest.yield_fixture(scope='function', autouse=True)
def around_all_tests():
    """
    Executes before and after EVERY test.

    Can be helpful for tracking bugs impacting test bed.
    """
    # before
    yield
    # after
<|endoftext|>"
},
{
"prompt": "fn save_cookies(driver, location)

fn load_cookies(driver, url, location)

fn delete_cookies(driver, domains)

fn set_cookies(driver, user, password)

fn admin_login(driver, user, password)

fn get_host_ip()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
import Cookies


@pytest.mark.usefixtures("setup_login")
class TestLogin:

    def wait_until(self, delay, condition):
        WebDriverWait(self.driver, delay).until(condition)

    def test_login(self, user="alice@foo.com", password="testtest"):
        driver = self.driver

        # Test invalid username
        driver.find_element_by_id("identifier").send_keys("wrong_alice")
        driver.find_element_by_id("password").send_keys(password)
        driver.find_element_by_id("submit").click()

        # Presence of the 'password' element implies a failed login
        self.wait_until(30, EC.presence_of_element_located((By.ID, "password")))

        # Test invalid password
        driver.find_element_by_id("identifier").send_keys(user)
        driver.find_element_by_id("password").send_keys("wrong_password")
        driver.find_element_by_id("submit").click()

        self.wait_until(30, EC.presence_of_element_located((By.ID, "password")))

        # Test valid login
        driver.find_element_by_id("identifier").send_keys(user)
        driver.find_element_by_id("password").send_keys(password)
        driver.find_element_by_id("submit").click()

        # Presence of the 'usermenu' element implies successful login
        self.wait_until(30, EC.presence_of_element_located((By.ID, "usermenu")))

    def test_logout(self):
        driver = self.driver

        driver.get("http://" + Cookies.get_host_ip() + ":9000/conferences")
        self.wait_until(30, EC.visibility_of_element_located((By.XPATH, '//*[@id="usermenu"]')))

        driver.find_element_by_id("usermenu").click()
        driver.find_element_by_partial_link_text("Logout").click()
        assert driver.find_element_by_partial_link_text("Login")
<|endoftext|>"
},
{
"prompt": "class BaseTest:
	fn initdir(self, tmpdir)
	fn ethane(self)
	fn methane(self)
	fn h2o(self)
	fn ch2(self)
	fn ester(self)
	fn ch3(self)
	fn c3(self)
	fn n4(self)
	fn hydrogen(self)
	fn betacristobalite(self)
	fn propyl(self)
	fn hexane(self, propyl)
	fn octane(self)
	fn sixpoints(self)
	fn benzene(self)
	fn rigid_benzene(self)
	fn benzene_from_parts(self)
	fn box_of_benzenes(self, benzene)
	fn rigid_ch(self)
	fn silane(self)
	fn chf(self)
	fn connect_and_reconnect(self, chf)
	fn copper_cell(self)
	fn graphene(self)
	fn cscl_crystal(self)


class DelayImportError(ImportError, SkipTest):


"""Import a module, and issue a nice message to stderr if the module isn't installed.

Parameters
----------
module : str
    The module you'd like to import, as a string

Returns
-------
module : {module, object}
    The module object

Examples
--------
>>> # the following two lines are equivalent. the difference is that the
>>> # second will check for an ImportError and print you a very nice
>>> # user-facing message about what's wrong (where you can install the
>>> # module from, etc) if the import fails
>>> import tables
>>> tables = import_('tables')

Notes
-----
The pybel/openbabel block is meant to resolve compatibility between
openbabel 2.x and 3.0.  There may be other breaking changes but the change
in importing them is the major one we are aware of. For details, see
https://open-babel.readthedocs.io/en/latest/UseTheLibrary/migration.html#python-module"""
fn import_(module: str) -> {module, object}

"""Get the full path to one of the reference files shipped for utils.

In the source distribution, these files are in ``mbuild/utils/reference``,
but on installation, they're moved to somewhere in the user's python
site-packages directory.

Parameters
----------
name : str
    Name of the file to load (with respect to the reference/ folder)."""
fn get_fn(name: str)

fn run_from_ipython()

"""Convert mb.Compound to Protobuf Message file

Parameters
---------
cmpd : mb.Compound
filename : str
binary: bool, default True 
    If True, will print a binary file
    If False, will print to a text file
    Todo: This could be more elegantly detected

Notes
----
Todo: Handle Ports in the protocol buffer (.proto) and in this writer/reader"""
fn write_pb2(cmpd, filename, binary)

"""Convert a Protobuf Message file into mb.Compound

Parameters
---------
filename : str
binary: bool, default True 
    If True, will print a binary file
    If False, will print to a text file
    Todo: This could be more elegantly detected

Returns
------
root_compound : mb.Compound"""
fn read_pb2(filename, binary)

"""Given mb.Compound, parse propertes into compound_pb2.Compound"""
fn _mb_to_proto(cmpd, proto)

"""Parse the mb.Compound bonds, add to the proto bonds

Parameters
---------
cmpd : mb.Compound
proto : compound_pb2.Compound"""
fn _add_proto_bonds(cmpd, proto)

"""Recurisve method to look for a compound_pb2's children

Parameters
---------
proto : compound_pb2

Notes
-----
Base Case: there are no children to the proto, just return 
Recursion: First look at proto's children and return these children (sub_proto)
    Then make the recursive call to look at all the sub_proto's successors
This is similar to mb.Compound().successors()
Unlike mb.Compound(), we need to also keep track of parents in this recursion"""
fn _proto_successors(proto)

"""Given compound_pb2.Compound, create mb.Compound 

Parameters
----------
proto: compound_pb2.Compound()"""
fn _proto_to_mb(proto: compound_pb2.Compound())

"""Parse the compound_pb2.Compound bonds, add to mb.Compound

Parameters
---------
proto : compound_pb2.Compound
cmpd : mb.Compound
proto_to_cmpd : dict
    keys : compound_pb2.Compound.id
    value : mb.Compound"""
fn _add_mb_bonds(proto, cmpd, proto_to_cmpd)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from mbuild.tests.base_test import BaseTest
from mbuild.formats.protobuf import write_pb2, read_pb2
from mbuild.utils.io import has_protobuf

class TestPB2(BaseTest):

    @pytest.mark.skipif(not has_protobuf, reason="Protobuf package not installed")
    def test_loop(self, ethane):
        write_pb2(ethane, 'ethane.pb2')
        proto = read_pb2('ethane.pb2')
        assert ethane.n_particles == proto.n_particles
        assert ethane.n_bonds == proto.n_bonds
        assert len(ethane.children) == len(proto.children)
<|endoftext|>"
},
{
"prompt": "fn match(command)

fn get_new_command(command)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from thefuck.rules.react_native_command_unrecognized import match, \
    get_new_command
from tests.utils import Command

stderr = 'Command `{}` unrecognized'.format

stdout = '''
Usage: react-native <command>

Commands:
  - start: starts the webserver
  - bundle: builds the javascript bundle for offline use
  - unbundle: builds javascript as "unbundle" for offline use
  - new-library: generates a native library bridge
  - android: generates an Android project for your app
  - run-android: builds your app and starts it on a connected Android emulator or device
  - log-android: print Android logs
  - run-ios: builds your app and starts it on iOS simulator
  - log-ios: print iOS logs
  - upgrade: upgrade your app's template files to the latest version; run this after updating the react-native version in your package.json and running npm install
  - link: link a library
'''


@pytest.mark.parametrize('command', [
    Command('react-native star', stderr=stderr('star')),
    Command('react-native android-logs', stderr=stderr('android-logs'))])
def test_match(command):
    assert match(command)


@pytest.mark.parametrize('command', [
    Command('gradle star', stderr=stderr('star')),
    Command('react-native start')])
def test_not_match(command):
    assert not match(command)


@pytest.mark.parametrize('command, result', [
    (Command('react-native star', stdout, stderr('star')),
     'react-native start'),
    (Command('react-native logsandroid -f', stdout, stderr('logsandroid')),
     'react-native log-android -f')])
def test_get_new_command(command, result):
    assert get_new_command(command)[0] == result
<|endoftext|>"
},
{
"prompt": """"world is going to be a nxn matrix
Now we say who is owning what. conq_countries will store
the number of countries per player
conq_countries[0] will correspond to player0..."""
fn start_game(n_players)

fn global_turn(world, n_players, select_grid, event, text, computer_turn)

fn individual_turn(world, current_player_id)

fn human_turn(text, world, select_grid, event)

"""Check the 4 neighbour countries and combat them
2 conditions
First to ensure that the i or j are not out of the matrix.
they should be smaller than the size of the matrix -1, since python
starts counting on 0"""
fn search(world, current_player_id, i, j)

fn combat(attacker_pos, defender_pos)

fn count_countries(x)

fn rand_num()

fn ranking(world, n_players)

fn human_combat(select_grid, world, text)

fn initialize()

fn calculate_colors(n_players)

fn initialize_draw()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# -*- coding: utf-8 -*-

"""
This is to be run in the command line with
pytest *nameOfFile*

To run all
python -m pytest
or
py.test

if you would like to see details, use -v (verbose)

"""
from .context import sample
import helpers as h
import numpy as np
import global_variables as g
import pytest

#We define the variables
current_player_id = 0            
n_players = 3
g.initialize(n_players)
world = np.array([[0,0,0],
                  [1,2,1],
                  [1,2,2]])


def test_rand_num():
   assert h.rand_num() < 3
        
def test_search():
        assert h.search(world,current_player_id,2,2) == -1
        assert h.search(world,current_player_id,0,2) == (1,2)        <|endoftext|>"
},
{
"prompt": "fn parse_event(event: ChannelEvent) -> ChannelEvent

fn make_message(event: ChannelEvent, topic: Topic, ref: Optional[str], payload: Optional[dict[(str, Any)]]) -> ChannelMessage

fn generate_reference(event: ChannelEvent) -> str

"""Provides the main functionality for running a client with asynchronous handlers

This class should generally be initialised and used as an asynchronous context manager.

Inside the `async with` block you should register for topics using `PHXChannelsClient.register_topic_subscription`
and register event handlers using `PHXChannelsClient.register_event_handler`.

Event handler functions can be `async` or normal functions.
* Async functions are run in the event loop `PHXChannelsClient._loop`
* Normal functions are run using the provided executor pool (`ThreadPoolExecutor` by default)"""
class PHXChannelsClient:
	channel_socket_url: str
	logger: Logger
	_client_start_event: Event
	_event_handler_config: dict[ChannelEvent, EventHandlerConfig]
	_topic_registration_status: dict[Topic, TopicRegistration]
	_loop: AbstractEventLoop
	_executor_pool: Optional[Executor]
	_registration_queue: Queue
	_topic_registration_task: Optional[Task]
	fn __init__(self, channel_socket_url: str, channel_auth_token: Optional[str], event_loop: Optional[AbstractEventLoop])
	fn _parse_message(self, socket_message: Union[(str, bytes)]) -> ChannelMessage
	fn shutdown(self, reason: str, websocket: Optional[client.WebSocketClientProtocol], executor_pool: Optional[Executor], wait_for_completion: bool) -> None
	fn register_event_handler(self, event: ChannelEvent, handlers: list[ChannelHandlerFunction], topic: Optional[Topic]) -> None
	fn register_topic_subscription(self, topic: Topic) -> Event


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from unittest.mock import AsyncMock

from hypothesis import given
import pytest

from phx_events import json_handler
from phx_events.client import PHXChannelsClient
from phx_events.utils import make_message
from tests.strategy_utils import channel_event_strategy


pytestmark = pytest.mark.asyncio


class TestPHXChannelsClientSendMessage:
    @given(channel_event_strategy())
    async def test_correct_message_passed_to_websocket_send(self, event_dict):
        mock_websocket = AsyncMock()
        message = make_message(**event_dict)

        async with PHXChannelsClient('ws://test.websocket/') as client:
            await client._send_message(mock_websocket, message)

        mock_websocket.send.assert_called_with(json_handler.dumps(message))
        mock_websocket.send.assert_awaited()
<|endoftext|>"
},
{
"prompt": "fn get_observable()

fn get_observable_to_21()

fn get_observable()

fn get_observable_to_21()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from async_rx.observable import rx_sum

from ..model import ObserverCounterCollector
from .model import get_observable_to_21


@pytest.mark.curio
async def test_rx_sum():

    seeker = ObserverCounterCollector()
    sub = await rx_sum(observable=get_observable_to_21()).subscribe(seeker)
    await sub()

    assert seeker.on_completed_count == 1
    assert seeker.on_next_count == 1
    assert seeker.on_error_count == 0
    assert seeker.items == [210]
<|endoftext|>"
},
{
"prompt": """"MutableMapping that automatically spills out dask key/value pairs to disk when
the total size of the stored data exceeds the target"""
class SpillBuffer(Buffer):
	spilled_by_key: dict[Hashable, int]
	spilled_total: int
	fn __init__(self, spill_directory: str, target: int)
	"""Key/value pairs stored in RAM. Alias of zict.Buffer.fast.
	For inspection only - do not modify directly!"""
	fn memory(self) -> Mapping[(Hashable, Any)]
	"""Key/value pairs spilled out to disk. Alias of zict.Buffer.slow.
	For inspection only - do not modify directly!"""
	fn disk(self) -> Mapping[(Hashable, Any)]
	fn _weight(key: Hashable, value: Any) -> int
	fn _on_evict(self, key: Hashable, value: Any) -> None
	fn _on_retrieve(self, key: Hashable, value: Any) -> None
	fn __setitem__(self, key: Hashable, value: Any) -> None
	fn __delitem__(self, key: Hashable) -> None


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from dask.sizeof import sizeof

from distributed.spill import SpillBuffer


def test_spillbuffer(tmpdir):
    buf = SpillBuffer(str(tmpdir), target=300)
    # Convenience aliases
    assert buf.memory is buf.fast
    assert buf.disk is buf.slow

    assert not buf.spilled_by_key
    assert buf.spilled_total == 0

    a, b, c, d = "a" * 100, "b" * 100, "c" * 100, "d" * 100
    s = sizeof(a)
    # Test assumption made by this test, mostly for non CPython implementations
    assert 100 < s < 200

    buf["a"] = a
    assert not buf.disk
    assert not buf.spilled_by_key
    assert buf.spilled_total == 0
    assert buf["a"] == a

    buf["b"] = b
    assert not buf.disk
    assert not buf.spilled_by_key
    assert buf.spilled_total == 0

    buf["c"] = c
    assert set(buf.disk) == {"a"}
    assert buf.spilled_by_key == {"a": s}
    assert buf.spilled_total == s

    assert buf["a"] == a
    assert set(buf.disk) == {"b"}
    assert buf.spilled_by_key == {"b": s}
    assert buf.spilled_total == s

    buf["d"] = d
    assert set(buf.disk) == {"b", "c"}
    assert buf.spilled_by_key == {"b": s, "c": s}
    assert buf.spilled_total == s * 2

    # Deleting an in-memory key does not automatically move spilled keys back to memory
    del buf["a"]
    assert set(buf.disk) == {"b", "c"}
    assert buf.spilled_by_key == {"b": s, "c": s}
    assert buf.spilled_total == s * 2
    with pytest.raises(KeyError):
        buf["a"]

    # Deleting a spilled key updates the metadata
    del buf["b"]
    assert set(buf.disk) == {"c"}
    assert buf.spilled_by_key == {"c": s}
    assert buf.spilled_total == s
    with pytest.raises(KeyError):
        buf["b"]

    # Updating a spilled key moves it to the top of the LRU and to memory
    buf["c"] = c * 2
    assert set(buf.disk) == {"d"}
    assert buf.spilled_by_key == {"d": s}
    assert buf.spilled_total == s

    # Single key is larger than target and goes directly into slow
    e = "e" * 500
    slarge = sizeof(e)
    buf["e"] = e
    assert set(buf.disk) == {"d", "e"}
    assert buf.spilled_by_key == {"d": s, "e": slarge}
    assert buf.spilled_total == s + slarge

    # Updating a spilled key with another larger than target updates slow directly
    buf["d"] = "d" * 500
    assert set(buf.disk) == {"d", "e"}
    assert buf.spilled_by_key == {"d": slarge, "e": slarge}
    assert buf.spilled_total == slarge * 2
<|endoftext|>"
},
{
"prompt": """"Check if is ResNet building block."""
fn is_block(modules)

"""Check if is one of the norms."""
fn is_norm(modules)

"""Check if norm layer is in correct train state."""
fn check_norm_state(modules, train_state)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
import torch

from mmdet.models.backbones import ResNeXt
from mmdet.models.backbones.resnext import Bottleneck as BottleneckX
from .utils import is_block


def test_renext_bottleneck():
    with pytest.raises(AssertionError):
        # Style must be in ['pytorch', 'caffe']
        BottleneckX(64, 64, groups=32, base_width=4, style='tensorflow')

    # Test ResNeXt Bottleneck structure
    block = BottleneckX(
        64, 64, groups=32, base_width=4, stride=2, style='pytorch')
    assert block.conv2.stride == (2, 2)
    assert block.conv2.groups == 32
    assert block.conv2.out_channels == 128

    # Test ResNeXt Bottleneck with DCN
    dcn = dict(type='DCN', deform_groups=1, fallback_on_stride=False)
    with pytest.raises(AssertionError):
        # conv_cfg must be None if dcn is not None
        BottleneckX(
            64,
            64,
            groups=32,
            base_width=4,
            dcn=dcn,
            conv_cfg=dict(type='Conv'))
    BottleneckX(64, 64, dcn=dcn)

    # Test ResNeXt Bottleneck forward
    block = BottleneckX(64, 16, groups=32, base_width=4)
    x = torch.randn(1, 64, 56, 56)
    x_out = block(x)
    assert x_out.shape == torch.Size([1, 64, 56, 56])

    # Test ResNeXt Bottleneck forward with plugins
    plugins = [
        dict(
            cfg=dict(
                type='GeneralizedAttention',
                spatial_range=-1,
                num_heads=8,
                attention_type='0010',
                kv_stride=2),
            stages=(False, False, True, True),
            position='after_conv2')
    ]
    block = BottleneckX(64, 16, groups=32, base_width=4, plugins=plugins)
    x = torch.randn(1, 64, 56, 56)
    x_out = block(x)
    assert x_out.shape == torch.Size([1, 64, 56, 56])


def test_resnext_backbone():
    with pytest.raises(KeyError):
        # ResNeXt depth should be in [50, 101, 152]
        ResNeXt(depth=18)

    # Test ResNeXt with group 32, base_width 4
    model = ResNeXt(depth=50, groups=32, base_width=4)
    for m in model.modules():
        if is_block(m):
            assert m.conv2.groups == 32
    model.init_weights()
    model.train()

    imgs = torch.randn(1, 3, 224, 224)
    feat = model(imgs)
    assert len(feat) == 4
    assert feat[0].shape == torch.Size([1, 256, 56, 56])
    assert feat[1].shape == torch.Size([1, 512, 28, 28])
    assert feat[2].shape == torch.Size([1, 1024, 14, 14])
    assert feat[3].shape == torch.Size([1, 2048, 7, 7])


regnet_test_data = [
    ('regnetx_400mf',
     dict(w0=24, wa=24.48, wm=2.54, group_w=16, depth=22,
          bot_mul=1.0), [32, 64, 160, 384]),
    ('regnetx_800mf',
     dict(w0=56, wa=35.73, wm=2.28, group_w=16, depth=16,
          bot_mul=1.0), [64, 128, 288, 672]),
    ('regnetx_1.6gf',
     dict(w0=80, wa=34.01, wm=2.25, group_w=24, depth=18,
          bot_mul=1.0), [72, 168, 408, 912]),
    ('regnetx_3.2gf',
     dict(w0=88, wa=26.31, wm=2.25, group_w=48, depth=25,
          bot_mul=1.0), [96, 192, 432, 1008]),
    ('regnetx_4.0gf',
     dict(w0=96, wa=38.65, wm=2.43, group_w=40, depth=23,
          bot_mul=1.0), [80, 240, 560, 1360]),
    ('regnetx_6.4gf',
     dict(w0=184, wa=60.83, wm=2.07, group_w=56, depth=17,
          bot_mul=1.0), [168, 392, 784, 1624]),
    ('regnetx_8.0gf',
     dict(w0=80, wa=49.56, wm=2.88, group_w=120, depth=23,
          bot_mul=1.0), [80, 240, 720, 1920]),
    ('regnetx_12gf',
     dict(w0=168, wa=73.36, wm=2.37, group_w=112, depth=19,
          bot_mul=1.0), [224, 448, 896, 2240]),
]
<|endoftext|>"
},
{
"prompt": """"Makes a data structure safe for JSON silently discarding invalid
objects from nested structures.  This also converts dates."""
fn to_safe_json(data)

"""Returns a date in iso8601 format."""
fn format_iso8601(d)

"""Parse an iso8601 date into a datetime object.  The timezone is
normalized to UTC."""
fn parse_iso8601(value)

fn get_application_name()

"""A property that is lazily calculated and then cached."""
class cached_property(object):
	fn __init__(self, func, name, doc)
	fn __get__(self, obj, type)


fn get_iterator_next_method(it)

fn is_unicode(x)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# -*- coding: utf-8 -*-
"""
    test utils for logbook
    ~~~~~~~~~~~~~~~~~~~~~~

    :copyright: (c) 2010 by Armin Ronacher, Georg Brandl.
    :license: BSD, see LICENSE for more details.
"""
import functools
import os
import sys
from contextlib import contextmanager

import logbook
from logbook.helpers import StringIO

import pytest

_missing = object()

LETTERS = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"


def get_total_delta_seconds(delta):
    """
    Replacement for datetime.timedelta.total_seconds() for Python 2.5, 2.6
    and 3.1
    """
    return (delta.microseconds + (delta.seconds + delta.days * 24 * 3600) * 10**6) / 10**6


require_py3 = pytest.mark.skipif(
    sys.version_info[0] < 3, reason="Requires Python 3")

appveyor = pytest.mark.skipif(
    os.environ.get('APPVEYOR') != 'True', reason='AppVeyor CI test')

travis = pytest.mark.skipif(
    os.environ.get('TRAVIS') != 'true', reason='Travis CI test')


def require_module(module_name):
    found = True
    try:
        __import__(module_name)
    except ImportError:
        found = False

    return pytest.mark.skipif(
        not found, reason='Module {0} is required'.format(module_name))


def make_fake_mail_handler(**kwargs):
    class FakeMailHandler(logbook.MailHandler):
        mails = []

        def get_connection(self):
            return self

        def close_connection(self, con):
            pass

        def sendmail(self, fromaddr, recipients, mail):
            self.mails.append((fromaddr, recipients, mail))

    kwargs.setdefault('level', logbook.ERROR)
    return FakeMailHandler('foo@example.com', ['bar@example.com'], **kwargs)


def missing(name):
    def decorate(f):
        @functools.wraps(f)
        def wrapper(*args, **kwargs):
            old = sys.modules.get(name, _missing)
            sys.modules[name] = None
            try:
                f(*args, **kwargs)
            finally:
                if old is _missing:
                    del sys.modules[name]
                else:
                    sys.modules[name] = old
        return wrapper
    return decorate


def activate_via_with_statement(handler):
    return handler


@contextmanager
def activate_via_push_pop(handler):
    handler.push_thread()
    try:
        yield handler
    finally:
        handler.pop_thread()


@contextmanager
def capturing_stderr_context():
    original = sys.stderr
    sys.stderr = StringIO()
    try:
        yield sys.stderr
    finally:
        sys.stderr = original
<|endoftext|>"
},
{
"prompt": """"Return a user object defined by a user_id."""
fn create_user(user_id: str) -> Dict[(str, Any)]

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """"Unittests for file service api."""
import os
import shutil
from datetime import datetime
from typing import Tuple

import pytest
from nameko.testing.services import worker_factory

from files.service import FilesService
from .utils import create_user

file_service = worker_factory(FilesService)


@pytest.mark.parametrize(
    'ref_path',
    ['final.txt', 'folder1/folder2/final.txt']
)
def test_upload_path(user_id_folder: Tuple[str, str], tmp_folder: str, upload_file: str, ref_path: str) -> None:
    """Upload file to a given path.

    Test a simple and a more complex path - file inside several folders.
    """
    user_folder, user_id = user_id_folder
    user = create_user(user_id)
    tmp_path = os.path.join(tmp_folder, 'upload.txt')
    shutil.copyfile(upload_file, tmp_path)
    assert os.path.isfile(tmp_path)

    result = file_service.upload(user=user, tmp_path=tmp_path, path=ref_path)
    assert datetime.strptime(result['data'].pop('modified'), '%Y-%m-%dT%H:%M:%SZ')
    assert result == {
        'code': 200,
        'data': {
            'path': f'{ref_path}',
            'size': 15
        },
        'status': 'success'}
    assert os.path.isfile(os.path.join(user_folder, 'files', ref_path))


@pytest.mark.parametrize(
    'folders_only',
    ['', 'folder1/folder2'])
def test_delete(user_id_folder: Tuple[str, str], upload_file: str, folders_only: str) -> None:
    """Test upload and delete of file in the main or inside another folder."""
    user_folder, user_id = user_id_folder
    user = create_user(user_id)
    folders_path = os.path.join(user_folder, 'files', folders_only)
    filepath = os.path.join(folders_path, 'delete.txt')
    if folders_only:
        os.makedirs(folders_path)
    shutil.copyfile(upload_file, filepath)
    assert os.path.isfile(filepath)

    result = file_service.delete(user=user, path=os.path.join(folders_only, 'delete.txt'))
    assert result == {'status': 'success', 'code': 204}
    assert not os.path.isfile(filepath)


def test_get_all(user_id_folder: Tuple[str, str], upload_file: str) -> None:
    """Upload and then retrieve several files and check the sorted response."""
    user_folder, user_id = user_id_folder
    user = create_user(user_id)

    shutil.copyfile(upload_file, os.path.join(user_folder, 'files', '1.txt'))
    folders2 = os.path.join(user_folder, 'files', 'folder1', 'folder2')
    os.makedirs(folders2)
    shutil.copyfile(upload_file, os.path.join(folders2, '2.txt'))
    folders3 = os.path.join(user_folder, 'files', 'folder3')
    os.makedirs(folders3)
    shutil.copyfile(upload_file, os.path.join(folders3, '3.txt'))
    shutil.copyfile(upload_file, os.path.join(folders3, '4.txt'))

    result = file_service.get_all(user=user)
    for i in range(len(result['data']['files'])):
        datetime.strptime(result['data']['files'][i].pop('modified'), '%Y-%m-%dT%H:%M:%SZ')
    assert result == {
        'status': 'success',
        'code': 200,
        'data': {
            'files': [
                {'path': '1.txt', 'size': 15},
                {'path': 'folder1/folder2/2.txt', 'size': 15},
                {'path': 'folder3/3.txt', 'size': 15},
                {'path': 'folder3/4.txt', 'size': 15},
            ],
            'links': []}}


def test_download(user_id_folder: Tuple[str, str], upload_file: str) -> None:
    """Test upload and download of file."""
    user_folder, user_id = user_id_folder
    user = create_user(user_id)

    filepath = os.path.join(user_folder, 'files', 'download.txt')
    shutil.copyfile(upload_file, filepath)
    assert os.path.isfile(filepath)

    result = file_service.download(user=user, path='download.txt')
    assert result == {
        'status': 'success',
        'code': 200,
        'headers': {'Content-Type': 'application/octet-stream'},
        'file': filepath
    }
    assert os.path.isfile(filepath)
<|endoftext|>"
},
{
"prompt": """"Mixin that calls a callback after each call to the `__delitem__()` function."""
class DelMixin:
	fn __init__(self, delete_callback: Callable[([str], Any)])
	fn __delitem__(self, key)


"""Behaves like a subclass of `cachetools.Cache`, but keeps a persistent copy
of the cache on disk.

The persistent copy is lazily instantiated at the first access to an attribute
of the underlying cache.

The persistent copy is updated after every write (add or delete item).

If items in the cache are modified without re-adding them to the dict, the
persistent cache will not be updated.

Persistency can be deactivated by providing `None` as the filename.

Internally, the `shelve` library is used to implement the cache.

Parameters
----------
wrapped_cache_cls: subclass of `Cache`
    the class of the cache that this PersistentCache should mimic.
filename: str or None
    filename for the persistent cache. A file extension may be appended. See
    `shelve.open()` for more information. If `None`, persistency is deactivated.
*args:
    forwarded to the init function of `wrapped_cache_cls`
*kwargs:
    forwarded to the init function of `wrapped_cache_cls`"""
class PersistentCache(MutableMapping):
	fn __init__(self, wrapped_cache_cls: Type[Cache], filename: str)
	"""Called when an item is deleted from the wrapped cache"""
	fn delete_callback(self, key)
	fn hash_key(key)
	fn __setitem__(self, key, value)
	fn setdefault(self, k, v)
	fn __getitem__(self, item)
	fn __getattr__(self, item)
	fn __delitem__(self, v) -> None
	fn __contains__(self, item)
	fn initialize_if_not_initialized(self)
	fn close(self)
	"""Try to tidy up.
	
	This is just for show since we sync the dict after every change anyway."""
	fn __del__(self)
	fn __len__(self) -> int
	fn __iter__(self) -> Iterator


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import os
import tempfile

import cachetools
import pytest
from cachetools import LRUCache

from shelved_cache.persistent_cache import PersistentCache


@pytest.fixture
def tmpfile():
    with tempfile.NamedTemporaryFile() as f:
        yield f.name


@pytest.fixture
def tmpdir():
    with tempfile.TemporaryDirectory() as d:
        yield d


def test_getsetitem(tmpdir):
    filename = os.path.join(tmpdir, "cache")
    pc = PersistentCache(LRUCache, filename=filename, maxsize=3)
    pc["a"] = 42
    assert pc["a"] == 42


def test_setdefault(tmpdir):
    filename = os.path.join(tmpdir, "cache")
    pc = PersistentCache(LRUCache, filename=filename, maxsize=3)

    # insert
    res = pc.setdefault("a", 42)
    assert res == 42
    assert pc.persistent_dict[str(hash("a"))] == ("a", 42)

    # retrieve
    res = pc.setdefault("a", 42)
    assert res == 42
    assert pc.persistent_dict[str(hash("a"))] == ("a", 42)


def test_eviction(tmpdir):
    filename = os.path.join(tmpdir, "cache")
    pc = PersistentCache(LRUCache, filename=filename, maxsize=2)
    pc["a"] = 42
    pc["b"] = 43
    pc["c"] = 44
    # "a" should be evicted now
    assert "a" not in pc
    assert PersistentCache.hash_key("a") not in pc.persistent_dict
    assert "b" in pc
    assert PersistentCache.hash_key("b") in pc.persistent_dict


def test_non_string_keys(tmpdir):
    filename = os.path.join(tmpdir, "cache")
    pc = PersistentCache(LRUCache, filename=filename, maxsize=3)
    pc[23] = 42
    assert pc[23] == 42


def test_persistency(tmpdir):
    filename = os.path.join(tmpdir, "cache")
    pc = PersistentCache(LRUCache, filename=filename, maxsize=2)
    pc["a"] = 42
    pc["b"] = 43
    pc["c"] = 44
    # "a" should be evicted now
    assert "a" not in pc
    assert "b" in pc
    pc.close()

    pc2 = PersistentCache(LRUCache, filename=filename, maxsize=2)
    assert "a" not in pc2
    assert pc2["b"] == 43
    assert pc2["c"] == 44


def test_no_persistency():
    filename = None
    pc = PersistentCache(LRUCache, filename=filename, maxsize=2)
    pc["a"] = 42
    pc["b"] = 43
    pc["c"] = 44
    # "a" should be evicted now
    assert "a" not in pc
    assert "b" in pc
    pc.close()

    pc2 = PersistentCache(LRUCache, filename=filename, maxsize=2)
    assert "a" not in pc2
    assert "b" not in pc2


def test_non_str_key_persistency(tmpdir):
    filename = os.path.join(tmpdir, "cache")
    pc = PersistentCache(LRUCache, filename=filename, maxsize=2)
    pc[23] = 42
    pc[24] = 43
    pc.close()

    pc2 = PersistentCache(LRUCache, filename=filename, maxsize=2)
    assert pc2[23] == 42
    assert pc2[24] == 43


def test_decorator(tmpdir):
    filename = os.path.join(tmpdir, "cache")
    pc = PersistentCache(LRUCache, filename, maxsize=2)

    @cachetools.cached(pc)
    def square(x):
        print("called")
        return x * x

    assert square(3) == 9
    # outputs "called"
    assert square(3) == 9
    # no output because the cache is used
<|endoftext|>"
},
{
"prompt": """"shortcut to instantiate a version plugin as well as a dummy repository"""
fn instantiate_version(tmpdir, ctlr)

"""shortcut to instantiate a version plugin as well as a dummy repository"""
fn instantiate_semver2(tmpdir, ctlr)

fn instantiate_test_plugin(typ, name, _ctl)

"""In order to test the versioning plugin we need a dummy
repository plugin - so we can test that actions are properly
propagated to a repository managed by the version plugin.

This plugin serves that purpose"""
class DummyRepositoryPlugin(RepositoryPlugin):
	fn init(self)
	fn uuid(self)
	fn is_cloned(self)
	fn is_clean(self)
	fn branch(self)
	fn commit(self)
	fn clone(self)
	fn pull(self)
	fn push(self)
	fn tag(self, version)
	fn checkout(self, branch)
	fn merge(self, a, b)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import os
import shutil

import pytest
from util import instantiate_test_plugin

import ctl
from ctl.plugins.changelog import ChangelogVersionMissing


def instantiate(tmpdir, ctlr=None, **kwargs):
    dirpath = f"{tmpdir}"
    md_file = os.path.join(dirpath, "CHANGELOG.md")
    data_file = os.path.join(dirpath, "CHANGELOG.yml")
    config = {
        "config": {
            "md_file": md_file,
            "data_file": data_file,
        }
    }
    config["config"].update(kwargs)
    plugin = instantiate_test_plugin("changelog", "test_changelog", _ctl=ctlr, **config)
    return plugin


def test_init():
    ctl.plugin.get_plugin_class("changelog")


def test_generate_clean(tmpdir, ctlr, data_changelog_generate_clean):
    plugin = instantiate(tmpdir, ctlr)
    data_file = plugin.get_config("data_file")
    plugin.generate_clean(data_file)

    assert plugin.load(data_file) == data_changelog_generate_clean.expected

    with pytest.raises(ValueError):
        plugin.generate_clean(data_file)


def test_generate(tmpdir, ctlr, data_changelog_generate):
    data_file = os.path.join(data_changelog_generate.path, "CHANGELOG.yml")
    plugin = instantiate(tmpdir, ctlr, data_file=data_file)
    md_file = plugin.get_config("md_file")
    plugin.generate(md_file, data_file)

    with open(md_file) as fh:
        content = fh.read()
        assert content.strip() == data_changelog_generate.md.strip()


def test_generate_datafile(tmpdir, ctlr, data_changelog_generate_datafile):
    md_file = os.path.join(data_changelog_generate_datafile.path, "CHANGELOG.md")
    plugin = instantiate(tmpdir, ctlr, md_file=md_file)
    data_file = plugin.get_config("data_file")
    plugin.generate_datafile(md_file, data_file)

    with open(data_file) as fh:
        content = fh.read()
        assert content.strip() == data_changelog_generate_datafile.yml.strip()


def test_release(tmpdir, ctlr, data_changelog_release):

    md_file_src = os.path.join(data_changelog_release.path, "CHANGELOG.md")

    plugin = instantiate(tmpdir, ctlr)
    md_file = plugin.get_config("md_file")
    data_file = plugin.get_config("data_file")
    shutil.copyfile(md_file_src, md_file)
    plugin.generate_datafile(md_file, data_file)
    plugin.release("1.1.0", data_file)

    with open(data_file) as fh:
        content = fh.read()
        print(content)
        assert content.strip() == data_changelog_release.yml.strip()


def test_validate(tmpdir, ctlr, data_changelog_generate):

    data_file = os.path.join(data_changelog_generate.path, "CHANGELOG.yml")
    plugin = instantiate(tmpdir, ctlr, data_file=data_file)

    plugin.validate(data_file, "1.0.0")

    with pytest.raises(ChangelogVersionMissing):
        plugin.validate(data_file, "1.1.0")
<|endoftext|>"
},
{
"prompt": "

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import sys
import subprocess
from pathlib import Path
from typing import List

import pytest
from syrupy import SnapshotAssertion

from prisma.generator import BASE_PACKAGE_DIR
from prisma.generator.utils import remove_suffix
from .utils import ROOTDIR


def get_files_from_templates(directory: Path) -> List[str]:
    """Return a list of all auto-generated python modules"""
    files: List[str] = []

    for template in directory.iterdir():
        if template.is_dir():
            files.extend(get_files_from_templates(template))
        elif template.name.endswith('.py.jinja') and not template.name.startswith('_'):
            if directory.name == 'templates':
                name = template.name
            else:
                name = str(template.relative_to(template.parent.parent))

            files.append(remove_suffix(name, '.jinja'))

    return files


SYNC_ROOTDIR = ROOTDIR / '__prisma_sync_output__' / 'prisma'
ASYNC_ROOTDIR = ROOTDIR / '__prisma_async_output__' / 'prisma'
FILES = get_files_from_templates(BASE_PACKAGE_DIR / 'generator' / 'templates')


@pytest.mark.parametrize('file', FILES)
def test_sync(snapshot: SnapshotAssertion, file: str) -> None:
    """Ensure synchronous client files match"""
    assert SYNC_ROOTDIR.joinpath(file).absolute().read_text() == snapshot


@pytest.mark.parametrize('file', FILES)
def test_async(snapshot: SnapshotAssertion, file: str) -> None:
    """Ensure asynchronous client files match"""
    assert ASYNC_ROOTDIR.joinpath(file).absolute().read_text() == snapshot


def test_sync_client_can_be_imported() -> None:
    """Synchronous client can be imported"""
    proc = subprocess.run(
        [sys.executable, '-c', 'import prisma; print(prisma.__file__)'],
        cwd=str(SYNC_ROOTDIR.parent),
        check=True,
        stdout=subprocess.PIPE,
    )
    assert proc.stdout.decode('utf-8').rstrip('\n') == str(SYNC_ROOTDIR / '__init__.py')


def test_async_client_can_be_imported() -> None:
    """Asynchronous client can be imported"""
    proc = subprocess.run(
        [sys.executable, '-c', 'import prisma; print(prisma.__file__)'],
        cwd=str(ASYNC_ROOTDIR.parent),
        check=True,
        stdout=subprocess.PIPE,
    )
    assert proc.stdout.decode('utf-8').rstrip('\n') == str(
        ASYNC_ROOTDIR / '__init__.py'
    )
<|endoftext|>"
},
{
"prompt": "fn send_simple_message(recipient)

class MainPage(webapp2.RequestHandler):
	fn get(self)


class SendEmailHandler(webapp2.RequestHandler):
	fn post(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Copyright 2016 Google Inc. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import mock
import pytest
import webtest

import main


@pytest.fixture
def app():
    return webtest.TestApp(main.app)


def test_get(app):
    response = app.get('/')
    if response.status_int != 200:
        raise AssertionError


@mock.patch('python_http_client.client.Client._make_request')
def test_post(make_request_mock, app):
    response = mock.Mock()
    response.getcode.return_value = 200
    response.read.return_value = 'OK'
    response.info.return_value = {}
    make_request_mock.return_value = response

    app.post('/send', {
        'recipient': 'user@example.com'
    })

    if not make_request_mock.called:
        raise AssertionError
    request = make_request_mock.call_args[0][1]
    if 'user@example.com' not in request.data:
        raise AssertionError
<|endoftext|>"
},
{
"prompt": "fn get_test_assets()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from traitlets import TraitError

from ipygany import PolyMesh, Warp

from .utils import get_test_assets


def test_default_input():
    vertices, triangles, data_1d, data_3d = get_test_assets()

    poly = PolyMesh(vertices=vertices, triangle_indices=triangles, data=[data_1d, data_3d])

    warped_mesh = Warp(poly)

    assert warped_mesh.input == (('1d', 'x'), 0, 0)

    poly = PolyMesh(vertices=vertices, triangle_indices=triangles, data=[data_3d])

    warped_mesh = Warp(poly)

    assert warped_mesh.input == '3d'


def test_input():
    vertices, triangles, data_1d, data_3d = get_test_assets()

    poly = PolyMesh(vertices=vertices, triangle_indices=triangles, data=[data_1d, data_3d])

    warped_mesh = Warp(poly)

    with pytest.raises(TraitError):
        warped_mesh.input = (('1d', 'x'), 0)

    warped_mesh.input = ('1d', 0, 0)
    assert warped_mesh.input == (('1d', 'x'), 0, 0)

    warped_mesh.input = ('1d', 0, 32)
    assert warped_mesh.input == (('1d', 'x'), 0, 32)

    warped_mesh.input = (0, 0, '1d')
    assert warped_mesh.input == (0, 0, ('1d', 'x'))

    with pytest.raises(TraitError):
        warped_mesh.input = ('3d', 0, 0)

    warped_mesh = Warp(poly, input=('1d', 0, 0))
    assert warped_mesh.input == (('1d', 'x'), 0, 0)

    warped_mesh = Warp(poly, input=(0, 0, '1d'))
    assert warped_mesh.input == (0, 0, ('1d', 'x'))

    warped_mesh = Warp(warped_mesh, input=(0, '1d', 0))
    assert warped_mesh.input == (0, ('1d', 'x'), 0)
<|endoftext|>"
},
{
"prompt": "class KeyboardButton(ListMainButton):
	fn __init__(self, title: str)


class QuickReplyContentBase:
	fn __init__(self, text: str, caption: str)


class QuickReplyContentText(QuickReplyContentBase):
	fn __init__(self, header: str, text: str, caption: str)
	fn json(self) -> Dict[(str, str)]


class QuickReplyContentImage(QuickReplyContentBase):
	fn __init__(self, url: str, text: str, caption: str)
	fn json(self) -> Dict[(str, str)]


class QuickReplyContentDocument(QuickReplyContentBase):
	fn __init__(self, url: str, filename: str, text: str, caption: str)
	fn json(self) -> Dict[(str, str)]


class QuickReplyContentVideo(QuickReplyContentBase):
	fn __init__(self, url: str, text: str, caption: str)
	fn json(self) -> Dict[(str, str)]


class QuickReply:
	fn __init__(self, callback_data: str, content: Union[(QuickReplyContentText, QuickReplyContentImage, QuickReplyContentDocument, QuickReplyContentVideo)], options: Optional[List[KeyboardButton]])
	fn add(self, element: KeyboardButton) -> QuickReply
	fn dict(self)
	fn json(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from waio.keyboard.reply import QuickReply, QuickReplyContentText, KeyboardButton, QuickReplyContentImage


def test_reply_keyboard_text():
    kb_content = QuickReplyContentText(
        header="this is the header",
        text="this is the body",
        caption="this is the footer"
    )
    kb = QuickReply(callback_data="callback_keyboard_123", content=kb_content)

    kb.add(KeyboardButton(title='keyboard1')).add(KeyboardButton(title='keyboard2'))

    assert kb.dict() == {
        "type": "quick_reply",
        "msgid": "callback_keyboard_123",
        "content": {
            "type": "text",
            "header": "this is the header",
            "text": "this is the body",
            "caption": "this is the footer"
        },
        "options": [
            {
                "type": "text",
                "title": "keyboard1"
            },
            {
                "type": "text",
                "title": "keyboard2"
            }
        ]
    }


def test_reply_keyboard_image():
    kb_content = QuickReplyContentImage(
        text="this is the body",
        caption="this is the footer",
        url="https://www.buildquickbots.com/whatsapp/media/sample/jpg/sample01.jpg"
    )
    kb = QuickReply(callback_data="callback_keyboard_123_img", content=kb_content)

    kb.add(KeyboardButton(title='keyboard1')).add(KeyboardButton(title='keyboard2'))

    assert kb.dict() == {
        "type": "quick_reply",
        "msgid": "callback_keyboard_123_img",
        "content": {
            "type": "image",
            "url": "https://www.buildquickbots.com/whatsapp/media/sample/jpg/sample01.jpg",
            "text": "this is the body",
            "caption": "this is the footer"
        },
        "options": [
            {
                "type": "text",
                "title": "keyboard1"
            },
            {
                "type": "text",
                "title": "keyboard2"
            }
        ]
    }
<|endoftext|>"
},
{
"prompt": "class ParserException(Exception):
	fn __init__(self, message, item)
	fn set_err_pos(self, lineno, col_offset)
	fn __str__(self)


class VariableDeclarationException(ParserException):


class StructureException(ParserException):


class ConstancyViolationException(ParserException):


class NonPayableViolationException(ParserException):


class InvalidLiteralException(ParserException):


class InvalidTypeException(ParserException):


class TypeMismatchException(ParserException):


class FunctionDeclarationException(ParserException):


class EventDeclarationException(ParserException):


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from pytest import raises

from vyper import compiler
from vyper.exceptions import FunctionDeclarationException

fail_list = [
    """
@public
def ő1qwerty(i: int128) -> int128:
    temp_var : int128 = i
    return temp_var
    """,
"""
@public
def int128(i: int128) -> int128:
    temp_var : int128 = i
    return temp_var
    """,
"""
@public
def decimal(i: int128) -> int128:
    temp_var : int128 = i
    return temp_var
    """,
    """
@public
def wei(i: int128) -> int128:
    temp_var : int128 = i
    return temp_var
    """,
    """
@public
def false(i: int128) -> int128:
    temp_var : int128 = i
    return temp_var
    """,
]


@pytest.mark.parametrize('bad_code', fail_list)
def test_varname_validity_fail(bad_code):
    with raises(FunctionDeclarationException):
        compiler.compile_code(bad_code)


valid_list = [
    """
@public
def func(i: int128) -> int128:
    variable : int128 = i
    return variable
    """,
    """
@public
def func_to_do_math(i: int128) -> int128:
    var_123 : int128 = i
    return var_123
    """,
    """
@public
def first1(i: int128) -> int128:
    _var123 : int128 = i
    return _var123
    """,
]


@pytest.mark.parametrize('good_code', valid_list)
def test_varname_validity_success(good_code):
    assert compiler.compile_code(good_code) is not None
<|endoftext|>"
},
{
"prompt": "fn utcnow()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from datetime import datetime

from dateutil.tz import UTC
from google.protobuf.wrappers_pb2 import StringValue
import pytest

from protobuf_serialization.tests.compiled.example_pb2 import Foo, Parent, Child, Coin, Category
from protobuf_serialization.utils import write_datetime_to_message
from protobuf_serialization.deserialization import protobuf_to_dict


@pytest.fixture
def foo():
    foo = Foo(
        id=123,
        name='val',
    )
    foo.content.value = 'crypto'
    foo.is_active.value = True
    foo.count.value = 501
    dt = datetime(2019, 1, 1, 12, 0, tzinfo=UTC)
    write_datetime_to_message(foo.created_at, dt)
    return foo


def test_protobuf_to_dict_basic():
    foo = Foo(
        id=123,
        name='val',
    )
    foo.content.value = 'crypto'
    foo.is_active.value = True
    foo.count.value = 501

    dt = datetime(2019, 1, 1, 12, 0, tzinfo=UTC)
    write_datetime_to_message(foo.created_at, dt)

    data = protobuf_to_dict(foo)

    assert data == dict(
        id=123,
        name='val',
        content='crypto',
        is_active=True,
        count=501,
        created_at=dt,
    )


def test_fields(foo):
    data = protobuf_to_dict(foo, fields=('id', 'content'))
    assert set(data.keys()) == {'id', 'content'}


def test_unset_wrapped_fields():
    # Test that None is returned for Wrapped fields that are unset
    foo = Foo(
        id=123,
        name='val',
    )
    data = protobuf_to_dict(foo)
    assert data == dict(
        id=123,
        name='val',
        content=None,
        is_active=None,
        count=None,
        created_at=None,
    )


def test_omit_keys_if_null():
    # Test omit_keys_if_null option
    foo = Foo(
        id=123,
        name='val',
    )
    data = protobuf_to_dict(foo, omit_keys_if_null=True)
    assert data == dict(
        id=123,
        name='val'
    )

    foo.content.value = 'stuff'
    data = protobuf_to_dict(foo, omit_keys_if_null=True)
    assert data == dict(
        id=123,
        name='val',
        content='stuff'
    )


def test_nested_deserialization():
    child = Child(name='child', label=StringValue(value='small'))
    parent = Parent(name='parent', label=StringValue(value='big'), child=child)

    data = protobuf_to_dict(parent)
    assert data == dict(
        name='parent',
        label='big',
        child=dict(
            name='child',
            label='small',
            category=None,
        )
    )


def test_advanced_fields():
    coin = Coin()
    data = protobuf_to_dict(coin)

    assert data == dict(
        tags={},
        names=[],
        categories=[],
    )

    coin.tags['USD'] = 'TUSD'
    coin.tags['HKD'] = 'THKD'
    coin.names.append('alpha')
    coin.names.append('beta')
    cat1 = Category(name='foo', type='cool')
    cat2 = Category(name='bar', type='lame')
    coin.categories.extend([cat1, cat2])

    data = protobuf_to_dict(coin)

    assert data == dict(
        tags=dict(USD='TUSD', HKD='THKD'),
        names=['alpha', 'beta'],
        categories=[
            dict(name='foo', type='cool'),
            dict(name='bar', type='lame'),
        ]
    )
<|endoftext|>"
},
{
"prompt": """"    """
fn verify_all(module, ALL)

class _TestHasProps(HasProps):
	x = Int(12)
	y = String('hello')
	z = List(Int, [1, 2, 3])
	zz = Dict(String, Int)
	s = String(None)


class _TestModel(HasProps):
	x = Int(12)
	y = String('hello')
	z = List(Int, [1, 2, 3])
	zz = Dict(String, Int)
	s = String(None)


class _TestModel2(HasProps):
	x = Int(12)
	y = String('hello')
	z = List(Int, [1, 2, 3])
	zz = Dict(String, Int)
	s = String(None)


"""Accepts only the string "auto".

Useful for properties that can be configured to behave "automatically".

Example:

    This property is often most useful in conjunction with the
    :class:`~bokeh.core.properties.Either` property.

    .. code-block:: python

        >>> class AutoModel(HasProps):
        ...     prop = Either(Float, Auto)
        ...

        >>> m = AutoModel()

        >>> m.prop = 10.2

        >>> m.prop = "auto"

        >>> m.prop = "foo"      # ValueError !!

        >>> m.prop = [1, 2, 3]  # ValueError !!"""
class Auto(Enum):
	fn __init__(self)
	fn __str__(self)
	fn _sphinx_type(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "#-----------------------------------------------------------------------------
# Copyright (c) 2012 - 2020, Anaconda, Inc., and Bokeh Contributors.
# All rights reserved.
#
# The full license is in the file LICENSE.txt, distributed with this software.
#-----------------------------------------------------------------------------

#-----------------------------------------------------------------------------
# Boilerplate
#-----------------------------------------------------------------------------
import pytest ; pytest

#-----------------------------------------------------------------------------
# Imports
#-----------------------------------------------------------------------------

# Bokeh imports
from _util_property import _TestHasProps, _TestModel
from bokeh._testing.util.api import verify_all

# Module under test
import bokeh.core.property.auto as bcpa # isort:skip

#-----------------------------------------------------------------------------
# Setup
#-----------------------------------------------------------------------------

ALL = (
    'Auto',
)

#-----------------------------------------------------------------------------
# General API
#-----------------------------------------------------------------------------

class Test_Auto(object):

    def test_valid(self) -> None:
        prop = bcpa.Auto()
        assert prop.is_valid(None)
        assert prop.is_valid("auto")

    def test_invalid(self) -> None:
        prop = bcpa.Auto()
        assert not prop.is_valid(False)
        assert not prop.is_valid(True)
        assert not prop.is_valid(0)
        assert not prop.is_valid(1)
        assert not prop.is_valid(0.0)
        assert not prop.is_valid(1.0)
        assert not prop.is_valid(1.0+1.0j)
        assert not prop.is_valid("")
        assert not prop.is_valid(())
        assert not prop.is_valid([])
        assert not prop.is_valid({})
        assert not prop.is_valid(_TestHasProps())
        assert not prop.is_valid(_TestModel())

    def test_has_ref(self) -> None:
        prop = bcpa.Auto()
        assert not prop.has_ref

    def test_str(self) -> None:
        prop = bcpa.Auto()
        assert str(prop) == "Auto"

#-----------------------------------------------------------------------------
# Dev API
#-----------------------------------------------------------------------------

#-----------------------------------------------------------------------------
# Private API
#-----------------------------------------------------------------------------

#-----------------------------------------------------------------------------
# Code
#-----------------------------------------------------------------------------

Test___all__ = verify_all(bcpa, ALL)
<|endoftext|>"
},
{
"prompt": "class CalDAVStorage(RustStorage):
	storage_name = 'caldav'
	fileext = '.ics'
	_repr_attributes = ('username', 'url')
	start_date = None
	end_date = None
	fn __init__(self, url, username, password, useragent, verify_cert, auth_cert, auth_cert_password, start_date, end_date, item_types)


class CardDAVStorage(RustStorage):
	storage_name = 'carddav'
	fileext = '.vcf'
	_repr_attributes = ('username', 'url')
	fn __init__(self, url, username, password, useragent, verify_cert, auth_cert, auth_cert_password)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# -*- coding: utf-8 -*-

import datetime
from textwrap import dedent

import pytest

from tests import EVENT_TEMPLATE, TASK_TEMPLATE, VCARD_TEMPLATE

from vdirsyncer.storage.dav import CalDAVStorage

from . import DAVStorageTests, dav_server
from .. import format_item


class TestCalDAVStorage(DAVStorageTests):
    storage_class = CalDAVStorage

    @pytest.fixture(params=['VTODO', 'VEVENT'])
    def item_type(self, request):
        return request.param

    def test_doesnt_accept_vcard(self, item_type, get_storage_args):
        args = get_storage_args()
        args['item_types'] = (item_type,)
        s = self.storage_class(**args)

        try:
            s.upload(format_item(item_template=VCARD_TEMPLATE))
        except Exception:
            pass
        assert not list(s.list())

    @pytest.mark.xfail(dav_server == 'radicale',
                       reason='Radicale doesn\'t support timeranges.')
    def test_timerange_correctness(self, get_storage_args):
        start_date = datetime.datetime(2013, 9, 10)
        end_date = datetime.datetime(2013, 9, 13)
        args = get_storage_args()
        args['start_date'] = start_date
        args['end_date'] = end_date
        s = self.storage_class(**args)

        too_old_item = format_item(item_template=dedent(u'''
            BEGIN:VCALENDAR
            VERSION:2.0
            PRODID:-//hacksw/handcal//NONSGML v1.0//EN
            BEGIN:VEVENT
            DTSTART:19970714T170000Z
            DTEND:19970715T035959Z
            SUMMARY:Bastille Day Party
            X-SOMETHING:{r}
            UID:{r}
            END:VEVENT
            END:VCALENDAR
            ''').strip())

        too_new_item = format_item(item_template=dedent(u'''
            BEGIN:VCALENDAR
            VERSION:2.0
            PRODID:-//hacksw/handcal//NONSGML v1.0//EN
            BEGIN:VEVENT
            DTSTART:20150714T170000Z
            DTEND:20150715T035959Z
            SUMMARY:Another Bastille Day Party
            X-SOMETHING:{r}
            UID:{r}
            END:VEVENT
            END:VCALENDAR
            ''').strip())

        good_item = format_item(item_template=dedent(u'''
            BEGIN:VCALENDAR
            VERSION:2.0
            PRODID:-//hacksw/handcal//NONSGML v1.0//EN
            BEGIN:VEVENT
            DTSTART:20130911T170000Z
            DTEND:20130912T035959Z
            SUMMARY:What's with all these Bastille Day Partys
            X-SOMETHING:{r}
            UID:{r}
            END:VEVENT
            END:VCALENDAR
            ''').strip())

        s.upload(too_old_item)
        s.upload(too_new_item)
        expected_href, _ = s.upload(good_item)

        (actual_href, _), = s.list()
        assert actual_href == expected_href

    @pytest.mark.skipif(dav_server == 'icloud',
                        reason='iCloud only accepts VEVENT')
    def test_item_types_general(self, get_storage_args):
        args = get_storage_args()
        s = self.storage_class(**args)
        event = s.upload(format_item(item_template=EVENT_TEMPLATE))[0]
        task = s.upload(format_item(item_template=TASK_TEMPLATE))[0]

        for item_types, expected_items in [
            (('VTODO', 'VEVENT'), {event, task}),
            (('VTODO',), {task}),
            (('VEVENT',), {event}),
        ]:
            args['item_types'] = item_types
            s = self.storage_class(**args)
            assert set(href for href, etag in s.list()) == expected_items
<|endoftext|>"
},
{
"prompt": "class UserChangeForm(admin_forms.UserChangeForm):


class UserCreationForm(admin_forms.UserCreationForm):


class UserDetailForm(forms.ModelForm):
	fn __init__(self)


class ApplicantUserForm(forms.ModelForm):
	fn __init__(self)
	"""Check if user exists in the database, if not add them and email a welcome message."""
	fn add_user(self, request, email, password)
	"""Check if user exists in the database, if not add them and email a welcome message."""
	fn authenticate_user(self, request, username, password)


"""User leaving information to be contacted by goldengate"""
class AccountHolderForm(forms.Form):
	username = forms.EmailField(max_length=100)
	phone_number = forms.RegexField(regex='^\\+?1?\\d{9,15}$', error_messages={'required': "Phone number must be entered in the format: '+999999999'. Up to 15 digits allowed."}, required=False)
	password = forms.CharField(widget=forms.PasswordInput())
	"""Check if user exists in the database, if not add them and email a welcome message."""
	fn authenticate_user(self, request, username, password)


"""Default user for GoldenGate."""
class ApplicantUser(AbstractBaseUser, PermissionsMixin):
	first_name = CharField(max_length=100)
	last_name = CharField(max_length=100)
	user_email = EmailField(max_length=100, unique=True)
	phone_number = PhoneNumberField(blank=True)
	county = CharField(max_length=100)
	district = CharField(max_length=100)
	division = CharField(max_length=100)
	is_staff = BooleanField(required=True)
	is_active = BooleanField(required=True)
	USERNAME_FIELD = 'user_email'
	REQUIRED_FIELDS = ['first_name', 'last_name']
	objects = ApplicantUserManager()
	fn __str__(self)
	"""Get url for user's detail view.
	Returns:
	    str: URL for user detail."""
	fn get_absolute_url(self) -> str
	fn has_perm(self, perm, obj)
	fn has_module_perms(self, app_label)
	"""Is the user a member of staff?"""
	fn is_staff(self)
	"""Is the user admin?"""
	fn is_admin(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """"
Module for all Form Tests.
"""
import pytest
from django.utils.translation import ugettext_lazy as _

from goldengate.users.forms import UserCreationForm
from goldengate.users.models import User

pytestmark = pytest.mark.django_db


class TestUserCreationForm:
    """
    Test class for all tests related to the UserCreationForm
    """

    def test_username_validation_error_msg(self, user: User):
        """
        Tests UserCreation Form's unique validator functions correctly by testing:
            1) A new user with an existing username cannot be added.
            2) Only 1 error is raised by the UserCreation Form
            3) The desired error message is raised
        """

        # The user already exists,
        # hence cannot be created.
        form = UserCreationForm(
            {
                "username": user.username,
                "password1": user.password,
                "password2": user.password,
            }
        )

        assert not form.is_valid()
        assert len(form.errors) == 1
        assert "username" in form.errors
        assert form.errors["username"][0] == _("This username has already been taken.")
<|endoftext|>"
},
{
"prompt": "

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# ============LICENSE_START=======================================================
#  Copyright (C) 2020 Nordix Foundation.
# ================================================================================
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0
# ============LICENSE_END=========================================================

import os
import socket
import ssl
import tarfile
import tempfile
import time
from io import StringIO
from typing import List

import docker
import pytest
from docker.models.containers import Container
from lxml import etree
from ncclient.transport.ssh import MSG_DELIM

import settings

HELLO_DTD = etree.DTD(StringIO("""
<!ELEMENT hello (capabilities, session-id)>
<!ATTLIST hello xmlns CDATA #REQUIRED>
<!ELEMENT capabilities (capability+)>
<!ELEMENT capability (#PCDATA)>
<!ELEMENT session-id (#PCDATA)>
"""))

INITIAL_CONFIG_DIR = "data/tls_initial"
NEW_CONFIG_DIR = "data/tls_new"


class TestTLS:
    container: Container

    @classmethod
    def setup_class(cls):
        dkr = docker.from_env()
        containers = dkr.containers.list(filters={"ancestor": "netconf-pnp-simulator:latest"})
        assert len(containers) == 1
        cls.container = containers[0]

    def test_tls_connect(self):
        nc_connect(INITIAL_CONFIG_DIR)

    @pytest.mark.parametrize("round_id", [f"round #{i + 1}" for i in range(6)])
    def test_tls_reconfiguration(self, round_id):
        # pylint: disable=W0613
        self.reconfigure_and_check(NEW_CONFIG_DIR, INITIAL_CONFIG_DIR)
        self.reconfigure_and_check(INITIAL_CONFIG_DIR, NEW_CONFIG_DIR)

    def reconfigure_and_check(self, good_config_dir: str, bad_config_dir: str):
        with simple_tar([f"{good_config_dir}/{b}.pem" for b in ["ca", "server_key", "server_cert"]]) as config_tar:
            status = self.container.put_archive(f"/config/tls", config_tar)
            assert status
        test_start = int(time.time())
        exit_code, (_, err) = self.container.exec_run("/opt/bin/reconfigure-tls.sh", demux=True)
        if exit_code != 0:
            print(f"reconfigure-tls.sh failed with rc={exit_code}")
            log_all("stderr", err)
            log_all("Container Logs", self.container.logs(since=test_start))
            assert False
        nc_connect(good_config_dir)
        # Exception matching must be compatible with Py36 and Py37+
        with pytest.raises(ssl.SSLError, match=r".*\[SSL: CERTIFICATE_VERIFY_FAILED\].*"):
            nc_connect(bad_config_dir)


def log_all(heading: str, lines: object):
    print(f"{heading}:")
    if isinstance(lines, bytes):
        lines = lines.decode("utf-8")
    if isinstance(lines, str):
        lines = lines.split("\n")
    for line in lines:
        print(" ", line)


def simple_tar(paths: List[str]):
    file = tempfile.NamedTemporaryFile()
    with tarfile.open(mode="w", fileobj=file) as tar:
        for path in paths:
            abs_path = os.path.abspath(path)
            tar.add(abs_path, arcname=os.path.basename(path), recursive=False)
    file.seek(0)
    return file


def nc_connect(config_dir: str):
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0) as sock:
        context = ssl.create_default_context()
        context.load_verify_locations(f"{config_dir}/ca.pem")
        context.load_cert_chain(certfile=f"{config_dir}/client_cert.pem", keyfile=f"{config_dir}/client_key.pem")
        context.check_hostname = False
        with context.wrap_socket(sock, server_side=False, server_hostname=settings.HOST) as conn:
            conn.connect((settings.HOST, settings.TLS_PORT))
            buf = nc_read_msg(conn)
            print(f"Received NETCONF HelloMessage:\n{buf}")
            conn.close()
            assert buf.endswith(MSG_DELIM)
            hello_root = etree.XML(buf[:-len(MSG_DELIM)])
            valid = HELLO_DTD.validate(hello_root)
            if not valid:
                log_all("Invalid NETCONF <hello> msg", list(HELLO_DTD.error_log.filter_from_errors()))
                assert False


def nc_read_msg(conn: ssl.SSLSocket):
    buf = ''
    while True:
        data = conn.recv(4096)
        if data:
            buf += data.decode(encoding="utf-8")
            if buf.endswith(MSG_DELIM):
                break
        else:
            break
    return buf
<|endoftext|>"
},
{
"prompt": "fn make_fake_event() -> Type[Event]

fn load_plugin(nonebug_init: None) -> Set[Plugin]

"""Make a snapshot for sys.modules before initializing.
Clear nonebot and other modules not in snapshot after test case running completed.
Ensure every test case has a clean nonebot environment.

By default, this fixture will be auto called by `nonebug_init`."""
fn nonebug_clear() -> Generator[(None, None, None)]

"""Initialize nonebot before test case running.
And clear nonebot after test case running completed."""
fn nonebug_init(nonebug_clear: None, request) -> None

"""Get a test app provided by nonebug.
Use app to define test cases and run them."""
fn nonebug_app(nonebug_init: None, monkeypatch: pytest.MonkeyPatch) -> App

"""Call `nonebug_init` and return a new instance of Processor Test App."""
fn processor_app(nonebug_init: None, monkeypatch: pytest.MonkeyPatch) -> ProcessorApp

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from utils import make_fake_event

from nonebug.fixture import *
from nonebug import ProcessorApp


@pytest.mark.asyncio
async def test_dependent(app: "ProcessorApp"):
    from nonebot.adapters import Event
    from nonebot.params import EventParam
    from nonebot.exception import TypeMisMatch

    FakeEvent = make_fake_event(test_field=(str, "test"))
    FakeEvent2 = make_fake_event(test_field2=(str, "test2"))

    def _handle(event: Event):
        ...

    def _handle_fake(event: FakeEvent):
        ...

    def _handle_return():
        return True

    async with app.test_dependent(_handle, allow_types=[EventParam]) as ctx:
        ctx.pass_params(event=FakeEvent())
    async with app.test_dependent(_handle_fake, allow_types=[EventParam]) as ctx:
        ctx.pass_params(event=FakeEvent())

    event = FakeEvent2()
    try:
        async with app.test_dependent(_handle_fake, allow_types=[EventParam]) as ctx:
            ctx.pass_params(event=event)
    except TypeMisMatch as e:
        assert e.param.name == "event"
        assert e.value is event
    else:
        assert False, "handler should be skipped"

    try:
        async with app.test_dependent(_handle_return) as ctx:
            ctx.should_return(False)
    except AssertionError as e:
        pass
    else:
        assert False, "Handler return value should be checked"
<|endoftext|>"
},
{
"prompt": "class Client:
	msg_sender: MessageSender
	disconnector: Disconnector
	name: Optional[str] = None
	fn __hash__(self) -> int


class Response:
	code: HTTPStatus
	msg: str


class Probe:


class ChatToClient:
	sender: str
	message: str
	room: Optional[str] = None


class MessageSender:
	fn __init__(self, serializer: Serializer, time_factory: TimeFactory) -> None
	fn send(self, msg: Message) -> None
	fn _convert(msg: Message) -> JSON


class Disconnector:
	fn __init__(self) -> None
	fn disconnect(self) -> None
	fn should_disconnect(self) -> bool


fn _auth_deco(holder: AuthClientsHolder, func: _ServerMessageHandler) -> _ServerMessageHandler

class AuthClientsHolder:
	fn __init__(self) -> None
	fn add_client(self, client: Client) -> None
	fn remove_client(self, client: Client) -> None
	fn is_authed(self, client: Client) -> bool
	fn find_client(self, name: str) -> Optional[Client]
	fn all(self) -> ValuesView[Client]
	fn required(self) -> Callable[([_ServerMessageHandler], _ServerMessageHandler)]


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from http import HTTPStatus
from unittest.mock import MagicMock

import pytest
from gb_chat.common.disconnector import Disconnector
from gb_chat.io.message_sender import MessageSender
from gb_chat.msg.server_to_client import Response
from gb_chat.server.auth_clients_holder import AuthClientsHolder
from gb_chat.server.client import Client


@pytest.fixture
def client():
    return Client(MagicMock(spec_set=MessageSender), MagicMock(spec_set=Disconnector))


@pytest.fixture
def sut():
    return AuthClientsHolder()


def test_add_client_raises_when_no_name(sut, client):
    with pytest.raises(ValueError):
        sut.add_client(client)


@pytest.fixture
def client_with_name():
    return Client(
        MagicMock(spec_set=MessageSender), MagicMock(spec_set=Disconnector), "username"
    )


def test_add_client_with_name(sut, client_with_name):
    sut.add_client(client_with_name)
    assert sut.is_authed(client_with_name)
    assert sut.find_client(client_with_name.name) is client_with_name
    assert [x for x in sut.all] == [client_with_name]


def test_remove_client_raises_when_no_name(sut, client):
    with pytest.raises(ValueError):
        sut.remove_client(client)


def test_remove_client_raises_when_client_not_in_list(sut, client_with_name):
    with pytest.raises(ValueError):
        sut.remove_client(client_with_name)


def test_remove_client(sut, client_with_name):
    sut.add_client(client_with_name)
    sut.remove_client(client_with_name)
    assert not sut.is_authed(client_with_name)
    assert sut.find_client(client_with_name.name) is None


class Sut:
    auth = AuthClientsHolder()

    def __init__(self, mock) -> None:
        self.mock = mock

    @auth.required
    def method(self, msg, from_client):
        self.mock(msg, from_client)


def test_required_disconnect_client_when_not_authed(client_with_name):
    mock = MagicMock()
    sut = Sut(mock)
    sut.method(MagicMock(), client_with_name)
    mock.assert_not_called()
    client_with_name.msg_sender.send.assert_called_once_with(
        Response(HTTPStatus.UNAUTHORIZED, "Allowed only for authed users")
    )
    client_with_name.disconnector.disconnect.assert_not_called()


def test_required_called_when_authed(client_with_name):
    mock = MagicMock()
    msg = MagicMock()
    sut = Sut(mock)
    sut.auth.add_client(client_with_name)
    sut.method(msg, client_with_name)
    mock.assert_called_once_with(msg, client_with_name)
    client_with_name.msg_sender.send.assert_not_called()
    client_with_name.disconnector.disconnect.assert_not_called()
<|endoftext|>"
},
{
"prompt": "class CustomSignupForm(SignupForm):
	fn __init__(self)
	fn clean_email(self)
	fn save(self, request)


"""Provide a custom form for setting password.

This causes resetting a password to imply the email has been verified."""
class CustomResetPasswordKeyForm(ResetPasswordKeyForm):
	fn save(self)


class AcceptInvitationForm(forms.Form):
	invitation_id = forms.IntegerField(widget=forms.HiddenInput())
	fn __init__(self)
	fn clean_invitation_id(self)


class CreateInvitationForm(forms.ModelForm):
	fn __init__(self)
	fn clean_recipient(self)


class TeamForm(forms.ModelForm):
	initial_invite_1 = forms.EmailField(required=False)
	initial_invite_2 = forms.EmailField(required=False)
	initial_invite_3 = forms.EmailField(required=False)
	fn __init__(self)
	fn clean(self)
	fn get_invites(self)


class CreateSubmissionForm(forms.ModelForm):
	fn __init__(self)
	fn clean_accepted_terms(self)
	fn clean_test_prediction_file(self)
	fn clean(self)


class ApproachForm(forms.ModelForm):
	fn __init__(self)
	fn clean_description(self)
	fn clean(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from datetime import timedelta

from django.core.files.uploadedfile import SimpleUploadedFile
from django.http import HttpRequest
from django.utils import timezone
import pytest

from stade.core.forms import CreateSubmissionForm


@pytest.fixture
def limited_approach(transactional_db, approach_factory):
    a = approach_factory(task__max_submissions_per_week=1, task__locked=False)
    # this object must be saved because we need an id to submit the form
    a.save()

    # add user to team
    a.team.users.add(a.team.creator)

    return a


@pytest.mark.django_db
def test_submission_throttling(limited_approach):
    request = HttpRequest()
    request.user = limited_approach.team.creator

    for _ in range(limited_approach.task.max_submissions_per_week):
        form = CreateSubmissionForm(
            data={'accepted_terms': True},
            files={'test_prediction_file': SimpleUploadedFile('some-file.csv', b'foo')},
            approach_id=limited_approach.id,
            request=request,
        )
        form.instance.approach = limited_approach
        form.instance.creator = limited_approach.team.creator
        assert form.is_valid(), form.errors
        form.save()

    form = CreateSubmissionForm(
        data={'accepted_terms': True},
        files={'test_prediction_file': SimpleUploadedFile('some-file.csv', b'foo')},
        approach_id=limited_approach.id,
        request=request,
    )
    form.instance.approach = limited_approach
    form.instance.creator = limited_approach.team.creator
    assert not form.is_valid(), form.errors


@pytest.mark.django_db
def test_next_available_submission(limited_approach, submission_factory):
    start = timezone.now()
    submission_creation_time = start - timedelta(days=1)

    submission_factory(created=submission_creation_time, approach=limited_approach)

    # The approach should now be rate limited
    assert (
        limited_approach.task.pending_or_succeeded_submissions(limited_approach.team).count()
        == limited_approach.task.max_submissions_per_week
    )

    # The next available submission should be in 6 days
    assert limited_approach.task.next_available_submission(
        limited_approach.team
    ) == submission_creation_time + timedelta(weeks=1)
<|endoftext|>"
},
{
"prompt": "class TestBaseClass:
	TEST_UNDEFINED_PARAMETER = 'this is an undefined parameter to work around pytest limitations'
	fn setup_class(cls)
	fn __pytest_empty_object_fixture(self, _input, default)
	fn _create_tempfile(self, _file, _input)
	fn _create_args(self, input, extraopts)
	fn _create_args_fix(self, input, extraopts)
	fn fix_and_check(self, args, id)
	fn _create_args_parser(self)
	fn check_for_id(self, args, id, occurences)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import os
import sys

import pytest

sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))

from base import TestBaseClass

class TestClassOelintVarRootFSCmd(TestBaseClass):

    @pytest.mark.parametrize('id', ['oelint.var.rootfspostcmd'])
    @pytest.mark.parametrize('occurance', [1])
    @pytest.mark.parametrize('input', 
        [
            {
            'oelint_adv_test.bb':
            '''
            ROOTFS_POSTPROCESS_COMMAND += "abc; adef ;;"
            '''
            },
        ],
    )
    def test_bad(self, input, id, occurance):
        self.check_for_id(self._create_args(input), id, occurance)

    @pytest.mark.parametrize('id', ['oelint.var.rootfspostcmd'])
    @pytest.mark.parametrize('occurance', [0])
    @pytest.mark.parametrize('input', 
        [
            {
            'oelint_adv_test.bb':
            '''
            ROOTFS_POSTPROCESS_COMMAND += "abc; adef;;"
            '''
            },
        ],
    )
    def test_good(self, input, id, occurance):
        self.check_for_id(self._create_args(input), id, occurance)<|endoftext|>"
},
{
"prompt": "

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from tests.fsm import TestStates
from waio import Dispatcher, Bot
from waio.factory.base import ResponseModel
from waio.factory.factory import factory_gupshup
from waio.rules import StateRule
from waio.storage import RedisStorage
from waio.types import Message

bot = Bot(
    apikey='FAKE_API_KEY',
    src_name='FAKE_SRC_NAME',
    phone_number=79289998877
)

storage = RedisStorage(prefix_fsm='fsm', redis_url="redis://localhost:6379")
dp = Dispatcher(bot=bot, storage=storage)


@pytest.mark.asyncio
async def test_check_state_filter_true():
    _state = dp.state(user_phone=79990000000)
    await _state.set_state(TestStates.email)
    message_json = {
        "app": "DemoApp",
        "timestamp": 1580227766370,
        "version": 2,
        "type": "message",
        "payload": {
            "id": "ABEGkYaYVSEEAhAL3SLAWwHKeKrt6s3FKB0c",
            "source": "79289998877",
            "type": "text",
            "payload": {
                "text": "Hi"
            },
            "sender": {
                "phone": "79990000000",
                "name": "Smit",
                "country_code": "7",
                "dial_code": "9990000000"
            }
        }
    }
    data_load = factory_gupshup.load(message_json, ResponseModel)
    message_model = Message(bot=dp.bot, message=data_load, state_func=dp.state)
    rule = StateRule(state=TestStates.email)

    check_filter = await rule.check(message=message_model)
    assert check_filter is True

    await _state.finish(clear_data=True)


@pytest.mark.asyncio
async def test_check_state_filter_false():
    _state = dp.state(user_phone=79990000000)
    await _state.set_state(TestStates.email)

    message_json = {
        "app": "DemoApp",
        "timestamp": 1580227766370,
        "version": 2,
        "type": "message",
        "payload": {
            "id": "ABEGkYaYVSEEAhAL3SLAWwHKeKrt6s3FKB0c",
            "source": "79289998877",
            "type": "text",
            "payload": {
                "text": "Hi"
            },
            "sender": {
                "phone": "79287776655",
                "name": "Anna",
                "country_code": "7",
                "dial_code": "9287776655"
            }
        }
    }
    data_load = factory_gupshup.load(message_json, ResponseModel)
    message_model = Message(bot=dp.bot, message=data_load, state_func=dp.state)
    rule = StateRule(state=TestStates.email)

    check_filter = await rule.check(message=message_model)
    assert check_filter is False

    await _state.finish(clear_data=True)<|endoftext|>"
},
{
"prompt": "class UserFactory(DjangoModelFactory):
	username = Faker('user_name')
	email = Faker('email')
	name = Faker('name')
	fn password(self, create: bool, extracted: Sequence[Any])


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from celery.result import EagerResult

from translations_tool.users.tasks import get_users_count
from translations_tool.users.tests.factories import UserFactory

pytestmark = pytest.mark.django_db


def test_user_count(settings):
    """A basic test to execute the get_users_count Celery task."""
    UserFactory.create_batch(3)
    settings.CELERY_TASK_ALWAYS_EAGER = True
    task_result = get_users_count.delay()
    assert isinstance(task_result, EagerResult)
    assert task_result.result == 3
<|endoftext|>"
},
{
"prompt": """"A helper for easy registering and unregistering event handlers."""
class EventWatcher:
	fn __init__(self, element: Element, element_dispatcher: ElementDispatcher | None, default_handler: Handler | None)
	"""Watch a certain path of elements starting with the DiagramItem. The
	handler is optional and will default the default provided at
	construction time.
	
	Watches should be set in the constructor, so they can be registered
	and unregistered in one shot.
	
	This interface is fluent (returns self)."""
	fn watch(self, path: str, handler: Handler | None) -> EventWatcher
	"""Unregister handlers.
	
	Extra arguments are ignored (makes connecting to destroy signals
	much easier though)."""
	fn unsubscribe_all(self)


"""The Element based Dispatcher allows handlers to receive only events
related to certain elements. Those elements should be registered too. A
path should be provided, that is used to find those changes.

The handlers are registered on their property attribute. This avoids
subclass lookups and is pretty specific. As a result this dispatcher is
tailored for dispatching events from the data model (ElementUpdated)

For example: if you're a TransitionItem (gaphor.core.modeling.Presentation
instance) and you're interested in the value of the guard attribute of the
model element that's represented by this item (gaphor.UML.Transition), you
can register a handler like this::

  dispatcher.subscribe(element,
          'guard.specification[LiteralSpecification].value', self._handler)

Note the '[' and ']'. This is because guard references ValueSpecification,
which does not have a value attribute. Therefore, the default reference type
is overruled in favour of the LiteralSpecification.

This dispatcher keeps track of the kind of events that are dispatched. The
dispatcher table is updated accordingly (so the right handlers are fired
every time)."""
class ElementDispatcher(Service):
	fn __init__(self, event_manager, modeling_language)
	fn shutdown(self) -> None
	fn subscribe(self, handler: Handler, element: Element, path: str) -> None
	"""Unregister a handler from the registry."""
	fn unsubscribe(self, handler: Handler) -> None
	"""Given a start element and a path, return a tuple of properties
	(association, attribute, etc.) representing the path."""
	fn _path_to_properties(self, element, path)
	"""Provided an element and a path of properties (props), register the
	handler for each property."""
	fn _add_handlers(self, element, props, handler)
	"""Remove the handler of the path of elements."""
	fn _remove_handlers(self, element, property, handler)
	fn on_element_change_event(self, event)
	fn on_model_loaded(self, event)


"""Mark a function/method as an event handler for a particular type of
event."""
fn event_handler()

"""The Event Manager."""
class EventManager(Service):
	fn __init__(self) -> None
	fn shutdown(self) -> None
	"""Register a handler.
	
	Handlers are triggered (executed) when specific events are
	emitted through the handle() method."""
	fn subscribe(self, handler: Handler) -> None
	"""Unregister a previously registered handler."""
	fn unsubscribe(self, handler: Handler) -> None
	"""Send event notifications to registered handlers."""
	fn handle(self) -> None


class CoreModelingLanguage(ModelingLanguage):
	fn name(self) -> str
	fn toolbox_definition(self)
	fn diagram_types(self)
	fn lookup_element(self, name)


"""This class can be used to instantly combine modeling languages."""
class MockModelingLanguage(ModelingLanguage):
	fn __init__(self)
	fn name(self) -> str
	fn toolbox_definition(self)
	fn diagram_types(self)
	fn lookup_element(self, name)


class UMLModelingLanguage(ModelingLanguage):
	fn name(self) -> str
	fn toolbox_definition(self) -> ToolboxDefinition
	"""Return an iterator (id, name) for each diagram type."""
	fn diagram_types(self) -> Iterable[DiagramType]
	fn lookup_element(self, name)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """"Basic test case for Gaphor tests.

Everything is about services so the Case can define it's required
services and start off.
"""
from __future__ import annotations

# isort: skip_file

from io import StringIO
from pathlib import Path

import pytest

# Load gaphor.ui first, so GTK library versions are set corrently
import gaphor.ui

from gaphor.core import Transaction
from gaphor.core.eventmanager import EventManager
from gaphor.core.modeling import Diagram, ElementFactory
from gaphor.core.modeling.elementdispatcher import ElementDispatcher
from gaphor.core.modeling.modelinglanguage import (
    CoreModelingLanguage,
    MockModelingLanguage,
)
from gaphor.storage import storage
from gaphor.UML.modelinglanguage import UMLModelingLanguage


@pytest.fixture
def event_manager():
    return EventManager()


@pytest.fixture
def element_factory(event_manager, modeling_language):
    element_factory = ElementFactory(
        event_manager, ElementDispatcher(event_manager, modeling_language)
    )
    yield element_factory
    element_factory.shutdown()


@pytest.fixture
def modeling_language():
    return MockModelingLanguage(CoreModelingLanguage(), UMLModelingLanguage())


@pytest.fixture
def diagram(element_factory, event_manager):
    with Transaction(event_manager):
        diagram = element_factory.create(Diagram)
    yield diagram
    with Transaction(event_manager):
        diagram.unlink()


@pytest.fixture
def create(diagram, element_factory):
    def _create(item_class, element_class=None):
        return diagram.create(
            item_class,
            subject=(element_factory.create(element_class) if element_class else None),
        )

    return _create


@pytest.fixture
def kindof(element_factory):
    def _kindof(type):
        return element_factory.lselect(type)

    return _kindof


@pytest.fixture
def saver(element_factory):
    def save():
        """Save diagram into string."""

        f = StringIO()
        storage.save(f, element_factory)
        data = f.getvalue()
        f.close()

        return data

    return save


@pytest.fixture
def loader(element_factory, modeling_language):
    def load(data):
        """Load data from specified string."""
        element_factory.flush()
        assert not list(element_factory.select())

        f = StringIO(data)
        storage.load(f, factory=element_factory, modeling_language=modeling_language)
        f.close()

    return load


@pytest.fixture
def test_models():
    """The folder where test models can be found."""
    return Path(__file__).parent.parent / "test-models"


@pytest.fixture
def models():
    """The folder where test models can be found."""
    return Path(__file__).parent.parent / "models"
<|endoftext|>"
},
{
"prompt": """"select the target columns

Args:
    source_df:
        input dataframe.
    column:
        Explicitly used columns. if set `"__all__"`, return all columns.
        column と exlucdes は同時に設定できません (どちらか一方だけ選択できます)
    excludes:
        Explicitly remove columns
Returns:"""
fn get_target_columns(source_df: pd.DataFrame, column: Union[(str, List)], excludes: Union[(None, List)]) -> Iterable

"""apply feature engineering for each columns."""
class ColumnWiseBlock(BaseBlock):
	engine = None
	"""Args:
	    name:
	        this block name.
	    column:
	        use columns. if set `"__all__"`, use all columns get from parent blocks
	    excludes:
	        if set, exclude these columns from column.
	        [NOTE]
	            when set specific column (ex. ['foo', 'bar']), excludes must be None.
	    **kwargs:"""
	fn __init__(self, name, column: Union[(str, List)], excludes: Union[(None, List)])
	fn create_new_engine(self, column_name: str) -> BaseEngine
	fn unzip(self, experiment: ExperimentBackend)
	fn frozen(self, experiment: ExperimentBackend)
	fn get_output_colname(self, column)
	fn fit(self, source_df, y, experiment) -> pd.DataFrame
	fn transform(self, source_df)


class FilterBlock(ColumnWiseBlock):
	fn fit(self, source_df, y, experiment) -> pd.DataFrame
	fn transform(self, source_df)
	fn frozen(self, experiment: ExperimentBackend)
	fn unzip(self, experiment: ExperimentBackend)


class BinningCountBlock(ColumnWiseBlock):
	engine = BinCountEncoder
	fn __init__(self, name, column, bins)
	fn create_new_engine(self, column_name: str)
	fn get_output_colname(self, column)


class OneHotEncodingBlock(ColumnWiseBlock):
	engine = OneHotEncoder
	fn create_new_engine(self, column_name: str) -> BaseEstimator
	fn transform(self, source_df)


class CountEncodingBlock(ColumnWiseBlock):
	engine = CountEncoder
	fn get_output_colname(self, column)


class FillnaBlock(BaseBlock):
	_save_attributes = ['fill_values_']
	fn fit(self, source_df: pd.DataFrame, y: Union[(None, np.ndarray)], experiment: ExperimentBackend) -> pd.DataFrame
	fn transform(self, source_df: pd.DataFrame) -> pd.DataFrame


class TunedLogisticBlock(TunerBlock):
	model_class = LogisticRegression
	initial_params = {'solver': 'liblinear', 'penalty': 'l2', 'input_scaling': 'standard'}
	fn generate_model_class_try_params(self, trial)


class TunedRidgeBlock(TunerBlock):
	model_class = Ridge
	initial_params = {'input_scaling': 'standard'}
	fn generate_model_class_try_params(self, trial)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pandas as pd
import pytest

from vivid import create_runner
from vivid.backends import LocalExperimentBackend
from vivid.estimators.linear import TunedRidgeBlock
from vivid.features.base import FillnaBlock
from vivid.features.base import get_target_columns


@pytest.fixture
def source_df():
    return pd.DataFrame({
        'a': [1, 2],
        'b': [1, 2]
    })


def test_fillna_block(regression_hasna_set, tmpdir):
    df, y = regression_hasna_set
    runner = create_runner(TunedRidgeBlock(name='ridge', n_trials=1))
    with pytest.raises(ValueError):
        runner.fit(df, y)

    fillna_block = FillnaBlock('FNA')
    ridge = TunedRidgeBlock(parent=fillna_block, name='ridge', n_trials=1)

    runner = create_runner(ridge, experiment=LocalExperimentBackend(tmpdir))
    runner.fit(df, y)

    # can predict re-define blocks (check save require field to tmpdir)
    fillna_block = FillnaBlock('FNA')
    ridge = TunedRidgeBlock(parent=fillna_block, name='ridge', n_trials=1)
    runner = create_runner(ridge, experiment=LocalExperimentBackend(tmpdir))
    runner.predict(df)


def test_target_columns(source_df):
    x = get_target_columns(source_df)
    assert x == ['a', 'b']

    x = get_target_columns(source_df, excludes=['a'])
    assert x == ['b']

    x = get_target_columns(source_df, column=['a'])
    assert x == ['a']

    x = get_target_columns(source_df, column=['a'], excludes=['a'])
    assert x == []


def test_raise_not_exist_column(source_df):
    with pytest.raises(ValueError, match='some specific column does not exist in source_df columns. i.e.'):
        get_target_columns(source_df, column=['a', 'b', 'c'])


def test_raise_not_exist_excludes(source_df):
    with pytest.raises(ValueError, match='some specific `excludes` columns does not exist'):
        get_target_columns(source_df, excludes=['c'])
<|endoftext|>"
},
{
"prompt": "fn print_log(volttron_home)

fn get_rand_ip_and_port()

fn get_rand_port(ip)

fn is_port_open(ip, port)

fn get_rand_vip()

fn get_rand_ipc_vip()

fn build_wrapper(vip_address)

fn cleanup_wrapper(wrapper)

fn cleanup_wrappers(platforms)

fn volttron_instance1(request)

fn volttron_instance2(request)

fn volttron_instance_msgdebug(request)

fn volttron_instance_encrypt(request)

fn volttron_instance1_web(request)

fn volttron_instance2_web(request)

fn volttron_instance_module_web(request)

"""Fixture that returns a single instance of volttron platform for testing

@param request: pytest request object
@return: volttron platform instance"""
fn volttron_instance(request: None)

"""Fixture to get more than 1 volttron instance for test
Use this fixture to get more than 1 volttron instance for test. This
returns a function object that should be called with number of instances
as parameter to get a list of volttron instnaces. The fixture also
takes care of shutting down all the instances at the end

Example Usage:

def test_function_that_uses_n_instances(get_volttron_instances):
    instance1, instance2, instance3 = get_volttron_instances(3)

@param request: pytest request object
@return: function that can used to get any number of
    volttron instances for testing."""
fn get_volttron_instances(request: None)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "
import pytest

from volttron.platform import get_services_core
from .volttron_platform_fixtures import print_log

PLATFORM_AGENT_CONFIG = {
    # Agent id is used in the display on volttron central.
    "agentid": "Platform Agent",

    # Configuration parameters.
    #
    # The period of time to go between attempting to report status to the
    # platform.historian.
    #
    # Default "report_status_period": 30
    "report_status_period": 15
}

VC_CONFIG = {
    # The agentid is used during display on the VOLTTRON central platform
    # it does not need to be unique.
    "agentid": "Volttron Central",

    # By default the webroot will be relative to the installation directory
    # of the agent when it is installed.  One can override this by specifying
    # the root directory here.
    # "webroot": "path/to/webroot",

    # Authentication for users is handled through a naive password algorithm
    # import hashlib
    # hashlib.sha512(password).hexdigest() where password is the plain text password.
    "users": {
        "reader": {
            "password": "2d7349c51a3914cd6f5dc28e23c417ace074400d7c3e176bcf5da72fdbeb6ce7ed767ca00c6c1fb754b8df5114fc0b903960e7f3befe3a338d4a640c05dfaf2d",
            "groups": [
                "reader"
            ]
        },
        "admin": {
            "password": "c7ad44cbad762a5da0a452f9e854fdc1e0e7a52a38015f23f3eab1d80b931dd472634dfac71cd34ebc35d16ab7fb8a90c81f975113d6c7538dc69dd8de9077ec",
            "groups": [
                "admin"
            ]
        },
        "dorothy": {
            "password": "cf1b67402d648f51ef6ff8805736d588ca07cbf018a5fba404d28532d839a1c046bfcd31558dff658678b3112502f4da9494f7a655c3bdc0e4b0db3a5577b298",
            "groups": [
                "reader, writer"
            ]
        }
    }
}


@pytest.fixture
def vc_instance(request, volttron_instance1_web):
    """
    Creates an instance of volttron with a `VolttronCentral` agent
    already installed and started.

    :returns tuple:
        - the PlatformWrapper
        - the uuid of the `VolttronCentral` agent.
        - the jsonrpc address to be used for communicating with the
          `VolttronCentral` agent.
    """
    agent_uuid = volttron_instance1_web.install_agent(
        agent_dir=get_services_core("VolttronCentral"),
        config_file=VC_CONFIG,
        start=True
    )

    rpc_addr = volttron_instance1_web.jsonrpc_endpoint

    # Allow all incoming connections that are encrypted.
    volttron_instance1_web.allow_all_connections()

    def cleanup():
        print('Cleanup vc_instance')
        volttron_instance1_web.remove_agent(agent_uuid)
        print_log(volttron_instance1_web.volttron_home)

    request.addfinalizer(cleanup)
    return volttron_instance1_web, agent_uuid, rpc_addr


@pytest.fixture
def pa_instance(request, volttron_instance2_web):
    """
    Creates an instance of volttron with a `VolttronCentralPlatform` agent
    already installed and started.

    :returns tuple: the platformwrapper and the uuid of the agaent installed.
    """
    agent_uuid = volttron_instance2_web.install_agent(
        agent_dir=get_services_core("VolttronCentralPlatform"),
        config_file=PLATFORM_AGENT_CONFIG,
        start=True
    )

    # Allow all incoming encrypted connections
    volttron_instance2_web.allow_all_connections()

    def cleanup():
        print('Cleanup pa_instance')
        volttron_instance2_web.remove_agent(agent_uuid)
        print_log(volttron_instance2_web.volttron_home)

    request.addfinalizer(cleanup)

    return volttron_instance2_web, agent_uuid
<|endoftext|>"
},
{
"prompt": "

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import json
from io import BytesIO

import pytest
from minio import Minio

from shepherd.constants import INPUT_DIR


@pytest.fixture()
def job(minio: Minio, bucket):
    job_id = bucket
    data = json.dumps({"content": "Lorem ipsum"}).encode()
    minio.put_object(job_id, INPUT_DIR + "/payload.json", BytesIO(data), len(data))
    yield job_id


async def test_get_input_success(job, aiohttp_client, app):
    job_id = job
    client = await aiohttp_client(app)
    response = await client.get("/jobs/{}/input/payload.json".format(job_id))
    assert response.status == 200

    data = await response.json()
    assert "content" in data


async def test_get_input_not_found(job, aiohttp_client, app):
    job_id = job
    client = await aiohttp_client(app)

    response = await client.get("/jobs/{}/input/i-dont-exist.json".format(job_id))
    assert response.status == 404
<|endoftext|>"
},
{
"prompt": "class UserFactory(DjangoModelFactory):
	username = Faker('user_name')
	email = Faker('email')
	name = Faker('name')
	fn password(self, create: bool, extracted: Sequence[Any])


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from devproject.users.forms import UserCreationForm
from devproject.users.tests.factories import UserFactory

pytestmark = pytest.mark.django_db


class TestUserCreationForm:
    def test_clean_username(self):
        # A user with proto_user params does not exist yet.
        proto_user = UserFactory.build()

        form = UserCreationForm(
            {
                "username": proto_user.username,
                "password1": proto_user._password,
                "password2": proto_user._password,
            }
        )

        assert form.is_valid()
        assert form.clean_username() == proto_user.username

        # Creating a user.
        form.save()

        # The user with proto_user params already exists,
        # hence cannot be created.
        form = UserCreationForm(
            {
                "username": proto_user.username,
                "password1": proto_user._password,
                "password2": proto_user._password,
            }
        )

        assert not form.is_valid()
        assert len(form.errors) == 1
        assert "username" in form.errors
<|endoftext|>"
},
{
"prompt": "fn pathway(id_1: str, id_2: str, path_length: int)

fn health_check()

fn get_random_image_id(excluding)

"""returns n evenly spaced points between start_coord and end_coord"""
fn get_ideal_coords(start_coord, end_coord, n)

"""Return the indexes of the points which most closely match an ideal path"""
fn get_path_indexes(closest_indexes, start_index, end_index)

fn get_pathway(id_1, id_2, n_nodes, ids, feature_index)

fn id_to_url(image_id)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import numpy as np
import time

import pytest
import requests
from starlette.testclient import TestClient

from .api import app
from .utils import ids

client = TestClient(app)


def test_health_check():
    response = client.get('/image_pathways/health_check')
    assert response.status_code == 200
    assert response.json() == {'status': 'healthy'}


def test_docs():
    response = client.get('/image_pathways/docs')
    assert response.status_code == 200


def test_redoc():
    response = client.get('/image_pathways/redoc')
    assert response.status_code == 200


def test_response_time():
    n, times = 100, []
    for _ in range(n):
        start_time = time.time()
        client.get('/image_pathways')
        times.append(time.time() - start_time)
    assert np.mean(times) <= 0.5


def test_reponse_contains_all_elements():
    response_elements = [
        'id_path',
        'image_url_path'
    ]
    response = client.get('/image_pathways').json()
    for element in response_elements:
        assert element in response


def test_response_length():
    for path_length in range(3, 25):
        response = client.get(
            '/image_pathways?path_length=' + str(path_length)
        ).json()
        assert len(response['id_path']) == path_length
        assert len(response['image_url_path']) == path_length


def test_response_accepts_image_id_1():
    for image_id in np.random.choice(ids, size=10):
        response = client.get('/image_pathways?id_1=' + image_id)
        assert response.status_code == 200


def test_response_accepts_image_id_2():
    for image_id in np.random.choice(ids, size=10):
        response = client.get('/image_pathways?id_2=' + image_id)
        assert response.status_code == 200


def test_does_not_accept_invalid_image_id_1():
    response = client.get('/image_pathways?id_1=some_rubbish')
    assert response.status_code == 404
    assert response.json() == {'detail': 'Invalid image_id'}


def test_does_not_accept_invalid_image_id_2():
    response = client.get('/image_pathways?id_2=some_rubbish')
    assert response.status_code == 404
    assert response.json() == {'detail': 'Invalid image_id'}


def test_does_not_accept_invalid_n():
    response = client.get('/image_pathways?path_length=some_rubbish')
    assert response.status_code == 422
<|endoftext|>"
},
{
"prompt": "

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
import xyzservices.providers as xyz
import requests
import mercantile

flat_free = xyz.filter(requires_token=False).flatten()


def check_provider(provider):
    for key in ["attribution", "name"]:
        assert key in provider.keys()
    assert provider.url.startswith("http")
    for option in ["{z}", "{y}", "{x}"]:
        assert option in provider.url


def get_tile(provider):
    bounds = provider.get("bounds", [[-180, -90], [180, 90]])
    lat = (bounds[0][0] + bounds[1][0]) / 2
    lon = (bounds[0][1] + bounds[1][1]) / 2
    zoom = (provider.get("min_zoom", 0) + provider.get("max_zoom", 20)) // 2
    tile = mercantile.tile(lon, lat, zoom)
    z = tile.z
    x = tile.x
    y = tile.y
    return (z, x, y)


def get_response(url):
    s = requests.Session()
    a = requests.adapters.HTTPAdapter(max_retries=3)
    s.mount("http://", a)
    s.mount("https://", a)
    r = s.get(url)
    return r.status_code


@pytest.mark.parametrize("provider_name", xyz.flatten())
def test_minimal_provider_metadata(provider_name):
    provider = xyz.flatten()[provider_name]
    check_provider(provider)


@pytest.mark.parametrize("name", flat_free)
def test_free_providers(name):
    provider = flat_free[name]

    if provider.get("status"):
        pytest.xfail("Provider is known to be broken.")

    z, x, y = get_tile(provider)

    try:
        r = get_response(provider.build_url(z=z, x=x, y=y))
        assert r == requests.codes.ok
    except AssertionError as e:
        if r == 403:
            pytest.xfail("Provider not available due to API restrictions (Error 403).")

        elif r == 503:
            pytest.xfail("Service temporarily unavailable (Error 503).")

        elif r == 502:
            pytest.xfail("Bad Gateway (Error 502).")

        # check another tiles
        elif r == 404:
            # in some cases, the computed tile is not availble. trying known tiles.
            options = [(12, 2154, 1363), (6, 13, 21), (16, 33149, 22973)]
            results = []
            for o in options:
                z, x, y = o
                r = get_response(provider.build_url(z=z, x=x, y=y))
                results.append(r)
            if not any([x == requests.codes.ok for x in results]):
                raise ValueError(f"Response code: {r}")
        else:
            raise ValueError(f"Response code: {r}")
<|endoftext|>"
},
{
"prompt": """"Get absolute path for ``fname``.

:param fname: Filename.
:type fname: str or pathlib.Path
:rtype: str"""
fn get_reference_file(fname: str or pathlib.Path) -> str

""":param fname: Filename.
:type fname: str or pathlib.Path"""
fn get_reference_image(fname: str or pathlib.Path)

""":param fname: Filename of the font.
:type fname: str or pathlib.Path"""
fn get_reference_font(fname: str or pathlib.Path, fsize)

"""Load :py:class:`PIL.ImageFont` type font from provided fname

:param fname: The name of the file that contains the PIL.ImageFont
:type fname: str
:rtype: :py:class:`PIL.ImageFont`"""
fn get_reference_pillow_font(fname: str) -> :py:class:`PIL.ImageFont`

fn get_spidev()

""":param img_path: Location of image.
:type img_path: str"""
fn assert_identical_image(reference, target, img_path: str)

fn i2c_error(path_name, err_no)

fn fib(n)

"""Create a mock "open" that will mock open multiple files in sequence
Args:
    *file_contents ([str]): a list of file contents to be returned by open
Returns:
    (MagicMock) a mock opener that will return the contents of the first
        file when opened the first time, the second file when opened the
        second time, etc."""
fn multi_mock_open()

fn skip_unsupported_platform(err)

fn _positional_args_list(mock)

fn assert_only_cleans_whats_setup(gpio)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Copyright (c) 2019-2020 Richard Hull and contributors
# See LICENSE.rst for details.

"""
Tests for the :py:class:`luma.core.interface.serial.ftdi_i2c` class.
"""

import pytest
from unittest.mock import Mock, patch
from luma.core.interface.serial import ftdi_i2c
from helpers import fib
import luma.core.error


@patch('pyftdi.i2c.I2cController')
def test_init(mock_controller):
    instance = Mock()
    instance.get_port = Mock()
    mock_controller.side_effect = [instance]

    ftdi_i2c(device='ftdi://dummy', address='0xFF')
    mock_controller.assert_called_with()
    instance.configure.assert_called_with('ftdi://dummy')
    instance.get_port.assert_called_with(0xFF)


@patch('pyftdi.i2c.I2cController')
def test_command(mock_controller):
    cmds = [3, 1, 4, 2]
    port = Mock()
    instance = Mock()
    instance.get_port = Mock(return_value=port)
    mock_controller.side_effect = [instance]

    serial = ftdi_i2c(device='ftdi://dummy', address=0x3C)
    serial.command(*cmds)
    port.write_to.assert_called_once_with(0x00, cmds)


@patch('pyftdi.i2c.I2cController')
def test_data(mock_controller):
    data = list(fib(100))
    port = Mock()
    instance = Mock()
    instance.get_port = Mock(return_value=port)
    mock_controller.side_effect = [instance]

    serial = ftdi_i2c(device='ftdi://dummy', address=0x3C)
    serial.data(data)
    port.write_to.assert_called_once_with(0x40, data)


@patch('pyftdi.i2c.I2cController')
def test_cleanup(mock_controller):
    port = Mock()
    instance = Mock()
    instance.get_port = Mock(return_value=port)
    mock_controller.side_effect = [instance]

    serial = ftdi_i2c(device='ftdi://dummy', address=0x3C)
    serial.cleanup()
    instance.terminate.assert_called_once_with()


@patch('pyftdi.i2c.I2cController')
def test_init_device_address_error(mock_controller):
    address = 'foo'
    with pytest.raises(luma.core.error.DeviceAddressError) as ex:
        ftdi_i2c(device='ftdi://dummy', address=address)
    assert str(ex.value) == f'I2C device address invalid: {address}'
<|endoftext|>"
},
{
"prompt": "fn tmp_h5_file(tmp_path)

fn h5_context(tmp_path)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import contextlib
from pathlib import Path

import h5py
import pytest
from bluesky.tests.conftest import RE  # noqa
from ophyd.tests.conftest import hw  # noqa

# from suitcase.utils.tests.conftest import (  # noqa
#     example_data,
#     generate_data,
#     plan_type,
#     detector_list,
#     event_type,
# )  # noqa


@pytest.fixture
def tmp_h5_file(tmp_path):
    return h5py.File(tmp_path / Path("test.h5"), "w")


@pytest.fixture
def h5_context(tmp_path):
    @contextlib.contextmanager
    def make_context():
        # print("  entering")
        try:
            f = h5py.File(tmp_path / Path("test.h5"), "w")
            yield f
        except RuntimeError as err:
            print("  ERROR:", err)
        finally:
            # print("  exiting")
            f.close()

    return make_context
<|endoftext|>"
},
{
"prompt": "class Context:
	fn __init__(self)
	"""Logs a message to stderr."""
	fn log(self, msg)
	"""Logs a message to stderr only if verbose is enabled."""
	fn vlog(self, msg)


class ComplexCLI(click.MultiCommand):
	fn list_commands(self, ctx)
	fn get_command(self, ctx, cmd_name)


"""A complex command line interface."""
fn cli(ctx, verbose)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import os
import shutil
import uuid
from pathlib import Path
import pytest
from click.testing import CliRunner
from git.repo import Repo
from envadmin.cli import cli
from tests.utilities import constants


PATH = os.path.join(Path.home(), "temp", "envadmin_sandbox", str(uuid.uuid4())[:8])


@pytest.fixture()
def runner():
    yield CliRunner()


@pytest.fixture()
def temp_folder():
    # Deletes and recreates PATH
    if os.path.isdir(PATH):
        shutil.rmtree(PATH)

    os.mkdir(PATH)

    yield PATH

    # Clear files at end.
    shutil.rmtree(PATH)


@pytest.fixture()
def temp_git_folder(temp_folder):
    repo = Repo.init(temp_folder)
    repo.create_remote(name="origin", url="git@gitlab.com:temp/sandbox.git")
    yield temp_folder


@pytest.fixture()
def temp_envadmin_folder(runner, temp_git_folder):
    runner.invoke(cli, ["-v", "init",
                        "-p", temp_git_folder,
                        "-c", temp_git_folder,
                        "-e", "test@test.com",
                        "--no-push"])

    runner.invoke(cli, ["namespace",
                        "-c", temp_git_folder,
                        "create",
                        "-n", constants.NAMESPACE1])
<|endoftext|>"
},
{
"prompt": "

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import inspect
import os
import pytest
import shutil
from unittest import mock

from hoverxref.translators import HoverXRefHTMLTranslatorMixin

from .utils import srcdir


@pytest.mark.sphinx(
    srcdir=srcdir,
    buildername='latex',
    confoverrides={
        'hoverxref_project': 'myproject',
        'hoverxref_version': 'myversion',
    },
)
def test_dont_override_translator_non_html_builder(app, status, warning):
    app.build()
    path = app.outdir / 'test.tex'
    assert path.exists() is True
    content = open(path).read()

    assert app.builder.format == 'latex'
    for name, klass in app.registry.translators.items():
        assert not issubclass(klass, HoverXRefHTMLTranslatorMixin)


@pytest.mark.sphinx(
    srcdir=srcdir,
    buildername='html',
    confoverrides={
        'hoverxref_project': 'myproject',
        'hoverxref_version': 'myversion',
    },
)
def test_override_translator_non_html_builder(app, status, warning):
    app.build()
    path = app.outdir / 'index.html'
    assert path.exists() is True
    content = open(path).read()

    assert app.builder.format == 'html'
    for name, klass in app.registry.translators.items():
        assert issubclass(klass, HoverXRefHTMLTranslatorMixin)


@pytest.mark.sphinx(
    srcdir=srcdir,
    buildername='latex',
    confoverrides={
        'hoverxref_project': 'myproject',
        'hoverxref_version': 'myversion',
        'hoverxref_auto_ref': True,
    },
)
def test_dont_fail_non_html_builder(app, status, warning):
    """
    Test our resolver is not used by non-HTML builder.

    When running the build with ``latex`` as builder and
    ``hoverxref_auto_ref=True`` it should not fail with

    def _get_docpath(self, builder, docname):
        docpath = builder.get_outfilename(docname)
        AttributeError: 'LaTeXBuilder' object has no attribute 'get_outfilename'

    LaTeXBuilder should never use our resolver.
    """

    with mock.patch('hoverxref.domains.HoverXRefBaseDomain._get_docpath') as _get_docpath:
        app.build()
        assert not _get_docpath.called
    path = app.outdir / 'test.tex'
    assert path.exists() is True
    content = open(path).read()

    assert app.builder.format == 'latex'
<|endoftext|>"
},
{
"prompt": "fn redirectTest(response, route)

fn redTestResponses(responses, route)

fn checkRendered(response)

fn basicResponses(route, client)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
import os
from helpers import redirectTest, checkRendered

def test_home(client, app):
    route = "/home"
    if "PLACES_KEY" in app.config:
        del app.config['PLACES_KEY']
    response = client.get(route)
    # there will be no key at this point should redirect to setKey page
    redirectTest(response, '/setKey')
    app.config.update(PLACES_KEY = 'test')
    response = client.get(route)
    #now there is a key there should be no redirect, check that the status code is 200 and
    #response contains html
    checkRendered(response)

<|endoftext|>"
},
{
"prompt": "class BaseFactory(SQLAlchemyModelFactory):


class UserFactory(BaseFactory):
	username = Sequence(lambda n: 'user{0}'.format(n))
	email = Sequence(lambda n: 'user{0}@example.com'.format(n))
	password = PostGenerationMethodCall('set_password', 'example')
	active = True


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# -*- coding: utf-8 -*-
"""Model unit tests."""
import datetime as dt

import pytest

from erdos.models.user import User, Role
from .factories import UserFactory


@pytest.mark.usefixtures('db')
class TestUser:

    def test_get_by_id(self):
        user = User('foo', 'foo@bar.com')
        user.save()

        retrieved = User.get_by_id(user.id)
        assert retrieved == user

    def test_created_at_defaults_to_datetime(self):
        user = User(username='foo', email='foo@bar.com')
        user.save()
        assert bool(user.created_at)
        assert isinstance(user.created_at, dt.datetime)

    def test_password_is_nullable(self):
        user = User(username='foo', email='foo@bar.com')
        user.save()
        assert user.password is None

    def test_factory(self, db):
        user = UserFactory(password="myprecious")
        db.session.commit()
        assert bool(user.username)
        assert bool(user.email)
        assert bool(user.created_at)
        assert user.is_admin is False
        assert user.active is True
        assert user.check_password('myprecious')

    def test_check_password(self):
        user = User.create(username="foo", email="foo@bar.com",
                    password="foobarbaz123")
        assert user.check_password('foobarbaz123') is True
        assert user.check_password("barfoobaz") is False

    def test_full_name(self):
        user = UserFactory(first_name="Foo", last_name="Bar")
        assert user.full_name == "Foo Bar"

    def test_roles(self):
        role = Role(name='admin')
        role.save()
        u = UserFactory()
        u.roles.append(role)
        u.save()
        assert role in u.roles
<|endoftext|>"
},
{
"prompt": """"Load npy file as numpy array."""
fn _load_npy(handle)

"""Load image file as numpy array."""
fn _load_img(handle, target_dtype, size)

"""Load json file as python object."""
fn _load_json(handle)

"""Load and decode a string."""
fn _load_text(handle, split, encoding)

"""Load GraphDef from a binary proto file."""
fn _load_graphdef_protobuf(handle)

"""Load a file.

File format is inferred from url. File retrieval strategy is inferred from
URL. Returned object type is inferred from url extension.

Args:
  url_or_handle: a (reachable) URL, or an already open file handle

Raises:
  RuntimeError: If file extension or URL is not supported."""
fn load(url_or_handle: None, cache)

fn load_using_loader(url_or_handle, loader, cache)

fn is_handle(url_or_handle)

fn get_extension(url_or_handle)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# -*- coding: UTF-8 -*-
from __future__ import absolute_import, division, print_function

import pytest

import numpy as np
from lucid.misc.io.loading import load
import io


def test_load_json():
  path = "./tests/fixtures/dictionary.json"
  dictionary = load(path)
  assert "key" in dictionary


def test_load_text():
  path = "./tests/fixtures/string.txt"
  string = load(path)
  assert u"🐕" in string


def test_load_multiline_text_as_list():
  path = "./tests/fixtures/multiline.txt"
  string_list = load(path, split=True)
  assert isinstance(string_list, list)
  assert all(isinstance(string, ("".__class__, u"".__class__)) for string in string_list)


def test_load_npy():
  path = "./tests/fixtures/array.npy"
  array = load(path)
  assert array.shape is not None


def test_load_npz():
  path = "./tests/fixtures/arrays.npz"
  arrays = load(path)
  assert isinstance(arrays, np.lib.npyio.NpzFile)


@pytest.mark.parametrize("path", [
  "./tests/fixtures/rgbeye.png",
  "./tests/fixtures/noise_uppercase.PNG",
  "./tests/fixtures/rgbeye.jpg",
  "./tests/fixtures/noise.jpeg",
  "./tests/fixtures/image.xyz",
])
def test_load_image(path):
  image = load(path)
  assert image.shape is not None
  assert all(dimension > 2 for dimension in image.shape)


@pytest.mark.parametrize("path", [
  "./tests/fixtures/rgbeye.png",
  "./tests/fixtures/noise.jpeg",
])
def test_load_image_resized(path):
  image = load(path)
  assert image.shape is not None
  assert all(dimension > 2 for dimension in image.shape)


def test_load_garbage_with_unknown_extension():
  path = "./tests/fixtures/string.XYZ"
  with pytest.raises(RuntimeError):
    load(path)


def test_load_json_with_file_handle():
  path = "./tests/fixtures/dictionary.json"
  with io.open(path, 'r') as handle:
    dictionary = load(handle)
  assert "key" in dictionary
<|endoftext|>"
},
{
"prompt": "class UserFactory(DjangoModelFactory):
	username = Faker('user_name')
	email = Faker('email')
	name = Faker('name')
	fn password(self, create: bool, extracted: Sequence[Any])


class ConsumerFactory(UserFactory):


class CoordinatorFactory(UserFactory):


class VendorFactory(UserFactory):


class InstructorFactory(UserFactory):


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from django.conf import settings
from django.contrib import messages
from django.contrib.auth.models import AnonymousUser
from django.contrib.messages.middleware import MessageMiddleware
from django.contrib.sessions.middleware import SessionMiddleware
from django.http import HttpRequest
from django.test import RequestFactory
from django.urls import reverse
from rest_framework import status

from server.users.forms import UserChangeForm
from server.users.models import User
from server.users.tests.factories import UserFactory
from server.users.views import UserRedirectView, UserUpdateView, user_detail_view

pytestmark = pytest.mark.django_db


class TestUserUpdateView:
    """
    TODO:
        extracting view initialization code as class-scoped fixture
        would be great if only pytest-django supported non-function-scoped
        fixture db access -- this is a work-in-progress for now:
        https://github.com/pytest-dev/pytest-django/pull/258
    """

    def dummy_get_response(self, request: HttpRequest):
        return None

    def test_get_success_url(self, user: User, rf: RequestFactory):
        view = UserUpdateView()
        request = rf.get("/fake-url/")
        request.user = user

        view.request = request

        assert view.get_success_url() == f"/users/{user.username}/"

    def test_get_object(self, user: User, rf: RequestFactory):
        view = UserUpdateView()
        request = rf.get("/fake-url/")
        request.user = user

        view.request = request

        assert view.get_object() == user

    def test_form_valid(self, user: User, rf: RequestFactory):
        view = UserUpdateView()
        request = rf.get("/fake-url/")

        # Add the session/message middleware to the request
        SessionMiddleware(self.dummy_get_response).process_request(request)
        MessageMiddleware(self.dummy_get_response).process_request(request)
        request.user = user

        view.request = request

        # Initialize the form
        form = UserChangeForm()
        form.cleaned_data = []
        view.form_valid(form)

        messages_sent = [m.message for m in messages.get_messages(request)]
        assert messages_sent == ["Information successfully updated"]


class TestUserRedirectView:
    def test_get_redirect_url(self, user: User, rf: RequestFactory):
        view = UserRedirectView()
        request = rf.get("/fake-url")
        request.user = user

        view.request = request

        assert view.get_redirect_url() == f"/users/{user.username}/"


class TestUserDetailView:
    def test_authenticated(self, user: User, rf: RequestFactory):
        request = rf.get("/fake-url/")
        request.user = UserFactory()

        response = user_detail_view(request, username=user.username)

        assert response.status_code == status.HTTP_200_OK

    def test_not_authenticated(self, user: User, rf: RequestFactory):
        request = rf.get("/fake-url/")
        request.user = AnonymousUser()

        response = user_detail_view(request, username=user.username)
        login_url = reverse(settings.LOGIN_URL)

        assert response.status_code == status.HTTP_302_FOUND
        assert response.url == f"{login_url}?next=/fake-url/"
<|endoftext|>"
},
{
"prompt": "class UserFactory(DjangoModelFactory):
	username = Faker('user_name')
	email = Faker('email')
	name = Faker('name')
	fn password(self, create: bool, extracted: Sequence[Any])


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from django.contrib.auth.models import AnonymousUser
from django.http.response import Http404
from django.test import RequestFactory

from bonds.users.models import User
from bonds.users.tests.factories import UserFactory
from bonds.users.views import (
    UserRedirectView,
    UserUpdateView,
    user_detail_view,
)

pytestmark = pytest.mark.django_db


class TestUserUpdateView:
    """
    TODO:
        extracting view initialization code as class-scoped fixture
        would be great if only pytest-django supported non-function-scoped
        fixture db access -- this is a work-in-progress for now:
        https://github.com/pytest-dev/pytest-django/pull/258
    """

    def test_get_success_url(self, user: User, rf: RequestFactory):
        view = UserUpdateView()
        request = rf.get("/fake-url/")
        request.user = user

        view.request = request

        assert view.get_success_url() == f"/users/{user.username}/"

    def test_get_object(self, user: User, rf: RequestFactory):
        view = UserUpdateView()
        request = rf.get("/fake-url/")
        request.user = user

        view.request = request

        assert view.get_object() == user


class TestUserRedirectView:
    def test_get_redirect_url(self, user: User, rf: RequestFactory):
        view = UserRedirectView()
        request = rf.get("/fake-url")
        request.user = user

        view.request = request

        assert view.get_redirect_url() == f"/users/{user.username}/"


class TestUserDetailView:
    def test_authenticated(self, user: User, rf: RequestFactory):
        request = rf.get("/fake-url/")
        request.user = UserFactory()

        response = user_detail_view(request, username=user.username)

        assert response.status_code == 200

    def test_not_authenticated(self, user: User, rf: RequestFactory):
        request = rf.get("/fake-url/")
        request.user = AnonymousUser()  # type: ignore

        response = user_detail_view(request, username=user.username)

        assert response.status_code == 302
        assert response.url == "/accounts/login/?next=/fake-url/"

    def test_case_sensitivity(self, rf: RequestFactory):
        request = rf.get("/fake-url/")
        request.user = UserFactory(username="UserName")

        with pytest.raises(Http404):
            user_detail_view(request, username="username")
<|endoftext|>"
},
{
"prompt": """"Replacement for the standard library deepcopy.

The stdlib's copy.deepcopy is notoriously slow. After benchmarking the REST Proxy produce API
the deepcopy was identified as a bottleneck. Currently it is faster to encode the objects as
json and decode the result instead of using copy.deepcopy."""
fn deepcopy(obj: Any) -> Any

"""Return datetime to ISO 8601 variant suitable for users.
Assume UTC for datetime objects without a timezone, always use
the Z timezone designator."""
fn isoformat(datetime_obj)

fn default_json_serialization(obj)

fn json_encode(obj)

fn assert_never(value: NoReturn) -> NoReturn

class Result:
	fn __init__(self, status, json_result, headers)
	fn json(self)
	fn __repr__(self)
	fn ok(self)


class Client:
	fn __init__(self, server_uri, client_factory, server_ca: Optional[str])


fn convert_to_int(object_: dict, key: str, content_type: str)

class KarapaceKafkaClient(KafkaClient):
	fn __init__(self)
	fn close_invalid_connections(self)
	fn _poll(self, timeout)
	"""Lifted from the parent class with the caveat that the node id will always belong to the bootstrap node,
	thus ensuring we do not end up in a stale metadata loop"""
	fn _maybe_refresh_metadata(self, wakeup)


class KarapaceBrokerConnection(BrokerConnection):
	fn __init__(self, host, port, afi)
	fn close(self, error)
	fn ns_blackout(self)
	fn blacked_out(self)


fn trim_margin(s: str) -> str

class KotlinRange:
	minimum: int
	maximum: int
	fn __str__(self) -> str


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """"
karapace - schema tests

Copyright (c) 2019 Aiven Ltd
See LICENSE for details
"""
from karapace.protobuf.kotlin_wrapper import trim_margin
from karapace.utils import Client
from tests.utils import create_subject_name_factory

import logging
import pytest

baseurl = "http://localhost:8081"


def add_slashes(text: str) -> str:
    escape_dict = {
        "\a": "\\a",
        "\b": "\\b",
        "\f": "\\f",
        "\n": "\\n",
        "\r": "\\r",
        "\t": "\\t",
        "\v": "\\v",
        "'": "\\'",
        '"': '\\"',
        "\\": "\\\\",
    }
    trans_table = str.maketrans(escape_dict)
    return text.translate(trans_table)


log = logging.getLogger(__name__)


@pytest.mark.parametrize("trail", ["", "/"])
async def test_protobuf_schema_compatibility(registry_async_client: Client, trail: str) -> None:
    subject = create_subject_name_factory(f"test_protobuf_schema_compatibility-{trail}")()

    res = await registry_async_client.put(f"config/{subject}{trail}", json={"compatibility": "BACKWARD"})
    assert res.status == 200

    original_schema = """
            |syntax = "proto3";
            |package a1;
            |message TestMessage {
            |    message Value {
            |        string str2 = 1;
            |        int32 x = 2;
            |    }
            |    string test = 1;
            |    .a1.TestMessage.Value val = 2;
            |}
            |"""

    original_schema = trim_margin(original_schema)

    res = await registry_async_client.post(
        f"subjects/{subject}/versions{trail}", json={"schemaType": "PROTOBUF", "schema": original_schema}
    )
    assert res.status == 200
    assert "id" in res.json()

    evolved_schema = """
            |syntax = "proto3";
            |package a1;
            |message TestMessage {
            |    message Value {
            |        string str2 = 1;
            |        Enu x = 2;
            |    }
            |    string test = 1;
            |    .a1.TestMessage.Value val = 2;
            |    enum Enu {
            |        A = 0;
            |        B = 1;
            |    }
            |}
            |"""
    evolved_schema = trim_margin(evolved_schema)

    res = await registry_async_client.post(
        f"compatibility/subjects/{subject}/versions/latest{trail}",
        json={"schemaType": "PROTOBUF", "schema": evolved_schema},
    )
    assert res.status == 200
    assert res.json() == {"is_compatible": True}

    res = await registry_async_client.post(
        f"subjects/{subject}/versions{trail}", json={"schemaType": "PROTOBUF", "schema": evolved_schema}
    )
    assert res.status == 200
    assert "id" in res.json()

    res = await registry_async_client.post(
        f"compatibility/subjects/{subject}/versions/latest{trail}",
        json={"schemaType": "PROTOBUF", "schema": original_schema},
    )
    assert res.json() == {"is_compatible": True}
    assert res.status == 200
    res = await registry_async_client.post(
        f"subjects/{subject}/versions{trail}", json={"schemaType": "PROTOBUF", "schema": original_schema}
    )
    assert res.status == 200
    assert "id" in res.json()
<|endoftext|>"
},
{
"prompt": "fn decode_timeout(value: str) -> float

fn encode_timeout(timeout: float) -> str

"""Represents request's deadline - fixed point in time
    """
class Deadline:
	fn __init__(self) -> None
	fn __lt__(self, other: object) -> bool
	fn __eq__(self, other: object) -> bool
	fn from_headers(cls, headers: _Headers) -> Optional[Deadline]
	fn from_timeout(cls, timeout: float) -> Deadline
	"""Calculates remaining time for the current request completion
	
	This function returns time in seconds as a floating point number,
	greater or equal to zero."""
	fn time_remaining(self) -> float


fn encode_grpc_message(message: str) -> str

fn decode_grpc_message(value: str) -> str

fn decode_bin_value(value: bytes) -> bytes

fn decode_metadata(headers: _Headers) -> _Metadata

fn encode_bin_value(value: bytes) -> bytes

fn encode_metadata(metadata: _MetadataLike) -> _Headers

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from multidict import MultiDict

from grpclib.metadata import Deadline
from grpclib.metadata import encode_timeout, decode_timeout
from grpclib.metadata import encode_metadata, decode_metadata
from grpclib.metadata import encode_grpc_message, decode_grpc_message


@pytest.mark.parametrize('value, expected', [
    (100, '100S'),
    (15, '15S'),
    (7, '7000m'),
    (0.02, '20m'),
    (0.001, '1000u'),
    (0.00002, '20u'),
    (0.000001, '1000n'),
    (0.00000002, '20n'),
])
def test_encode_timeout(value, expected):
    assert encode_timeout(value) == expected


@pytest.mark.parametrize('value, expected', [
    ('5H', 5 * 3600),
    ('4M', 4 * 60),
    ('3S', 3),
    ('200m', pytest.approx(0.2)),
    ('100u', pytest.approx(0.0001)),
    ('50n', pytest.approx(0.00000005)),
])
def test_decode_timeout(value, expected):
    assert decode_timeout(value) == expected


def test_deadline():
    assert Deadline.from_timeout(1) < Deadline.from_timeout(2)

    with pytest.raises(TypeError) as err:
        Deadline.from_timeout(1) < 'invalid'
    err.match('comparison is not supported between instances '
              'of \'Deadline\' and \'str\'')

    assert Deadline(_timestamp=1) == Deadline(_timestamp=1)

    assert Deadline.from_timeout(1) != 'invalid'


@pytest.mark.parametrize('value, output', [
    ('а^2+б^2=ц^2', '%D0%B0^2+%D0%B1^2=%D1%86^2'),
])
def test_grpc_message_encoding(value, output):
    assert encode_grpc_message(value) == output
    encode_grpc_message(value).encode('ascii')
    assert decode_grpc_message(output) == value


def test_grpc_message_decode_safe():
    # 0xFF is invalid byte in utf-8:
    # https://www.cl.cam.ac.uk/~mgk25/ucs/examples/UTF-8-test.txt
    assert (decode_grpc_message('%FF^2+%FF^2=%FF^2')
            == '\ufffd^2+\ufffd^2=\ufffd^2')


@pytest.mark.parametrize('value, output', [
    ({'regular': 'value'}, [('regular', 'value')]),  # dict-like
    ([('regular', 'value')], [('regular', 'value')]),  # list of pairs
    ({'binary-bin': b'value'}, [('binary-bin', 'dmFsdWU')])
])
def test_encode_metadata(value, output):
    assert encode_metadata(value) == output


def test_encode_metadata_errors():
    with pytest.raises(TypeError) as e1:
        encode_metadata({'regular': b'invalid'})
    e1.match('Invalid metadata value type, str expected')

    with pytest.raises(TypeError) as e2:
        encode_metadata({'binary-bin': 'invalid'})
    e2.match('Invalid metadata value type, bytes expected')


@pytest.mark.parametrize('key', [
    'grpc-internal',
    'Upper-Case',
    'invalid~character',
    ' spaces ',
])
def test_encode_metadata_invalid_key(key):
    with pytest.raises(ValueError) as err:
        encode_metadata({key: 'anything'})
    err.match('Invalid metadata key')


def test_decode_metadata_empty():
    metadata = decode_metadata([
        (':method', 'POST'),
        ('te', 'trailers'),
        ('content-type', 'application/grpc'),
        ('user-agent', 'Test'),
        ('grpc-timeout', '100m'),
    ])
    assert metadata == MultiDict()


@pytest.mark.parametrize('key, value, expected', [
    ('regular', 'value', 'value'),
    ('binary-bin', 'dmFsdWU', b'value'),
])
def test_decode_metadata_regular(key, value, expected):
    metadata = decode_metadata([
        (':method', 'POST'),
        ('te', 'trailers'),
        ('content-type', 'application/grpc'),
        ('user-agent', 'Test'),
        ('grpc-timeout', '100m'),
        (key, value),
    ])
    assert metadata == MultiDict({key: expected})
    assert type(metadata[key]) is type(expected)
<|endoftext|>"
},
{
"prompt": "

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """"Tests for ClimaCell const."""
import pytest

from homeassistant.components.climacell.const import ClimaCellSensorEntityDescription
from homeassistant.const import TEMP_FAHRENHEIT


async def test_post_init():
    """Test post initialization check for ClimaCellSensorEntityDescription."""

    with pytest.raises(RuntimeError):
        ClimaCellSensorEntityDescription(
            key="a", name="b", unit_imperial=TEMP_FAHRENHEIT
        )
<|endoftext|>"
},
{
"prompt": """"Just a BytesIO with an async read method."""
class AsyncBuffer(BytesIO):
	readexactly = read
	"""Append data to the buffer without changing the current position."""
	fn append(self, data)


fn inc_nonce(nonce)

"""Split sequence in equal-sized chunks.
The last chunk is not padded."""
fn split_chunks(seq, n)

fn b(s)

"""long_to_bytes(n:long, blocksize:int) : string
Convert a long integer to a byte string.
If optional blocksize is given and greater than zero, pad the front of the
byte string with binary zeros so that the length is a multiple of
blocksize."""
fn long_to_bytes(n, blocksize)

"""bytes_to_long(string) : long
Convert a byte string to a long integer.
This is (essentially) the inverse of long_to_bytes()."""
fn bytes_to_long(s)

fn server()

fn client()

fn test_handshake(client, server)

"""Return a tuple with `(unbox_stream, box_stream)` (reader/writer).

:return: (:class:`secret_handshake.boxstream.UnboxStream`,
          :class:`secret_handshake.boxstream.BoxStream`) """
fn get_stream_pair(reader, writer)

class UnboxStream(object):
	fn __init__(self, reader, key, nonce)


class BoxStream(object):
	fn __init__(self, writer, key, nonce)
	fn write(self, data)
	fn close(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Copyright (c) 2017 PySecretHandshake contributors (see AUTHORS for more details)
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.


import pytest

from secret_handshake.boxstream import HEADER_LENGTH, BoxStream, UnboxStream
from secret_handshake.util import AsyncBuffer, async_comprehend

from .test_crypto import CLIENT_ENCRYPT_KEY, CLIENT_ENCRYPT_NONCE

MESSAGE_1 = (b'\xcev\xedE\x06l\x02\x13\xc8\x17V\xfa\x8bZ?\x88B%O\xb0L\x9f\x8e\x8c0y\x1dv\xc0\xc9\xf6\x9d\xc2\xdf\xdb'
             b'\xee\x9d')
MESSAGE_2 = b"\x141\xd63\x13d\xd1\xecZ\x9b\xd0\xd4\x03\xcdR?'\xaa.\x89I\x92I\xf9guL\xaa\x06?\xea\xca/}\x88*\xb2"
MESSAGE_3 = (b'\xcbYY\xf1\x0f\xa5O\x13r\xa6"\x15\xc5\x9d\r.*\x0b\x92\x10m\xa6(\x0c\x0c\xc61\x80j\x81)\x800\xed\xda'
             b'\xad\xa1')
MESSAGE_CLOSED = b'\xb1\x14hU\'\xb5M\xa6"\x03\x9duy\xa1\xd4evW,\xdcE\x18\xe4+ C4\xe8h\x96\xed\xc5\x94\x80'


@pytest.mark.asyncio
async def test_boxstream():
    buffer = AsyncBuffer()
    box_stream = BoxStream(buffer, CLIENT_ENCRYPT_KEY, CLIENT_ENCRYPT_NONCE)
    box_stream.write(b'foo')
    buffer.seek(0)
    assert await buffer.read() == MESSAGE_1

    pos = buffer.tell()
    box_stream.write(b'foo')
    buffer.seek(pos)
    assert await buffer.read() == MESSAGE_2

    pos = buffer.tell()
    box_stream.write(b'bar')
    buffer.seek(pos)
    assert await buffer.read() == MESSAGE_3

    pos = buffer.tell()
    box_stream.close()
    buffer.seek(pos)
    assert await buffer.read() == MESSAGE_CLOSED


@pytest.mark.asyncio
async def test_unboxstream():
    buffer = AsyncBuffer(MESSAGE_1 + MESSAGE_2 + MESSAGE_3 + MESSAGE_CLOSED)
    buffer.seek(0)

    unbox_stream = UnboxStream(buffer, CLIENT_ENCRYPT_KEY, CLIENT_ENCRYPT_NONCE)
    assert not unbox_stream.closed
    assert (await async_comprehend(unbox_stream)) == [b'foo', b'foo', b'bar']
    assert unbox_stream.closed


@pytest.mark.asyncio
async def test_long_packets():
    data_size = 6 * 1024
    data = bytes(n % 256 for n in range(data_size))

    # box 6K buffer
    buffer = AsyncBuffer()
    box_stream = BoxStream(buffer, CLIENT_ENCRYPT_KEY, CLIENT_ENCRYPT_NONCE)
    box_stream.write(data)
    # the size overhead corresponds to the two packet headers
    assert buffer.tell() == data_size + (HEADER_LENGTH * 2)
    buffer.seek(0)

    # now let's unbox it and check whether it's OK
    unbox_stream = UnboxStream(buffer, CLIENT_ENCRYPT_KEY, CLIENT_ENCRYPT_NONCE)
    first_packet = await unbox_stream.read()
    assert first_packet == data[:4096]
    second_packet = await unbox_stream.read()
    assert second_packet == data[4096:]
<|endoftext|>"
},
{
"prompt": "

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# (C) Datadog, Inc. 2018
# All rights reserved
# Licensed under a 3-clause BSD style license (see LICENSE)

import pytest
from requests.exceptions import ConnectionError

from datadog_checks.gitlab_runner import GitlabRunnerCheck

from .common import ALLOWED_METRICS, BAD_CONFIG, CONFIG, CUSTOM_TAGS, GITLAB_RUNNER_TAGS, HOST


def test_check(aggregator):
    """
    Basic Test for gitlab integration.
    """
    gitlab_runner = GitlabRunnerCheck('gitlab_runner', CONFIG['init_config'], {}, instances=CONFIG['instances'])

    gitlab_runner.check(CONFIG['instances'][0])

    aggregator.assert_service_check(
        GitlabRunnerCheck.MASTER_SERVICE_CHECK_NAME,
        status=GitlabRunnerCheck.OK,
        tags=GITLAB_RUNNER_TAGS + CUSTOM_TAGS,
        count=1,
    )

    aggregator.assert_service_check(
        GitlabRunnerCheck.PROMETHEUS_SERVICE_CHECK_NAME, status=GitlabRunnerCheck.OK, tags=CUSTOM_TAGS, count=1
    )

    for metric in ALLOWED_METRICS:
        if metric.startswith('ci_runner'):
            aggregator.assert_metric("gitlab_runner.{}".format(metric))
        else:
            aggregator.assert_metric("gitlab_runner.{}".format(metric), tags=CUSTOM_TAGS, count=1)


def test_connection_failure(aggregator):
    """
    Make sure we're failing when the URL isn't right
    """

    gitlab_runner = GitlabRunnerCheck('gitlab', BAD_CONFIG['init_config'], {}, instances=BAD_CONFIG['instances'])

    with pytest.raises(ConnectionError):
        gitlab_runner.check(BAD_CONFIG['instances'][0])

    # We should get two failed service checks
    aggregator.assert_service_check(
        GitlabRunnerCheck.MASTER_SERVICE_CHECK_NAME,
        status=GitlabRunnerCheck.CRITICAL,
        tags=['gitlab_host:{}'.format(HOST), 'gitlab_port:1234'] + CUSTOM_TAGS,
        count=1,
    )

    aggregator.assert_service_check(
        GitlabRunnerCheck.PROMETHEUS_SERVICE_CHECK_NAME, status=GitlabRunnerCheck.CRITICAL, tags=CUSTOM_TAGS, count=1
    )
<|endoftext|>"
},
{
"prompt": """"Base factory."""
class BaseFactory(SQLAlchemyModelFactory):


"""User factory."""
class UserFactory(BaseFactory):
	username = Sequence(lambda n: f'user{n}')
	password = PostGenerationMethodCall('set_password', 'example')
	active = True


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# -*- coding: utf-8 -*-
"""Model unit tests."""
import datetime as dt

import pytest

from radio_chaser.user.models import Role, User

from .factories import UserFactory


@pytest.mark.usefixtures("db")
class TestUser:
    """User tests."""

    def test_get_by_id(self):
        """Get user by ID."""
        user = User("foo", "foo@bar.com")
        user.save()

        retrieved = User.get_by_id(user.id)
        assert retrieved == user

    def test_created_at_defaults_to_datetime(self):
        """Test creation date."""
        user = User(username="foo")
        user.save()
        assert bool(user.created_at)
        assert isinstance(user.created_at, dt.datetime)

    def test_password_is_nullable(self):
        """Test null password."""
        user = User(username="foo")
        user.save()
        assert user.password is None

    def test_factory(self, db):
        """Test user factory."""
        user = UserFactory(password="myprecious")
        db.session.commit()
        assert bool(user.username)
        assert bool(user.created_at)
        assert user.is_admin is False
        assert user.active is True
        assert user.check_password("myprecious")

    def test_check_password(self):
        """Check password."""
        user = User.create(username="foo", password="foobarbaz123")
        assert user.check_password("foobarbaz123") is True
        assert user.check_password("barfoobaz") is False

    def test_full_name(self):
        """User full name."""
        user = UserFactory(first_name="Foo", last_name="Bar")
        assert user.full_name == "Foo Bar"

    def test_roles(self):
        """Add a role to a user."""
        role = Role(name="admin")
        role.save()
        user = UserFactory()
        user.roles.append(role)
        user.save()
        assert role in user.roles
<|endoftext|>"
},
{
"prompt": "class UserFactory(DjangoModelFactory):
	username = Faker('user_name')
	email = Faker('email')
	name = Faker('name')
	fn password(self, create: bool, extracted: Sequence[Any])


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from celery.result import EagerResult

from great_url_shortener.users.tasks import get_users_count
from great_url_shortener.users.tests.factories import UserFactory

pytestmark = pytest.mark.django_db


def test_user_count(settings):
    """A basic test to execute the get_users_count Celery task."""
    UserFactory.create_batch(3)
    settings.CELERY_TASK_ALWAYS_EAGER = True
    task_result = get_users_count.delay()
    assert isinstance(task_result, EagerResult)
    assert task_result.result == 3
<|endoftext|>"
},
{
"prompt": "class cq_directive(Directive):
	has_content = True
	required_arguments = 0
	optional_arguments = 2
	option_spec = {'height': directives.length_or_unitless, 'width': directives.length_or_percentage_or_unitless, 'align': directives.unchanged}
	fn run(self)


fn setup(app)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from glob import glob
from itertools import chain, count

from docutils.parsers.rst import directives, Directive
from docutils.core import publish_doctree
from docutils.utils import Reporter

import cadquery as cq
from cadquery import cqgi
from cadquery.cq_directive import cq_directive


def find_examples(pattern="examples/*.py"):

    for p in glob(pattern):
        with open(p, encoding="UTF-8") as f:
            code = f.read()

        yield code


def find_examples_in_docs(pattern="doc/*.rst"):

    # dummy CQ directive for code
    class dummy_cq_directive(cq_directive):

        codes = []

        def run(self):

            self.codes.append("\n".join(self.content))

            return []

    directives.register_directive("cadquery", dummy_cq_directive)

    # read and parse all rst files
    for p in glob(pattern):
        with open(p, encoding="UTF-8") as f:
            doc = f.read()

        publish_doctree(
            doc, settings_overrides={"report_level": Reporter.SEVERE_LEVEL + 1}
        )

    # yield all code snippets
    for c in dummy_cq_directive.codes:

        yield c


@pytest.mark.parametrize(
    "code", chain(find_examples(), find_examples_in_docs()), ids=count(0)
)
def test_example(code):

    # build
    res = cqgi.parse(code).build()

    assert res.exception is None

    # check if the resulting objects are valid
    for r in res.results:
        r = r.shape
        if isinstance(r, cq.Workplane):
            for v in r.vals():
                if isinstance(v, cq.Shape):
                    assert v.isValid()
        elif isinstance(r, cq.Shape):
            assert r.isValid()
<|endoftext|>"
},
{
"prompt": "class Blocked(Exception):


class TestRun:
	dut = None
	executor = None
	LOGGER: Log = None
	plugin_manager = None
	fn step(cls, message)
	fn group(cls, message)
	fn iteration(cls, iterable, group_name)
	fn fail(cls, message)
	fn block(cls, message)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "#
# Copyright(c) 2020 Intel Corporation
# SPDX-License-Identifier: BSD-3-Clause-Clear
#

import pytest
import sys
import importlib
from core.test_run import TestRun


class PluginManager:
    def __init__(self, item, config):
        if 'plugins_dir' in config:
            sys.path.append(config['plugins_dir'])
        self.plugins = {}

        self.plugins_config = config.get('plugins', {})

        self.req_plugins = config.get('req_plugins', {})
        self.opt_plugins = config.get('opt_plugins', {})

        self.req_plugins.update(dict(map(lambda mark: (mark.args[0], mark.kwargs),
                                item.iter_markers(name="require_plugin"))))

        req_plugin_mod = {}
        opt_plugin_mod = {}

        for name in self.req_plugins:
            try:
                req_plugin_mod[name] = self.__import_plugin(name)
            except ModuleNotFoundError:
                pytest.skip("Unable to find requested plugin!")

        for name in self.opt_plugins:
            try:
                opt_plugin_mod[name] = self.__import_plugin(name)
            except ModuleNotFoundError as e:
                TestRun.LOGGER.debug(
                    f"Failed to import '{name}' - optional plugin. " f"Reason: {e}"
                )
                continue

        for name, mod in req_plugin_mod.items():
            try:
                self.plugins[name] = mod.plugin_class(
                    self.req_plugins[name],
                    self.plugins_config.get(name, {}).get("config", {}))
            except Exception:
                pytest.skip(f"Unable to initialize plugin '{name}'")

        for name, mod in opt_plugin_mod.items():
            try:
                self.plugins[name] = mod.plugin_class(
                    self.opt_plugins[name],
                    self.plugins_config.get(name, {}).get("config", {}))
            except Exception as e:
                TestRun.LOGGER.debug(
                    f"Failed to initialize '{name}' - optional plugin. " f"Reason: {e}"
                )
                continue

    def __import_plugin(self, name):
        provided_by = self.plugins_config.get(name, {}).get("provided_by")
        if provided_by:
            return importlib.import_module(provided_by)

        try:
            return importlib.import_module(f"internal_plugins.{name}")
        except ModuleNotFoundError:
            pass

        return importlib.import_module(f"external_plugins.{name}")

    def hook_pre_setup(self):
        for plugin in self.plugins.values():
            plugin.pre_setup()

    def hook_post_setup(self):
        for plugin in self.plugins.values():
            plugin.post_setup()

    def hook_teardown(self):
        for plugin in self.plugins.values():
            plugin.teardown()

    def get_plugin(self, name):
        if name not in self.plugins:
            raise KeyError("Requested plugin does not exist")
        return self.plugins[name]
<|endoftext|>"
},
{
"prompt": "

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from icevision.all import *
from icevision.models.ultralytics.yolov5.backbones import *


def _test_preds(preds):
    assert len(preds) == 3
    assert isinstance(preds[0].detection.bboxes[0], BBox)
    assert len(preds[0].detection.scores) == len(preds[0].detection.labels)
    assert len(preds[2].detection.scores) == len(preds[2].detection.labels)


@pytest.mark.parametrize(
    "backbone",
    [small, medium, large],
)
def test_yolo_predict(fridge_ds, backbone):
    _, valid_ds = fridge_ds
    model = models.ultralytics.yolov5.model(
        num_classes=5, img_size=384, backbone=backbone(pretrained=True)
    )
    preds = models.ultralytics.yolov5.predict(model, valid_ds, detection_threshold=0.0)
    _test_preds(preds)


@pytest.mark.parametrize(
    "backbone",
    [small, medium, large],
)
def test_yolo_predict_from_dl(fridge_ds, backbone):
    _, valid_ds = fridge_ds
    infer_dl = models.ultralytics.yolov5.infer_dl(valid_ds, batch_size=1, shuffle=False)
    model = models.ultralytics.yolov5.model(
        num_classes=5, img_size=384, backbone=backbone(pretrained=True)
    )
    preds = models.ultralytics.yolov5.predict_from_dl(
        model, infer_dl, detection_threshold=0.0
    )
    _test_preds(preds)
<|endoftext|>"
},
{
"prompt": "class ErrorHandler(list):
	fn append(self, error)


fn process(input, filepath)

fn process_all(input, filepath, error_handler)

fn assert_like(d1, d2)

fn outdent(doc)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import glob
import os

import pytest

from .utils import assert_like, process_all


def simple_name(filename):
    return os.path.splitext(os.path.basename(filename))[0]


path = os.path.dirname(os.path.realpath(__file__))
files = [
    pytest.param(f, id=simple_name(f))
    for f in glob.glob(path + "/samples/**/*.yml", recursive=True)
]

os.environ["YGLU_ENABLE_ENV"] = "true"


@pytest.mark.parametrize("filename", files)
def test_sample(filename):
    with open(filename) as file_handle:
        (input, output) = process_all(file_handle, filename, FailingErrorHandler())
        assert_like(input, output)

class FailingErrorHandler(list):
    def __init__(self):
        self.nodes = set()

    def append(self, error):
        raise error
<|endoftext|>"
},
{
"prompt": "class CounterWrapper:
	fn __init__(self, func)
	fn __call__(self)
	fn count(self, integrator)


fn valid_args_id(args)

fn valid_args(request)

fn invalid_args_id(args)

fn invalid_args(request)

fn _test_moves_id(args)

fn _test_moves_args(_valid_args)

fn test_moves_args(request)

fn _cpp_args(_valid_args)

fn cpp_args_id(args)

fn cpp_args(request)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Copyright (c) 2009-2022 The Regents of the University of Michigan.
# Part of HOOMD-blue, released under the BSD 3-Clause License.

"""Test hoomd.hpmc.nec.integrate.Sphere."""

import hoomd
import hoomd.hpmc.tune
from hoomd.conftest import operation_pickling_check

import pytest
import math

# from test_sphere_eos.py for hoomd-2.9
# see for example Guang-Wen Wu and Richard J. Sadus, doi:10.1002.aic10233
_phi_p_ref = [(0.29054, 0.1), (0.91912, 0.2), (2.2768, 0.3), (5.29102, 0.4),
              (8.06553, 0.45), (9.98979, 0.475)]
rel_err_cs = 0.0015


@pytest.mark.parametrize("betap,phi", _phi_p_ref)
@pytest.mark.serial
@pytest.mark.validate
def test_sphere_eos_nec(betap, phi, simulation_factory,
                        lattice_snapshot_factory):
    """Test that NEC runs and computes the pressure correctly."""
    n = 7

    v_particle = 4 / 3 * math.pi * (0.5)**3
    lattice_length = (v_particle / phi)**(1. / 3)
    snap = lattice_snapshot_factory(n=n, a=lattice_length)

    sim = simulation_factory(snap)
    sim.seed = 123456
    sim.state.thermalize_particle_momenta(hoomd.filter.All(), kT=1)

    mc = hoomd.hpmc.nec.integrate.Sphere(default_d=0.05, update_fraction=0.05)
    mc.shape['A'] = dict(diameter=1)
    mc.chain_time = 0.05
    sim.operations.integrator = mc

    triggerTune = hoomd.trigger.Periodic(50, 0)
    tune_nec_d = hoomd.hpmc.tune.MoveSize.scale_solver(
        triggerTune,
        moves=['d'],
        target=0.10,
        tol=0.001,
        max_translation_move=0.15)
    sim.operations.tuners.append(tune_nec_d)

    tune_nec_ct = hoomd.hpmc.nec.tune.ChainTime.scale_solver(triggerTune,
                                                             target=20.0,
                                                             tol=1.0,
                                                             gamma=20.0)
    sim.operations.tuners.append(tune_nec_ct)

    # equilibrate
    sim.run(1000)
    sim.operations.tuners.clear()

    pressures = []
    mc.nselect = 50
    for i in range(100):
        sim.run(1)
        pressures.append(mc.virial_pressure)

    mean = sum(pressures) / len(pressures)
    variance = sum(p**2 for p in pressures) / len(pressures) - mean**2
    std_dev = variance**0.5
    error = std_dev / (len(pressures) - 1)**0.5

    # confidence interval, 0.95 quantile of the normal distribution
    ci = 1.96

    assert math.isclose(betap, mean, abs_tol=ci * (error + rel_err_cs * betap))
    assert error < 1

    mcCounts = mc.counters
    necCounts = mc.nec_counters

    assert mcCounts.overlap_errors == 0
    assert necCounts.overlap_errors == 0


nec_test_parameters = [
    (
        hoomd.hpmc.nec.integrate.Sphere,
        dict(default_d=0.2, chain_time=0.75, update_fraction=0.125, nselect=2),
        dict(diameter=1),
    ),
    (
        hoomd.hpmc.nec.integrate.ConvexPolyhedron,
        dict(default_d=0.2,
             default_a=0.1,
             chain_probability=0.5,
             chain_time=0.75,
             update_fraction=0.125,
             nselect=2),
        dict(vertices=[[1, 1, 1], [1, 1, -1], [1, -1, 1], [1, -1, -1],
                       [-1, 1, 1], [-1, 1, -1], [-1, -1, 1], [-1, -1, -1]]),
    ),
]


@pytest.mark.serial
@pytest.mark.parametrize('integrator_cls, integrator_args, shape',
                         nec_test_parameters)
def test_pickling(integrator_cls, integrator_args, shape, simulation_factory,
                  two_particle_snapshot_factory):
    mc = integrator_cls(**integrator_args)
    mc.shape["A"] = shape
    sim = simulation_factory(two_particle_snapshot_factory(L=1000))
    operation_pickling_check(mc, sim)
<|endoftext|>"
},
{
"prompt": "class Persistence:
	fn __get__(self, keyring, type)
	"""Set the persistence value on the Keyring. Value may be
	one of the win32cred.CRED_PERSIST_* constants or a
	string representing one of those constants. For example,
	'local machine' or 'session'."""
	fn __set__(self, keyring, value)


"""WinVaultKeyring stores encrypted passwords using the Windows Credential
Manager.

Requires pywin32

This backend does some gymnastics to simulate multi-user support,
which WinVault doesn't support natively. See
https://bitbucket.org/kang/python-keyring-lib/issue/47/winvaultkeyring-only-ever-returns-last#comment-731977
for details on the implementation, but here's the gist:

Passwords are stored under the service name unless there is a collision
(another password with the same service name but different user name),
in which case the previous password is moved into a compound name:
{username}@{service}"""
class WinVaultKeyring(KeyringBackend):
	persist = Persistence()
	"""If available, the preferred backend on Windows."""
	fn priority(cls)
	fn _compound_name(username, service)
	fn get_password(self, service, username)
	fn _get_password(self, target)
	fn set_password(self, service, username, password)
	fn _set_password(self, target, username, password)
	fn delete_password(self, service, username)
	fn _delete_password(self, target)
	fn get_credential(self, service, username)


"""A compatibility wrapper for old PyWin32 errors, such as reported in
https://bitbucket.org/kang/python-keyring-lib/issue/140/"""
class OldPywinError:
	fn __init__(self, orig)
	fn funcname(self)
	fn winerror(self)
	fn wrap(cls, orig_err)


fn is_ascii_printable(s)

"""Test for the keyring's basic functions. password_set and password_get
    """
class BackendBasicTests:
	DIFFICULT_CHARS = string.whitespace + string.punctuation
	fn _init_properties(self, request)
	fn cleanup(self)
	fn set_password(self, service, username, password)
	fn check_set_get(self, service, username, password)
	fn test_password_set_get(self)
	fn test_difficult_chars(self)
	fn test_delete_present(self)
	fn test_delete_not_present(self)
	fn test_delete_one_in_group(self)
	fn test_name_property(self)
	fn test_unicode_chars(self)
	fn test_unicode_and_ascii_chars(self)
	"""Issue #47 reports that WinVault isn't storing passwords for
	multiple users. This test exercises that test for each of the
	backends."""
	fn test_different_user(self)
	fn test_credential(self)
	fn test_set_properties(self, monkeypatch)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import sys

import pytest

import keyring.backends.Windows
from keyring.testing.backend import BackendBasicTests


@pytest.mark.skipif(
    not keyring.backends.Windows.WinVaultKeyring.viable, reason="Needs Windows"
)
class WinVaultKeyringTestCase(BackendBasicTests):
    def tearDown(self):
        # clean up any credentials created
        for cred in self.credentials_created:
            try:
                self.keyring.delete_password(*cred)
            except Exception as e:
                print(e, file=sys.stderr)

    def init_keyring(self):
        return keyring.backends.Windows.WinVaultKeyring()


@pytest.mark.skipif('sys.platform != "win32"')
def test_winvault_always_viable():
    """
    The WinVault backend should always be viable on Windows.
    """
    assert keyring.backends.Windows.WinVaultKeyring.viable
<|endoftext|>"
},
{
"prompt": "fn calculate_price_without_vat(price_with_vat)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from decimal import Decimal

import pytest

from parking_permits_app.pricing.value_added_tax import calculate_price_without_vat


@pytest.mark.parametrize(
    "price_with_vat, price_without_vat",
    [
        (Decimal("124"), Decimal("100")),
        (Decimal("60"), Decimal("48.39")),
        (Decimal("10000"), Decimal("8064.52")),
        (Decimal("32.75"), Decimal("26.41")),
        (Decimal("0"), Decimal("0")),
    ],
)
def test_calculate_price_without_vat_function_returns_correct_result(
    price_with_vat, price_without_vat
):
    assert calculate_price_without_vat(price_with_vat) == price_without_vat
<|endoftext|>"
},
{
"prompt": """"Function to load sample datasets.

Order of read:
(1) Tries to read dataset from local folder first.
(2) Then tries to read dataset from folder in GitHub "address" (see below)
(3) Then tries to read from sktime (if installed)
(4) Raises error if none exist

List of available datasets on GitHub can be checked using
(1) ``get_data('index')`` or
(2) ``get_data('index', folder='time_series/seasonal)``
(see available "folder" options below)


Example
-------
>>> from pycaret.datasets import get_data
>>> all_datasets = get_data('index')
>>> juice = get_data('juice')


dataset: str, default = 'index'
    Index value of dataset.


folder: Optional[str], default = None
    The folder from which to get the data.
    If 'None', gets it from the "common" folder. Other options are:
        - time_series/seasonal
        - time_series/random_walk
        - time_series/white_noise


save_copy: bool, default = False
    When set to true, it saves a copy in current working directory.


profile: bool, default = False
    When set to true, an interactive EDA report is displayed.


verbose: bool, default = True
    When set to False, head of data is not displayed.

address: string, default = None
    Download url of dataset. Defaults to None which fetches the dataset from
    "https://raw.githubusercontent.com/pycaret/datasets/main/". For people
    having difficulty linking to github, they can change the default address
    to their own
    (e.g. "https://gitee.com/IncubatorShokuhou/pycaret/raw/master/datasets/")


Returns:
    pandas.DataFrame


Warnings
--------
- Use of ``get_data`` requires internet connection.


Raises
------
ImportError
    (1) When trying to import time series datasets that require sktime,
    but sktime has not been installed.
    (2) If the data does not exist"""
fn get_data(dataset: str, folder: Optional[str], save_copy: bool, profile: bool, verbose: bool, address: Optional[str])

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import os, sys

sys.path.insert(0, os.path.abspath(".."))

import pandas as pd
import numpy as np
import pytest
import pycaret.classification
import pycaret.datasets
from IPython.display import display

def test():

    # loading dataset
    data = pycaret.datasets.get_data("blood")

    # initialize setup
    clf1 = pycaret.classification.setup(
        data,
        target="Class",
        silent=True,
        html=False,
        n_jobs=1,
    )

    # EDA
    pycaret.classification.eda(display_format = 'svg')

    # assert
    assert 1 == 1

if __name__ == "__main__":
    test()
<|endoftext|>"
},
{
"prompt": "class UserFactory(DjangoModelFactory):
	username = Faker('user_name')
	email = Faker('email')
	name = Faker('name')
	fn password(self, create: bool, extracted: Sequence[Any])


"""Default user for Qiubit Test."""
class User(AbstractUser):
	name = CharField(_('Name of User'), blank=True, max_length=255)
	first_name = None
	last_name = None
	"""Get url for user's detail view.
	
	Returns:
	    str: URL for user detail."""
	fn get_absolute_url(self) -> str


class UserDetailView(LoginRequiredMixin, DetailView):
	model = User
	slug_field = 'username'
	slug_url_kwarg = 'username'


class UserUpdateView(LoginRequiredMixin, SuccessMessageMixin, UpdateView):
	model = User
	fields = ['name']
	success_message = _('Information successfully updated')
	fn get_success_url(self)
	fn get_object(self)


class UserRedirectView(LoginRequiredMixin, RedirectView):
	permanent = False
	fn get_redirect_url(self)


class UserChangeForm(admin_forms.UserChangeForm):


class UserCreationForm(admin_forms.UserCreationForm):


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from django.conf import settings
from django.contrib import messages
from django.contrib.auth.models import AnonymousUser
from django.contrib.messages.middleware import MessageMiddleware
from django.contrib.sessions.middleware import SessionMiddleware
from django.test import RequestFactory
from django.urls import reverse

from qiubit.users.forms import UserChangeForm
from qiubit.users.models import User
from qiubit.users.tests.factories import UserFactory
from qiubit.users.views import (
    UserRedirectView,
    UserUpdateView,
    user_detail_view,
)

pytestmark = pytest.mark.django_db


class TestUserUpdateView:
    """
    TODO:
        extracting view initialization code as class-scoped fixture
        would be great if only pytest-django supported non-function-scoped
        fixture db access -- this is a work-in-progress for now:
        https://github.com/pytest-dev/pytest-django/pull/258
    """

    def test_get_success_url(self, user: User, rf: RequestFactory):
        view = UserUpdateView()
        request = rf.get("/fake-url/")
        request.user = user

        view.request = request

        assert view.get_success_url() == f"/users/{user.username}/"

    def test_get_object(self, user: User, rf: RequestFactory):
        view = UserUpdateView()
        request = rf.get("/fake-url/")
        request.user = user

        view.request = request

        assert view.get_object() == user

    def test_form_valid(self, user: User, rf: RequestFactory):
        view = UserUpdateView()
        request = rf.get("/fake-url/")

        # Add the session/message middleware to the request
        SessionMiddleware().process_request(request)
        MessageMiddleware().process_request(request)
        request.user = user

        view.request = request

        # Initialize the form
        form = UserChangeForm()
        form.cleaned_data = []
        view.form_valid(form)

        messages_sent = [m.message for m in messages.get_messages(request)]
        assert messages_sent == ["Information successfully updated"]


class TestUserRedirectView:
    def test_get_redirect_url(self, user: User, rf: RequestFactory):
        view = UserRedirectView()
        request = rf.get("/fake-url")
        request.user = user

        view.request = request

        assert view.get_redirect_url() == f"/users/{user.username}/"


class TestUserDetailView:
    def test_authenticated(self, user: User, rf: RequestFactory):
        request = rf.get("/fake-url/")
        request.user = UserFactory()

        response = user_detail_view(request, username=user.username)

        assert response.status_code == 200

    def test_not_authenticated(self, user: User, rf: RequestFactory):
        request = rf.get("/fake-url/")
        request.user = AnonymousUser()

        response = user_detail_view(request, username=user.username)
        login_url = reverse(settings.LOGIN_URL)

        assert response.status_code == 302
        assert response.url == f"{login_url}?next=/fake-url/"
<|endoftext|>"
},
{
"prompt": "fn somar(x, y)

fn multiplicar(x, y)

fn subtrair(x, y)

fn divisao(x, y)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from Principal import somar
from principal import multiplicar
from principal import subtrair
from principal import divisao


def teste_somar():
    assert somar(2,4)==6

def teste_multiplicar():
    assert multiplicar(6,12)==72

def teste_subtrair():
    assert subtrair(290-800)==610

def teste_divisao():
    assert divisao(6,2)== 3<|endoftext|>"
},
{
"prompt": "fn dynamoGetMap(DiningCommon, metric)

fn doesNotHaveMeal(diningCommon, mealTime)

fn hours(diningCommon, mealTime)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "#test_hours.py

import boto3
import json
import os
import pytest
import decimal
from boto3.dynamodb.conditions import Key, Attr

from hoursFunctions import *

client = boto3.resource('dynamodb',aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'], aws_secret_access_key=os.environ[
'AWS_SECRET_ACCESS_KEY'], region_name='us-east-1')
table = client.Table('GauchoEats')

def test_hours():
    isWeekend = True if (str(dynamoGetMap("dlg", "isWeekend")) == "True") else False
    print("testing dlg breakfast: ")
    dlgBreakfast = hours("dlg", "breakfast")
    assert dlgBreakfast == "Dlg doesn't have breakfast."
    if isWeekend == False:
        print("testing dlg lunch: ")
        assert hours("dlg", "lunch") == "D.L.G. is open from 11:00 AM to 2:30 PM for lunch"
        print("testing dlg dinner: ")
        assert hours("dlg", "dinner") == "D.L.G. is open from 5:00 PM to 8:00 PM for dinner"
        print("testing dlg late-night: ")
        assert hours("dlg", "late-night") == "D.L.G. is open from 9:00 PM to 12:30 AM for late-night"
        print("testing carrillo breakfast:")
        assert hours("carrillo", "breakfast") == "Carrillo is open from 7:15 AM to 10:00 AM for breakfast"
        print("testing carrillo lunch")
        assert hours("carrillo", "lunch") == "Carrillo is open from 11:00 AM to 2:30 PM for lunch"
        print("testing carrillo dinner:")
        assert hours("carrillo", "dinner") == "Carrillo is open from 5:00 PM to 8:00 PM for dinner"
        print("testing ortega breakfast:")
        assert hours("ortega", "breakfast") == "Ortega is open from 7:15 AM to 10:00 AM for breakfast"
        print("testing ortega lunch:")
        assert hours("ortega", "lunch") == "Ortega is open from 11:00 AM to 2:30 PM for lunch"
        print("testing ortega dinner:")
        assert hours("ortega", "dinner") == "Ortega is open from 5:00 PM to 8:00 PM for dinner"
    else:
        print("testing carrillo brunch:")
        assert hours("carrillo", "brunch") == "Carrillo is open from 10:30 AM to 2:00 PM for brunch"
        print("testing carrillo dinner:")
        assert hours("carrillo", "dinner") == "Carrillo is open from 5:00 PM to 8:00 PM for dinner"
        print("testing dlg brunch:")
        assert hours("dlg", "brunch") == "D.L.G. is open from 10:30 AM to 2:00 PM for brunch"
        print("testing dlg dinner: ")
        assert hours("dlg", "dinner") == "D.L.G. is open from 5:00 PM to 8:00 PM for dinner"
<|endoftext|>"
},
{
"prompt": """"GFGenome class"""
class GFGenome(pyensembl.Genome):
	fn __init__(self, reference_name: object, annotation_name: object, annotation_version: object, gtf_path_or_url: object, transcript_fasta_paths_or_urls: object, protein_fasta_paths_or_urls: object, decompress_on_download: object, copy_local_files_to_cache: object, require_ensembl_ids: object, cache_directory_path: object, copy_genome)
	fn check_fasta_dictionary(self)
	"""Get GFTranscript objects. Optionally restrict to a particular chromosome and strand
	
	:param contig: optional, chromosome transcripts for which are required
	:type contig: int
	:param strand: optional, chromosome strand transcripts for which are required
	:type strand: str
	:return: list of gfeat.transcript.GFTranscript"""
	fn transcripts(self, contig: int, strand: str)
	"""Get GFTranscript objects by the specified transcript name
	
	:param transcript_name: name of the transcripts
	:type transcript_name: str
	:return: gfeat.transcript.GFTranscript object"""
	fn transcripts_by_name(self, transcript_name: str)
	"""Get GFTranscript objects by the specified protein id
	
	:param protein_id: protein id transcript for which is required
	:type protein_id: str
	:return: gfeat.transcript.GFTranscript object"""
	fn transcript_by_protein_id(self, protein_id: str)
	"""Get GFTranscript object by the transcript id
	
	:param transcript_id: id of the transcript
	:type transcript_id: str
	:return: gfeat.transcript.GFTranscript object"""
	fn gftranscript_by_id(self, transcript_id: str)
	"""Get the consensus Kozak sequence (Kozak sequence which is present more times than all other for this being)
	
	:param seq: True – to return the letter representation of the sequence
	
	    False – to return the digit representation of the sequence: 0 – A, 1 – C, 2 – G, 3 – T
	
	:type seq: bool
	:return: string, consensus Kozak sequence constituting of either digits or letters depending on the specified
	        seq parameter"""
	fn get_consensus_Kozak_seq(self, seq: bool)
	"""Get Kozak matrix with all transcripts for this being. Consensus columns are removed
	
	:return: pandas.DataFrame,
	
	        column names – first number corresponds to the base position in the Kozak sequence,
	        e.g. 0A means base A 5 bases upstream from the start codon
	
	        rows – 1 if it has the corresponding base, 0 otherwise
	
	NOTE: columns corresponding to the most frequent bases at each position are removed"""
	fn get_Kozak_matrix(self)
	"""Get the consensus stop codon context sequence (stop codon context sequence which is present more times than
	all other for this being)
	
	:param seq: True – to return the letter representation of the sequence
	
	                  False – to return the digit representation of the sequence: 0 – A, 1 – C, 2 – G, 3 – T
	:type seq: bool
	:return: string, consensus stop codon context sequence constituting of either digits or letters
	        depending on the specified seq parameter"""
	fn get_consensus_stop_codon_context(self, seq: bool)
	"""Get stop codon context matrix with all transcripts for this being. Consensus columns are removed
	
	:return: pandas.DataFrame,
	
	        column names – first number corresponds to the base position in the stop codon
	        context sequence, e.g. 0A means base A 5 bases upstream from the start codon
	
	        rows – 1 if it has the corresponding base, 0 otherwise
	
	NOTE: columns corresponding to the most frequent bases at each position are removed"""
	fn get_stop_codon_context_matrix(self)
	"""Get truncated PCA of the codon frequency matrix with 2 principal components
	
	:return: pandas.DataFrame, truncated PCA of the codon frequency matrix with 2 principal components"""
	fn get_codon_pair_bias(self)
	"""Get a table which shows whether a certain nucleobase in Kozak sequence or stop codon context was mutated or not.
	
	:param vcf: path to the vcf.gz or file opened using cyvcf2
	:type vcf: string or an "opened" file
	:return: pd.DataFrame,
	
	        column names – K_i – where i shows position in Kozak sequence;
	        S_i – where i shows position in stop codon context; gene_id; transcript_id
	
	        rows – NaN – no variant, 1 – heterozygous variant, 2 – homozygous variant"""
	fn get_nucleobase_mutation_table(self, vcf: string or an "opened" file)


class GFGene(pyensembl.Gene):
	fn from_pyensembl(cls, obj)
	fn iter_all(cls, genome)
	"""Calculate how many times the given codon is present in all transcript sequences corresponding to this gene
	:param codon: string, codon to be found in the sequence
	:return: how many times the given codon is present in the sequence"""
	fn coding_sequence_codon_count(self, codon: None)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from gfeat.genome import GFGenome
from gfeat.gene import GFGene


@pytest.fixture()
def gene():
    data1 = GFGenome(reference_name='hg38_test',
                     annotation_name='hg38_chr22_test',
                     gtf_path_or_url="./tests/data/gencode.v24.annotation_chr22.gtf",
                     transcript_fasta_paths_or_urls="./tests/data/hg38_chr22.fa",
                     )
    test_gene = GFGene('ENSG00000008735.13', 'MAPK8IP2', '22', 50600685, 50613981, '+', None, data1)
    return test_gene
<|endoftext|>"
},
{
"prompt": "class Node:
	fn __init__(self, val)
	fn __repr__(self)
	fn __str__(self)


class BST:
	fn __init__(self, iter)
	fn __repr__(self)
	fn __str__(self)
	fn in_order(self, operation)
	fn pre_order(self, operation)
	fn post_order(self, operation)
	fn insert(self, val)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from bst import BST as bst

@pytest.fixture
def empty_bst():
    """ 
    returns empty tree
    """
    return bst()

@pytest.fixture
def full_bst():
    """ 
    creates a tree with values to check
    """
    return bst([3, 5, 8, 15, 30, 12, 1, 21])


<|endoftext|>"
},
{
"prompt": "class HTTPClient:
	fn __init__(self, base_url: str, session: requests.Session)
	fn get_html(self, params: dict, url_path: str) -> str


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
import requests
from mockito import mock
from requests import Response
from io import BytesIO as BufferIO

from projectx1.http_client import HTTPClient


@pytest.fixture
def session_factory(when):
    def factory(return_value):
        session = mock(requests.Session)
        if isinstance(return_value, requests.exceptions.RequestException):
            when(session).get(...).thenRaise(return_value)
        else:
            when(session).get(...).thenReturn(return_value)
        
        return session

    return factory


def test_http_client_get_html_successful_connection_to_server(session_factory):
    expected_page_html = '<html>website</html>'
    
    expected_response = Response()
    expected_response.status_code = 200
    expected_response.encoding = 'utf-8'
    expected_response.raw = BufferIO(expected_page_html.encode('utf-8'))

    session = session_factory(return_value=expected_response)
    http_client = HTTPClient(base_url='https://www.example.com', session=session)
    page_html = http_client.get_html(params={'page': 0})
    
    assert expected_page_html == page_html


def test_http_client_get_html_request_exception_occurred(session_factory):
    expected_page_html = ''

    session = session_factory(return_value=requests.exceptions.ConnectionError())
    http_client = HTTPClient(base_url='https://www.example.com', session=session)
    page_html = http_client.get_html(params={'page': 0})

    assert expected_page_html == page_html
<|endoftext|>"
},
{
"prompt": """"Ask a yes/no question via raw_input() and return their answer.

Args:
    question (str): Question hat is presented to the user.
    default (str): The presumed answer, if the user just hits <Enter>.
        It must be "yes" (the default), "no" or None (meaning
        an answer is required of the user).

Returns (boolean):
    True, if 'yes', False otherwise."""
fn query_yes_no(question: str, default: str)

"""Ask for user input.

This asks a yes/no question with a preset default.
You can bypass the user-input and fetch the default answer, if
you set

Args:
    question: The question to ask on stdout.
    default_answer: The default value to return.
    default_answer_str:
        The default answer string that we present to the user."""
fn ask(question: str, default_answer: bool, default_answer_str: str) -> bool

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """"Test user interface utility methods."""
import os

import pytest

from benchbuild.utils.user_interface import ask


@pytest.fixture
def env():
    os.putenv("TEST", "yes")


def test_ask(env):
    assert ask("Test?", default_answer=True) == True
    assert ask("Test?", default_answer=False) == False
<|endoftext|>"
},
{
"prompt": "fn select(value: Union[(Selector, Condition, str)]) -> Selector

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import marshmallow
import pytest

from aiosnow import fields
from aiosnow.query.utils import select


def test_fields_boolean_type():
    assert isinstance(fields.Boolean(), fields.Boolean)


def test_fields_boolean_deserialize_plain_valid(mock_boolean_field):
    f_bool = mock_boolean_field("test_bool")

    assert f_bool.deserialize(True) is True
    assert f_bool.deserialize(1) is True
    assert f_bool.deserialize(False) is False
    assert f_bool.deserialize(0) is False


def test_fields_boolean_deserialize_plain_invalid(mock_boolean_field):
    with pytest.raises(marshmallow.ValidationError):
        mock_boolean_field("test_bool").deserialize(3)


def test_fields_boolean_select_falsy(mock_boolean_field):
    assert (
        select(mock_boolean_field("test_bool").is_falsy()).sysparms == "test_bool!=true"
    )


def test_fields_boolean_select_true(mock_boolean_field):
    assert (
        select(mock_boolean_field("test_bool").is_true()).sysparms == "test_bool=true"
    )
<|endoftext|>"
},
{
"prompt": """"Validation for the essential parameters of step class"""
class EssentialParameters(object):
	"""Args:
	    cls_name: class name which has validation target parameters
	    param_list: list of validation target parameters"""
	fn __init__(self, cls_name, param_list)
	fn __call__(self)


"""Validation for the table of sqlite"""
class SqliteTableExistence(object):
	"""Args:
	    dbname: database name
	    tblname: table name
	    returns_bool: return bool or not"""
	fn __init__(self, dbname, tblname, returns_bool)
	fn __call__(self)


"""Validation for io: input"""
class IOInput(object):
	fn __init__(self, io)
	fn __call__(self)


"""Validation for io: input"""
class IOOutput(object):
	fn __init__(self, io)
	fn __call__(self)


class CliboaException(Exception):


class DirStructureInvalid(CliboaException):


class FileNotFound(CliboaException):


class InvalidFileCount(CliboaException):


class InvalidParameter(CliboaException):


class InvalidCount(CliboaException):


class InvalidFormat(CliboaException):


class ScenarioFileInvalid(CliboaException):


class StepExecutionFailed(CliboaException):


class SqliteInvalid(CliboaException):


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "#
# Copyright 2019 BrainPad Inc. All Rights Reserved.
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
import os
import shutil
import sqlite3

import pytest

from cliboa.conf import env
from cliboa.scenario.validator import EssentialParameters, SqliteTableExistence
from cliboa.util.exception import CliboaException, SqliteInvalid


class TestEssentialParameters(object):
    def test_essential_parameters_ng(self):
        """
        EssentialParameters invalid case
        """
        with pytest.raises(CliboaException) as excinfo:
            valid = EssentialParameters("DummyClass", [""])
            valid()
        assert "is not specified" in str(excinfo.value)


class TestSqliteTableExistence(object):
    def setup_method(self, method):
        self.__db_dir = os.path.join(env.BASE_DIR, "db")

    def test_table_existence_ng_with_exc(self):
        """
        SqliteTableExistende invalid case
        """
        # create test db and insert dummy data
        os.makedirs(self.__db_dir)
        db_file = os.path.join(self.__db_dir, "spam.db")
        conn = sqlite3.connect(db_file)
        conn.execute("create table spam_table (id, name, age);")
        conn.execute("insert into spam_table (id, name, age) values(1,1,1);")
        conn.commit()
        conn.close()

        with pytest.raises(SqliteInvalid) as excinfo:
            valid = SqliteTableExistence(db_file, "spam_table2")
            valid()
        shutil.rmtree(self.__db_dir)
        assert "not found" in str(excinfo.value)

    def test_table_existence_ng_with_bool(self):
        """
        SqliteTableExistende invalid case
        """
        # create test db and insert dummy data
        os.makedirs(self.__db_dir)
        db_file = os.path.join(self.__db_dir, "spam.db")
        conn = sqlite3.connect(db_file)
        conn.execute("create table spam_table (id, name, age);")
        conn.execute("insert into spam_table (id, name, age) values(1,1,1);")
        conn.commit()
        conn.close()

        valid = SqliteTableExistence(db_file, "spam_table2", True)
        exists_tbl = valid()
        shutil.rmtree(self.__db_dir)
        assert exists_tbl is False

    def test_table_existence_ok_with_bool(self):
        """
        SqliteTableExistende invalid case
        """
        # create test db and insert dummy data
        os.makedirs(self.__db_dir)
        db_file = os.path.join(self.__db_dir, "spam.db")
        conn = sqlite3.connect(db_file)
        conn.execute("create table spam_table (id, name, age);")
        conn.execute("insert into spam_table (id, name, age) values(1,1,1);")
        conn.commit()
        conn.close()

        valid = SqliteTableExistence(db_file, "spam_table", True)
        exists_tbl = valid()
        shutil.rmtree(self.__db_dir)
        assert exists_tbl is True
<|endoftext|>"
},
{
"prompt": """"Client to use in the SQL databases integration. Overrides BaseClient Overrides BaseClient
makes the connection to the DB server"""
class Client:
	fn __init__(self, dialect: str, host: str, username: str, password: str, port: str, database: str, connect_parameters: str)
	"""Converting a dialect to the correct string needed in order to connect the wanted dialect
	:param dialect: the SQL db
	:return: a key string needed for the connection"""
	fn _convert_dialect_to_module(dialect: str) -> str
	"""Creating and engine according to the instance preferences and connecting
	:return: a connection object that will be used in order to execute SQL queries"""
	fn _create_engine_and_connect(self) -> sqlalchemy.engine.base.Connection
	"""Execute query in DB via engine
	:param bind_vars: in case there are names and values - a bind_var dict, in case there are only values - list
	:param sql_query: the SQL query
	:return: results of query, table headers"""
	fn sql_query_execute_request(self, sql_query: str, bind_vars: Any) -> Tuple[(Dict, List)]


"""In case no port was chosen, a default port will be chosen according to the SQL db type
:param dialect: sql db type
:return: default port needed for connection"""
fn generate_default_port_by_dialect(dialect: str) -> str

"""The bind variables can be given in 2 legal ways: as 2 lists - names and values, or only values
any way defines a different executing way, therefore there are 2 legal return types
:param bind_variables_names: the names of the bind variables, must be in the length of the values list
:param bind_variables_values: the values of the bind variables, can be in the length of the names list
        or in case there is no name lists - at any length
:return: a dict or lists of the bind variables"""
fn generate_bind_vars(bind_variables_names: str, bind_variables_values: str) -> Any

"""If the connection in the client was successful the test will return OK
if it wasn't an exception will be raised"""
fn test_module(client: Client) -> Tuple[(str, Dict[(Any, Any)], List[Any])]

"""Executes the sql query with the connection that was configured in the client
:param client: the client object with the db connection
:param args: demisto.args() including the sql query
:return: Demisto outputs"""
fn sql_query_execute(client: Client, args: dict) -> Tuple[(str, Dict[(str, Any)], List[Dict[(str, Any)]])]

fn main()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from GenericSQL import Client, sql_query_execute
import pytest

args1 = {
    'query': "select Name from city",
    'limit': 5,
    'skip': 0
}

raw1 = [{'Name': 'Kabul'}, {'Name': 'Qandahar'}, {'Name': 'Herat'}, {'Name': 'Mazar-e-Sharif'}]

expected_output = {
    'GenericSQL(val.Query && val.Query === obj.Query)':
        {'GenericSQL': {'Result': [{'Name': 'Kabul'},
                                   {'Name': 'Qandahar'},
                                   {'Name': 'Herat'},
                                   {'Name': 'Mazar-e-Sharif'}],
                        'Query': 'select Name from city',
                        'InstanceName': 'sql_dialect_database'}}
}


@pytest.mark.parametrize('command, args, response, expected_result', [
    (sql_query_execute, args1, raw1, expected_output),
])
def test_sql_queries(command, args, response, expected_result, mocker):
    """Unit test
    Given
    - select query
    - raw response of the database
    When
    - mock the database result
    Then
    - convert the result to human readable table
    - create the context
    validate the expected_result and the created context
    """
    mocker.patch.object(Client, '_create_engine_and_connect')  # needed in order not to make a connection in tests
    mocker.patch.object(Client, 'sql_query_execute_request', return_value=(raw1, ['Name']))
    client = Client('sql_dialect', 'server_url', 'username', 'password', 'port', 'database', "")
    result = command(client, args)
    assert expected_result == result[1]  # entry context is found in the 2nd place in the result of the command
<|endoftext|>"
},
{
"prompt": "class ApplicationError(RuntimeError):
	code = 1


class CLIArgumentError(ApplicationError):
	code = 2


fn printerror()

"""Parse arguments for `callmain` API.

>>> parse_callmain("PkgName=u-u-i-d:main.function.name") == {
...     "pkgname": "PkgName",
...     "pkguuid": "u-u-i-d",
...     "main": "main.function.name",
... }
True"""
fn parse_callmain(callmain: str)

fn default_stdio_path(name)

fn default_stdin_path()

fn default_stdout_path()

fn default_stderr_path()

fn compile_request(eval, callmain, callany, adhoccli, julia, project, stdin, stdout, stderr, script, args, usemain, ignorereturn)

fn send_request(request, connection)

fn handle_response(response, print_result)

fn jlcli()

fn default_connection()

class CustomFormatter(argparse.RawDescriptionHelpFormatter, argparse.ArgumentDefaultsHelpFormatter):


fn parse_args(args)

fn main(args)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import os
import shlex

import pytest

from jlcli import CLIArgumentError, compile_request, handle_response, parse_args


def compile_request_from_cliargs(cliargs):
    request, kwargs = compile_request(**vars(parse_args(shlex.split(cliargs))))
    assert request["jsonrpc"] == "2.0"
    return request


def test_require_one_command():
    with pytest.raises(CLIArgumentError):
        compile_request_from_cliargs("")


def test_require_only_one_command():
    with pytest.raises(SystemExit):
        compile_request_from_cliargs(
            """--callany='Statistics=10745b16-79ce-11e8-11f9-7d13ad32a3b2:std'"""
            """ --eval=CODE --"""
        )


def test_request_eval():
    request = compile_request_from_cliargs("--eval=CODE a b c")
    assert request["params"]["code"] == "CODE"
    assert request["params"]["args"] == ["a", "b", "c"]


def test_request_callmain():
    request = compile_request_from_cliargs(
        "--callmain=PkgName=u-u-i-d:main.function.name a b c"
    )
    assert request["params"]["pkgname"] == "PkgName"
    assert request["params"]["pkguuid"] == "u-u-i-d"
    assert request["params"]["main"] == "main.function.name"
    assert request["params"]["args"] == ["a", "b", "c"]


def test_request_callany():
    request = compile_request_from_cliargs(
        """--callany='Statistics=10745b16-79ce-11e8-11f9-7d13ad32a3b2:std'"""
        """ '{"args":[[1,2,3]]}'"""
    )
    assert request["params"]["pkgname"] == "Statistics"
    assert request["params"]["pkguuid"] == "10745b16-79ce-11e8-11f9-7d13ad32a3b2"
    assert request["params"]["main"] == "std"
    assert request["params"]["args"] == [[1, 2, 3]]


def test_request_adhoccli():
    request = compile_request_from_cliargs(
        """--print-result --adhoccli='Unicode=4ec0a83e-493e-50e2-b9ac-8f72acf5a8f5:normalize'"""
        """ -- JuLiA --casefold"""
    )
    assert request["params"]["pkgname"] == "Unicode"
    assert request["params"]["pkguuid"] == "4ec0a83e-493e-50e2-b9ac-8f72acf5a8f5"
    assert request["params"]["main"] == "normalize"
    assert request["params"]["args"] == ["JuLiA", "--casefold"]


def test_request_script():
    scriptfullpath = os.devnull
    request = compile_request_from_cliargs(scriptfullpath + " -h")
    assert request["params"]["script"] == scriptfullpath
    assert request["params"]["args"] == ["-h"]


def handle(**response):
    response["id"] = 0
    return handle_response(response, print_result=True)


def test_response_stdout(capsys):
    err = handle(result={"stdout": "hello"})
    captured = capsys.readouterr()
    assert captured.out == "hello"
    assert not err


def test_response_result(capsys):
    err = handle(result={"result": {"some": "result"}})
    captured = capsys.readouterr()
    assert "some" in captured.out
    assert "result" in captured.out
    assert not err


def test_response_error(capsys):
    err = handle(error={"message": "hello"})
    captured = capsys.readouterr()
    assert captured.err == "hello\n"
    assert err


def test_response_invalid(capsys):
    err = handle()
    captured = capsys.readouterr()
    assert "Invalid response:" in captured.err
    assert err
<|endoftext|>"
},
{
"prompt": """"A variant of `functools.singledispatch` which dispatches on type of the second argument."""
fn seconddispatch(func)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """"
These tests are borrowed from CPython
"""
import sys
import unittest
import collections

import pytest

from sunpy.util.functools import seconddispatch


class TestSingleDispatch(unittest.TestCase):
    def test_simple_overloads(self):

        @seconddispatch
        def g(dummy, obj):
            return "base"

        def g_int(dummy, i):
            return "integer"
        g.register(int, g_int)
        self.assertEqual(g(None, "str"), "base")
        self.assertEqual(g(None, 1), "integer")
        self.assertEqual(g(None, [1,2,3]), "base")

    def test_mro(self):

        @seconddispatch
        def g(obj):
            return "base"

        class A:
            pass

        class C(A):
            pass

        class B(A):
            pass

        class D(C, B):
            pass

        def g_A(dummy, a):
            return "A"

        def g_B(dummy, b):
            return "B"
        g.register(A, g_A)
        g.register(B, g_B)
        self.assertEqual(g(None, A()), "A")
        self.assertEqual(g(None, B()), "B")
        self.assertEqual(g(None, C()), "A")
        self.assertEqual(g(None, D()), "B")

    def test_register_decorator(self):

        @seconddispatch
        def g(dummy, obj):
            return "base"

        @g.register(int)
        def g_int(dummy, i):
            return "int %s" % (i,)
        self.assertEqual(g(None, ""), "base")
        self.assertEqual(g(None, 12), "int 12")
        self.assertIs(g.dispatch(int), g_int)
        self.assertIs(g.dispatch(object), g.dispatch(str))
        # Note: in the assert above this is not g.
        # @singledispatch returns the wrapper.

    def test_wrapping_attributes(self):
        @seconddispatch
        def g(dummy, obj):
            "Simple test"
            return "Test"
        self.assertEqual(g.__name__, "g")
        if sys.flags.optimize < 2:
            self.assertEqual(g.__doc__, "Simple test")

    @pytest.mark.skipif(sys.version_info < (3, 7), reason="requires python3.7 or higher")
    def test_annotations(self):
        @seconddispatch
        def i(dummy, arg):
            return "base"

        @i.register
        def _(dummy, arg: collections.abc.Mapping):
            return "mapping"

        @i.register
        def _(dummy, arg: "collections.abc.Sequence"):
            return "sequence"
        self.assertEqual(i(None, None), "base")
        self.assertEqual(i(None, {"a": 1}), "mapping")
        self.assertEqual(i(None, [1, 2, 3]), "sequence")
        self.assertEqual(i(None, (1, 2, 3)), "sequence")
        self.assertEqual(i(None, "str"), "sequence")

        # Registering classes as callables doesn't work with annotations,
        # you need to pass the type explicitly.
        @i.register(str)
        class _:
            def __init__(self, dummy, arg):
                self.arg = arg

            def __eq__(self, other):
                return self.arg == other
        self.assertEqual(i(None, "str"), "str")
<|endoftext|>"
},
{
"prompt": "fn make_tempfile(mode)

fn make_tempdir()

"""Create Doc object from given vocab, words and annotations."""
fn get_doc(vocab, words, pos, heads, deps, tags, ents)

"""Perform a series of pre-specified transitions, to put the parser in a
desired state."""
fn apply_transition_sequence(parser, doc, sequence)

"""Add list of vector tuples to given vocab. All vectors need to have the
same length. Format: [("text", [1, 2, 3])]"""
fn add_vecs_to_vocab(vocab, vectors)

"""Get cosine for two given vectors"""
fn get_cosine(vec1, vec2)

"""Compare two Doc objects and assert that they're equal. Tests for tokens,
tags, dependencies and entities."""
fn assert_docs_equal(doc1, doc2)

"""Assert that two packed msgpack messages are equal."""
fn assert_packed_msg_equal(b1, b2)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# coding: utf-8
from __future__ import unicode_literals

import pytest
from spacy import displacy
from spacy.displacy.render import DependencyRenderer
from spacy.tokens import Span
from spacy.lang.fa import Persian

from .util import get_doc


def test_displacy_parse_ents(en_vocab):
    """Test that named entities on a Doc are converted into displaCy's format."""
    doc = get_doc(en_vocab, words=["But", "Google", "is", "starting", "from", "behind"])
    doc.ents = [Span(doc, 1, 2, label=doc.vocab.strings["ORG"])]
    ents = displacy.parse_ents(doc)
    assert isinstance(ents, dict)
    assert ents["text"] == "But Google is starting from behind "
    assert ents["ents"] == [{"start": 4, "end": 10, "label": "ORG"}]


def test_displacy_parse_deps(en_vocab):
    """Test that deps and tags on a Doc are converted into displaCy's format."""
    words = ["This", "is", "a", "sentence"]
    heads = [1, 0, 1, -2]
    pos = ["DET", "VERB", "DET", "NOUN"]
    tags = ["DT", "VBZ", "DT", "NN"]
    deps = ["nsubj", "ROOT", "det", "attr"]
    doc = get_doc(en_vocab, words=words, heads=heads, pos=pos, tags=tags, deps=deps)
    deps = displacy.parse_deps(doc)
    assert isinstance(deps, dict)
    assert deps["words"] == [
        {"lemma": None, "text": "This", "tag": "DET"},
        {"lemma": None, "text": "is", "tag": "AUX"},
        {"lemma": None, "text": "a", "tag": "DET"},
        {"lemma": None, "text": "sentence", "tag": "NOUN"},
    ]
    assert deps["arcs"] == [
        {"start": 0, "end": 1, "label": "nsubj", "dir": "left"},
        {"start": 2, "end": 3, "label": "det", "dir": "left"},
        {"start": 1, "end": 3, "label": "attr", "dir": "right"},
    ]


def test_displacy_invalid_arcs():
    renderer = DependencyRenderer()
    words = [{"text": "This", "tag": "DET"}, {"text": "is", "tag": "VERB"}]
    arcs = [
        {"start": 0, "end": 1, "label": "nsubj", "dir": "left"},
        {"start": -1, "end": 2, "label": "det", "dir": "left"},
    ]
    with pytest.raises(ValueError):
        renderer.render([{"words": words, "arcs": arcs}])


def test_displacy_spans(en_vocab):
    """Test that displaCy can render Spans."""
    doc = get_doc(en_vocab, words=["But", "Google", "is", "starting", "from", "behind"])
    doc.ents = [Span(doc, 1, 2, label=doc.vocab.strings["ORG"])]
    html = displacy.render(doc[1:4], style="ent")
    assert html.startswith("<div")


def test_displacy_raises_for_wrong_type(en_vocab):
    with pytest.raises(ValueError):
        displacy.render("hello world")


def test_displacy_rtl():
    # Source: http://www.sobhe.ir/hazm/ – is this correct?
    words = ["ما", "بسیار", "کتاب", "می\u200cخوانیم"]
    # These are (likely) wrong, but it's just for testing
    pos = ["PRO", "ADV", "N_PL", "V_SUB"]  # needs to match lang.fa.tag_map
    deps = ["foo", "bar", "foo", "baz"]
    heads = [1, 0, 1, -2]
    nlp = Persian()
    doc = get_doc(nlp.vocab, words=words, pos=pos, tags=pos, heads=heads, deps=deps)
    doc.ents = [Span(doc, 1, 3, label="TEST")]
    html = displacy.render(doc, page=True, style="dep")
    assert "direction: rtl" in html
    assert 'direction="rtl"' in html
    assert 'lang="{}"'.format(nlp.lang) in html
    html = displacy.render(doc, page=True, style="ent")
    assert "direction: rtl" in html
    assert 'lang="{}"'.format(nlp.lang) in html


def test_displacy_render_wrapper(en_vocab):
    """Test that displaCy accepts custom rendering wrapper."""

    def wrapper(html):
        return "TEST" + html + "TEST"

    displacy.set_render_wrapper(wrapper)
    doc = get_doc(en_vocab, words=["But", "Google", "is", "starting", "from", "behind"])
    doc.ents = [Span(doc, 1, 2, label=doc.vocab.strings["ORG"])]
    html = displacy.render(doc, style="ent")
    assert html.startswith("TEST<div")
    assert html.endswith("/div>TEST")
    # Restore
    displacy.set_render_wrapper(lambda html: html)
<|endoftext|>"
},
{
"prompt": "fn get_json(file_name: str)

fn get_file(file_name: str)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
import typing
from httpx import Response

if typing.TYPE_CHECKING:
    import sys
    sys.path.append('./src/plugins')
    import nonebot_bison

from .utils import get_json

@pytest.fixture(scope='module')
def bing_dy_list():
    return get_json('bilibili_bing_list.json')['data']['cards']

@pytest.fixture
def bilibili(plugin_module: 'nonebot_bison'):
    return plugin_module.platform.platform_manager['bilibili']

@pytest.mark.asyncio
async def test_video_forward(bilibili, bing_dy_list):
    post = await bilibili.parse(bing_dy_list[1]) 
    assert(post.text == '答案揭晓：宿舍！来看看投票结果\nhttps://t.bilibili.com/568093580488553786\n--------------\n#可露希尔的秘密档案# \n11：来宿舍休息一下吧 \n档案来源：lambda:\\罗德岛内务\\秘密档案 \n发布时间：9/12 1:00 P.M. \n档案类型：可见 \n档案描述：今天请了病假在宿舍休息。很舒适。 \n提供者：赫默')

@pytest.mark.asyncio
async def test_article_forward(bilibili, bing_dy_list):
    post = await bilibili.parse(bing_dy_list[4]) 
    assert(post.text == '#明日方舟##饼学大厦#\n9.11专栏更新完毕，这还塌了实属没跟新运营对上\n后边除了周日发饼和PV没提及的中文语音，稳了\n别忘了来参加#可露希尔的秘密档案#的主题投票\nhttps://t.bilibili.com/568093580488553786?tab=2' +
        '\n--------------\n' +
        '【明日方舟】饼学大厦#12~14（风暴瞭望&玛莉娅·临光&红松林&感谢庆典）9.11更新 更新记录09.11更新：覆盖09.10更新；以及排期更新，猜测周一周五开活动09.10更新：以周五开活动为底，PV/公告调整位置，整体结构更新09.08更新：饼学大厦#12更新，新增一件六星商店服饰（周日发饼）09.06更新：饼学大厦整栋整栋翻新，改为9.16开主线（四日无饼！）09.05凌晨更新：10.13后的排期（两日无饼，鹰角背刺，心狠手辣）前言感谢楪筱祈ぺ的动态-哔哩哔哩 (bilibili.com) 对饼学的贡献！后续排期：9.17【风暴瞭望】、10.01【玛莉娅·临光】复刻、10.1')

@pytest.mark.asyncio
async def test_dynamic_forward(bilibili, bing_dy_list):
    post = await bilibili.parse(bing_dy_list[5])
    assert(post.text == '饼组主线饼学预测——9.11版\n①今日结果\n9.11 殿堂上的游禽-星极(x，新运营实锤了)\n②后续预测\n9.12 #罗德岛相簿#+#可露希尔的秘密档案#11话\n9.13 六星先锋(执旗手)干员-琴柳\n9.14 宣传策略-空弦+家具\n9.15 轮换池（+中文语音前瞻）\n9.16 停机\n9.17 #罗德岛闲逛部#+新六星EP+EP09·风暴瞭望开启\n9.19 #罗德岛相簿#' +
            '\n--------------\n' +
            '#明日方舟#\n【新增服饰】\n//殿堂上的游禽 - 星极\n塞壬唱片偶像企划《闪耀阶梯》特供服饰/殿堂上的游禽。星极自费参加了这项企划，尝试着用大众能接受的方式演绎天空之上的故事。\n\n_____________\n谦逊留给观众，骄傲发自歌喉，此夜，唯我璀璨。 ')
<|endoftext|>"
},
{
"prompt": """"A role for a user."""
class Role(SurrogatePK, Model):
	__tablename__ = 'roles'
	name = Column(db.String(80), unique=True, nullable=False)
	user_id = reference_col('users', nullable=True)
	user = relationship('User', backref='roles')
	"""Create instance."""
	fn __init__(self, name)
	"""Represent instance as a unique string."""
	fn __repr__(self)


"""A user of the app."""
class User(UserMixin, SurrogatePK, Model):
	__tablename__ = 'users'
	username = Column(db.String(80), unique=True, nullable=False)
	email = Column(db.String(80), unique=True, nullable=False)
	password = Column(db.Binary(128), nullable=True)
	created_at = Column(db.DateTime, nullable=False, default=dt.datetime.utcnow)
	first_name = Column(db.String(30), nullable=True)
	last_name = Column(db.String(30), nullable=True)
	active = Column(db.Boolean(), default=False)
	is_admin = Column(db.Boolean(), default=False)
	"""Create instance."""
	fn __init__(self, username, email, password)
	"""Set password."""
	fn set_password(self, password)
	"""Check password."""
	fn check_password(self, value)
	"""Full user name."""
	fn full_name(self)
	"""Represent instance as a unique string."""
	fn __repr__(self)


"""Base factory."""
class BaseFactory(SQLAlchemyModelFactory):


"""User factory."""
class UserFactory(BaseFactory):
	username = Sequence(lambda n: 'user{0}'.format(n))
	email = Sequence(lambda n: 'user{0}@example.com'.format(n))
	password = PostGenerationMethodCall('set_password', 'example')
	active = True


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# -*- coding: utf-8 -*-
"""Model unit tests."""
import datetime as dt

import pytest

from arthistapp.user.models import Role, User

from .factories import UserFactory


@pytest.mark.usefixtures('db')
class TestUser:
    """User tests."""

    def test_get_by_id(self):
        """Get user by ID."""
        user = User('foo', 'foo@bar.com')
        user.save()

        retrieved = User.get_by_id(user.id)
        assert retrieved == user

    def test_created_at_defaults_to_datetime(self):
        """Test creation date."""
        user = User(username='foo', email='foo@bar.com')
        user.save()
        assert bool(user.created_at)
        assert isinstance(user.created_at, dt.datetime)

    def test_password_is_nullable(self):
        """Test null password."""
        user = User(username='foo', email='foo@bar.com')
        user.save()
        assert user.password is None

    def test_factory(self, db):
        """Test user factory."""
        user = UserFactory(password='myprecious')
        db.session.commit()
        assert bool(user.username)
        assert bool(user.email)
        assert bool(user.created_at)
        assert user.is_admin is False
        assert user.active is True
        assert user.check_password('myprecious')

    def test_check_password(self):
        """Check password."""
        user = User.create(username='foo', email='foo@bar.com',
                           password='foobarbaz123')
        assert user.check_password('foobarbaz123') is True
        assert user.check_password('barfoobaz') is False

    def test_full_name(self):
        """User full name."""
        user = UserFactory(first_name='Foo', last_name='Bar')
        assert user.full_name == 'Foo Bar'

    def test_roles(self):
        """Add a role to a user."""
        role = Role(name='admin')
        role.save()
        user = UserFactory()
        user.roles.append(role)
        user.save()
        assert role in user.roles
<|endoftext|>"
},
{
"prompt": "fn get_open_port()

class ParityWeb3ModuleTest(Web3ModuleTest):
	fn _check_web3_clientVersion(self, client_version)


class ParityEthModuleTest(EthModuleTest):
	fn test_eth_uninstallFilter(self, web3)
	fn test_eth_newBlockFilter(self, web3)
	fn test_eth_replaceTransaction(self, web3, unlocked_account)
	fn test_eth_replaceTransaction_already_mined(self, web3, unlocked_account)
	fn test_eth_replaceTransaction_incorrect_nonce(self, web3, unlocked_account)
	fn test_eth_replaceTransaction_gas_price_too_low(self, web3, unlocked_account)
	fn test_eth_replaceTransaction_gas_price_defaulting_minimum(self, web3, unlocked_account)
	fn test_eth_replaceTransaction_gas_price_defaulting_strategy_higher(self, web3, unlocked_account)
	fn test_eth_replaceTransaction_gas_price_defaulting_strategy_lower(self, web3, unlocked_account)
	fn test_eth_modifyTransaction(self, web3, unlocked_account)


class ParityPersonalModuleTest(PersonalModuleTest):
	fn test_personal_importRawKey(self, web3)
	fn test_personal_listAccounts(self, web3)
	fn test_personal_lockAccount(self, web3, unlocked_account)
	fn test_personal_unlockAccount_success(self, web3)
	fn test_personal_unlockAccount_failure(self, web3, unlockable_account)
	fn test_personal_newAccount(self, web3)
	fn test_personal_sendTransaction(self, web3, unlockable_account, unlockable_account_pw)
	fn test_personal_sign_and_ecrecover(self, web3, unlockable_account, unlockable_account_pw)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import os
import pytest
import tempfile

from tests.integration.parity.utils import (
    wait_for_socket,
)
from web3 import Web3
from web3.utils.module_testing import (
    NetModuleTest,
    VersionModuleTest,
)

from .common import (
    ParityEthModuleTest,
    ParityPersonalModuleTest,
    ParityWeb3ModuleTest,
)


@pytest.fixture(scope='module')
def ipc_path(datadir):
    ipc_dir_path = tempfile.mkdtemp()
    _ipc_path = os.path.join(ipc_dir_path, 'jsonrpc.ipc')
    yield _ipc_path

    if os.path.exists(_ipc_path):
        os.remove(_ipc_path)


@pytest.fixture(scope="module")
def parity_command_arguments(
    parity_import_blocks_process,
    parity_binary,
    ipc_path,
    datadir,
    passwordfile,
    author
):
    return (
        parity_binary,
        '--chain', os.path.join(datadir, 'chain_config.json'),
        '--ipc-path', ipc_path,
        '--base-path', datadir,
        '--unlock', author,
        '--password', passwordfile,
        '--no-jsonrpc',
        '--no-ws',
    )


@pytest.fixture(scope="module")
def parity_import_blocks_command(parity_binary, ipc_path, datadir, passwordfile):
    return (
        parity_binary,
        'import', os.path.join(datadir, 'blocks_export.rlp'),
        '--chain', os.path.join(datadir, 'chain_config.json'),
        '--ipc-path', ipc_path,
        '--base-path', datadir,
        '--password', passwordfile,
        '--no-jsonrpc',
        '--no-ws',
    )


@pytest.fixture(scope="module")  # noqa: F811
def web3(parity_process, ipc_path):
    wait_for_socket(ipc_path)
    _web3 = Web3(Web3.IPCProvider(ipc_path))
    return _web3


class TestParityWeb3ModuleTest(ParityWeb3ModuleTest):
    pass


class TestParityEthModuleTest(ParityEthModuleTest):
    pass


class TestParityVersionModule(VersionModuleTest):
    pass


class TestParityNetModule(NetModuleTest):
    pass


class TestParityPersonalModuleTest(ParityPersonalModuleTest):
    pass
<|endoftext|>"
},
{
"prompt": "fn generate_point(item_type)

fn create_options(item_type)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
import fixtures
from source.user_defined_functions.queue_sender.alert_definition import AlertDefinition

@pytest.mark.parametrize("item_type", ["tiltmeter"])
def test_alert_definitions(item_type, alert_definition):
    alert = alert_definition
    alert["alert"]["description"] = ""
    assert alert == fixtures.EXPECTED_ALERT

@pytest.fixture(scope="function")
def alert_definition(item_type):
    options = fixtures.create_options(item_type)
    point = fixtures.generate_point(item_type)
    definition = AlertDefinition(options)
    return definition.get_alert(point)
<|endoftext|>"
},
{
"prompt": """"GloVe is pre-trained word vector named `Global Vectors for Word Representation`.

References:

        [1] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014.
        GloVe: Global Vectors for Word Representation.

Arguments:
        file_id (str): a str indicates the source of GloVe word vectors. If it is local file,
                it can be a directory contains 'glove.txt' or just a text file.
                Default: ``resources://Glove300d``.     A 300d glove is downloaded and cached."""
class Glove(WordVector):
	fn __init__(self, file_id)
	fn _load_raw_word2vec(self)
	"""Refer to :meth:`.WordVector.load`."""
	fn load_matrix(self, n_dims, vocab_list, mean, std, default_embeddings)
	"""Refer to :meth:`.WordVector.load_pretrain_embed`."""
	fn load_dict(self, vocab_list)


"""Base of all word vector loader
        """
class WordVector(LoadClassInterface):
	fn __init__(self)
	"""Load pretrained word vector and return a numpy 2-d array. The ith row is the feature
	of the ith word in ``vocab_list``. If some feature is not included in pretrained
	word vector, it will be initialized by:
	
	* ``default_embeddings``, if it is not ``None``.
	* normal distribution with ``mean`` and ``std``, otherwise.
	
	Parameters:
	        n_dims (int): specify the dimension size of word vector. If ``n_dims``
	                is bigger than size of pretrained word vector, the rest embedding will be
	                initialized by ``default_embeddings`` or a normal distribution.
	        vocab_list (list): specify the vocab list used in data loader. If there
	                is any word not appeared in pretrained word vector, the embedding will be
	                initialized by ``default_embeddings`` or a normal distribution.
	        mean: The mean of normal distribution. Default: 0.
	        std: The standard deviation of normal distribution. Default: 0.1.
	        default_embeddings: The default embeddings, it size should be
	                ``[len(vocab_list), ndims]``.
	
	
	Returns:
	
	        (:class:`numpy.ndarray`): A  2-d array. Size:``[len(vocab_list), n_dims]``."""
	fn load_matrix(self, n_dims: int, vocab_list: list, mean: None, std: None, default_embeddings: None) -> (
	"""Load word vector and return a dict that maps words to vectors.
	
	Parameters:
	        vocab_list (list): specify the vocab list used in data loader. If there
	                is any word not appeared in pretrained word vector, the feature will
	                not be returned.
	
	Returns:
	
	        (dict): maps a word (str) to its pretrained embedding (:class:`numpy.ndarray`)
	                where its shape is [ndims]."""
	fn load_dict(self, vocab_list: list) -> (dict)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import copy

import pytest
import numpy as np
from cotk.dataloader import LanguageGeneration, MSCOCO
from cotk.metric import MetricBase
from cotk.wordvector.wordvector import WordVector
from cotk.wordvector.gloves import Glove
import logging

def setup_module():
	import random
	random.seed(0)
	import numpy as np
	np.random.seed(0)

class TestWordVector():
	def base_test_init(self, dl):
		assert isinstance(dl, WordVector)
		with pytest.raises(Exception):
			WordVector.load(None, None, None)
		WordVector.get_all_subclasses()
		assert WordVector.load_class('Glove') == Glove
		assert WordVector.load_class('not_subclass') == None

	def base_test_load(self, dl):
		# test WordVector.load_matrix
		vocab_list = ['the', 'of']
		n_dims = 300
		wordvec = dl.load_matrix(n_dims, vocab_list)
		assert isinstance(wordvec, np.ndarray)
		assert wordvec.shape == (len(vocab_list), n_dims)
		print(wordvec[1])
		assert wordvec[1][0] == -0.076947


		vocab_list = ['the', 'word_not_exist']
		n_dims = 300
		wordvec = dl.load_matrix(n_dims, vocab_list)
		assert isinstance(wordvec, np.ndarray)
		assert wordvec.shape == (len(vocab_list), n_dims)
		assert wordvec[0][0] == 0.04656

		# test WordVector.load_dict
		oov_list = ['oov', 'unk', '']
		for vocab_list in (['the', 'and'], ['the', 'of'], ['the', 'oov'], ['oov'], ['of', 'unk', ''], []):
			wordvec = dl.load_dict(vocab_list)
			assert isinstance(wordvec, dict)
			assert set(wordvec) == set([word for word in vocab_list if word not in oov_list])
			if not wordvec:
				continue
			vec_shape = next(iter(wordvec.values())).shape
			assert len(vec_shape) == 1
			for vec in wordvec.values():
				assert isinstance(vec, np.ndarray)
				assert vec.shape == vec_shape
			if 'the' in wordvec:
				assert (wordvec['the'][-2:] == [-0.20989, 0.053913]).all()
			if 'and' in wordvec:
				assert (wordvec['and'][-2:] == [0.011807, 0.059703]).all()
			if 'of' in wordvec:
				assert (wordvec['of'][-2:] == [-0.29183, -0.046533]).all()		


@pytest.fixture
def load_glove():
	def _load_glove():
		return Glove("./tests/wordvector/dummy_glove/300d")
	return _load_glove

class TestGlove(TestWordVector):
	def test_init(self, load_glove):
		super().base_test_init(load_glove())

	def test_load(self, load_glove):
		super().base_test_load(load_glove())
<|endoftext|>"
},
{
"prompt": """"Widget to add a CodeMirror or DjangoCodeMirror instance on a textarea
Take the same arguments than ``forms.Textarea`` and accepts one
suplementary optionnal arguments :

Arguments:
    config_name (string): A Codemirror config name available in
        ``settings.CODEMIRROR_SETTINGS``. Default is ``empty``.
    embed_config (bool): If ``True`` will add Codemirror Javascript config
        just below the input. Default is ``False``.

Attributes:
    config_name (string): For given config name.
    template_name (string): Template path for widget rendering."""
class CodeMirrorWidget(forms.Textarea):
	codemirror_field_js = settings.CODEMIRROR_FIELD_INIT_JS
	template_name = 'djangocodemirror/widget.html'
	fn __init__(self)
	"""Initialize a manifest instance
	
	Arguments:
	    name (string): Config name to register.
	
	Returns:
	    CodeMirrorManifest: A manifest instance where config (from
	    ``config_name`` attribute) is registred."""
	fn init_manifest(self, name: string) -> CodeMirrorManifest
	"""Return CodeMirror HTML template from
	``CodeMirrorWidget.codemirror_field_js``.
	
	Returns:
	    string: HTML template string."""
	fn get_codemirror_field_js(self) -> string
	"""Shortcut to get Codemirror parameters.
	
	Returns:
	    dict: CodeMirror parameters."""
	fn codemirror_config(self) -> dict
	"""Build CodeMirror HTML script tag which contains CodeMirror init.
	
	Arguments:
	    inputid (string): Input id.
	
	Returns:
	    string: HTML for field CodeMirror instance."""
	fn codemirror_script(self, inputid: string) -> string
	fn get_context(self, name, value, attrs)
	"""Returns this Widget rendered as HTML, as a Unicode string."""
	fn render(self, name, value, attrs, renderer)
	"""Adds necessary files (Js/CSS) to the widget's medias.
	
	Returns:
	    django.forms.Media: Media object with all assets from registered
	    config."""
	fn media(self) -> django.forms.Media


"""CodeMirror widget suited for usage in models admins.

Act like CodeMirrorWidget but allways embed Codemirror Javascript config."""
class CodeMirrorAdminWidget(CodeMirrorWidget):
	fn __init__(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """"
Tests against field widgets
"""
import pytest

from djangocodemirror.widgets import CodeMirrorWidget, CodeMirrorAdminWidget

from tests.utils import assert_and_parse_html


def test_widget_init_manifest():
    """Check registered config"""
    widget = CodeMirrorWidget(config_name="empty")

    config = widget.init_manifest("empty")

    assert config.get_configs() == {
        'empty': {
            'modes': [],
            'addons': [],
            'themes': [],
            'css_bundle_name': 'dcm-empty_css',
            'js_bundle_name': 'dcm-empty_js',
        }
    }


def test_widget_basic():
    """Basic widget usage"""
    widget = CodeMirrorWidget(config_name="basic")

    rendered = widget.render("sample", "Hello World!")

    expected = ("""<textarea id="id_sample" name="sample" rows="10" cols="40">"""
                """Hello World!</textarea>""")

    assert assert_and_parse_html(rendered) == assert_and_parse_html(expected)


@pytest.mark.parametrize('name,expected', [
    (
        'basic',
        ("""<textarea id="id_sample" name="sample" rows="10" cols="40">"""
         """Hello World!</textarea>\n"""
         """<script>var id_sample_codemirror = """
         """CodeMirror.fromTextArea("""
         """document.getElementById("id_sample"),"""
         """{"mode": "rst"});</script>"""),
    ),
    (
        'with-options',
        ("""<textarea id="id_sample" name="sample" rows="10" cols="40">"""
         """Hello World!</textarea>\n"""
         """<script>var id_sample_codemirror = """
         """CodeMirror.fromTextArea("""
         """document.getElementById("id_sample"),"""
         """{"lineNumbers": true, "lineWrapping": true, "mode": "rst"});"""
         """</script>"""),
    ),
], ids=["basic", "with-options"])
def test_admin_widget(name, expected):
    """Admin widget usage"""
    widget = CodeMirrorAdminWidget(config_name=name)

    rendered = widget.render("sample", "Hello World!")

    assert assert_and_parse_html(rendered) == assert_and_parse_html(expected)


@pytest.mark.parametrize('name,expected', [
    (
        'basic',
        ("""<link href="/static/CodeMirror/lib/codemirror.css" type="text/css" media="all" rel="stylesheet">\n"""
         """<script type="text/javascript" src="/static/CodeMirror/lib/codemirror.js"></script>""")
    ),
    (
        'with-modes',
        ("""<link href="/static/CodeMirror/lib/codemirror.css" type="text/css" media="all" rel="stylesheet">\n"""
         """<script type="text/javascript" src="/static/CodeMirror/lib/codemirror.js"></script>\n"""
         """<script type="text/javascript" src="/static/CodeMirror/mode/rst/rst.js"></script>\n"""
         """<script type="text/javascript" src="/static/CodeMirror/mode/python/python.js"></script>""")
    ),
], ids=["basic", "with-modes"])
def test_widget_medias(name, expected):
    """Get widget medias"""
    widget = CodeMirrorWidget(config_name=name)
    rendered = str(widget.media)

    assert assert_and_parse_html(rendered) == assert_and_parse_html(expected)
<|endoftext|>"
},
{
"prompt": """"This is a wrapper over :func:`subprocess.check_output`.
It adds all directories containing runnable scripts (the CLI tool and the demos) to PATH to make them invokable.

:param args: The args to run.

:param timeout: Give up waiting if the command could not be completed in this much time and raise TimeoutExpired.
    No limit by default.

:return: stdout of the command.

>>> run_process('ping', '127.0.0.1', timeout=0.1)
Traceback (most recent call last):
...
subprocess.TimeoutExpired: ..."""
fn run_process() -> str

"""A wrapper over :func:`run_process` that runs the CLI tool with the specified arguments."""
fn run_cli_tool() -> str

"""A wrapper over :class:`subprocess.Popen`.
This wrapper allows collection of stdout upon completion. At first I tried using a background reader
thread that was blocked on ``stdout.readlines()``, but that solution ended up being dysfunctional because
it is fundamentally incompatible with internal stdio buffering in the monitored process which
we have absolutely no control over from our local process. Sure, there exist options to suppress buffering,
such as the ``-u`` flag in Python or the PYTHONUNBUFFERED env var, but they would make the test environment
unnecessarily fragile, so I opted to use a simpler approach where we just run the process until it kicks
the bucket and then loot the output from its dead body.

>>> p = BackgroundChildProcess('ping', '127.0.0.1')
>>> p.wait(0.5)
Traceback (most recent call last):
...
subprocess.TimeoutExpired: ...
>>> p.kill()"""
class BackgroundChildProcess:
	fn __init__(self)
	"""A convenience factory for running the CLI tool."""
	fn cli() -> BackgroundChildProcess
	fn wait(self, timeout: float, interrupt: typing.Optional[bool]) -> typing.Tuple[(int, str)]
	fn kill(self) -> None
	fn interrupt(self) -> None
	fn pid(self) -> int
	fn alive(self) -> bool
	fn __del__(self) -> None


fn _get_env(environment_variables: typing.Optional[typing.Dict[(str, str)]]) -> typing.Dict[(str, str)]

fn _make_process_args(executable: str) -> typing.Sequence[str]

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "#
# Copyright (c) 2019 UAVCAN Development Team
# This software is distributed under the terms of the MIT License.
# Author: Pavel Kirienko <pavel.kirienko@zubax.com>
#

import pytest
import subprocess
from ._subprocess import run_cli_tool


def _unittest_trivial() -> None:
    run_cli_tool('show-transport', timeout=2.0)

    with pytest.raises(subprocess.CalledProcessError):
        run_cli_tool(timeout=2.0)

    with pytest.raises(subprocess.CalledProcessError):
        run_cli_tool('invalid-command', timeout=2.0)

    with pytest.raises(subprocess.CalledProcessError):
        run_cli_tool('dsdl-gen-pkg', 'nonexistent/path', timeout=2.0)

    with pytest.raises(subprocess.CalledProcessError):  # Look-up of a nonexistent package requires large timeout
        run_cli_tool('pub', 'nonexistent.data.Type.1.0', '{}', '--tr=Loopback(None)', timeout=5.0)
<|endoftext|>"
},
{
"prompt": """"Short term objective intelligibility
Computes the STOI (See [1][2]) of a denoised signal compared to a clean
signal, The output is expected to have a monotonic relation with the
subjective speech-intelligibility, where a higher score denotes better
speech intelligibility.

# Arguments
    x (np.ndarray): clean original speech
    y (np.ndarray): denoised speech
    fs_sig (int): sampling rate of x and y
    extended (bool): Boolean, whether to use the extended STOI described in [3]

# Returns
    float: Short time objective intelligibility measure between clean and
    denoised speech

# Raises
    AssertionError : if x and y have different lengths

# Reference
    [1] C.H.Taal, R.C.Hendriks, R.Heusdens, J.Jensen 'A Short-Time
        Objective Intelligibility Measure for Time-Frequency Weighted Noisy
        Speech', ICASSP 2010, Texas, Dallas.
    [2] C.H.Taal, R.C.Hendriks, R.Heusdens, J.Jensen 'An Algorithm for
        Intelligibility Prediction of Time-Frequency Weighted Noisy Speech',
        IEEE Transactions on Audio, Speech, and Language Processing, 2011.
    [3] Jesper Jensen and Cees H. Taal, 'An Algorithm for Predicting the
        Intelligibility of Speech Masked by Modulated Noise Maskers',
        IEEE Transactions on Audio, Speech and Language Processing, 2016."""
fn stoi(x, y, fs_sig, extended)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
import matlab.engine
import numpy as np
import scipy
from numpy.testing import assert_allclose
from pystoi.stoi import N_FRAME, NFFT, FS

ATOL = 1e-5

eng = matlab.engine.start_matlab()
eng.cd('matlab/')


def test_hanning():
    """ Compare scipy and Matlab hanning window.
        Matlab returns a N+2 size window without first and last samples"""
    hanning = scipy.hanning(N_FRAME+2)[1:-1]
    hanning_m = eng.hanning(float(N_FRAME))
    hanning_m = np.array(hanning_m._data)
    assert_allclose(hanning, hanning_m, atol=ATOL)


def test_fft():
    x = np.random.randn(N_FRAME, )
    x_m = matlab.double(list(x))
    fft_m = eng.fft(x_m, NFFT)
    fft_m = np.array(fft_m).transpose()
    fft_m = fft_m[0:NFFT//2+1, 0]
    fft = np.fft.rfft(x, n=NFFT)
    assert_allclose(fft, fft_m, atol=ATOL)


def test_resampy():
    """ Compare matlab and librosa resample : FAILING """
    from resampy import resample
    from pystoi.stoi import FS
    import matlab_wrapper
    matlab = matlab_wrapper.MatlabSession()
    matlab.put('FS', float(FS))
    RTOL = 1e-4

    for fs in [8000, 11025, 16000, 22050, 32000, 44100, 48000]:
        x = np.random.randn(2*fs,)
        x_r = resample(x, fs, FS)
        matlab.put('x', x)
        matlab.put('fs', float(fs))
        matlab.eval('x_r = resample(x, FS, fs)')
        assert_allclose(x_r, matlab.get('x_r'), atol=ATOL, rtol=RTOL)


def test_nnresample():
    """ Compare matlab and nnresample resample : FAILING """
    from nnresample import resample
    from pystoi.stoi import FS
    import matlab_wrapper
    matlab = matlab_wrapper.MatlabSession()
    matlab.put('FS', float(FS))
    RTOL = 1e-4

    for fs in [8000, 11025, 16000, 22050, 32000, 44100, 48000]:
        x = np.random.randn(2*fs,)
        x_r = resample(x, FS, fs)
        matlab.put('x', x)
        matlab.put('fs', float(fs))
        matlab.eval('x_r = resample(x, FS, fs)')
        assert_allclose(x_r, matlab.get('x_r'), atol=ATOL, rtol=RTOL)


if __name__ == '__main__':
    pytest.main([__file__])
<|endoftext|>"
},
{
"prompt": """"An exception denoting an error during a native call."""
class NativeContractException(Exception):


""":param data:
:return:"""
fn ecrecover(data: List[int]) -> List[int]

""":param data:
:return:"""
fn sha256(data: List[int]) -> List[int]

""":param data:
:return:"""
fn ripemd160(data: List[int]) -> List[int]

""":param data:
:return:"""
fn identity(data: List[int]) -> List[int]

"""TODO: Some symbolic parts can be handled here
Modular Exponentiation
:param data: Data with <length_of_BASE> <length_of_EXPONENT> <length_of_MODULUS> <BASE> <EXPONENT> <MODULUS>
:return: modular exponentiation"""
fn mod_exp(data: List[int]) -> List[int]

fn ec_add(data: List[int]) -> List[int]

fn ec_mul(data: List[int]) -> List[int]

fn ec_pair(data: List[int]) -> List[int]

"""Takes integer address 1, 2, 3, 4.

:param address:
:param data:
:return:"""
fn native_contracts(address: int, data: BaseCalldata) -> List[int]

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from eth_utils import decode_hex
from mythril.laser.ethereum.natives import mod_exp
from ethereum.utils import big_endian_to_int


EIP198_VECTOR_A = decode_hex(
    "0000000000000000000000000000000000000000000000000000000000000001"
    "0000000000000000000000000000000000000000000000000000000000000020"
    "0000000000000000000000000000000000000000000000000000000000000020"
    "03"
    "fffffffffffffffffffffffffffffffffffffffffffffffffffffffefffffc2e"
    "fffffffffffffffffffffffffffffffffffffffffffffffffffffffefffffc2f"
)

EIP198_VECTOR_B = decode_hex(
    "0000000000000000000000000000000000000000000000000000000000000000"
    "0000000000000000000000000000000000000000000000000000000000000020"
    "0000000000000000000000000000000000000000000000000000000000000020"
    "fffffffffffffffffffffffffffffffffffffffffffffffffffffffefffffc2e"
    "fffffffffffffffffffffffffffffffffffffffffffffffffffffffefffffc2f"
)

EIP198_VECTOR_C = decode_hex(
    "0000000000000000000000000000000000000000000000000000000000000001"
    "0000000000000000000000000000000000000000000000000000000000000002"
    "0000000000000000000000000000000000000000000000000000000000000020"
    "03"
    "ffff"
    "8000000000000000000000000000000000000000000000000000000000000000"
    "07"
)

EIP198_VECTOR_D = decode_hex(
    "0000000000000000000000000000000000000000000000000000000000000001"
    "0000000000000000000000000000000000000000000000000000000000000002"
    "0000000000000000000000000000000000000000000000000000000000000020"
    "03"
    "ffff"
    "80"
)


@pytest.mark.parametrize(
    "data,expected",
    (
        (EIP198_VECTOR_A, 1),
        (EIP198_VECTOR_B, 0),
        (
            EIP198_VECTOR_C,
            26689440342447178617115869845918039756797228267049433585260346420242739014315,
        ),
        (
            EIP198_VECTOR_D,
            26689440342447178617115869845918039756797228267049433585260346420242739014315,
        ),
    ),
)
def test_modexp_result(data, expected):
    actual = mod_exp(data)
    assert big_endian_to_int(actual) == expected
<|endoftext|>"
},
{
"prompt": "fn test_new_job(db)

"""Test that passing a release's id will cause the relevant release to have
a job log attached to it"""
fn test_related_models_by_pk(db)

"""Test that passing an instance of the model directly will cause a job log
to be attached to it"""
fn test_related_models_by_instance(db)

"""Test that models that do not have a job_log field do not have job logs
attached to them."""
fn test_related_models_no_property(db, mocker)

"""Test that no relations are formed if the specified argument cant' be found"""
fn test_related_models_arg_not_found(db, mocker)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import os
import pytest
from creator.decorators import task
from creator.jobs.models import Job, JobLog
from creator.studies.models import Study
from creator.studies.factories import StudyFactory
from creator.releases.models import Release
from creator.releases.factories import ReleaseFactory, ReleaseTaskFactory


def test_new_job(db):
    @task("myjob")
    def my_job():
        pass

    my_job()

    assert Job.objects.count() == 1
    assert JobLog.objects.count() == 1
    assert Job.objects.first().scheduled is False
    assert Job.objects.first().name == "myjob"
    assert os.path.exists(JobLog.objects.first().log_file.path)


def test_related_models_by_pk(db):
    """
    Test that passing a release's id will cause the relevant release to have
    a job log attached to it
    """

    @task("myjob", related_models={Release: "release"})
    def my_job(release):
        pass

    release = ReleaseFactory()

    my_job(release=release.pk)

    release.refresh_from_db()
    assert Job.objects.count() == 1
    assert JobLog.objects.count() == 1
    assert Job.objects.first().scheduled is False
    assert Job.objects.first().name == "myjob"
    assert release.job_log == JobLog.objects.first()


def test_related_models_by_instance(db):
    """
    Test that passing an instance of the model directly will cause a job log
    to be attached to it
    """

    @task("myjob", related_models={Release: "release"})
    def my_job(release):
        pass

    release = ReleaseFactory()

    my_job(release=release)

    release.refresh_from_db()
    assert Job.objects.count() == 1
    assert JobLog.objects.count() == 1
    assert Job.objects.first().scheduled is False
    assert Job.objects.first().name == "myjob"
    assert release.job_log == JobLog.objects.first()


def test_related_models_no_property(db, mocker):
    """
    Test that models that do not have a job_log field do not have job logs
    attached to them.
    """

    @task("myjob", related_models={Study: "study"})
    def my_job(study):
        pass

    study = StudyFactory()
    mock = mocker.patch("creator.decorators.logger.warning")

    my_job(study=study)

    assert mock.call_count == 1
    assert "must have a" in mock.call_args_list[0].args[0]


def test_related_models_arg_not_found(db, mocker):
    """
    Test that no relations are formed if the specified argument cant' be found
    """

    @task("myjob", related_models={Release: "release_id"})
    def my_job(release):
        pass

    mock = mocker.patch("creator.decorators.logger.warning")

    release = ReleaseFactory()

    my_job(release=release)

    assert mock.call_count == 1
    assert "Could not find" in mock.call_args_list[0].args[0]
<|endoftext|>"
},
{
"prompt": "

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "#from server import mycrt
import pytest
import unittest
import requests
import json
from datetime import datetime
import boto3
from .context import *

#TOTAL NUMBER OF FUNCTIONS FROM REPLAY FILE: 15
#TOTAL NUMBER OF FUNCTIONS TESTED: 1


"""
if __name__ == '__main__':

    if __package__ is None:
        import sys
        from os import path
        sys.path.append( path.dirname( path.dirname( path.abspath(__file__) ) ) )
        from server.mycrt import *
    else:
        from ..server.mycrt import *
"""
#mock class for the RDS Client
class FakeRDSClient():
    db_id = "blackfoot-rds"
    def describe_db_instances(DBInstanceIdentifier):
        return {
            'DBInstances':[{
                'Endpoint': [
                    {'Address': 'test'}
                ]
            }]
        }

class TestFlaskApi(unittest.TestCase):
    def setUp(self):
        self.app = server.mycrt.application.test_client()
#Do not work without credentials currently
#TODO: Fix this test
    # def test_get_active_db(self):
    #     fakeRDSClient = FakeRDSClient()
    #     hostname = server.utility.replay._get_hostname(fakeRDSClient, "blackfoot-rds")
    #     print('heerree')
        # print(hostname)

    # def test_get_active_db(self):
    #     test = server.utility.replay.get_active_replays()
    #     print('HERE')
    #     print(test)

    def test_get_replays_from_table(self):
        tester = server.utility.replay.get_replays_from_table()
        exprectedReplays = {
            "replay":[{
                "replay" : 'test_replay', 
                "capture" : 'test_capture', 
                "db" : 'cdb', 
                "mode" : 'fast'
            }]
        }
        print('HERE')
        print(tester)


    
<|endoftext|>"
},
{
"prompt": "class DisallowAuthInterceptor(grpc.ServerInterceptor):
	fn __init__(self)
	fn intercept_service(self, continuation, handler_call_details)


class AllowAuthInterceptor(grpc.ServerInterceptor):
	fn __init__(self)
	fn intercept_service(self, continuation, handler_call_details)


class CoreServicer(Core.CoreServiceServicer):
	fn __init__(self)
	fn GetFeastCoreVersion(self, request, context)
	fn GetFeatureTable(self, request: GetFeatureTableRequest, context)
	fn ListFeatureTables(self, request: ListFeatureTablesRequest, context)
	fn ApplyFeatureTable(self, request: ApplyFeatureTableRequest, context)
	fn DeleteFeatureTable(self, request: DeleteFeatureTableRequest, context)
	fn GetEntity(self, request: GetEntityRequest, context)
	fn ListEntities(self, request: ListEntitiesRequest, context)
	fn ListProjects(self, request, context)
	fn ApplyEntity(self, request: ApplyEntityRequest, context)


fn serve()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Copyright 2020 The Feast Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import socket
from concurrent import futures
from contextlib import closing

import grpc
import pytest

from feast.client import Client
from feast.core import CoreService_pb2_grpc as Core
from feast.entity import Entity
from feast.value_type import ValueType
from feast_core_server import CoreServicer


def find_free_port():
    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:
        s.bind(("", 0))
        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        return s.getsockname()[1]


free_port = find_free_port()


class TestEntity:
    @pytest.fixture(scope="function")
    def server(self):
        server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
        Core.add_CoreServiceServicer_to_server(CoreServicer(), server)
        server.add_insecure_port(f"[::]:{free_port}")
        server.start()
        yield server
        server.stop(0)

    @pytest.fixture
    def client(self, server):
        return Client(core_url=f"localhost:{free_port}")

    def test_entity_import_export_yaml(self):

        test_entity = Entity(
            name="car_driver_entity",
            description="Driver entity for car rides",
            value_type=ValueType.STRING,
            labels={"team": "matchmaking"},
        )

        # Create a string YAML representation of the entity
        string_yaml = test_entity.to_yaml()

        # Create a new entity object from the YAML string
        actual_entity_from_string = Entity.from_yaml(string_yaml)

        # Ensure equality is upheld to original entity
        assert test_entity == actual_entity_from_string


def test_entity_class_contains_labels():
    entity = Entity(
        "my-entity",
        description="My entity",
        value_type=ValueType.STRING,
        labels={"key1": "val1", "key2": "val2"},
    )
    assert "key1" in entity.labels.keys() and entity.labels["key1"] == "val1"
    assert "key2" in entity.labels.keys() and entity.labels["key2"] == "val2"


def test_entity_without_labels_empty_dict():
    entity = Entity("my-entity", description="My entity", value_type=ValueType.STRING)
    assert entity.labels == dict()
    assert len(entity.labels) == 0
<|endoftext|>"
},
{
"prompt": """"Closes QMessageBox's that can appear when testing.

You can use this with QTimer to close a QMessageBox.
Before calling anything that may show a QMessageBox call:
QTimer.singleShot(1000, lambda: close_message_box(qtbot))"""
fn close_message_box(qtbot)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# -*- coding: utf-8 -*-
#
# Copyright © Spyder Project Contributors
# Licensed under the terms of the MIT License
#

"""
Tests for environ.py
"""

# Standard library imports
import os

# Test library imports
import pytest

# Third party imports
from qtpy.QtCore import QTimer

# Local imports
from spyder.utils.test import close_message_box

@pytest.fixture
def setup_environ(qtbot):
    "Setup the Environment variables Dialog taking into account the os."    
    if os.name == 'nt':
        from spyder.utils.environ import WinUserEnvDialog
        dialog = WinUserEnvDialog()
    else:        
        from spyder.utils.environ import EnvDialog
        dialog = EnvDialog()
    qtbot.addWidget(dialog)
    
    return dialog

def test_environ(qtbot):
    """Test the environment variables dialog."""
    QTimer.singleShot(1000, lambda: close_message_box(qtbot))
    dialog = setup_environ(qtbot)
    dialog.show()
    assert dialog


if __name__ == "__main__":
    pytest.main()
<|endoftext|>"
},
{
"prompt": """"monkeypatch for get_or_create_project"""
fn goc_project()

"""Load test suites from file in YAML format:
- test_1:
    - /first/step/url:
        method: GET
        query_string: {"a": "b"}
        data: {"json": "payload"}
        status_code: 200
        response: {"json": "response"}
    - /second/step/url:
- test_2:"""
fn load_test_suite_from_file(test_suite_file)

fn get_test_ids(test_val)

fn test_suite_items(current_test_suite)

fn make_request(client, url, params)

fn compare_response_values(client_value, expected_value)

fn compare_response(client_response, expected_response)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from .base import load_test_suite_from_file, test_suite_items, get_test_ids, make_request, compare_response


@pytest.mark.parametrize('test_suite', load_test_suite_from_file('data_import.yml'), ids=get_test_ids)
def test_import(client, test_suite):
    for url, params in test_suite_items(test_suite):
        r = make_request(client, url, params)
        assert r.status_code == params['status_code']
        is_equal, error_message = compare_response(r.json, params['response'])
        assert is_equal, error_message
<|endoftext|>"
},
{
"prompt": "fn elasticsearch(host)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from .fixtures import elasticsearch
import pytest


def test_setting_node_name_with_an_environment_variable(elasticsearch):
    # The fixture for this test comes from tests/docker-compose.yml
    assert elasticsearch.get_root_page()['name'].startswith('docker-test-node')


def test_setting_cluster_name_with_an_environment_variable(elasticsearch):
    # The fixture for this test comes from tests/docker-compose.yml
    assert elasticsearch.get_root_page()['cluster_name'] == ('docker-test-cluster')


def test_setting_heapsize_with_an_environment_variable(elasticsearch):
    # The fixture for this test comes from tests/docker-compose.yml.
    #
    # The number of bytes that we assert is not exactly what we
    # specify in the fixture. This is due to jvm honoring the
    # generation and survivor ratios, rounding and alignments.  It is
    # enough if Elasticsearch reports a max heap size within 64MB of
    # the target value set in the fixture (=1152MB).

    mem_delta_mb = 64
    for jvm in elasticsearch.get_node_jvm_stats():
        reported_heap_max_in_mb = int(jvm['mem']['heap_max_in_bytes'] / (1024**2))
        assert abs(reported_heap_max_in_mb - 1152) < mem_delta_mb


def test_parameter_containing_underscore_with_an_environment_variable(elasticsearch):
    # The fixture for this test comes from tests/docker-compose.yml
    for thread_pool_queue_size in elasticsearch.get_node_thread_pool_bulk_queue_size():
        assert '500' == thread_pool_queue_size


def test_setting_processors(elasticsearch):
    # The fixture for this test comes from tests/docker-compose.yml
    for processors in elasticsearch.get_processors_setting():
        assert '1' == processors


def test_envar_not_including_a_dot_is_not_presented_to_elasticsearch(elasticsearch):
    # The fixture for this test comes from tests/docker-compose.yml
    assert 'irrelevantsetting' not in elasticsearch.es_cmdline()


def test_capitalized_envvar_is_not_presented_to_elasticsearch(elasticsearch):
    # The fixture for this test comes from tests/docker-compose.yml
    assert 'NonESRelatedVariable' not in elasticsearch.es_cmdline()


def test_setting_boostrap_memory_lock_with_an_environment_variable(elasticsearch):
    # The fixture for this test comes from tests/docker-compose.yml
    #
    # When memory_lock=true ES bootstrap checks expect the memlock ulimit set to unlimited.
    # https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration-memory.html#mlockall
    for mlockall_node_value in elasticsearch.get_node_mlockall_state():
        assert mlockall_node_value is True
<|endoftext|>"
},
{
"prompt": "class UserFactory(DjangoModelFactory):
	username = Faker('user_name')
	email = Faker('email')
	name = Faker('name')
	fn password(self, create: bool, extracted: Sequence[Any])


"""A pointless Celery task to demonstrate usage."""
fn get_users_count()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from celery.result import EagerResult

from redirink.users.tasks import get_users_count
from redirink.users.tests.factories import UserFactory

pytestmark = pytest.mark.django_db


def test_user_count(settings):
    """A basic test to execute the get_users_count Celery task."""
    UserFactory.create_batch(3)
    settings.CELERY_TASK_ALWAYS_EAGER = True
    task_result = get_users_count.delay()
    assert isinstance(task_result, EagerResult)
    assert task_result.result == 3
<|endoftext|>"
},
{
"prompt": "class _FileLike:
	BLOCKSIZE = 1024 * 32
	fn __init__(self, o)
	fn set_descriptor(self, o)
	fn __getattr__(self, attr)
	"""Starts or resets the log.
	
	This will store all bytes read or written."""
	fn start_log(self)
	"""Stops the log."""
	fn stop_log(self)
	fn is_logging(self)
	"""Returns the log as a string."""
	fn get_log(self)
	fn add_log(self, v)
	fn reset_timestamps(self)


class Writer(_FileLike):
	"""May raise exceptions.TcpDisconnect"""
	fn flush(self)
	"""May raise exceptions.TcpDisconnect"""
	fn write(self, v)


class Reader(_FileLike):
	"""If length is -1, we read until connection closes."""
	fn read(self, length)
	fn readline(self, size)
	"""Like .read, but is guaranteed to either return length bytes, or
	raise an exception."""
	fn safe_read(self, length)
	"""Tries to peek into the underlying file object.
	
	Returns:
	    Up to the next N bytes if peeking is successful.
	
	Raises:
	    exceptions.TcpException if there was an error with the socket
	    TlsException if there was an error with pyOpenSSL.
	    NotImplementedError if the underlying file object is not a [pyOpenSSL] socket"""
	fn peek(self, length)


"""This is a wrapper around select.select() which also works for SSL.Connections
by taking ssl_connection.pending() into account.

Caveats:
    If .pending() > 0 for any of the connections in rlist, we avoid the select syscall
    and **will not include any other connections which may or may not be ready**.

Args:
    rlist: wait until ready for reading

Returns:
    subset of rlist which is ready for reading."""
fn ssl_read_select(rlist: None, timeout)

"""Does a hard close of a socket, without emitting a RST."""
fn close_socket(sock)

class _Connection:
	rbufsize = -1
	wbufsize = -1
	"""Set up .rfile and .wfile attributes from .connection"""
	fn _makefile(self)
	fn __init__(self, connection)
	fn get_current_cipher(self)
	fn finish(self)


class ConnectionCloser:
	fn __init__(self, conn)
	"""Cancel the current closer, and return a fresh one."""
	fn pop(self)
	fn __enter__(self)
	fn __exit__(self)


class TCPClient(_Connection):
	fn __init__(self, address, source_address, spoof_source_address)
	fn ssl_verification_error(self) -> Optional[exceptions.InvalidCertificateException]
	fn close(self)
	fn convert_to_ssl(self, sni, alpn_protos)
	fn makesocket(self, family, type, proto)
	fn create_connection(self, timeout)
	fn connect(self)
	fn settimeout(self, n)
	fn gettimeout(self)
	fn get_alpn_proto_negotiated(self)


"""The instantiator is expected to call the handle() and finish() methods."""
class BaseHandler(_Connection):
	fn __init__(self, connection, address, server)
	"""Convert connection to SSL.
	For a list of parameters, see tls.create_server_context(...)"""
	fn convert_to_ssl(self, cert, key)
	fn handle(self)
	fn settimeout(self, n)
	fn get_alpn_proto_negotiated(self)


class Counter:
	fn __init__(self)
	fn count(self)
	fn __enter__(self)
	fn __exit__(self)


class TCPServer:
	request_queue_size = 20
	fn __init__(self, address)
	fn connection_thread(self, connection, client_address)
	fn serve_forever(self, poll_interval)
	fn shutdown(self)
	"""Called when handle_client_connection raises an exception."""
	fn handle_error(self, connection_, client_address, fp)
	"""Called after client connection."""
	fn handle_client_connection(self, conn, client_address)
	"""Called after server shutdown."""
	fn handle_shutdown(self)
	fn wait_for_silence(self, timeout)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from mitmproxy import exceptions
from mitmproxy.net import tls
from mitmproxy.net.tcp import TCPClient
from test.mitmproxy.net.test_tcp import EchoHandler
from . import tservers


class TestMasterSecretLogger(tservers.ServerTestBase):
    handler = EchoHandler
    ssl = dict(
        cipher_list="AES256-SHA"
    )

    def test_log(self, tmpdir):
        testval = b"echo!\n"
        _logfun = tls.log_master_secret

        logfile = str(tmpdir.join("foo", "bar", "logfile"))
        tls.log_master_secret = tls.MasterSecretLogger(logfile)

        c = TCPClient(("127.0.0.1", self.port))
        with c.connect():
            c.convert_to_ssl()
            c.wfile.write(testval)
            c.wfile.flush()
            assert c.rfile.readline() == testval
            c.finish()

            tls.log_master_secret.close()
            with open(logfile, "rb") as f:
                assert f.read().count(b"CLIENT_RANDOM") == 2

        tls.log_master_secret = _logfun

    def test_create_logfun(self):
        assert isinstance(
            tls.MasterSecretLogger.create_logfun("test"),
            tls.MasterSecretLogger)
        assert not tls.MasterSecretLogger.create_logfun(False)


class TestTLSInvalid:
    def test_invalid_ssl_method_should_fail(self):
        fake_ssl_method = 100500
        with pytest.raises(exceptions.TlsException):
            tls.create_client_context(method=fake_ssl_method)

    def test_alpn_error(self):
        with pytest.raises(exceptions.TlsException, match="must be a function"):
            tls.create_client_context(alpn_select_callback="foo")

        with pytest.raises(exceptions.TlsException, match="ALPN error"):
            tls.create_client_context(alpn_select="foo", alpn_select_callback="bar")
<|endoftext|>"
},
{
"prompt": "class ConstantOfShape(nn.Module):
	fn __init__(self, constant)
	fn forward(self, shape: torch.Tensor)
	fn extra_repr(self) -> str


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import numpy as np
import torch
import pytest

from onnx2pytorch.operations.constantofshape import ConstantOfShape


def test_constantofshape_float_ones():
    op = ConstantOfShape()
    x = torch.tensor([4, 3, 2], dtype=torch.int64)
    y = torch.ones(*x, dtype=torch.float32)
    assert torch.equal(op(x), y)


def test_constantofshape_int32_shape_zero():
    constant = torch.tensor([0], dtype=torch.int32)
    op = ConstantOfShape(constant=constant)
    x = torch.tensor([0], dtype=torch.int64)
    y = torch.zeros(*x, dtype=torch.int32)
    assert torch.equal(op(x), y)


def test_constantofshape_int32_zeros():
    constant = torch.tensor([0], dtype=torch.int32)
    op = ConstantOfShape(constant=constant)
    x = torch.tensor([10, 6], dtype=torch.int64)
    y = torch.zeros(*x, dtype=torch.int32)
    assert torch.equal(op(x), y)
<|endoftext|>"
},
{
"prompt": """"Simple entity document model."""
class MyEntityDocument(AbstractEntityDocumentModel):
	my_field = models.CharField(max_length=MAX_LENGTH)
	"""Returns URL to download endpoint."""
	fn url(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from django.utils.timezone import now

from api.documents.tasks import delete_document
from .my_entity_document.models import MyEntityDocument
from api.documents.utils import get_bucket_name

from api.documents.utils import get_s3_client_for_bucket
from botocore.stub import Stubber


# mark the whole module for db use
pytestmark = pytest.mark.django_db


@pytest.fixture()
def s3_stubber():
    """S3 stubber using the botocore Stubber class."""
    s3_client = get_s3_client_for_bucket("default")
    with Stubber(s3_client) as s3_stubber:
        yield s3_stubber


def test_delete_document(s3_stubber):
    """Tests if delete_document task deletes s3 document."""
    entity_document = MyEntityDocument.objects.create(
        original_filename="test.txt", my_field="lions"
    )
    document = entity_document.document
    document.uploaded_on = now()
    document.mark_deletion_pending()

    bucket_name = get_bucket_name(document.bucket_id)

    s3_stubber.add_response(
        "delete_object",
        {"ResponseMetadata": {"HTTPStatusCode": 204}},
        expected_params={"Bucket": bucket_name, "Key": document.path},
    )

    result = delete_document.apply(args=(document.pk,)).get()
    assert result is None

    with pytest.raises(MyEntityDocument.DoesNotExist):
        MyEntityDocument.objects.include_objects_deletion_pending().get(
            pk=entity_document.pk
        )


def test_delete_document_s3_failure(s3_stubber):
    """
    Tests if delete_document task won't delete document from the
    database if deletion from S3 fails.
    """
    entity_document = MyEntityDocument.objects.create(
        original_filename="test.txt", my_field="lions"
    )
    document = entity_document.document
    document.uploaded_on = now()
    document.mark_deletion_pending()

    bucket_name = get_bucket_name(document.bucket_id)

    s3_stubber.add_client_error(
        "delete_object",
        service_error_code=500,
        expected_params={"Bucket": bucket_name, "Key": document.path},
    )

    with pytest.raises(Exception):
        delete_document.apply(args=(document.pk,)).get()

    qs = MyEntityDocument.objects.include_objects_deletion_pending()
    assert qs.filter(pk=entity_document.pk).exists() is True
<|endoftext|>"
},
{
"prompt": """"Create temporary directory with copy of requirements."""
fn temp_dir()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """"End to end tests for CLI v2"""

from functools import partial

try:
    from unittest import mock
except ImportError:
    import mock
from click.testing import CliRunner

import pytest

from pipcompilemulti.cli_v2 import cli, read_config
from .utils import temp_dir


@pytest.fixture(autouse=True)
def requirements_dir():
    """Create temporary requirements directory for test time."""
    with temp_dir() as tmp_dir:
        patch = partial(patched_config, tmp_dir)
        with mock.patch('pipcompilemulti.cli_v2.read_config', patch):
            yield


@pytest.mark.parametrize('command', ['lock', 'upgrade', 'verify'])
def test_command_exits_with_zero(command):
    """Run requirements command on self"""
    # pylint: disable=redefined-outer-name
    runner = CliRunner()
    result = runner.invoke(cli, [command])
    assert result.exit_code == 0


def patched_config(base_dir):
    """Override base_dir in each section of config."""
    config_sections = read_config()
    for _, section in config_sections:
        section['base_dir'] = base_dir
    return config_sections
<|endoftext|>"
},
{
"prompt": "fn random_string(n)

fn get_source_function(func)

"""Get a unique name based on the prefix
which does not coincide with a module which
already exists and is not being created by
another thread

Parameters
----------
prefix : str
         The starting string of the random name
path   : str
         The folder where the lock file should be saved

Returns
-------
module_name : str
              A unique name for the new module
module_lock : FileLock
              A file lock preventing other threads
              from creating a module with the same name"""
fn get_unique_name(prefix: str, path: str) -> FileLock

fn epyccel_seq(function_or_module)

"""Accelerate Python function or module using Pyccel in "embedded" mode.

Parameters
----------
python_function_or_module : function | module
    Python function or module to be accelerated.

verbose : bool
    Print additional information (default: False).

language : {'fortran', 'c', 'python'}
    Language of generated code (default: 'fortran').

accelerators : iterable of str, optional
    Parallel multi-threading acceleration strategy
    (currently supported: 'mpi', 'openmp', 'openacc').

Options for parallel mode
-------------------------
comm : mpi4py.MPI.Comm, optional
    MPI communicator for calling Pyccel in parallel mode (default: None).

root : int, optional
    MPI rank of process in charge of accelerating code (default: 0).

bcast : {True, False}
    If False, only root process loads accelerated function/module (default: True).

Other options
-------------
compiler : str, optional
    User-defined command for compiling generated source code.

Returns
-------
res : object
    Accelerated function or module.

Examples
--------
>>> def one(): return 1
>>> from pyccel.epyccel import epyccel
>>> one_f = epyccel(one, language='fortran')
>>> one_c = epyccel(one, language='c')"""
fn epyccel(python_function_or_module: function | module) -> object

fn lambdify(f)

fn python(f)

fn sympy(f)

fn bypass(f)

fn types()

"""template decorator."""
fn template(name, types)

fn pure(f)

fn private(f)

fn elemental(f)

"""Decorator indicates that all arrays mentioned as args should be stored
on the stack.

Parameters
----------
f : Function
    The function to which the decorator is applied
args : list of str
    A list containing the names of all arrays which should be stored on the stack"""
fn stack_array(f: Function)

"""Decorator indicates that all arrays mentioned as args can be accessed with
negative indexes. As a result all non-constant indexing uses a modulo
function. This can have negative results on the performance

Parameters
----------
f : Function
    The function to which the decorator is applied
args : list of str
    A list containing the names of all arrays which can be accessed
    with non-constant negative indexes"""
fn allow_negative_index(f: Function)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# pylint: disable=missing-function-docstring, missing-module-docstring/
# coding: utf-8
import pytest
import numpy as np

from pyccel.epyccel import epyccel
from pyccel.decorators import types

@pytest.fixture(params=[
    pytest.param('fortran', marks = pytest.mark.fortran),
    pytest.param('c'      , marks = pytest.mark.c),
    pytest.param("python", marks = [
        pytest.mark.skip(reason="Confusion around ValuedVariable means it cannot be used in python"),
        pytest.mark.python]
    )]
)
def language(request):
    return request.param

#------------------------------------------------------------------------------
def test_f1(language):
    @types('int')
    def f1(x = 1):
        y = x - 1
        return y

    f = epyccel(f1, language = language)

    # ...
    assert f(2) == f1(2)
    assert f() == f1()
    # ...
#------------------------------------------------------------------------------
def test_f2(language):
    @types('real [:]', 'int')
    def f5(x, m1 = 2):
        x[:] = 0.
        for i in range(0, m1):
            x[i] = i * 1.

    f = epyccel(f5, language=language)

    # ...
    m1 = 3

    x = np.zeros(m1)
    f(x)

    x_expected = np.zeros(m1)
    f5(x_expected)

    assert np.allclose( x, x_expected, rtol=1e-15, atol=1e-15 )
    # ...

    f(x, m1 = m1)

    f5(x_expected, m1)

    assert np.allclose( x, x_expected, rtol=1e-15, atol=1e-15 )


#------------------------------------------------------------------------------
def test_f3(language):
    @types('real','real')
    def f3(x = 1.5, y = 2.5):
        return x+y

    f = epyccel(f3, language=language)

    # ...
    assert f(19.2,6.7) == f3(19.2,6.7)
    assert f(4.5) == f3(4.5)
    assert f(y = 8.2) == f3(y = 8.2)
    assert f() == f3()
    # ...

#------------------------------------------------------------------------------
def test_f4(language):
    @types('bool')
    def f4(x = True):
        if x:
            return 1
        else:
            return 2

    f = epyccel(f4, language = language)

    # ...
    assert f(True)  == f4(True)
    assert f(False) == f4(False)
    assert f()      == f4()
    # ...

#------------------------------------------------------------------------------
def test_f5(language):
    @types('complex')
    def f5(x = 1j):
        y = x - 1
        return y

    f = epyccel(f5, language = language)

    # ...
    assert f(2.9+3j) == f5(2.9+3j)
    assert f()       == f5()
    # ...
<|endoftext|>"
},
{
"prompt": "fn pytest_addoption(parser)

fn num_cpus(pytestconfig)

fn io_type(pytestconfig)

"""Used as base class for pytests of qubit classes"""
class BaseTest:
	qbt = None
	"""Pytest fixture that provides a temporary directory for writing test files"""
	fn set_tmpdir(self, request)
	fn teardown_class(cls)
	fn eigenvals(self, io_type, evals_reference)
	fn eigenvecs(self, io_type, evecs_reference)
	fn plot_evals_vs_paramvals(self, num_cpus, param_name, param_list)
	fn get_spectrum_vs_paramvals(self, num_cpus, io_type, param_name, param_list, evals_reference, evecs_reference)
	fn matrixelement_table(self, io_type, op, matelem_reference)
	fn plot_matrixelements(self, op, evals_count)
	fn print_matrixelements(self, op)
	fn plot_matelem_vs_paramvals(self, num_cpus, op, param_name, param_list, select_elems)
	fn test_file_io(self)


class StandardTests(BaseTest):
	fn setup_class(cls)
	fn test_hamiltonian_is_hermitean(self, io_type)
	fn test_eigenvals(self, io_type)
	fn test_eigenvecs(self, io_type)
	fn test_plot_wavefunction(self, io_type)
	fn test_plot_evals_vs_paramvals(self, num_cpus, io_type)
	fn test_get_spectrum_vs_paramvals(self, num_cpus, io_type)
	fn test_matrixelement_table(self, io_type)
	fn test_plot_matrixelements(self, io_type)
	fn test_print_matrixelements(self, io_type)
	fn test_plot_matelem_vs_paramvals(self, num_cpus, io_type)
	fn test_plot_potential(self, io_type)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# testing.py
#
# This file is part of scqubits.
#
#    Copyright (c) 2019 and later, Jens Koch and Peter Groszkowski
#    All rights reserved.
#
#    This source code is licensed under the BSD-style license found in the
#    LICENSE file in the root directory of this source tree.
############################################################################

import pytest

from scqubits.tests.conftest import TESTDIR


def run():
    """
    Run the pytest scripts for scqubits.
    """
    # runs tests in scqubits.tests directory
    pytest.main(["-v", TESTDIR])
<|endoftext|>"
},
{
"prompt": """"Utility Class to check if a payee is Sanction Hit/No with percentage."""
class SanctionsScreening(object):
	fn __init__(self, sanction_list_file, matching_percentage)
	"""Check if payee is a Hit with matched percentage."""
	fn payee_sanction_screening(self, payee_name)
	fn __read_sanction_list(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from SanctionsScreening import SanctionsScreening


class TestSanctionScreening(object):

    @pytest.fixture
    def screening_instance(self):
        return SanctionsScreening("sanctions_list.csv", 0.75)

    def testHitWithPercentage1(self, screening_instance):
        the_payee_name = "Kristopher Toe"
        result = screening_instance.payee_sanction_screening(the_payee_name)
        expected = ('Hit', 0.9285714285714286)
        assert result == expected

    def testHitWithPercentage2(self, screening_instance):
        the_payee_name = "Kristophre Doe"
        result = screening_instance.payee_sanction_screening(the_payee_name)
        expected = ('Hit', 0.8571428571428571)
        assert result == expected

    def testHitWithPercentage3(self, screening_instance):
        the_payee_name = "Kristopher Doe"
        result = screening_instance.payee_sanction_screening(the_payee_name)
        expected = ('Hit', 1.0)
        assert result == expected

    def testNoHitWithPercentage(self, screening_instance):
        the_payee_name = "Tianze Zhang"
        result = screening_instance.payee_sanction_screening(the_payee_name)
        expected = ('No', 0.0)
        assert result == expected

    if __name__ == '__main__':
        pytest.main(['-sv', 'SanctionScreening.py'])
<|endoftext|>"
},
{
"prompt": "class ExeParams:
	parallel: Any = '-'
	proc: Any = '-'
	marshal: Any = '-'


"""Collapse any whitespace in stringified `s` into a single space. """
fn oneliner(s) -> str

fn flat_dict(d)

"""Asserts captured nodes have `attr` satisfying `regex` one by one (in a cycle).

:param count:
    The number-of-nodes expected.
    If ``True``, this number becomes the number-of-`regex`;
    if none, no count check happens."""
fn attr_check(attr)

fn check_xpath(etree, fname, path, check, be_found)

fn dilled(i)

fn pickled(i)

"""Same as a + b + ...."""
fn addall()

fn abspow(a, p)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import logging
import os
from collections import namedtuple
from multiprocessing import Pool
from multiprocessing import dummy as mp_dummy
from multiprocessing import get_context
from operator import add

import pytest

from graphtik import compose, operation
from graphtik.config import debug_enabled, execution_pool_plugged, tasks_marshalled

from .helpers import (
    _marshal,
    _parallel,
    _proc,
    _slow,
    _thread,
    dilled,
    exe_params,
    pickled,
)

# Enable pytest-sphinx fixtures
# See https://www.sphinx-doc.org/en/master/devguide.html#unit-testing
pytest_plugins = "sphinx.testing.fixtures"

# TODO: is this needed along with norecursedirs?
# See https://stackoverflow.com/questions/33508060/create-and-import-helper-functions-in-tests-without-creating-packages-in-test-di
collect_ignore = ["helpers.py"]

########
## From https://stackoverflow.com/a/57002853/548792
##
def pytest_addoption(parser):
    """Add a command line option to disable logger."""
    parser.addoption(
        "--logger-disabled",
        action="append",
        default=[],
        help="disable specific loggers",
    )


def pytest_configure(config):
    """Disable the loggers from CLI and silence sphinx markers warns."""
    for name in config.getoption("--logger-disabled", default=[]):
        logger = logging.getLogger(name)
        logger.propagate = False

    config.addinivalue_line("markers", "sphinx: parametrized sphinx test-launches")
    config.addinivalue_line(
        "markers", "test_params: for parametrized sphinx test-launches"
    )


##
########


@pytest.fixture
def debug_mode():
    from graphtik.config import debug_enabled

    with debug_enabled(True):
        yield


@pytest.fixture(params=[dilled, pickled])
def ser_method(request):
    return request.param


@pytest.fixture(
    params=[
        # PARALLEL?, Thread/Proc?, Marshalled?
        (None, None, None),
        pytest.param((1, 0, 0), marks=(_parallel, _thread)),
        pytest.param((1, 0, 1), marks=(_parallel, _thread, _marshal)),
        pytest.param((1, 1, 1), marks=(_parallel, _proc, _marshal, _slow)),
        pytest.param(
            (1, 1, 0),
            marks=(
                _parallel,
                _proc,
                _slow,
                pytest.mark.xfail(reason="ProcessPool non-marshaled may fail."),
            ),
        ),
    ]
)
def exemethod(request):
    """Returns exe-method combinations, and store them globally, for xfail checks."""
    parallel, proc_pool, marshal = request.param
    exe_params.parallel, exe_params.proc, exe_params.marshal = request.param

    nsharks = None  # number of pool swimmers....

    with tasks_marshalled(marshal):
        if parallel:
            if proc_pool:
                if os.name == "posix":  # Allthough it is the default ...
                    # NOTE: "spawn" DEADLOCKS!.
                    pool = get_context("fork").Pool(nsharks)
                else:
                    pool = Pool(nsharks)
            else:
                pool = mp_dummy.Pool(nsharks)

            with execution_pool_plugged(pool), pool:
                yield parallel
        else:
            yield parallel


@pytest.fixture
def samplenet():
    """sum1 = (a + b), sum2 = (c + d), sum3 = c + (c + d)"""
    sum_op1 = operation(name="sum_op1", needs=["a", "b"], provides="sum1")(add)
    sum_op2 = operation(name="sum_op2", needs=["c", "d"], provides="sum2")(add)
    sum_op3 = operation(name="sum_op3", needs=["c", "sum2"], provides="sum3")(add)
    return compose("test_net", sum_op1, sum_op2, sum_op3)


@pytest.fixture(params=[10, 20])
def log_levels(request, caplog):
    caplog.set_level(request.param)
    return
<|endoftext|>"
},
{
"prompt": """"Generates a full mapping by using a simulated annealing algorithm from:
Orsila, H., Kangas, T., Salminen, E., Hämäläinen, T. D., & Hännikäinen, M.
(2007). Automated memory-aware application distribution for multi-processor
system-on-chips. Journal of Systems Architecture, 53(11), 795-815.e.

Args:
    platform (Platform): A platform
    random_seed (int, optional): A random seed for the RNG. Defautls to 42.
    record_statistics (bool, optional): Record statistics on mappings
        evaluated? Defautls to False.
    initial_temperature (float, optional): Initial temperature for
        simmulated annealing. Defaults to 1.0.
    final_temperature (float, optional): Final temperature for simmulated
        annealing. Defaults to 0.1.
    temperature_proportionality_constant (float, optional): Temperature
        prop. constant for simmulated annealing. Defaults to 0.5.
    radius (float, optional): The radius for searching when moving.
        Defaults to 3.0.
    dump_cache (bool, optional): Dump the mapping cache? Defaults to False.
    chunk_size (int, optional): Size of chunks for parallel simulation.
        Defaults to 10.
    progress (bool, optional): Display simulation progress visually?
        Defaults to False.
    parallel (bool, optional): Execute simulations in parallel?
        Defaults to False.
    jobs (int, optional): Number of jobs for parallel simulation.
        Defaults to 1."""
class SimulatedAnnealingMapper(BaseMapper):
	fn __init__(self, platform, random_seed, record_statistics, initial_temperature, final_temperature, temperature_proportionality_constant, radius, dump_cache, chunk_size, progress, parallel, jobs)
	fn temperature_cooling(self, temperature, iteration, max_rejections)
	fn query_accept(self, time, temperature)
	fn move(self, representation, mapping, temperature)
	"""Generate a full mapping using simulated annealing.
	
	Args:
	    graph (DataflowGraph): a dataflow graph
	    trace (TraceGenerator, optional): a trace generator
	    representation (MappingRepresentation, optional): a mapping
	        representation object
	    processors (:obj:`list` of :obj:`Processor`, optional): a list of
	        processors to map to.
	    partial_mapping (Mapping, optional): a partial mapping to complete
	
	Returns:
	    Mapping: the generated mapping."""
	fn generate_mapping(self, graph: DataflowGraph, trace: TraceGenerator, representation: MappingRepresentation, processors, partial_mapping: Mapping) -> Mapping


class MockMappingCache:
	fn __init__(self, simres_evaluation_function, mocker)
	fn reset_statistics(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Copyright (C) 2020 TU Dresden
# Licensed under the ISC license (see LICENSE.txt)
#
# Authors: Felix Teweleit, Andres Goens

from mocasin.mapper.test.mock_cache import MockMappingCache
from mocasin.mapper.simulated_annealing import SimulatedAnnealingMapper
from itertools import product
import pytest
import numpy as np


@pytest.fixture
def conf():
    return {
        "mapper": {
            "random_seed": 42,
            "record_statistics": False,
            "initial_temperature": 1.0,
            "final_temperature": 0.01,
            "temperature_proportionality_constant": 0.5,
            "radius": 2,
            "dump_cache": False,
            "chunk_size": 10,
            "progress": False,
            "parallel": True,
            "jobs": 4,
        },
        "norm_p": 2,
        "periodic_boundary_conditions": False,
        "representation": "SimpleVector",
        "channels": False,
    }


@pytest.fixture
def mapper(
    graph, platform, trace, representation, simres_evaluation_function, mocker
):
    m = SimulatedAnnealingMapper(
        graph,
        platform,
        trace,
        representation,
        42,
        False,
        1.0,
        0.01,
        0.5,
        2,
        False,
        10,
        False,
        True,
        4,
    )
    m.simulation_manager = MockMappingCache(simres_evaluation_function, mocker)
    return m


def test_sa(mapper, evaluation_function):
    result_mapper = mapper.generate_mapping()
    results = [
        (evaluation_function([x, y]), x, y)
        for x, y in product(range(7), range(7))
    ]
    expected = set([(x, y) for (_, x, y) in sorted(results)[:5]])

    # result is top 5 best
    assert tuple(result_mapper.to_list()) in expected


def test_temperature_cooling(conf, mapper):
    timeout = 10000
    temperature = conf["mapper"]["initial_temperature"]

    for i in range(timeout):
        temperature = mapper.temperature_cooling(temperature, i)
        if temperature <= conf["mapper"]["final_temperature"]:
            break

    assert temperature <= conf["mapper"]["final_temperature"]


def test_query_accept(mapper):
    mapper.initial_cost = 1

    assert np.isclose(mapper.query_accept(1000, 0.0001), 0)

    positive = mapper.query_accept(1.000001, 1)
    assert positive > 0
    assert positive < 1
<|endoftext|>"
},
{
"prompt": "fn generate_url(endpoint)

"""Get Rate Limit Information from response headers (A Dictionary)
:returns: Dictionary of 'remaining' (int), 'limit' (int), 'reset' (time)"""
fn get_rate_limit_info(headers)

fn get_result(response, return_args)

"""Get access token for Twitter OAuth2 authentication.
:param consumer_key: Twitter application consumer key
:param consumer_secret: Twitter application consumer secret"""
class TwitterOAuth2(requests.auth.AuthBase):
	fn __init__(self, consumer_key, consumer_secret, proxies)
	fn _get_credentials(self)
	fn get_access_token(self)
	fn __call__(self, r)


"""Raised when request fails"""
class TwitterError(Exception):
	fn __init__(self, status_code, error_messages)
	fn __str__(self)


"""HTTP Status Code < HTTP_STATUS_INTERNAL_SERVER_ERROR (Should not retry)"""
class TwitterRequestError(TwitterError):


"""HTTP Status Code >= HTTP_STATUS_INTERNAL_SERVER_ERROR (You may retry)"""
class TwitterServerError(TwitterError):


"""HTTP Status Code 304 """
class TwitterNotModifiedError(TwitterRequestError):
	fn __init__(self, error_messages)


"""HTTP Status Code 400 """
class TwitterBadRequestError(TwitterRequestError):
	fn __init__(self, error_messages)


"""HTTP Status Code 401 """
class TwitterUnauthorizedError(TwitterRequestError):
	fn __init__(self, error_messages)


"""HTTP Status Code 403 """
class TwitterForbiddenError(TwitterRequestError):
	fn __init__(self, error_messages)


"""HTTP Status Code 404 """
class TwitterNotFoundError(TwitterRequestError):
	fn __init__(self, error_messages)


"""HTTP Status Code 410 """
class TwitterGoneError(TwitterRequestError):
	fn __init__(self, error_messages)


"""HTTP Status Code 420 """
class TwitterEnhanceYourCalmError(TwitterRequestError):
	fn __init__(self, error_messages)


"""HTTP Status Code 422 """
class TwitterUnprocessableEntityError(TwitterRequestError):
	fn __init__(self, error_messages)


"""HTTP Status Code 429 """
class TwitterTooManyRequestsError(TwitterRequestError):
	fn __init__(self, error_messages)


"""HTTP Status Code 500 """
class TwitterInternalServerError(TwitterServerError):
	fn __init__(self, error_messages)


"""HTTP Status Code 502 """
class TwitterBadGatewayError(TwitterServerError):
	fn __init__(self, error_messages)


"""HTTP Status Code 503 """
class TwitterServiceUnavailableError(TwitterServerError):
	fn __init__(self, error_messages)


"""HTTP Status Code 504 """
class TwitterGatewayTimeoutError(TwitterServerError):
	fn __init__(self, error_messages)


fn raise_twitter_error(status_code, error_messages)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# -*- coding: utf-8 -*-

from .twitter_auth import TwitterOAuth2 
from .twitter_error import *
from .utils import generate_url
from .variables import ENDPOINT_AUTH

import pytest
import requests

def test_get_access_token(requests_mock):
    requests_mock.post(generate_url(ENDPOINT_AUTH), json={'token_type': 'bearer', 'access_token': 'ACCESSTOKENAAAAAAAAAALYX8wAAAAAAsgM6MD8%2F%2BqU75lSu8vhowzkfuZQ%3Db2MlUpIakaSUtLi0JsSSCsg53v5BTBPu9e3t4Bcgt7i5rzn6e9'})
    consumer_key = 'key'
    consumer_secret = 'secret'
    client = TwitterOAuth2(consumer_key, consumer_secret)
    token = client.get_access_token()
    assert token  == 'ACCESSTOKENAAAAAAAAAALYX8wAAAAAAsgM6MD8%2F%2BqU75lSu8vhowzkfuZQ%3Db2MlUpIakaSUtLi0JsSSCsg53v5BTBPu9e3t4Bcgt7i5rzn6e9'

def test_get_access_token(requests_mock):
    requests_mock.post(generate_url(ENDPOINT_AUTH), status_code=403, json={'errors': [{'code': 99, 'message': 'Unable to verify your credentials', 'label': 'authenticity_token_error'}]})
    consumer_key = 'key'
    consumer_secret = ''
    client = TwitterOAuth2(consumer_key, consumer_secret)
    expected = "'code': 99, 'message': 'Unable to verify your credentials'"
    with pytest.raises(TwitterRequestError, match=expected) as exc_info:
        token = client.get_access_token()
    assert exc_info.value.errors == [{'code': 99, 'message': 'Unable to verify your credentials', 'label': 'authenticity_token_error'}]<|endoftext|>"
},
{
"prompt": "class PoolTestBase(TestScenarioBase):


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """"
Created on Dec 8, 2017

@author: nhan.nguyen
Verify that user can open a pool ledger with valid data.
"""

from indy import pool

from utilities import utils
from utilities import common, constant
from test_scripts.functional_tests.pool.pool_test_base import PoolTestBase
import pytest


class TestOpenPoolLedgerConfig(PoolTestBase):

    @pytest.mark.asyncio
    async def test(self):
        # 1. Create pool ledger configuration.
        self.steps.add_step("Create pool ledger config")
        await utils.perform(self.steps, common.create_pool_ledger_config,
                            self.pool_name, constant.pool_genesis_txn_file)
        # 2. Open pool ledger.
        self.steps.add_step("Open pool ledger")
        self.pool_handle = await \
            utils.perform(self.steps, pool.open_pool_ledger,
                          self.pool_name, None, ignore_exception=True)

        # 3. Verify that returned pool_handle is a positive integer.
        self.steps.add_step("Verify that returned pool_"
                            "handle is a positive integer")
        utils.check(self.steps, error_message="Cannot open pool ledger",
                    condition=lambda: isinstance(self.pool_handle, int) and
                    self.pool_handle >= 0)
<|endoftext|>"
},
{
"prompt": """"Load the UCI multiple features dataset [#1Data]_, taken from the UCI
Machine Learning Repository at
https://archive.ics.uci.edu/ml/datasets/Multiple+Features. This data set
consists of 6 views of handwritten digit images, with classes 0-9. The
6 views are the following:

1. 76 Fourier coefficients of the character shapes
2. 216 profile correlations
3. 64 Karhunen-Love coefficients
4. 240 pixel averages of the images from 2x3 windows
5. 47 Zernike moments
6. 6 morphological features

Each class contains 200 labeled examples.

Parameters
----------
select_labeled : optional, array-like, shape (n_features,) default (all)
    A list of the examples that the user wants by label. If not
    specified, all examples in the dataset are returned. Repeated labels
    are ignored.

views : optional, array-like, shape (n_views,) default (all)
    A list of the data views that the user would like in the indicated
    order. If not specified, all data views will be returned. Repeated
    views are ignored.

shuffle : bool, default=False
    If ``True``, returns each array with its rows and corresponding
    labels shuffled randomly according to ``random_state``.

random_state : int, default=None
    Determines the order data is shuffled if ``shuffle=True``. Used so
    that data loaded is reproducible but shuffled.

Returns
-------
data : list of np.ndarray, each of size (200*num_classes, n_features)
    List of length 6 with each element being the data for one of the
    views.

labels : np.ndarray
    Array of labels for the digit

References
----------
.. [#1Data] M. van Breukelen, R.P.W. Duin, D.M.J. Tax, and J.E. den Hartog,
        Handwritten digit recognition by combined classifiers, Kybernetika,
        vol. 34, no. 4, 1998, 381-386

Examples
--------
>>> from mvlearn.datasets import load_UCImultifeature
>>> # Load 6-view dataset with all 10 classes
>>> mv_data, labels = load_UCImultifeature()
>>> print(len(mv_data))
6
>>> print([mv_data[i].shape for i in range(6)])
[(2000, 76), (2000, 216), (2000, 64), (2000, 240), (2000, 47), (2000, 6)]
>>> print(labels.shape)
(2000,)"""
fn load_UCImultifeature(select_labeled: optional, array-like, shape (n_features,) default (all), views: optional, array-like, shape (n_views,) default (all), shuffle: bool, default=False, random_state: int, default=None) -> np.ndarray

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Gavin Mischler
# 10/17/2019

import pytest
import numpy as np
from mvlearn.datasets.base import load_UCImultifeature

def test_UCImultifeature_dataloader():
    # load data
    data, labels = load_UCImultifeature()

    assert len(data) == 6
    assert labels.shape[0] == 2000

    # check size of data
    for i in range(6):
        assert data[i].shape[0] == 2000

    data1, labels1 = load_UCImultifeature()

    # check data and labels are same
    assert(np.allclose(data[0], data1[0]))
    assert(np.allclose(labels, labels1))

def test_UCImultifeature_randomstate_sameordifferent():

    # load data
    data, labels = load_UCImultifeature(shuffle=True, random_state=2)
    data1, labels1 = load_UCImultifeature(shuffle=True, random_state=5)
    data2, labels2 = load_UCImultifeature(shuffle=True, random_state=2)
    data3, labels3 = load_UCImultifeature(shuffle=False)

    assert len(data) == 6
    assert labels.shape[0] == 2000

    # check size of data
    for i in range(6):
        assert data[i].shape[0] == 2000

    # check data is same
    for idx in range(6):
        assert(np.allclose(data[idx], data2[idx]))
        assert(not np.allclose(data[idx], data1[idx]))
        assert(not np.allclose(data[idx], data3[idx]))
        assert(not np.allclose(data1[idx], data3[idx]))

def test_UCImultifeature_dataloader_select_labels():
    # load data
    lab = [0,1,2]
    data, labels = load_UCImultifeature(select_labeled=lab)

    assert len(data) == 6

    assert labels.shape[0] == 600
    labels_set = list(set(labels))
    assert len(labels_set) == len(lab)
    for j, lab_in_set in enumerate(labels_set):
        assert lab_in_set == lab[j]

    # check size of data
    for i in range(6):
        assert data[i].shape[0] == 600

def test_UCImultifeature_dataloader_select_views():
    # load data
    views = [4, 5, 1]
    o_data, o_labels = load_UCImultifeature()
    data, labels = load_UCImultifeature(views=views)

    assert len(data) == len(views)
    assert labels.shape[0] == 2000

    # Check the shape of the data
    for i in range(len(views)):
        assert data[i].shape == o_data[views[i]].shape

        
def test_UCImultifeature_dataloader_badselect():
    with pytest.raises(ValueError):
        data, labels = load_UCImultifeature(select_labeled=[])

def test_UCImultifeature_dataloader_badselect2():
    long_list = [0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
    with pytest.raises(ValueError):
        data, labels = load_UCImultifeature(select_labeled=long_list)

def test_UCImultifeature_dataloader_badselect3():
    bad_list = [0,2,4,-2]
    with pytest.raises(ValueError):
        data, labels = load_UCImultifeature(select_labeled=bad_list)

def test_dataloader_badviews1():
    v_list = []
    with pytest.raises(ValueError):
        data, labels = load_UCImultifeature(views=v_list)

def test_dataloader_badviews2():
    v_list = [4, 6]
    with pytest.raises(ValueError):
        data, labels = load_UCImultifeature(views=v_list)
<|endoftext|>"
},
{
"prompt": "

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from __future__ import annotations

import threading
import time
from typing import Any, List
from unittest import TestCase

import pytest

from joulehunter.low_level.stat_profile import setstatprofile

from ..util import busy_wait, do_nothing


class CallCounter:
    def __init__(self, thread) -> None:
        self.thread = thread
        self.count = 0

    def __call__(self, *args: Any, **kwds: Any) -> Any:
        assert self.thread is threading.current_thread()
        self.count += 1


def test_threaded():
    # assert that each thread gets its own callbacks, and check that it
    # doesn't crash!

    counters: list[CallCounter | None] = [None for _ in range(10)]
    stop = False

    def profile_a_busy_wait(i):
        thread = threads[i]
        counter = CallCounter(thread)
        counters[i] = counter

        setstatprofile(counter, 0.001)
        while not stop:
            do_nothing()
        setstatprofile(None)

    threads = [threading.Thread(target=profile_a_busy_wait, args=(i,)) for i in range(10)]
    for thread in threads:
        thread.start()

    while not stop:
        stop = all(c is not None and c.count > 10 for c in counters)

    for thread in threads:
        thread.join()
<|endoftext|>"
},
{
"prompt": "class BaseSerializer(contracts.RpcEntitySerializer):
	fn __init__(self, version: str)
	fn handle_request(self, data: dict) -> protocol.RpcRequest
	fn handle_notification(self, data: dict) -> protocol.RpcNotification
	fn handle_result(self, data: dict) -> protocol.RpcResult
	fn handle_error(self, data: dict) -> protocol.RpcEntity
	fn handle_entity(self, data: dict) -> protocol.RpcEntity


class JsonSerializer(BaseSerializer):
	fn __init__(self, encoding: str, version: str)
	fn entity_to_bytes(self, entity: protocol.RpcEntity) -> bytes
	fn bytes_to_entity(self, data: bytes) -> protocol.RpcEntity


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from jsonrpc_stream.serializers import JsonSerializer
from jsonrpc_stream import protocol as pro

import json
import pytest


@pytest.fixture
def utf8() -> JsonSerializer: return JsonSerializer('utf-8')


def test_request_to_bytes(utf8: JsonSerializer):
    r = pro.RpcRequest(0, 'kek', None)
    data = {'jsonrpc': "2.0", 'method': 'kek', 'id': 0}
    serialized = json.loads(
        utf8.entity_to_bytes(r).decode('utf-8')
    )
    assert serialized == data


def test_bytes_to_request(utf8: JsonSerializer):
    r = pro.RpcRequest(0, 'kek', None)
    expectation = r.to_dict()
    reality = utf8.bytes_to_entity(utf8.entity_to_bytes(r)).to_dict()

    assert expectation == reality


def test_bytes_to_notification(utf8: JsonSerializer):
    r = pro.RpcNotification('kek', None)
    expectation = r.to_dict()
    reality = utf8.bytes_to_entity(utf8.entity_to_bytes(r)).to_dict()

    assert expectation == reality


def test_bytes_to_result(utf8: JsonSerializer):
    r = pro.RpcResult(0, 'yeet')
    expectation = r.to_dict()
    reality = utf8.bytes_to_entity(utf8.entity_to_bytes(r)).to_dict()

    assert expectation == reality


def test_bytes_to_error_nodata(utf8: JsonSerializer):
    r = pro.RpcError(
        0, pro.RpcErrorDetails(
            123, 'yeet', None
        )
    )
    expectation = r.to_dict()
    reality = utf8.bytes_to_entity(utf8.entity_to_bytes(r)).to_dict()

    assert expectation == reality


def test_bytes_to_error_data(utf8: JsonSerializer):
    r = pro.RpcError(
        0, pro.RpcErrorDetails(
            123, 'yeet', 'additional errorz'
        )
    )
    expectation = r.to_dict()
    reality = utf8.bytes_to_entity(utf8.entity_to_bytes(r)).to_dict()

    assert expectation == reality


def test_deserialize_doesnt_raise(utf8: JsonSerializer):
    malformed = utf8.bytes_to_entity(b'{"jsonrpc": "2.0", "kek": "top"}')
    assert isinstance(malformed, pro.RpcMalformed)
    assert malformed.id is None


def test_deserialize_doesnt_raise_with_id(utf8: JsonSerializer):
    malformed = utf8.bytes_to_entity(b'{"jsonrpc": "2.0", "id": "yeet"}')
    assert isinstance(malformed, pro.RpcMalformed)
    assert malformed.id == 'yeet'


def test_parser_error_doesnt_raise(utf8: JsonSerializer):
    malformed = utf8.bytes_to_entity(b'{yeet}')
    assert isinstance(malformed, pro.RpcMalformed)
    assert malformed.id is None


def test_batch_with_malformed_to_entity(utf8: JsonSerializer):
    r = [
        {"jsonrpc": "2.0", "method": "sum", "params": [1, 2, 4], "id": "1"},
        {"jsonrpc": "2.0", "method": "notify_hello", "params": [7]},
        {"jsonrpc": "2.0", "method": "a", "params": [42, 23], "id": "2"},
        {"foo": "boo"},
        {"jsonrpc": "2.0", "method": "b", "params": {"x": "y"}, "id": "5"},
        {"jsonrpc": "2.0", "method": "get_data", "id": "9"}
    ]
    reality = utf8.bytes_to_entity(json.dumps(r).encode())
    expectation = pro.RpcBatch(entities=[
        pro.RpcRequest(id='1', method='sum', params=[1, 2, 4], jsonrpc='2.0'),
        pro.RpcNotification(method='notify_hello', params=[7], jsonrpc='2.0'),
        pro.RpcRequest(id='2', method='a', params=[42, 23], jsonrpc='2.0'),
        pro.RpcRequest(id='5', method='b', params={'x': 'y'}, jsonrpc='2.0'),
        pro.RpcRequest(id='9', method='get_data', params=None, jsonrpc='2.0')]
    )

    assert isinstance(reality, pro.RpcBatch)
    #  test if all entities are present except the malformed one
    #  because exceptions are sadly not equatable
    matches = len([x for x in expectation.entities if x in reality.entities])
    assert matches == len(expectation.entities)
    x = [x for x in reality.entities if isinstance(x, pro.RpcMalformed)]
    assert len(x) == 1
    assert not x[0].id
<|endoftext|>"
},
{
"prompt": """"trims trailing data from the central directory
code taken from http://stackoverflow.com/a/7457686/570216, courtesy of Uri Cohen"""
fn repair_central_directory(zipFile, is_file_instance)

"""Check the file is a valid zipfile"""
fn _validate_archive(filename)

fn _find_workbook_part(package)

"""Open the given filename and return the workbook

:param filename: the path to open or a file-like object
:type filename: string or a file-like object open in binary mode c.f., :class:`zipfile.ZipFile`

:param read_only: optimised for reading, content cannot be edited
:type read_only: bool

:param keep_vba: preseve vba content (this does NOT mean you can use it)
:type keep_vba: bool

:param guess_types: guess cell content type and do not read it from the file
:type guess_types: bool

:param data_only: controls whether cells with formulae have either the formula (default) or the value stored the last time Excel read the sheet
:type data_only: bool

:param keep_links: whether links to external workbooks should be preserved. The default is True
:type keep_links: bool

:rtype: :class:`openpyxl2.workbook.Workbook`

.. note::

    When using lazy load, all worksheets will be :class:`openpyxl.worksheet.iter_worksheet.IterableWorksheet`
    and the returned workbook will be read-only."""
fn load_workbook(filename: string or a file-like object open in binary mode c.f., :class:`zipfile.ZipFile`, read_only: bool, keep_vba: bool, data_only: bool, guess_types: bool, keep_links: bool) -> :class:`openpyxl2.workbook.Workbook`
.. note::

    When using lazy load, all worksheets will be :class:`openpyxl.worksheet.iter_worksheet.IterableWorksheet`
    and the returned workbook will be read-only.

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from __future__ import absolute_import
# Copyright (c) 2010-2018 openpyxl

import pytest

# compatibility imports
from openpyxl2.compat import unicode

# package imports
from openpyxl2.styles import numbers
from openpyxl2.reader.excel import load_workbook


@pytest.mark.parametrize("cell, number_format",
                    [
                        ('A1', numbers.FORMAT_GENERAL),
                        ('A2', numbers.FORMAT_DATE_XLSX14),
                        ('A3', numbers.FORMAT_NUMBER_00),
                        ('A4', numbers.FORMAT_DATE_TIME3),
                        ('A5', numbers.FORMAT_PERCENTAGE_00),
                    ]
                    )
def test_read_general_style(datadir, cell, number_format):
    datadir.join("genuine").chdir()
    wb = load_workbook('empty-with-styles.xlsx')
    ws = wb["Sheet1"]
    assert ws[cell].number_format == number_format


def test_read_no_theme(datadir):
    datadir.join("genuine").chdir()
    wb = load_workbook('libreoffice_nrt.xlsx')
    assert wb


@pytest.mark.parametrize("guess_types, dtype",
                         (
                             (True, float),
                             (False, unicode),
                         )
                        )
def test_guess_types(datadir, guess_types, dtype):
    datadir.join("genuine").chdir()
    wb = load_workbook('guess_types.xlsx', guess_types=guess_types)
    ws = wb.active
    assert isinstance(ws['D2'].value, dtype)
<|endoftext|>"
},
{
"prompt": """"Serves as a base exception for all exceptions in this library.

.. todo::
    Ensure this is actually the case given we call into both ``aiodocker``
    and ``docker``"""
class DockerException(Exception):


"""Raised when the end of a stream is not valid."""
class ChunkedStreamingError(RuntimeError):


"""An attribute or function that to to be removed before reaching version 1."""
class DeprecatedInVersion1(DeprecationWarning):


"""An attribute or function that is kept for ``aiodocker`` compatibility.

These will eventually be removed to ensure a consistent API"""
class AioDockerDeprecationWarning(DeprecatedInVersion1):


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import asyncio

import pytest

import adocker.api as a_api
import adocker.errors as a_errors


@pytest.fixture()
def adocker_api(request, event_loop):
    asyncio.set_event_loop(event_loop)
    with pytest.warns(a_errors.AioDockerDeprecationWarning):
        api = a_api.APIClient()

    def cleanup():
        event_loop.run_until_complete(api.close())
    request.addfinalizer(cleanup)
    return api
<|endoftext|>"
},
{
"prompt": "

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Copyright (c) 2022 OpenCyphal
# This software is distributed under the terms of the MIT License.
# Author: Pavel Kirienko <pavel@opencyphal.org>

from __future__ import annotations
import asyncio
from typing import Any
import pytest
import pycyphal
from pycyphal.transport.loopback import LoopbackTransport
from yakut.subject_specifier_processor import process_subject_specifier, SubjectResolver
from yakut.subject_specifier_processor import BadSpecifierError, NoFixedPortIDError, NetworkDiscoveryError
from yakut import dtype_loader


@pytest.mark.asyncio
async def _unittest_without_subject_resolver(compiled_dsdl: Any) -> None:
    _ = compiled_dsdl
    asyncio.get_running_loop().slow_callback_duration = 5.0

    import uavcan.primitive
    import uavcan.node

    def get_subject_resolver() -> SubjectResolver:
        raise RuntimeError("Subject resolver shall not be used in this test")

    async def once(specifier: str) -> tuple[int, Any]:
        return await process_subject_specifier(specifier, get_subject_resolver)

    assert (123, uavcan.primitive.Empty_1) == await once("123:uavcan.primitive.Empty.1")

    fpid = pycyphal.dsdl.get_fixed_port_id(uavcan.node.Heartbeat_1)
    assert fpid is not None
    assert (fpid, uavcan.node.Heartbeat_1) == await once("uavcan.node.Heartbeat.1")

    with pytest.raises(BadSpecifierError):
        await once("99999999:uavcan.primitive.Empty.1")

    with pytest.raises(BadSpecifierError):
        await once("not_a_number:uavcan.primitive.Empty.1")

    with pytest.raises(dtype_loader.FormatError):
        await once("bad:format:error")

    with pytest.raises(dtype_loader.FormatError):
        await once("123:123.123.123")

    with pytest.raises(dtype_loader.NotFoundError):
        await once("123:uavcan.primitive.Empty.1.250")

    with pytest.raises(NoFixedPortIDError):
        await once("uavcan.primitive.Empty")


@pytest.mark.asyncio
async def _unittest_with_subject_resolver(compiled_dsdl: Any) -> None:
    _ = compiled_dsdl
    asyncio.get_running_loop().slow_callback_duration = 5.0

    from pycyphal.application import make_node, NodeInfo, register
    import uavcan.primitive.scalar

    local_node = make_node(info=NodeInfo(), transport=LoopbackTransport(2222))
    subject_resolver = SubjectResolver(local_node)

    def advertise(kind: str, name: str, dtype_name: str, port_id: int) -> None:
        local_node.registry[f"uavcan.{kind}.{name}.id"] = register.ValueProxy(register.Natural16([port_id]))
        local_node.registry[f"uavcan.{kind}.{name}.type"] = register.ValueProxy(register.String(dtype_name))

    advertise("pub", "foo", "uavcan.primitive.scalar.Bit.1.200", 500)
    advertise("sub", "bar", "uavcan.primitive.scalar.Integer8.1.200", 600)  # minor version ignored
    advertise("pub", "bar", "uavcan.node.GetInfo.1.0", 600)  # service type ignored
    advertise("pub", "bad", "nonexistent.DataType.1.0", 600)  # nonexistent type ignored
    advertise("sub", "baz", "uavcan.primitive.scalar.Real16.1.200", 0xFFFF)

    local_node.start()

    async def once(specifier: str) -> tuple[int, Any]:
        return await process_subject_specifier(specifier, lambda: subject_resolver)

    assert (500, uavcan.primitive.scalar.Bit_1_0) == await once("500")
    assert (600, uavcan.primitive.scalar.Integer8_1) == await once("600")  # minor version ignored

    with pytest.raises(NetworkDiscoveryError):
        await once("700")  # nonexistent

    subject_resolver.close()
    local_node.close()
    await asyncio.sleep(1.0)
<|endoftext|>"
},
{
"prompt": "fn z_norm(a, axis, threshold)

fn distance(a, b, axis)

fn apply_exclusion_zone(D, trivial_idx, excl_zone)

fn distance_profile(Q, T, m)

fn distance_matrix(T_A, T_B, m)

fn mass(Q, T, m, trivial_idx, excl_zone, ignore_trivial)

fn stamp(T_A, m, T_B, exclusion_zone)

"""Traverse distance matrix along the diagonals and update the matrix profile and
matrix profile indices"""
fn stump(T_A, m, T_B, exclusion_zone)

fn replace_inf(x, value)

fn multi_mass(Q, T, m, include, discords)

fn PI(D, trivial_idx, excl_zone)

fn mstump(T, m, excl_zone, include, discords)

fn get_array_ranges(a, n_chunks, truncate)

fn aamp(T_A, m, T_B, exclusion_zone)

class aampi_egress(object):
	fn __init__(self, T, m, excl_zone)
	fn update(self, t)


class stumpi_egress(object):
	fn __init__(self, T, m, excl_zone)
	fn update(self, t)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import numpy as np
import numpy.testing as npt
import pandas as pd
from stumpy import stumped, core
from dask.distributed import Client, LocalCluster
import pytest
import warnings
import naive


@pytest.fixture(scope="module")
def dask_cluster():
    cluster = LocalCluster(n_workers=2, threads_per_worker=2)
    yield cluster
    cluster.close()


test_data = [
    (
        np.array([9, 8100, -60, 7], dtype=np.float64),
        np.array([584, -11, 23, 79, 1001, 0, -19], dtype=np.float64),
    ),
    (
        np.random.uniform(-1000, 1000, [8]).astype(np.float64),
        np.random.uniform(-1000, 1000, [64]).astype(np.float64),
    ),
]

substitution_locations = [0, -1, slice(1, 3), [0, 3]]


@pytest.mark.filterwarnings("ignore:numpy.dtype size changed")
@pytest.mark.filterwarnings("ignore:numpy.ufunc size changed")
@pytest.mark.filterwarnings("ignore:numpy.ndarray size changed")
@pytest.mark.filterwarnings("ignore:\\s+Port 8787 is already in use:UserWarning")
@pytest.mark.parametrize("T_A, T_B", test_data)
@pytest.mark.parametrize("substitution_location_B", substitution_locations)
def test_stumped_one_subsequence_inf_A_B_join(
    T_A, T_B, substitution_location_B, dask_cluster
):
    with Client(dask_cluster) as dask_client:
        m = 3

        T_B_sub = T_B.copy()
        T_B_sub[substitution_location_B] = np.inf

        ref_mp = naive.stump(T_A, m, T_B=T_B_sub)
        comp_mp = stumped(dask_client, T_A, m, T_B_sub, ignore_trivial=False)
        naive.replace_inf(ref_mp)
        naive.replace_inf(comp_mp)
        npt.assert_almost_equal(ref_mp, comp_mp)
<|endoftext|>"
},
{
"prompt": """"Extracts an hour of test data for one hour and ground truth prediction
5,10,15,30,45 and 60 minutes into the future.

Parameters
----------

data
    tensor of shape (24+, 495, 436, 8) of  type uint8
offset
to_torch:bool
    convert to torch float tensor.

Returns
-------
    test_data
        tensor of shape (12, 495, 436, 8) of  type uint8
    ground_truth_prediction
        tensor of shape (6, 495, 436, 8) of  type uint8"""
fn prepare_test(data: np.ndarray, offset: None, to_torch: bool) -> Tuple[(Union[(np.ndarray, torch.Tensor)], Union[(np.ndarray, torch.Tensor)])]

"""Parameters
----------
offset: int

Returns
-------
    the 6 indices for the prediction horizon, i.e. offset+12, offset+13, ...., offset+23"""
fn prepare_within_day_indices_for_ground_truth(offset: int) -> np.ndarray

fn _generate_test_slots()

fn _create_test_manifest(config, test_slot_directory)

fn _check_test_manifest(config, test_slot_directory)

fn _check_exists_dir(city_output_directory)

fn _get_contents_from_tarballs_and_directories(dynamic_data_path, check_tars, check_dirs) -> Dict[(Tuple[(str, str)], Tuple[(str, str)])]

fn _extract_h5_for_city_and_date(city: str, city_date_finder, date: str, output_directory: str)

fn _prepare_test_slots_city(arg, city_date_finder, competition, expected_number_of_slots, output_directory, output_directory_ground_truth, tempdir)

fn _extract_and_slice_h5_files_for_testing(city_date_finder, output_directory, output_directory_ground_truth, test_slots_dir, expected_number_of_slots_per_city)

fn _check_expanded_test_slots(expanded_test_slots, city_date_finder, output_directory, output_directory_ground_truth)

"""Creates the argument parser for this program."""
fn create_parser() -> argparse.ArgumentParser

fn main(params)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "#  Copyright 2021 Institute of Advanced Research in Artificial Intelligence (IARAI) GmbH.
#  IARAI licenses this file to You under the Apache License, Version 2.0
#  (the "License"); you may not use this file except in compliance with
#  the License. You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
import numpy as np
import pytest

from competition.prepare_test_data.prepare_test_data import prepare_test
from data.data_layout import channel_labels
from data.data_layout import speed_channel_indices
from data.data_layout import static_channel_labels
from data.data_layout import volume_channel_indices


@pytest.mark.parametrize("offset", [0, 5, 6])
def test_prepare_test(offset):
    data = np.random.rand(24 + offset, 495, 436, 8)
    test_data, ground_truth_prediction = prepare_test(data=data, offset=offset)
    assert test_data.shape == (12, 495, 436, 8)
    assert ground_truth_prediction.shape == (6, 495, 436, 8)
    ub = offset + 12
    assert (test_data == data[offset:ub]).all()
    # 5,10,15,30,45 and 60 into the future
    assert (ground_truth_prediction[0] == data[offset + 11 + 5 // 5]).all()
    assert (ground_truth_prediction[1] == data[offset + 11 + 10 // 5]).all()
    assert (ground_truth_prediction[2] == data[offset + 11 + 15 // 5]).all()
    assert (ground_truth_prediction[3] == data[offset + 11 + 30 // 5]).all()
    assert (ground_truth_prediction[4] == data[offset + 11 + 45 // 5]).all()
    assert (ground_truth_prediction[5] == data[offset + 11 + 60 // 5]).all()


def test_channel_labels():
    print(channel_labels)
    print(static_channel_labels)
    print(volume_channel_indices)
    print(speed_channel_indices)
<|endoftext|>"
},
{
"prompt": "fn random_lower_string() -> str

fn random_email() -> str

fn get_superuser_token_headers(client: TestClient) -> Dict[(str, str)]

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from typing import Dict, Generator

import pytest
from fastapi.testclient import TestClient

from db_api.main import app

from .utils import get_superuser_token_headers


@pytest.fixture(scope="module")
def client() -> Generator:
    with TestClient(app) as c:
        yield c


@pytest.fixture(scope="module")
def superuser_token_headers(client: TestClient) -> Dict[str, str]:
    return get_superuser_token_headers(client)
<|endoftext|>"
},
{
"prompt": "fn wait_for_socket(ipc_path, timeout)

fn wait_for_http(endpoint_uri, timeout)

fn wait_for_popen(proc, timeout)

fn kill_proc_gracefully(proc)

class GoPlatonTest(Web3ModuleTest):
	fn _check_web3_clientVersion(self, client_version)


class GoPlatonPlatonModuleTest(PlatonModuleTest):
	fn test_platon_sign_typed_data(self, web3, unlocked_account_dual_type)
	fn test_invalid_platon_sign_typed_data(self, web3, unlocked_account_dual_type)
	fn test_platon_protocol_version(self, web3)
	fn test_platon_protocolVersion(self, web3)


class GoPlatonVersionModuleTest(VersionModuleTest):
	fn test_platon_protocol_version(self, web3)
	fn test_platon_protocolVersion(self, web3)


class GoPlatonNetModuleTest(NetModuleTest):


class GoPlatonAdminModuleTest(GoPlatonAdminModuleTest):


class GoPlatonPersonalModuleTest(GoPlatonPersonalModuleTest):


class GoPlatonAsyncPlatonModuleTest(AsyncPlatonModuleTest):


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from tests.utils import (
    get_open_port,
)
from platon import Web3
from platon.platon import (
    AsyncPlaton,
)
from platon.middleware import (
    async_buffered_gas_estimate_middleware,
    async_gas_price_strategy_middleware,
)
from platon.providers.async_rpc import (
    AsyncHTTPProvider,
)

from .common import (
    GoPlatonAdminModuleTest,
    GoPlatonAsyncPlatonModuleTest,
    GoPlatonPlatonModuleTest,
    GoPlatonNetModuleTest,
    GoPlatonPersonalModuleTest,
    GoPlatonTest,
    GoPlatonVersionModuleTest,
)
from .utils import (
    wait_for_aiohttp,
    wait_for_http,
)


@pytest.fixture(scope="module")
def rpc_port():
    return get_open_port()


@pytest.fixture(scope="module")
def endpoint_uri(rpc_port):
    return 'http://localhost:{0}'.format(rpc_port)


def _node_command_arguments(rpc_port,
                            base_node_command_arguments,
                            node_version):
    yield from base_node_command_arguments
    if node_version.major == 1:
        yield from (
            '--http',
            '--http.port', rpc_port,
            '--http.api', 'admin,platon,net,platon,personal,miner',
            '--ipcdisable',
            '--allow-insecure-unlock'
        )
    else:
        raise AssertionError("Unsupported Node version")


@pytest.fixture(scope='module')
def node_command_arguments(rpc_port,
                           base_node_command_arguments,
                           get_node_version):

    return _node_command_arguments(
        rpc_port,
        base_node_command_arguments,
        get_node_version
    )


@pytest.fixture(scope="module")
def web3(node_process, endpoint_uri):
    wait_for_http(endpoint_uri)
    _web3 = Web3(Web3.HTTPProvider(endpoint_uri))
    return _web3


@pytest.fixture(scope="module")
async def async_w3(node_process, endpoint_uri):
    await wait_for_aiohttp(endpoint_uri)
    _web3 = Web3(
        AsyncHTTPProvider(endpoint_uri),
        middlewares=[
            async_gas_price_strategy_middleware,
            async_buffered_gas_estimate_middleware
        ],
        modules={'platon': (AsyncPlaton,)})
    return _web3


class TestGoPlatonTest(GoPlatonTest):
    pass


class TestGoPlatonAdminModuleTest(GoPlatonAdminModuleTest):
    @pytest.mark.xfail(reason="running node with the --nodiscover flag doesn't allow peer addition")
    def test_admin_peers(self, web3: "Web3") -> None:
        super().test_admin_peers(web3)

    def test_admin_start_stop_rpc(self, web3: "Web3") -> None:
        # This test causes all tests after it to fail on CI if it's allowed to run
        pytest.xfail(reason='Only one RPC endpoint is allowed to be active at any time')
        super().test_admin_start_stop_rpc(web3)

    def test_admin_start_stop_ws(self, web3: "Web3") -> None:
        # This test causes all tests after it to fail on CI if it's allowed to run
        pytest.xfail(reason='Only one WS endpoint is allowed to be active at any time')
        super().test_admin_start_stop_ws(web3)


class TestGoPlatonPlatonModuleTest(GoPlatonPlatonModuleTest):
    pass


class TestGoPlatonVersionModuleTest(GoPlatonVersionModuleTest):
    pass


class TestGoPlatonNetModuleTest(GoPlatonNetModuleTest):
    pass


class TestGoPlatonPersonalModuleTest(GoPlatonPersonalModuleTest):
    pass


class TestGoPlatonAsyncPlatonModuleTest(GoPlatonAsyncPlatonModuleTest):
    pass
<|endoftext|>"
},
{
"prompt": "fn rpc(method: str, param_dict: Optional[dict], _id: str) -> dict

"""Represents a signal-cli/auxin-cli session.
Lifecycle: Downloads the datastore, runs and restarts signal client,
tries to gracefully kill signal and upload before exiting.
I/O: reads signal client's output into inbox,
has methods for sending commands to the signal client, and
actually writes those json blobs to signal client's stdin."""
class Signal:
	fn __init__(self, bot_number: Optional[str]) -> None
	sigints = 0
	"""Try to start async_shutdown and/or just sys.exit"""
	fn sync_signal_handler(self) -> None
	"""Done callback which logs task done result and/or restarts a task on error
	args:
	    task (asyncio.task): Finished task
	    _func (AsyncFunc): Async function to restart
	    f_args (dict): Args to pass to function on restart"""
	fn handle_task(self, task: asyncio.Task) -> None
	pending_requests: dict[str, asyncio.Future[Message]] = {}
	pending_messages_sent: dict[str, dict] = {}
	backoff = False
	messages_until_rate_limit = 50.0
	last_update = time.time()
	"""Returns whether we think signal server will rate limit us for sending a
	message right now"""
	fn update_and_check_rate_limit(self) -> bool


fn is_admin(msg: Message) -> bool

fn requires_admin(command: Callable) -> Callable

fn hide(command: Callable) -> Callable

"""Handles messages and command dispatch, as well as basic commands.
Must be instantiated within a running async loop.
Subclass this with your own commands."""
class Bot(Signal):
	"""Creates AND STARTS a bot that routes commands to do_x handlers"""
	fn __init__(self, bot_number: Optional[str]) -> None
	fn is_command(self, msg: Message) -> bool
	fn match_command(self, msg: Message) -> str
	fn documented_commands(self) -> str
	"""Returns a list of all known recipients by parsing underlying datastore."""
	fn get_recipients(self) -> list[dict[(str, str)]]
	"""Queries the recipients-store file for a UUID, provided a phone number."""
	fn get_uuid_by_phone(self, phonenumber: str) -> Optional[str]
	"""Queries the recipients-store file for a phone number, provided a uuid."""
	fn get_number_by_uuid(self, uuid_: str) -> Optional[str]


class PayBot(Bot):
	PAYMENTS_HELPTEXT = 'Enable Signal Pay:\n\n    1. In Signal, tap “🠔“ & tap on your profile icon in the top left & tap *Settings*\n\n    2. Tap *Payments* & tap *Activate Payments*\n\n    For more information on Signal Payments visit:\n\n    https://support.signal.org/hc/en-us/articles/360057625692-In-app-Payments'


"""A common pattern is to store intermediate state for individual users as a dictionary.
Users can be referred to by some combination of source (a phone number) or uuid (underlying user ID)
This abstracts over the possibility space, returning a boolean indicator of whether the sender of a Message
is referenced in a dict, and the value pointed at (if any)."""
fn get_source_or_uuid_from_dict(msg: Message, dict_: dict[(str, Any)]) -> Tuple[(bool, Any)]

fn is_first_device(msg: Message) -> bool

class QuestionBot(PayBot):
	fn __init__(self, bot_number: Optional[str]) -> None


fn fmt_ms(ts: int) -> str

fn run_bot(bot: Type[Bot], local_app: web.Application) -> None

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import asyncio
import os
import pathlib
from importlib import reload
import pytest
from forest import utils
from forest.core import Message, QuestionBot


def test_secrets(tmp_path: pathlib.Path) -> None:
    open(tmp_path / "dev_secrets", "w").write("A=B\nC=D")
    os.chdir(tmp_path)
    reload(utils)

    assert utils.get_secret("A") == "B"
    assert utils.get_secret("C") == "D"
    assert utils.get_secret("E") == ""


def test_root(tmp_path: pathlib.Path) -> None:
    assert reload(utils).ROOT_DIR == "."
    os.chdir(tmp_path)
    open(tmp_path / "dev_secrets", "w").write("DOWNLOAD=1")
    assert reload(utils).ROOT_DIR == "/tmp/local-signal"
    os.environ["FLY_APP_NAME"] = "A"
    assert reload(utils).ROOT_DIR == "/app"


class MockMessage(Message):
    def __init__(self, text: str) -> None:
        self.text = text
        self.source = "+" + "2" * 11
        self.uuid = "cf3d7d34-2dcd-4fcd-b193-cbc6a666758b"
        super().__init__({})


class MockBot(QuestionBot):
    async def start_process(self) -> None:
        pass

    async def get_output(self, text: str) -> str:
        await self.inbox.put(MockMessage(text))
        try:
            msg = await asyncio.wait_for(self.outbox.get(), timeout=1)
            return msg["params"]["message"]
        except asyncio.TimeoutError:
            return ""


alice = "+" + "1" * 11


@pytest.mark.asyncio
async def test_commands() -> None:
    bot = MockBot(alice)
    os.environ["ENABLE_MAGIC"] = "1"
    assert await bot.get_output("/pingg foo") == "/pong foo"
    # slightly slow
    # assert "printer" in (await bot.get_output("/printerfactt")).lower()
    assert (await bot.get_output("uptime")).startswith("Uptime: ")
    assert (
        await bot.get_output("/eval 1+1")
    ) == "you must be an admin to use this command"
<|endoftext|>"
},
{
"prompt": "fn medias()

fn creatable_media()

fn no_desc_media(medias)

"""For use when building GuestMedia records """
fn no_prefs_media(no_desc_media)

fn open_door_media(medias)

fn not_authorized_media(medias)

fn authorized_media_no_guest(medias)

"""This media should be used when creating a media_perm record."""
fn media_for_creatable_media_perm(medias)

"""This media should be used when creating a guest_media record."""
fn media_for_creatable_guest_media(medias)

"""Each media returned should be associated with a guest """
fn media_for_guests(medias)

"""Each media returned should NOT be associated with a guest """
fn media_without_guests(medias)

"""Each media returned should be associated with a permission """
fn media_for_permissions(medias)

fn add_media_helpers(monkeypatch_session)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from rfidsecuritysvc.model.media import Media


@pytest.fixture(scope='session')
def medias():
    # The DB will return these ordered by id, please build the list accordingly
    return [
        Media('NOT AUTHORIZED', 'Not Authorized', 'This media should never be used in a media_perm record.'),
        Media('TEST FOR AUTHORIZED NO GUEST', 'test for authorized media without a guest media record', None),
        Media('TEST FOR GUESTMEDIA CREATION', 'test for GuestMedia creation', None),
        Media('TEST FOR MEDIAPERM CREATION', 'test for MediaPerm creation', None),
        Media('TEST MEDIA 1', 'test media 1', 'Media for testing (1)'),
        Media('TEST MEDIA 2', 'test media 2', 'Media for testing (2)'),
        Media('TEST MEDIA 3', 'test media 3', 'Media for testing (3)'),
        Media('TEST MEDIA 4', 'test media 4', 'Media for testing (4)'),
        Media('TEST MEDIA 5', 'test media 5', 'Media for testing (5)'),
        Media('TEST OPEN DOOR', 'test open door', 'This media will be assigned the permission Open Door'),
        Media('TEST WITHOUT DESC', 'test without desc', None),
    ]


@pytest.fixture(scope='session')
def creatable_media():
    return Media('CREATABLE ID', 'creatable name', 'creatable desc')


@pytest.fixture(scope='session')
def no_desc_media(medias):
    for m in medias:
        if m.id == 'TEST WITHOUT DESC':
            return m


@pytest.fixture(scope='session')
def no_prefs_media(no_desc_media):
    """ For use when building GuestMedia records """
    return no_desc_media


@pytest.fixture(scope='session')
def open_door_media(medias):
    for m in medias:
        if m.id == 'TEST OPEN DOOR':
            return m


@pytest.fixture(scope='session')
def not_authorized_media(medias):
    for m in medias:
        if m.id == 'NOT AUTHORIZED':
            return m


@pytest.fixture(scope='session')
def authorized_media_no_guest(medias):
    for m in medias:
        if m.id == 'TEST FOR AUTHORIZED NO GUEST':
            return m


@pytest.fixture(scope='session')
def media_for_creatable_media_perm(medias):
    """ This media should be used when creating a media_perm record."""
    for m in medias:
        if m.id == 'TEST FOR MEDIAPERM CREATION':
            return m


@pytest.fixture(scope='session')
def media_for_creatable_guest_media(medias):
    """ This media should be used when creating a guest_media record."""
    for m in medias:
        if m.id == 'TEST FOR GUESTMEDIA CREATION':
            return m


@pytest.fixture(scope='session')
def media_for_guests(medias):
    """ Each media returned should be associated with a guest """
    return medias[4:9]


@pytest.fixture(scope='session')
def media_without_guests(medias):
    """ Each media returned should NOT be associated with a guest """
    return medias[0:4]


@pytest.fixture(scope='session')
def media_for_permissions(medias):
    """ Each media returned should be associated with a permission """
    return medias[4:]


@pytest.fixture(autouse=True, scope='session')
def add_media_helpers(monkeypatch_session):
    def create(self):
        return self.to_json()

    def update(self):
        copy = self.to_json()
        del copy['id']
        return copy

    monkeypatch_session.setattr(Media, 'test_create', create, raising=False)
    monkeypatch_session.setattr(Media, 'test_update', update, raising=False)
<|endoftext|>"
},
{
"prompt": """"Manage an `Ingest Pipeline <https://www.elastic.co/guide/en/elasticsearch/reference/master/put-pipeline-api.html>`__

:param id: ID of an ingest pipeline
:type id: str

:param processors: List of `Processors <https://www.elastic.co/guide/en/elasticsearch/reference/master/processors.html>`__ for an ingest pipeline.
:type processors: list

:param description: Description of the Ingest Pipeline.
:type description: str, optional

:param on_failure: List of `Processors <https://www.elastic.co/guide/en/elasticsearch/reference/master/processors.html>`__ in case of failure.
:type on_failure: list

:param version: Version number of the ingest pipeline
:type version: str, optional

:param _meta: Optional metadata about the ingest pipeline
:type _meta: dict, optional"""
class IngestPipeline(BaseResource):
	id: str
	processors: list
	description: Optional[str] = None
	on_failure: Optional[list] = None
	version: Optional[int] = None
	_meta: Optional[dict] = None
	"""Get an ingest pipeline from Elasticsearch
	
	:param client: Elasticsearch Client
	:type client: :py:mod:`Elasticsearch`
	
	:param pipeline_id: Ingest pipeline id
	:type pipeline_id: str"""
	fn get(cls, client: :py:mod:`Elasticsearch`, pipeline_id: str)
	"""Create an ingest pipeline in Elasticsearch
	
	:param client: Elasticsearch Client
	:type client: :py:mod:`Elasticsearch`"""
	fn create(self, client: :py:mod:`Elasticsearch`)
	"""Delete an ingestpipeline from Elasticsearch
	
	:param client: Elasticsearch Client
	:type client: :py:mod:`Elasticsearch`"""
	fn delete(self, client: :py:mod:`Elasticsearch`)
	"""Update an ingest pipeline in Elasticsearch
	
	:param client: Elasticsearch Client
	:type client: :py:mod:`Elasticsearch`"""
	fn update(self, client: :py:mod:`Elasticsearch`)
	"""Simulate an ingest pipeline in Elasticsearch
	
	:param client: Elasticsearch Client
	:type client: :py:mod:`Elasticsearch`
	
	:param docs: List of Documents to simulate in the ingest pipeline
	:type docs: list"""
	fn simulate(self, client: :py:mod:`Elasticsearch`, docs: list)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "#!/usr/bin/env python
# -*- coding: utf-8 -*-

import pytest
from dataclasses import FrozenInstanceError
from nemesis.resources.elasticsearch.ingest_pipeline import IngestPipeline
from elasticsearch import Elasticsearch, RequestError, NotFoundError

from nemesis.tests import es_client, nemesis_client


@pytest.fixture
def pipeline(es_client):
    pipeline = IngestPipeline(
        id="test-pipeline",
        processors=[
            {"pipeline": {"name": "pipelineA"}},
            {"set": {"field": "outer_pipeline_set", "value": "outer"}},
        ],
    )
    yield pipeline

    # clear remote resources if they exist
    try:
        result = pipeline.delete(es_client)
        assert result == {"acknowledged": True}
    except NotFoundError:
        pass


def test_asdict(pipeline):
    """ """
    assert pipeline.asdict() == {
        "id": "test-pipeline",
        "processors": [
            {"pipeline": {"name": "pipelineA"}},
            {"set": {"field": "outer_pipeline_set", "value": "outer"}},
        ],
    }


def test_frozen_object(pipeline):
    with pytest.raises(FrozenInstanceError):
        pipeline.processors = [{}]


def test_id(pipeline):
    assert pipeline.id == "test-pipeline"


def test_get_not_found(es_client):
    assert IngestPipeline.get(es_client, "notfound") is None


def test_create_multiple(es_client, pipeline):
    result = pipeline.create(es_client)
    assert result == {"acknowledged": True}

    result = pipeline.create(es_client)
    assert result == {"acknowledged": True}


def test_create_update_delete(pipeline, es_client):
    result = pipeline.create(es_client)
    assert result == {"acknowledged": True}

    result = IngestPipeline.get(es_client, pipeline.id)
    assert result == pipeline

    # Test that create called multiple times is ok.
    result = pipeline.create(es_client)
    assert result == {"acknowledged": True}

    pipeline.processors.append(
        {"set": {"field": "inner_pipeline_set", "value": "inner"}}
    )
    result = pipeline.update(es_client)
    assert result == {"acknowledged": True}

    result = pipeline.delete(es_client)
    assert result == {"acknowledged": True}
<|endoftext|>"
},
{
"prompt": "fn test_data()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
import os
import numpy as np
from math import cos, sin, pi

from pclpy import pcl
import pclpy
from .utils import test_data


def test_fit_line():
    line = np.array([(1, 2, 3), (2, 4, 6), (3, 7, 9), (5, 10, 15)])
    pc = pcl.PointCloud.PointXYZ(line)
    inliers, coefficients = pclpy.fit(pc, "line", distance=0.1)
    assert len(inliers.indices) == 3
    assert np.allclose(coefficients.values, pcl.vectors.Float([2.66667, 5.33333, 8, 0.267261, 0.534522, 0.801784]))


def test_fit_cylinder():
    points = np.array([(cos(a), sin(a), z) for z in np.linspace(0, 5, num=10) for a in np.linspace(0, 2 * pi, num=20)])
    points = np.vstack((np.array([10, 10, 10]), points))
    pc = pcl.PointCloud.PointXYZ(points)
    inliers, coefficients = pclpy.fit(pc, "circle2d", distance=0.01, indices=np.arange(100))
    assert 0 not in inliers.indices
    assert len(inliers.indices) == 99
    assert np.allclose(coefficients.values, [0., 0., 1.], atol=0.00001)
<|endoftext|>"
},
{
"prompt": "fn pytest_sim_params(metafunc)

fn outlines(capsys)

fn define_simple_circuit(T, circ_name, has_clk)

class TestPeekCircuit(m.Circuit):
	__test__ = False
	IO = ['I', m.In(T), 'O0', m.Out(T), 'O1', m.Out(T)]
	fn definition(io)


class ConfigReg(m.Circuit):
	IO = ['D', m.In(m.Bits[2]), 'Q', m.Out(m.Bits[2])] + m.ClockInterface(has_ce=True)
	fn definition(io)


class SimpleALU(m.Circuit):
	IO = ['a', m.In(m.UInt[16]), 'b', m.In(m.UInt[16]), 'c', m.Out(m.UInt[16]), 'config_data', m.In(m.Bits[2]), 'config_en', m.In(m.Enable)] + m.ClockInterface()
	fn definition(io)


class AndCircuit(m.Circuit):
	IO = ['I0', m.In(m.Bit), 'I1', m.In(m.Bit), 'O', m.Out(m.Bit)]
	fn definition(io)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import fault
import magma as m
from pathlib import Path
from .common import pytest_sim_params


def pytest_generate_tests(metafunc):
    pytest_sim_params(metafunc, 'system-verilog')


def test_def_vlog(target, simulator, n_bits=8, b_val=42):
    # declare circuit
    defadd = m.DeclareCircuit(
        'defadd',
        'a_val', m.In(m.Bits[n_bits]),
        'c_val', m.Out(m.Bits[n_bits])
    )

    # instantiate tester
    tester = fault.Tester(defadd)

    # define test
    tester.poke(defadd.a_val, 12)
    tester.eval()
    tester.expect(defadd.c_val, 54)
    tester.poke(defadd.a_val, 34)
    tester.eval()
    tester.expect(defadd.c_val, 76)

    # run simulation
    tester.compile_and_run(
        target=target,
        simulator=simulator,
        ext_libs=[Path('tests/verilog/defadd.sv').resolve()],
        defines={'N_BITS': n_bits, 'B_VAL': b_val},
        ext_model_file=True,
        tmp_dir=True
    )
<|endoftext|>"
},
{
"prompt": """"A tool for visualizing the structure and performance of Random Forests"""
fn main()

"""Web-based graphical user interface.

FOREST_DATA: Path to the JSON file that contains the forest's data. Can also take a directory or tar file which
    contains the JSON."""
fn gui(forest_data, port)

"""Command line interface to generate SVGs.

As Python is unable to render React components, we make a subprocess call to a small Node.js application which
will do the rendering and also store the created SVG files. This command requires that Node.js is installed on your
system!

FOREST_DATA: Path to the JSON file that contains the forest's data. Can also take a directory or tar file which
    contains the JSON."""
fn cli(forest_data, out, width, height, trunk_length, display_depth, branch_color, leaf_color)

"""Reads forest data from the filesystem

Args:
    forest_data (str): Either the path a folder or .tar.gz that contains a file called "forest.json" or the path
        to the forest JSON file itself.
Returns:
    (dict) Forest data"""
fn _read_data(forest_data: str)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import os
import tarfile

import pytest

from rfvis.__main__ import _read_data


@pytest.fixture(params=["dir", "tar", "json"])
def forest_data(request, tmp_path):
    forest_dir = os.path.join(os.path.dirname(__file__), "resources")
    if request.param == "dir":
        return forest_dir
    forest_json = os.path.join(forest_dir, "forest.json")
    if request.param == "json":
        return forest_json
    forest_tar = os.path.join(tmp_path, "forest.tar.gz")
    with tarfile.open(forest_tar, mode="w:gz") as tar:
        for p in os.listdir(forest_dir):
            tar.add(os.path.join(forest_dir, p), arcname=p)
    if request.param == "tar":
        return forest_tar
    raise ValueError("Unknown parameter {}".format(request.param))


def test_read_data(forest_data):
    forest = _read_data(forest_data)
    assert len(forest["trees"]) == 2
    assert forest["trees"][0]["error"] == 0.278413
<|endoftext|>"
},
{
"prompt": "fn left_join(h1, h2: HashTable) -> list

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from left_join import left_join
from hashtable import HashTable  # noqa E402


def test_func_exists():
    left_join(HashTable(), None)


def test_func_secondhashisnone():
    expected = []
    actual = left_join(HashTable(), None)
    assert expected == actual


def test_func_onematch_onedoesnt():

    h1 = HashTable()
    h1.add('fond', 'enamored')
    h1.add('wrath', 'anger')

    h2 = HashTable()
    h2.add('fond', 'averse')

    expected = [
        {'word': 'fond', 'synonym': 'enamored', 'antonym': 'averse' },  # noqa: E202
        {'word': 'wrath', 'synonym': 'anger', 'antonym': None }         # noqa: E202
    ]
    actual = left_join(h1, h2)
    assert expected == actual


def test_func_fullsample():
    h1 = HashTable()
    h1.add('fond', 'enamored')
    h1.add('wrath', 'anger')
    h1.add('diligent', 'employed')
    h1.add('outfit', 'garb')
    h1.add('guide', 'usher')

    h2 = HashTable()
    h2.add('fond', 'averse')
    h2.add('wrath', 'delight')
    h2.add('diligent', 'idle')
    h2.add('guide', 'follow')
    h2.add('flow', 'jam')

    expected = [
        {'word': 'diligent', 'synonym': 'employed', 'antonym': 'idle',    },  # noqa E202
        {'word': 'outfit',   'synonym': 'garb',     'antonym': None,      },  # noqa E202
        {'word': 'fond',     'synonym': 'enamored', 'antonym': 'averse',  },  # noqa E202
        {'word': 'guide',    'synonym': 'usher',    'antonym': 'follow',  },  # noqa E202
        {'word': 'wrath',    'synonym': 'anger',    'antonym': 'delight', }   # noqa E202
    ]

    actual = left_join(h1, h2)
    assert expected == actual
<|endoftext|>"
},
{
"prompt": """"A subclass of scipy.sparse.csr_matrix that skips the data format
checks that are run everytime a new csr_matrix is created."""
class fast_csr_matrix(csr_matrix):
	fn __init__(self, args, shape, dtype, copy)
	"""Do the binary operation fn to two sparse matrices using
	fast_csr_matrix only when other is also a fast_csr_matrix."""
	fn _binopt(self, other, op)
	"""Point-wise multiplication by another matrix, vector, or
	scalar."""
	fn multiply(self, other)
	"""Do the sparse matrix mult returning fast_csr_matrix only
	when other is also fast_csr_matrix."""
	fn _mul_sparse_matrix(self, other)
	"""Scalar version of self._binopt, for cases in which no new nonzeros
	are added. Produces a new spmatrix in canonical form."""
	fn _scalar_binopt(self, other, op)
	fn __eq__(self, other)
	fn __ne__(self, other)
	fn _inequality(self, other, op, op_name, bad_scalar_msg)
	"""Returns a matrix with the same sparsity structure as self,
	but with different data.  By default the structure arrays
	(i.e. .indptr and .indices) are copied."""
	fn _with_data(self, data, copy)
	"""Returns the transpose of the matrix, keeping
	it in fast_csr format."""
	fn transpose(self)
	"""Same as transpose"""
	fn trans(self)
	"""Returns the conjugate-transpose of the matrix, keeping
	it in fast_csr format."""
	fn getH(self)
	"""Same as getH"""
	fn adjoint(self)


fn csr2fast(A, copy)

"""Generates a sparse identity matrix in
fast_csr format."""
fn fast_identity(N)

fn _all_true(shape)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
import scipy.sparse
import qutip
from qutip.fastsparse import fast_csr_matrix


class TestOperationEffectsOnType:
    @pytest.mark.parametrize("operation", [
        pytest.param(lambda x: x, id="identity"),
        pytest.param(lambda x: x + x, id="addition"),
        pytest.param(lambda x: x - x, id="subtraction"),
        pytest.param(lambda x: x * x, id="multiplication by op"),
        pytest.param(lambda x: 2*x, id="multiplication by scalar"),
        pytest.param(lambda x: x/3, id="division by scalar"),
        pytest.param(lambda x: -x, id="negation"),
        pytest.param(lambda x: x.copy(), id="copy"),
        pytest.param(lambda x: x.T, id="transpose [.T]"),
        pytest.param(lambda x: x.trans(), id="transpose [.trans()]"),
        pytest.param(lambda x: x.transpose(), id="transpose [.transpose()]"),
        pytest.param(lambda x: x.H, id="adjoint [.H]"),
        pytest.param(lambda x: x.getH(), id="adjoint [.getH()]"),
        pytest.param(lambda x: x.adjoint(), id="adjoint [.adjoint()]"),
    ])
    def test_operations_preserve_type(self, operation):
        op = qutip.rand_herm(5).data
        assert isinstance(operation(op), fast_csr_matrix)

    @pytest.mark.parametrize("operation", [
        pytest.param(lambda x, y: y, id="identity of other"),
        pytest.param(lambda x, y: x + y, id="addition"),
        pytest.param(lambda x, y: y + x, id="r-addition"),
        pytest.param(lambda x, y: x - y, id="subtraction"),
        pytest.param(lambda x, y: y - x, id="r-subtraction"),
        pytest.param(lambda x, y: x * y, id="multiplication"),
        pytest.param(lambda x, y: y * x, id="r-multiplication"),
    ])
    def test_mixed_operations_yield_type(self, operation):
        op = qutip.rand_herm(5).data
        other = scipy.sparse.csr_matrix((op.data, op.indices, op.indptr),
                                        copy=True, shape=op.shape)
        assert not isinstance(operation(op, other), fast_csr_matrix)
<|endoftext|>"
},
{
"prompt": "fn setup_function(f)

fn teardown_function(f)

"""Resolves a filename relative to the "tests" directory."""
fn get_filename(fn)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# -*- coding: utf-8 -*-

# The MIT License (MIT) - Copyright (c) 2016-2021 Dave Vandenbout.

import pytest

from skidl import TEMPLATE, Net, Part, generate_pcb

from .setup_teardown import setup_function, teardown_function


def test_pcb_1():
    """Test PCB generation."""

    l1 = Part(
        "Device.lib",
        "L",
        footprint="Inductor_SMD:L_0805_2012Metric_Pad1.15x1.40mm_HandSolder",
    )
    r1, r2 = (
        Part(
            "Device.lib",
            "R",
            dest=TEMPLATE,
            value="200.0",
            footprint="Resistor_SMD:R_0805_2012Metric",
        )
        * 2
    )
    q1 = Part(
        "Device.lib", "Q_NPN_CBE", footprint="Package_TO_SOT_SMD:SOT-223-3_TabPin2"
    )
    c1 = Part(
        "Device.lib",
        "C",
        value="10pF",
        footprint="Capacitor_SMD:C_0805_2012Metric_Pad1.18x1.45mm_HandSolder",
    )
    r3 = r2(value="1K", footprint="Resistor_SMD:R_0805_2012Metric")
    vcc, vin, vout, gnd = Net("VCC"), Net("VIN"), Net("VOUT"), Net("GND")
    vcc & r1 & vin & r2 & gnd
    vcc & r3 & vout & q1["C,E"] & gnd
    q1["B"] += vin
    vout & (l1 | c1) & gnd
    rly = Part("Relay", "TE_PCH-1xxx2M", footprint="Relay_THT:Relay_SPST_TE_PCH-1xxx2M")
    rly[1, 2, 3, 5] += gnd
    led = Part("Device.lib", "LED_ARGB", symtx="RH", footprint="LED_SMD:LED_RGB_1210")
    r, g, b = Net("R"), Net("G"), Net("B")
    led["A,RK,GK,BK"] += vcc, r, g, b
    Part(
        lib="MCU_Microchip_PIC10.lib",
        name="PIC10F200-IMC",
        footprint="Package_DFN_QFN:DFN-8-1EP_2x3mm_P0.5mm_EP0.61x2.2mm",
    )

    generate_pcb()
<|endoftext|>"
},
{
"prompt": "fn all_valid_kwargs(valid_kwargs)

fn all_invalid_kwargs(valid_kwargs, invalid_kwargs)

fn build_valid_kwargs_list(base, optional_kwargs)

class SometimesIncompleteReaderIO(io.BytesIO):
	fn __init__(self)
	"""Every other read request, return fewer than the requested number of bytes if more than one byte requested."""
	fn read(self, size)


class NothingButRead(object):
	fn __init__(self, data)
	fn read(self, size)


class ExactlyTwoReads(SometimesIncompleteReaderIO):
	fn read(self, size)


class FailingTeller(object):
	fn tell(self)


fn assert_prepped_stream_identity(prepped_stream, wrapped_type)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
# http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""Unit test suite to validate aws_encryption_sdk.key_providers.raw.RawMasterKeyConfig"""
import pytest
import six

from aws_encryption_sdk.identifiers import EncryptionKeyType, WrappingAlgorithm
from aws_encryption_sdk.internal.crypto.wrapping_keys import WrappingKey
from aws_encryption_sdk.key_providers.base import MasterKeyConfig
from aws_encryption_sdk.key_providers.raw import RawMasterKeyConfig

from .unit_test_utils import all_invalid_kwargs, all_valid_kwargs

pytestmark = [pytest.mark.unit, pytest.mark.local]

STATIC_WRAPPING_KEY = WrappingKey(
    wrapping_algorithm=WrappingAlgorithm.AES_256_GCM_IV12_TAG16_NO_PADDING,
    wrapping_key=b"_________a symmetric key________",
    wrapping_key_type=EncryptionKeyType.SYMMETRIC,
)
VALID_KWARGS = {
    RawMasterKeyConfig: [
        dict(key_id=b"a raw key", provider_id="a provider", wrapping_key=STATIC_WRAPPING_KEY),
        dict(key_id=b"a raw key", provider_id=b"a provider", wrapping_key=STATIC_WRAPPING_KEY),
    ]
}


@pytest.mark.parametrize("cls, kwargs", all_valid_kwargs(VALID_KWARGS))
def test_attributes_valid_kwargs(cls, kwargs):
    cls(**kwargs)


@pytest.mark.parametrize("cls, kwargs", all_invalid_kwargs(VALID_KWARGS))
def test_attributes_invalid_kwargs(cls, kwargs):
    with pytest.raises(TypeError):
        cls(**kwargs)


def test_parent():
    assert issubclass(RawMasterKeyConfig, MasterKeyConfig)


@pytest.mark.parametrize("cls, kwargs", all_valid_kwargs(VALID_KWARGS))
def test_attributes_converts(cls, kwargs):
    test = cls(**kwargs)
    assert isinstance(test.provider_id, six.string_types)
<|endoftext|>"
},
{
"prompt": """"Document iterator for in-memory documents."""
class InMemoryDocumentIterator(DocumentIterator):
	"""Initialize the list of document rows.
	
	Parameters
	----------
	rows: list
	    List of document rows. Each row in the list is a tuple of row
	    position, row index, and cell values."""
	fn __init__(self, rows: List[Tuple[(int, RowIndex, DataRow)]])
	"""Set list of rows to None."""
	fn close(self)
	"""Read the next row in the document.
	
	Returns the row position, row index and the list of cell values for each
	of the document columns. Raises a StopIteration error if an attempt is
	made to read past the end of the document.
	
	Returns
	-------
	tuple of int, histore.document.base.RowIndex, histore.document.base.DataRow"""
	fn next(self) -> Tuple[(int, RowIndex, DataRow)]


"""The in-memory document maintains a array of document rows (each as a
tuple of row position, row index, and cell values). The row position is the
original position of the row in a sorted document."""
class InMemoryDocument(Document):
	"""Initialize document schema and rows.
	
	Parameters
	----------
	columns: list
	    List of column names. The number of values in each row is expected
	    to be the same as the number of columns and the order of values in
	    each row is expected to correspond to their respective column in
	    this list.
	rows: list
	    List of document rows. Each row in the list is a tuple of row
	    position, row index, and cell values."""
	fn __init__(self, columns: DocumentSchema, rows: List[Tuple[(int, RowIndex, DataRow)]])
	"""Set list of rows to None when closing the document."""
	fn close(self)
	"""Open the document to get a iterator for the rows in the document.
	
	Returns
	-------
	histore.document.base.DocumentIterator"""
	fn open(self) -> InMemoryDocumentIterator
	"""Create data frame from the document rows.
	
	Returns
	-------
	pd.DataFrame"""
	fn to_df(self) -> pd.DataFrame
	"""Sort the document rows based on the values in the key columns.
	
	Key columns are specified by their index position. Returns a new
	document.
	
	Parameters
	----------
	keys: list of int
	    Index position of sort columns.
	buffersize: float, default=None
	    Maximum size (in bytes) of file blocks that are kept in main-memory.
	    Ignored. Included for API completeness.
	
	Returns
	-------
	histore.document.mem.InMemoryDocument"""
	fn sorted(self, keys: List[int], buffersize: Optional[float]) -> InMemoryDocument


"""Empty document that only defines the document schema."""
class Schema(InMemoryDocument):
	"""Initialize document schema.
	
	Parameters
	----------
	columns: list
	    List of column names."""
	fn __init__(self, columns: DocumentSchema)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# This file is part of the History Store (histore).
#
# Copyright (C) 2018-2021 New York University.
#
# The History Store (histore) is released under the Revised BSD License. See
# file LICENSE for full license details.

"""Unit tests for the in-memory document and document reader."""

import pytest

from histore.document.mem import InMemoryDocument, Schema


@pytest.fixture
def document():
    """Get a document for test purposes."""
    return InMemoryDocument(
        columns=['Name', 'Age', 'Dept'],
        rows=[
            (0, 0, ['Alice', 23, 'R&D']),
            (1, 1, ['Bob', 35, 'Finance']),
            (2, 3, ['Dave', 28, 'Sales']),
            (3, 2, ['Claire', 33, 'Finance'])
        ]
    )


def test_document_iterrows(document):
    """Test the reader for an in-memory document."""
    rows = list()
    for rowidx, row in document.iterrows():
        rows.append((rowidx, row))
    assert rows == [
        (0, ['Alice', 23, 'R&D']),
        (1, ['Bob', 35, 'Finance']),
        (3, ['Dave', 28, 'Sales']),
        (2, ['Claire', 33, 'Finance'])
    ]


def test_empty_document():
    """Test the reader for an empty document."""
    doc = Schema(columns=['Name', 'Age'])
    assert doc.columns == ['Name', 'Age']
    rows = list()
    for rowidx, row in doc.iterrows():
        rows.append((rowidx, row))
    assert rows == []


def test_error_read_after_close(document):
    """Error when reading closed document."""
    for rowidx, row in document.iterrows():
        pass
    document.close()
    with document.open() as reader:
        with pytest.raises(StopIteration):
            next(reader)
    document.close()


def test_to_df(document):
    """Test reading the document as a data frame."""
    df = document.to_df()
    assert df.shape == (4, 3)
    assert list(df.index) == [0, 1, 3, 2]


def test_sort_document(document):
    """Test sorting the document."""
    doc = document.sorted(keys=[0])
    with document.open() as reader:
        names = [r[0] for _, _, r in reader]
        assert names == ['Alice', 'Bob', 'Dave', 'Claire']
    with doc.open() as reader:
        names = [r[0] for _, _, r in reader]
        assert names == ['Alice', 'Bob', 'Claire', 'Dave']
    doc = document.sorted(keys=[2, 0])
    with doc.open() as reader:
        names = [r[0] for _, _, r in reader]
        assert names == ['Bob', 'Claire', 'Alice', 'Dave']
<|endoftext|>"
},
{
"prompt": """"Mock for logger class """
class LoggerMock(object):
	fn log(self, module, message)


"""Common class for integration tests """
class CommonIntegration(object):
	db = None
	"""Prepare class for run tests """
	fn setup_class(self)
	"""Clean tables for tests """
	fn _clean_db(self)
	"""Add hashlist record """
	fn _add_hashlist(self, id, name, alg_id, have_salts, status, common_by_alg, parsed, tmp_path, last_finder_checked, uncracked)
	"""Add hash record """
	fn _add_hash(self, hashlist_id, hash, salt, summ, password, cracked, id)
	"""Add work task record """
	fn _add_work_task(self, id, hashlist_id, task_id, status, priority, out_file)
	"""Add task record """
	fn _add_task(self, id, name, group_id, type, source)
	"""Add dict record """
	fn _add_dict(self, id, group_id, name, hash)
	"""Add dict group record """
	fn _add_dict_group(self, id, name)
	"""Add rule record """
	fn _add_rule(self, id, name, hash, count)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# -*- coding: utf-8 -*-
"""
This is part of HashBruteStation software
Docs EN: http://hack4sec.pro/wiki/index.php/Hash_Brute_Station_en
Docs RU: http://hack4sec.pro/wiki/index.php/Hash_Brute_Station
License: MIT
Copyright (c) Anton Kuzmin <http://anton-kuzmin.ru> (ru) <http://anton-kuzmin.pro> (en)

Integration tests for FinderInsideProThread
"""

import sys
import time
import pytest

sys.path.append('../../')

from libs.common import md5
from classes.FinderInsideProThread import FinderInsideProThread
from CommonIntegration import CommonIntegration

class Test_HashlistsLoaderThread(CommonIntegration):
    """ Integration tests for FinderInsideProThread """
    thrd = None

    def setup(self):
        """ Tests setup """
        self._clean_db()
        self.thrd = FinderInsideProThread()
        self.thrd.catch_exceptions = False
        self.db.update("algs", {'finder_insidepro_allowed': 1}, "id")


    def teardown(self):
        """ Tests teardown """
        if isinstance(self.thrd, FinderInsideProThread):
            self.thrd.available = False
            time.sleep(1)
            del self.thrd
        self._clean_db()

    test_data = [
        (
            74,
            1,
            [
                {'id': 1, 'hash': '0065ffe5f9e4e5996c2c3f52f81c6e31', 'salt': 'cB6Ar', 'cracked': 1,
                 'summ': md5("0065ffe5f9e4e5996c2c3f52f81c6e31:cB6Ar"), 'password': 'y0007171'},
                {'id': 2, 'hash': '20e153b046072c949562f3c939611db8', 'salt': '0RTV', 'cracked': 0,
                 'summ': md5("20e153b046072c949562f3c939611db8:0RTV"), 'password': ''},
            ]
        ),
        (
            4,
            0,
            [
                {'id': 1, 'hash': md5('aaa'), 'salt': '', 'cracked': 1,
                 'summ': md5(md5('aaa')), 'password': 'aaa'},
                {'id': 2, 'hash': '10e153b046072c949562f3c939611db7', 'salt': '', 'cracked': 0,
                 'summ': md5("10e153b046072c949562f3c939611db7"), 'password': ''},
            ]
        )
    ]
    @pytest.mark.parametrize("alg_id,have_salt,hashes", test_data)
    def test_run(self, alg_id, have_salt, hashes):
        """
        Test simple run
        :param alg_id:
        :param have_salt: does alg has salts?
        :param hashes: Hashes rows
        :return:
        """
        self._add_hashlist(common_by_alg=alg_id, alg_id=alg_id, have_salts=have_salt,
                           last_finder_checked=int(time.time()))
        for _hash in hashes:
            self._add_hash(id=_hash['id'], hash=_hash['hash'], salt=_hash['salt'], summ=_hash['summ'])

        self.thrd.start()

        time.sleep(10)

        assert self.db.fetch_one("SELECT 1 FROM hashes WHERE id = 1 AND cracked = 0 AND password=''") == 1
        assert self.db.fetch_one("SELECT 1 FROM hashes WHERE id = 2 AND cracked = 0 AND password=''") == 1
        assert self.db.fetch_one("SELECT last_finder_checked FROM hashlists WHERE id = 1") < time.time()

        self.db.update("hashlists", {"last_finder_checked": 0}, "id = 1")

        time.sleep(10)

        for _hash in hashes:
            test_data = self.db.fetch_row("SELECT * FROM hashes WHERE id = {0}".format(_hash['id']))
            assert test_data['summ'] == _hash['summ']
            assert test_data['cracked'] == _hash['cracked']
            assert test_data['password'] == _hash['password']

        assert self.db.fetch_one("SELECT last_finder_checked FROM hashlists WHERE id = 1") > time.time() - 20
<|endoftext|>"
},
{
"prompt": "class LinearNNModel(tf.keras.Model):
	fn __init__(self)
	fn call(self, x)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements. See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License. You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import unittest
from datetime import datetime
from os import environ

import pytest
import tensorflow

import submarine
from submarine.artifacts.repository import Repository
from submarine.store.database import models
from submarine.store.database.models import SqlExperiment, SqlMetric, SqlParam
from submarine.tracking.client import SubmarineClient

from .tf_model import LinearNNModel

JOB_ID = "application_123456789"
REGISTERED_MODEL_NAME = "registerd_model_name"
MLFLOW_S3_ENDPOINT_URL = "http://localhost:9000"


@pytest.mark.e2e
class TestTracking(unittest.TestCase):
    def setUp(self):
        environ["JOB_ID"] = JOB_ID
        submarine.set_db_uri(
            "mysql+pymysql://submarine_test:password_test@localhost:3306/submarine_test"
        )
        self.db_uri = submarine.get_db_uri()
        self.client = SubmarineClient(
            db_uri=self.db_uri,
            s3_registry_uri=MLFLOW_S3_ENDPOINT_URL,
        )
        from submarine.store.tracking.sqlalchemy_store import SqlAlchemyStore

        self.store = SqlAlchemyStore(self.db_uri)
        from submarine.store.model_registry.sqlalchemy_store import SqlAlchemyStore

        self.model_registry = SqlAlchemyStore(self.db_uri)
        # TODO: use submarine.tracking.fluent to support experiment create
        with self.store.ManagedSessionMaker() as session:
            instance = SqlExperiment(
                id=JOB_ID,
                experiment_spec='{"value": 1}',
                create_by="test",
                create_time=datetime.now(),
                update_by=None,
                update_time=None,
            )
            session.add(instance)
            session.commit()

    def tearDown(self):
        submarine.set_db_uri(None)
        models.Base.metadata.drop_all(self.store.engine)
        environ["MLFLOW_S3_ENDPOINT_URL"] = MLFLOW_S3_ENDPOINT_URL
        environ["AWS_ACCESS_KEY_ID"] = "submarine_minio"
        environ["AWS_SECRET_ACCESS_KEY"] = "submarine_minio"
        Repository().delete_folder(f"experiment/{JOB_ID}")
        Repository().delete_folder(f"registry/{REGISTERED_MODEL_NAME}")

    def test_log_param(self):
        submarine.log_param("name_1", "a")
        # Validate params
        with self.store.ManagedSessionMaker() as session:
            params = session.query(SqlParam).options().filter(SqlParam.id == JOB_ID).all()
            assert params[0].key == "name_1"
            assert params[0].value == "a"
            assert params[0].id == JOB_ID

    def test_log_metric(self):
        submarine.log_metric("name_1", 5)
        submarine.log_metric("name_1", 6)
        # Validate params
        with self.store.ManagedSessionMaker() as session:
            metrics = session.query(SqlMetric).options().filter(SqlMetric.id == JOB_ID).all()
            assert len(metrics) == 2
            assert metrics[0].key == "name_1"
            assert metrics[0].value == 5
            assert metrics[0].id == JOB_ID
            assert metrics[1].value == 6

    @pytest.mark.skipif(tensorflow.version.VERSION < "2.0", reason="using tensorflow 2")
    def test_save_model(self):
        input_arr = tensorflow.random.uniform((1, 5))
        model = LinearNNModel()
        model(input_arr)
        self.client.save_model(model, "tensorflow", "name_1", REGISTERED_MODEL_NAME)
        self.client.save_model(model, "tensorflow", "name_2", REGISTERED_MODEL_NAME)
        # Validate model_versions
        model_versions = self.model_registry.list_model_versions(REGISTERED_MODEL_NAME)
        assert len(model_versions) == 2
        assert model_versions[0].name == REGISTERED_MODEL_NAME
        assert model_versions[0].version == 1
        assert model_versions[1].name == REGISTERED_MODEL_NAME
        assert model_versions[1].version == 2
<|endoftext|>"
},
{
"prompt": """"Take a dictionary with microphone array
capsules and 3D polar coordinates to
convert them from degrees to radians

colatitude, azimuth, and radius (radius
is left intact)"""
fn _deg2rad(coords_dict)

"""Take a dictionary with microphone array
capsules and polar coordinates and convert
to cartesian

Parameters:
    units: (str) indicating 'degrees' or 'radians'"""
fn _polar2cart(coords_dict, units: None)

"""Take a dictionary with microphone array
capsules and cartesian coordinates, find the center
of such coordinates and subtract it to center all
the coordinates around zero."""
fn _centercoords(coords_dict)

"""Take a dictionary with microphone array
capsules and cartesian coordinates, and convert
to polar: colatitude (radians), azimuth (radians), radius"""
fn _cart2polar(coords_dict)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from micarraylib.datasets import tau2021sse_nigens_loader
from micarraylib.arraycoords.array_shapes_utils import _polar2cart
import numpy as np
import soundata
import pytest
import librosa


def test_tau2021sse_nigens_init():

    a = tau2021sse_nigens_loader.tau2021sse_nigens(download=False, data_home="~/")
    assert a.name == "tau2021sse_nigens"
    assert a.fs == 24000
    assert len(a.array_format) == 1
    assert a.array_format["Eigenmike"] == "A"
    assert len(a.capsule_coords["Eigenmike"]) == 4
    assert list(a.capsule_coords["Eigenmike"].keys()) == ["6", "10", "22", "26"]
    b = _polar2cart(a.capsule_coords["Eigenmike"], "radians")
    # TODO: analize why the atol is needed
    assert np.allclose(
        np.mean(np.array([c for c in b.values()]), axis=0), [0, 0, 0], atol=1e-4
    )
    assert (
        len(a.micarray_clip_ids["Eigenmike"])
        == len(soundata.initialize("tau2021sse_nigens", data_home="~/").clip_ids) / 2
    )  # because soundata has clip_ids for each format A and B
    assert (
        len(a.clips_list)
        == len(soundata.initialize("tau2021sse_nigens", data_home="~/").clip_ids) / 2
    )  # because soundata has clip_ids for each format A and B


def test_tau2021sse_nigens_get_audio_numpy():

    a = tau2021sse_nigens_loader.tau2021sse_nigens(
        download=False, data_home="tests/resources/datasets/tau2021sse_nigens"
    )
    A = a.get_audio_numpy("dev/dev-train/fold1_room1_mix001")
    B = a.get_audio_numpy("dev/dev-train/fold1_room1_mix001", fmt="B")

    Al = librosa.load(
        "tests/resources/datasets/tau2021sse_nigens/mic_dev/dev-train/fold1_room1_mix001.wav",
        sr=24000,
        mono=False,
    )[0]
    Bl = librosa.load(
        "tests/resources/datasets/tau2021sse_nigens/foa_dev/dev-train/fold1_room1_mix001.wav",
        sr=24000,
        mono=False,
    )[0]

    assert (B == Bl).all()
    assert (A == Al).all()

    with pytest.raises(ValueError):
        a.get_audio_numpy("a", "b")
    with pytest.raises(ValueError):
        a.get_audio_numpy("a", "Eigenmike")


def test_tau2021sse_nigens_get_audio_events():

    a = tau2021sse_nigens_loader.tau2021sse_nigens(
        download=False, data_home="tests/resources/datasets/tau2021sse_nigens"
    )
    A = a.get_audio_events("dev/dev-train/fold1_room1_mix001")

    with pytest.raises(ValueError):
        a.get_audio_events("a")
<|endoftext|>"
},
{
"prompt": "fn uuid()

fn oauth_redirect_request(app, make_request)

fn oauth_callback_request(app, make_request)

fn config(config)

fn get_email(profile_data)

fn set_email(profile_data, email)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from .conftest import PROVIDERS_TO_TEST, PROVIDER_PARAMS


@pytest.mark.django_db
async def test_oauth_redirect_unknown_provider(oauth_redirect_request):
    await oauth_redirect_request(provider_name='xxx', expected_status=400)


@pytest.mark.django_db
async def test_oauth_redirect_provider_not_configured(oauth_redirect_request):
    await oauth_redirect_request(provider_name='google', expected_status=404)


@pytest.mark.django_db
@pytest.mark.parametrize('provider', PROVIDERS_TO_TEST)
async def test_oauth_redirect_successful(provider, oauth_redirect_request, config):
    response = await oauth_redirect_request(provider_name=provider, expected_status=302)
    location = response.headers['Location']
    assert location.startswith(PROVIDER_PARAMS[provider]['auth_url'])
    assert config.social[provider]['client_id'] in location
    assert 'response_type=code' in location
<|endoftext|>"
},
{
"prompt": "fn number0() -> NumberExpressionSyntax

fn number1() -> NumberExpressionSyntax

fn number2() -> NumberExpressionSyntax

fn open_paranthesis() -> SyntaxToken

fn close_paranthesis() -> SyntaxToken

fn addition_operator() -> SyntaxToken

fn subtraction_operator() -> SyntaxToken

fn multiplication_operator() -> SyntaxToken

fn division_operator() -> SyntaxToken

fn binary_expression_syntax_simple(number1, number2, addition_operator) -> BinaryExpressionSyntax

fn paranthesized_expression_simple(number1, number2, addition_operator, open_paranthesis, close_paranthesis) -> ParanthesizedExpressionSyntax

fn parser_simple(number1, number2, addition_operator, open_paranthesis, close_paranthesis) -> Parser

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from codeanalysis.syntaxkind import SyntaxKind
from codeanalysis.tests.conftest import paranthesized_expression_simple
import pytest


class TestParanthesizedExpressionSyntax:
    def test_kind(self, paranthesized_expression_simple) -> None:
        assert (
            paranthesized_expression_simple.kind() == SyntaxKind.paranthesizedexpression
        )

    def test_getChildren(self, paranthesized_expression_simple) -> None:
        token_gernator = paranthesized_expression_simple.get_children()
        assert next(token_gernator).kind() == SyntaxKind.openparanthesis
        assert next(token_gernator).kind() == SyntaxKind.binaryexpression
        assert next(token_gernator).kind() == SyntaxKind.closeparanthesis
        with pytest.raises(Exception) as excinfo:
            next(token_gernator)
        assert str(excinfo) == "<ExceptionInfo StopIteration() tblen=1>"

    def test_getLastChild(self, paranthesized_expression_simple) -> None:
        assert (
            paranthesized_expression_simple.get_last_child().kind()
            == SyntaxKind.closeparanthesis
        )

<|endoftext|>"
},
{
"prompt": "class UserDetailView(LoginRequiredMixin, DetailView):
	model = User
	slug_field = 'username'
	slug_url_kwarg = 'username'


class UserUpdateView(LoginRequiredMixin, UpdateView):
	model = User
	fields = ['name']
	fn get_success_url(self)
	fn get_object(self)
	fn form_valid(self, form)


class UserRedirectView(LoginRequiredMixin, RedirectView):
	permanent = False
	fn get_redirect_url(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from django.conf import settings
from django.test import RequestFactory

from opensteer.users.views import UserRedirectView, UserUpdateView

pytestmark = pytest.mark.django_db


class TestUserUpdateView:
    """
    TODO:
        extracting view initialization code as class-scoped fixture
        would be great if only pytest-django supported non-function-scoped
        fixture db access -- this is a work-in-progress for now:
        https://github.com/pytest-dev/pytest-django/pull/258
    """

    def test_get_success_url(
        self, user: settings.AUTH_USER_MODEL, request_factory: RequestFactory
    ):
        view = UserUpdateView()
        request = request_factory.get("/fake-url/")
        request.user = user

        view.request = request

        assert view.get_success_url() == f"/users/{user.username}/"

    def test_get_object(
        self, user: settings.AUTH_USER_MODEL, request_factory: RequestFactory
    ):
        view = UserUpdateView()
        request = request_factory.get("/fake-url/")
        request.user = user

        view.request = request

        assert view.get_object() == user


class TestUserRedirectView:
    def test_get_redirect_url(
        self, user: settings.AUTH_USER_MODEL, request_factory: RequestFactory
    ):
        view = UserRedirectView()
        request = request_factory.get("/fake-url")
        request.user = user

        view.request = request

        assert view.get_redirect_url() == f"/users/{user.username}/"
<|endoftext|>"
},
{
"prompt": "fn assert_equal(left, right)

fn skipif_unsupported(f)

fn skipif_backend(skip_backend)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from posixpath import join as pjoin

import pytest

import ibis
from ibis.tests.util import assert_equal

pytest.importorskip('hdfs')
pytest.importorskip('sqlalchemy')
pytest.importorskip('impala.dbapi')

from ibis.impala.compat import HS2Error  # noqa: E402

pytestmark = pytest.mark.impala


def test_cleanup_tmp_table_on_gc(con, test_data_dir):
    import gc
    hdfs_path = pjoin(test_data_dir, 'parquet/tpch_region')
    table = con.parquet_file(hdfs_path)
    name = table.op().name
    table = None
    gc.collect()
    assert not con.exists_table(name)


def test_persist_parquet_file_with_name(con, test_data_dir, temp_table_db):
    import gc
    hdfs_path = pjoin(test_data_dir, 'parquet/tpch_region')

    tmp_db, name = temp_table_db
    schema = ibis.schema([('r_regionkey', 'int16'),
                          ('r_name', 'string'),
                          ('r_comment', 'string')])
    con.parquet_file(hdfs_path, schema=schema, name=name, database=tmp_db,
                     persist=True)
    gc.collect()

    # table still exists
    con.table(name, database=tmp_db)


def test_query_parquet_file_with_schema(con, test_data_dir):
    hdfs_path = pjoin(test_data_dir, 'parquet/tpch_region')

    schema = ibis.schema([('r_regionkey', 'int16'),
                          ('r_name', 'string'),
                          ('r_comment', 'string')])

    table = con.parquet_file(hdfs_path, schema=schema)

    name = table.op().name

    # table exists
    con.table(name)

    expr = table.r_name.value_counts()
    expr.execute()

    assert table.count().execute() == 5


def test_query_parquet_file_like_table(con, test_data_dir):
    hdfs_path = pjoin(test_data_dir, 'parquet/tpch_region')

    ex_schema = ibis.schema([('r_regionkey', 'int16'),
                             ('r_name', 'string'),
                             ('r_comment', 'string')])

    table = con.parquet_file(hdfs_path, like_table='tpch_region')

    assert_equal(table.schema(), ex_schema)


def test_query_parquet_infer_schema(con, test_data_dir):
    hdfs_path = pjoin(test_data_dir, 'parquet/tpch_region')
    table = con.parquet_file(hdfs_path)

    # NOTE: the actual schema should have an int16, but bc this is being
    # inferred from a parquet file, which has no notion of int16, the
    # inferred schema will have an int32 instead.
    ex_schema = ibis.schema([('r_regionkey', 'int32'),
                             ('r_name', 'string'),
                             ('r_comment', 'string')])

    assert_equal(table.schema(), ex_schema)


def test_create_table_persist_fails_if_called_twice(
    con, temp_table_db, test_data_dir
):
    tmp_db, tname = temp_table_db

    hdfs_path = pjoin(test_data_dir, 'parquet/tpch_region')
    con.parquet_file(hdfs_path, name=tname, persist=True, database=tmp_db)

    with pytest.raises(HS2Error):
        con.parquet_file(hdfs_path, name=tname, persist=True, database=tmp_db)
<|endoftext|>"
},
{
"prompt": """"Represents the binary state of events publication: ``published`` or
``unpublished``"""
class EventStatus:
	UNPUBLISHED = 'unpublished'
	PUBLISHED = 'published'


"""An event that can be published.

Args:
    eventtype (:obj:`baroque.entities.eventtype.EventType` instance or `type` object): the type of the event
    payload (dict, optional): the content of this event
    description (str, optional): the description of this event
    owner (str, optional): the owner of this event"""
class Event:
	fn __init__(self, eventtype, payload, description, owner)
	"""Sets the status of this event to published."""
	fn set_published(self)
	"""Sets the status of this event to unpublished."""
	fn set_unpublished(self)
	"""Sets the current time as timestamp of this event"""
	fn touch(self)
	"""Dumps this object to a JSON string.
	
	Returns:
	    str"""
	fn json(self)
	"""Returns the MD5 hash of this object.
	
	Returns:
	    str"""
	fn md5(self)
	fn __repr__(self)


"""Describes generic events with a free-form content.

Args:
    owner (str, optional): ID of the owner of this event type."""
class GenericEventType(EventType):
	fn __init__(self, owner)


"""Describes events cast when something changes its state.
Suitable i.e. to track state machines changes.
Old and new states are conveyed in the event payload, as well as
the cause of the state transition.

Args:
    owner (str, optional): ID of the owner of this event type."""
class StateTransitionEventType(EventType):
	fn __init__(self, owner)


"""Describes events cast when some kind of operation is done on a
piece of data.
Suitable i.e. to track CRUD operations on DB tables or whole datastores.
Details about the impacted data entity and the operation are
conveyed in the event payload.

Args:
    owner (str, optional): ID of the owner of this event type."""
class DataOperationEventType(EventType):
	fn __init__(self, owner)


"""Describes events carrying metric data.
Suitable i.e. to track values about measured physical quantities.
The metric name and value are conveyed in the event payload.

Args:
    owner (str, optional): ID of the owner of this event type."""
class MetricEventType(EventType):
	fn __init__(self, owner)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from baroque.entities.event import Event, EventStatus
from baroque.defaults.eventtypes import GenericEventType


def test_constructor_failures():
    with pytest.raises(TypeError):
        Event()
        pytest.fail()
    with pytest.raises(AssertionError):
        Event(None)
        pytest.fail()
    with pytest.raises(AssertionError):
        Event(123)
        pytest.fail()
    with pytest.raises(AssertionError):
        Event(GenericEventType(), payload=123)
        pytest.fail()


def test_constructor():
    e = Event(GenericEventType(), payload=dict(a=1, b=2), description='hello',
              owner=1234)
    assert isinstance(e.type, GenericEventType)
    assert e.id is not None
    assert not e.tags
    assert e.status == EventStatus.UNPUBLISHED
    assert e.timestamp is not None


def test_constructor_with_type_objects():
    e1 = Event(GenericEventType(), payload=dict(a=1, b=2), description='hello',
               owner=1234)
    e2 = Event(GenericEventType, payload=dict(a=1, b=2), description='hello',
               owner=1234)
    assert isinstance(e1.type, GenericEventType)
    assert isinstance(e2.type, GenericEventType)


def test_touch():
    e = Event(GenericEventType(), payload=dict(a=1, b=2), description='hello',
              owner=1234)
    ts1 = e.timestamp
    e = Event(GenericEventType(), payload=dict(a=1, b=2), description='hello',
              owner=1234)
    ts2 = e.timestamp
    assert ts2 > ts1


def test_md5():
    e = Event(GenericEventType(), payload=dict(a=1, b=2), description='hello',
              owner=1234)
    assert e.md5() is not None


def test_print():
    print(Event(GenericEventType()))
<|endoftext|>"
},
{
"prompt": "fn test_boolean()

fn test_str_list()

fn test_int_list()

fn test_objectid_list()

fn test_datetime()

fn test_date()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from datetime import datetime as dt

import pytest
from bson import ObjectId

from flask_stupe.inputs import (boolean, date, datetime, int_list,
                                objectid_list, str_list)


def test_boolean():
    assert boolean(True) is True
    assert boolean(False) is False

    with pytest.raises(ValueError):
        boolean(None)

    assert boolean("0") is False
    assert boolean("1") is True

    assert boolean("true") is True
    assert boolean("false") is False

    assert boolean("tRuE") is True
    assert boolean("fAlSE") is False

    with pytest.raises(ValueError):
        boolean("invalid_string")


def test_str_list():
    assert str_list("") == []
    assert str_list("foo") == ["foo"]
    assert str_list("foo,bar") == ["foo", "bar"]
    assert str_list("foo,baz,bar") == ["foo", "baz", "bar"]


def test_int_list():
    assert int_list("") == []
    assert int_list("42") == [42]
    assert int_list("13,37") == [13, 37]
    assert int_list("4,2,0") == [4, 2, 0]


def test_objectid_list():
    objectids = [ObjectId() for i in range(5)]
    assert objectid_list("") == []
    assert objectid_list(str(objectids[0])) == [objectids[0]]

    objectids_string = ','.join(str(o) for o in objectids)
    assert objectid_list(objectids_string) == objectids


def test_datetime():
    now = dt.now()
    assert datetime(str(now)) == now


def test_date():
    now = dt.now().date()
    assert date(str(now)) == now
<|endoftext|>"
},
{
"prompt": "fn pytest_sim_params(metafunc)

fn outlines(capsys)

fn define_simple_circuit(T, circ_name, has_clk)

class TestPeekCircuit(m.Circuit):
	__test__ = False
	io = m.IO(I=m.In(T), O0=m.Out(T), O1=m.Out(T))


class ConfigReg(m.Circuit):
	io = m.IO(D=m.In(m.Bits[2]), Q=m.Out(m.Bits[2])) + m.ClockIO(has_ce=True)
	reg = mantle.Register(2, has_ce=True, name='conf_reg')


class SimpleALU(m.Circuit):
	io = m.IO(a=m.In(m.UInt[16]), b=m.In(m.UInt[16]), c=m.Out(m.UInt[16]), config_data=m.In(m.Bits[2]), config_en=m.In(m.Enable)) + m.ClockIO()
	opcode = ConfigReg(name='config_reg')(io.config_data, CE=io.config_en)


class AndCircuit(m.Circuit):
	io = m.IO(I0=m.In(m.Bit), I1=m.In(m.Bit), O=m.Out(m.Bit))


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from pathlib import Path
import fault
import magma as m
from .common import pytest_sim_params


def pytest_generate_tests(metafunc):
    pytest_sim_params(metafunc, 'system-verilog')


def test_ext_vlog(target, simulator):
    # declare circuit
    class myinv(m.Circuit):
        io = m.IO(
            in_=m.In(m.Bit),
            out=m.Out(m.Bit)
        )

    # define test
    tester = fault.InvTester(myinv)

    # run the test
    tester.compile_and_run(
        target=target,
        simulator=simulator,
        ext_srcs=[Path('tests/verilog/myinv_extra_module.v').resolve()],
        ext_model_file=True,
        tmp_dir=True
    )
<|endoftext|>"
},
{
"prompt": "fn gpu_list(region)

fn cpu_list(region)

"""Decorator for running an Integ test with an instance_list and
break on first success

Args:
    instance_list (list): List of Compute instances for integ test.
Usage:
    @retry_with_instance_list(instance_list=["ml.g3.2", "ml.g2"])
    def sample_function():
        print("xxxx....")"""
fn retry_with_instance_list(instance_list: list)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from __future__ import absolute_import

import time

import pytest

from sagemaker import FactorizationMachines, FactorizationMachinesModel
from sagemaker.utils import unique_name_from_base
from tests.integ import datasets, TRAINING_DEFAULT_TIMEOUT_MINUTES
from tests.integ.timeout import timeout, timeout_and_delete_endpoint_by_name


@pytest.fixture
def training_set():
    return datasets.one_p_mnist()


def test_factorization_machines(sagemaker_session, cpu_instance_type, training_set):
    job_name = unique_name_from_base("fm")

    with timeout(minutes=TRAINING_DEFAULT_TIMEOUT_MINUTES):
        fm = FactorizationMachines(
            role="SageMakerRole",
            instance_count=1,
            instance_type=cpu_instance_type,
            num_factors=10,
            predictor_type="regressor",
            epochs=2,
            clip_gradient=1e2,
            eps=0.001,
            rescale_grad=1.0 / 100,
            sagemaker_session=sagemaker_session,
        )

        # training labels must be 'float32'
        fm.fit(
            fm.record_set(training_set[0][:200], training_set[1][:200].astype("float32")),
            job_name=job_name,
        )

    with timeout_and_delete_endpoint_by_name(job_name, sagemaker_session):
        model = FactorizationMachinesModel(
            fm.model_data, role="SageMakerRole", sagemaker_session=sagemaker_session
        )
        predictor = model.deploy(1, cpu_instance_type, endpoint_name=job_name)
        result = predictor.predict(training_set[0][:10])

        assert len(result) == 10
        for record in result:
            assert record.label["score"] is not None


def test_async_factorization_machines(sagemaker_session, cpu_instance_type, training_set):
    job_name = unique_name_from_base("fm")

    with timeout(minutes=5):
        fm = FactorizationMachines(
            role="SageMakerRole",
            instance_count=1,
            instance_type=cpu_instance_type,
            num_factors=10,
            predictor_type="regressor",
            epochs=2,
            clip_gradient=1e2,
            eps=0.001,
            rescale_grad=1.0 / 100,
            sagemaker_session=sagemaker_session,
        )

        # training labels must be 'float32'
        fm.fit(
            fm.record_set(training_set[0][:200], training_set[1][:200].astype("float32")),
            job_name=job_name,
            wait=False,
        )

        print("Detached from training job. Will re-attach in 20 seconds")
        time.sleep(20)
        print("attaching now...")

    with timeout_and_delete_endpoint_by_name(job_name, sagemaker_session):
        estimator = FactorizationMachines.attach(
            training_job_name=job_name, sagemaker_session=sagemaker_session
        )
        model = FactorizationMachinesModel(
            estimator.model_data, role="SageMakerRole", sagemaker_session=sagemaker_session
        )
        predictor = model.deploy(1, cpu_instance_type, endpoint_name=job_name)
        result = predictor.predict(training_set[0][:10])

        assert len(result) == 10
        for record in result:
            assert record.label["score"] is not None
<|endoftext|>"
},
{
"prompt": "

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "#!/usr/bin/env python

import os
import boto3
import pytest
from moto import mock_s3
from moto import mock_ssm

# our module(s)
from nicolet_processor import NicoletProcessor
from base_processor.tests import init_ssm
from base_processor.timeseries.tests import timeseries_test, channels_test

from params import params_channel

@mock_s3
@mock_ssm
@pytest.mark.parametrize("ts_expected", params_channel)
def test_channel(ts_expected):
    init_ssm()

    task = NicoletProcessor(inputs=ts_expected.inputs)
    channels_test(task, ts_expected)
<|endoftext|>"
},
{
"prompt": "fn start_service(service_name, host, port)

fn stop_process(process)

fn dynamodb2_server()

fn s3_server()

fn kms_server()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import boto3
from contextlib import contextmanager
import pytest
from moto import mock_s3
from mock_server import *  # noqa
from pytest_lazyfixture import lazy_fixture
import tempfile
import os

import ray
from ray.experimental import workflow
from ray.experimental.workflow import storage
from ray.tests.conftest import get_default_fixture_ray_kwargs


@pytest.fixture(scope="session")
def filesystem_storage():
    with tempfile.TemporaryDirectory() as d:
        yield d


@pytest.fixture(scope="session")
def aws_credentials():
    old_env = os.environ
    os.environ["AWS_ACCESS_KEY_ID"] = "testing"
    os.environ["AWS_SECRET_ACCESS_KEY"] = "testing"
    os.environ["AWS_SECURITY_TOKEN"] = "testing"
    os.environ["AWS_SESSION_TOKEN"] = "testing"
    yield (f"aws_access_key_id={os.environ['AWS_ACCESS_KEY_ID']}&"
           f"aws_secret_access_key={os.environ['AWS_SECRET_ACCESS_KEY']}&"
           f"aws_session_token={os.environ['AWS_SESSION_TOKEN']}")
    os.environ = old_env


@pytest.fixture(scope="session")
def s3_storage(aws_credentials, s3_server):
    with mock_s3():
        client = boto3.client(
            "s3", region_name="us-west-2", endpoint_url=s3_server)
        client.create_bucket(Bucket="test_bucket")
        url = ("s3://test_bucket/workflow"
               f"?region_name=us-west-2&endpoint_url={s3_server}"
               f"&{aws_credentials}")
        yield url


@contextmanager
def _workflow_start(storage_url, shared, **kwargs):
    init_kwargs = get_default_fixture_ray_kwargs()
    init_kwargs.update(kwargs)
    if ray.is_initialized():
        ray.shutdown()
        storage.set_global_storage(None)
    # Sometimes pytest does not cleanup all global variables.
    # we have to manually reset the workflow storage. This
    # should not be an issue for normal use cases, because global variables
    # are freed after the driver exits.
    address_info = ray.init(**init_kwargs)
    workflow.init(storage_url)
    yield address_info
    # The code after the yield will run as teardown code.
    ray.shutdown()
    storage.set_global_storage(None)


@pytest.fixture(scope="function")
def workflow_start_regular(storage_url, request):
    param = getattr(request, "param", {})
    with _workflow_start(storage_url, False, **param) as res:
        yield res


@pytest.fixture(scope="session")
def workflow_start_regular_shared(storage_url, request):
    param = getattr(request, "param", {})
    with _workflow_start(storage_url, True, **param) as res:
        yield res


def pytest_generate_tests(metafunc):
    if "storage_url" in metafunc.fixturenames:
        metafunc.parametrize(
            "storage_url",
            [lazy_fixture("s3_storage"),
             lazy_fixture("filesystem_storage")],
            scope="session")
<|endoftext|>"
},
{
"prompt": "fn countdown(n)

fn multiply(x, y)

fn count_letters(tup)

fn count_letters2(obj)

fn word_multiply(x, word)

fn count_forever()

fn get_task_name(task)

fn get_user_id(user)

fn hello()

fn result(obj)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from django_q.tasks import async
from django_q.brokers import get_broker
from django_q.cluster import Cluster
from django_q.monitor import monitor, info
from django_q.status import Stat
from django_q.conf import Conf


@pytest.mark.django_db
def test_monitor(monkeypatch):
    assert Stat.get(0).sentinel == 0
    c = Cluster()
    c.start()
    stats = monitor(run_once=True)
    c.stop()
    assert len(stats) > 0
    found_c = False
    for stat in stats:
        if stat.cluster_id == c.pid:
            found_c = True
            assert stat.uptime() > 0
            assert stat.empty_queues() is True
            break
    assert found_c is True
    # test lock size
    monkeypatch.setattr(Conf, 'ORM', 'default')
    b = get_broker('monitor_test')
    b.enqueue('test')
    b.dequeue()
    assert b.lock_size() == 1
    monitor(run_once=True, broker=b)
    b.delete_queue()


@pytest.mark.django_db
def test_info():
    info()
    do_sync()
    info()
    for _ in range(24):
        do_sync()
    info()


def do_sync():
    async('django_q.tests.tasks.countdown', 1, sync=True, save=True)
<|endoftext|>"
},
{
"prompt": "class TaxonomyData:
	kingdom: str = dataclasses.field(metadata={'key': True})
	order: str = dataclasses.field(metadata={'key': True})
	family: str = dataclasses.field(metadata={'key': True})
	genus: str = dataclasses.field(metadata={'key': True})


class TreeData:
	serial_number: int = dataclasses.field(metadata={'key': True})
	taxonomy: TaxonomyData = dataclasses.field(metadata={'key': True})
	specie: str = dataclasses.field(metadata={'key': True})
	diameter_m: float = None
	long_description: bytes = None
	has_flower: bool = None
	plantation_datetime: datetime.datetime = None
	last_pruning_date: datetime.date = None


class PlantMagnoliaTask(Task):
	fn __init__(self, serial_number, specie, model)
	fn name(self)


class FailedTask(Task):
	fn name(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """""""

# Standard library modules.

# Third party modules.
import pytest

# Local modules.
from pipeline_async.task import Task
import mock

# Globals and constants variables.


@pytest.mark.asyncio
async def test_task_run_nomodel():
    task = mock.PlantMagnoliaTask(1, 'Magnolia grandiflora')
    assert await task.run()
    assert await task.run() # No check that result was previously calculated



<|endoftext|>"
},
{
"prompt": "

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from async_asgi_testclient import TestClient
from myapp import main
import pytest


@pytest.mark.asyncio
async def test_willpyre_app():

    async with TestClient(main) as client:
        resp = await client.get("/")
        assert resp.status_code == 200
        assert resp.text == "index page"


@pytest.mark.asyncio
async def test_willpyre_post():

    async with TestClient(main) as client:
        resp = await client.post("/login/", data="a=anything")
        assert resp.status_code == 200
        assert resp.text == "anything"


@pytest.mark.asyncio
async def test_willpyre_get():

    async with TestClient(main) as client:
        resp = await client.get("/login/?user=admin")
        assert resp.status_code == 200
        assert resp.text == "Welcome admin"


@pytest.mark.asyncio
async def test_trailing_slash():

    async with TestClient(main) as client:
        resp = await client.get("/login")
        assert resp.status_code == 200
        assert resp.text == "Welcome ordinary user"


@pytest.mark.asyncio
async def test_url_vars():

    async with TestClient(main) as client:
        resp = await client.get("/api/hello")
        assert resp.status_code == 200
        assert resp.text == "You requested the variable hello"


@pytest.mark.asyncio
async def test_url_many():

    async with TestClient(main) as client:
        resp = await client.get("/static/foo/bar/baz")
        assert resp.status_code == 200
        assert resp.text == "foobarbaz"


@pytest.mark.asyncio
async def test_utils():

    async with TestClient(main) as client:
        resp = await client.get("/json")
        assert resp.json() == {'a': 'b'}
        assert resp.headers["Content-Type"] == "application/json"


@pytest.mark.asyncio
async def test_response404():
    async with TestClient(main) as client:
        resp = await client.get("/non-exhistent")
        assert resp.text == "Not found"
        assert resp.status_code == 404


@pytest.mark.asyncio
async def test_response405():
    async with TestClient(main) as client:
        resp = await client.open("/login", method="NO_SUCH_METHOD")
        assert resp.text == "Method not allowed"
        assert resp.status_code == 405


@pytest.mark.asyncio
async def test_put():

    async with TestClient(main) as client:
        resp = await client.put("/others")
        assert resp.text == "others"


@pytest.mark.asyncio
async def test_patch():

    async with TestClient(main) as client:
        resp = await client.patch("/others")
        assert resp.text == "others"
<|endoftext|>"
},
{
"prompt": "fn _verify_attr(name)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from konst._common import _verify_attr


@pytest.mark.parametrize('name', ['lower', 'UPPER', 'PascalCase', 'camelCase'])
def test_regular_name(name):
    _verify_attr(name)


@pytest.mark.parametrize('name', ['_single', '__double', '__dunder__'])
def test_bad_name(name):
    with pytest.raises(AttributeError):
        _verify_attr(name)
<|endoftext|>"
},
{
"prompt": "fn nodeIds(nodeSet)

fn validUpgrade(nodeIds, tconf)

fn sendUpgrade(client, wallet, upgradeData)

fn ensureUpgradeSent(looper, trustee, trusteeWallet, upgradeData)

fn checkUpgradeScheduled(nodes, version)

fn checkNoUpgradeScheduled(nodes)

fn codeVersion()

fn bumpVersion(v)

fn bumpedVersion()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from copy import deepcopy

from plenum.common.eventually import eventually
from plenum.common.txn import VERSION, NAME
from plenum.common.util import randomString
from plenum.test.test_node import checkNodesConnected
from sovrin.common.txn import SHA256, CANCEL, ACTION
from sovrin.test.conftest import tconf, testNodeClass, testClientClass
from plenum.test.conftest import allPluginsPath
from sovrin.test.helper import TestNode
from sovrin.test.upgrade.helper import bumpVersion, sendUpgrade, \
    ensureUpgradeSent, checkUpgradeScheduled


@pytest.fixture(scope="module")
def txnPoolNodeSet(tconf, nodeSet):
    for node in nodeSet:
        node.config = tconf
        node.upgrader.config = tconf
    return nodeSet


def testUpgradeLatestUncancelledVersion(looper,
                                        txnPoolNodeSet, tconf, nodeThetaAdded,
                                        validUpgrade, trustee, trusteeWallet,
                                        tdirWithPoolTxns, allPluginsPath):
    """
    A node starts and finds several upgrades but selects the latest one which
    is not cancelled, eg node is on version 1.2 but finds 1.3, 1.4 and 1.5 but
    since 1.5 is cancelled, it selects 1.4
    """
    nodeSet = txnPoolNodeSet
    newSteward, newStewardWallet, newNode = nodeThetaAdded
    for node in nodeSet[:-1]:
        node.nodestack.removeRemoteByName(newNode.nodestack.name)
        newNode.nodestack.removeRemoteByName(node.nodestack.name)
    newNode.stop()
    nodeSet = nodeSet[:-1]
    looper.removeProdable(newNode)

    upgr1 = deepcopy(validUpgrade)

    upgr2 = deepcopy(upgr1)
    upgr2[VERSION] = bumpVersion(upgr1[VERSION])
    upgr2[NAME] += randomString(3)
    upgr2[SHA256] = randomString(32)

    upgr3 = deepcopy(upgr2)
    upgr3[VERSION] = bumpVersion(upgr2[VERSION])
    upgr3[NAME] += randomString(3)
    upgr3[SHA256] = randomString(32)

    upgr4 = deepcopy(upgr3)
    upgr4[ACTION] = CANCEL

    ensureUpgradeSent(looper, trustee, trusteeWallet, upgr1)
    looper.run(eventually(checkUpgradeScheduled, nodeSet[:-1], upgr1[VERSION],
                          retryWait=1, timeout=5))

    ensureUpgradeSent(looper, trustee, trusteeWallet, upgr2)
    looper.run(eventually(checkUpgradeScheduled, nodeSet[:-1], upgr2[VERSION],
                          retryWait=1, timeout=5))

    ensureUpgradeSent(looper, trustee, trusteeWallet, upgr3)
    looper.run(eventually(checkUpgradeScheduled, nodeSet[:-1], upgr3[VERSION],
                          retryWait=1, timeout=5))

    ensureUpgradeSent(looper, trustee, trusteeWallet, upgr4)
    looper.run(eventually(checkUpgradeScheduled, nodeSet[:-1], upgr2[VERSION],
                          retryWait=1, timeout=5))

    trustee.stopRetrying()

    newNode = TestNode(newNode.name, basedirpath=tdirWithPoolTxns,
                       config=tconf, pluginPaths=allPluginsPath,
                       ha=newNode.nodestack.ha, cliha=newNode.clientstack.ha)
    looper.add(newNode)
    nodeSet.append(newNode)
    looper.run(checkNodesConnected(nodeSet, overrideTimeout=30))

    looper.run(eventually(checkUpgradeScheduled, [newNode, ], upgr2[VERSION],
                          retryWait=1, timeout=10))


<|endoftext|>"
},
{
"prompt": "fn _skip_word(word: str, no_skip_spaces: bool) -> bool

fn _skip_date(date_from_word: str, cut_off_date: str) -> bool

fn _scrape_once(data, config: Config) -> Iterator[WordPronPair]

"""Scrapes with a given configuration."""
fn scrape(config: Config) -> Iterator[WordPronPair]

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import collections

import pytest

from wikipron.scrape import scrape, _skip_word, _skip_date
from wikipron.extract import EXTRACTION_FUNCTIONS

from . import can_connect_to_wiktionary, config_factory


SmokeTestLanguage = collections.namedtuple(
    "SmokeTestLanguage", ("key", "wik_name", "config_params")
)
SmokeTestLanguage.__doc__ = """
Represents a language to run a smoke test on.

Parameters
----------
key : str
    An ISO 639 code or language name.
wik_name : str
    The language name used by Wiktionary.
config_params : dict
    Parameters for the Config class.
"""

_SMOKE_TEST_LANGUAGES = [
    SmokeTestLanguage("eng", "English", {}),
    # Test that 'sup[a[@title = "wikipedia:Slovak phonology"]]' works.
    SmokeTestLanguage("slk", "Slovak", {}),
    # Test that the extra "span" layer for Korean is handled.
    # Korean data is mostly phonetic transcription only.
    SmokeTestLanguage("kor", "Korean", {"phonetic": True}),
    SmokeTestLanguage("khb", "Lü", {}),
    SmokeTestLanguage("khm", "Khmer", {}),
    SmokeTestLanguage("shn", "Shan", {}),
    SmokeTestLanguage("tha", "Thai", {}),
    SmokeTestLanguage("lat", "Latin", {}),
    # Japanese data is mostly phonetic transcription.
    SmokeTestLanguage("jpn", "Japanese", {"phonetic": True}),
    SmokeTestLanguage("cmn", "Chinese", {}),
    # Vietnamese data is mostly phonetic transcription.
    SmokeTestLanguage("vie", "Vietnamese", {"phonetic": True}),
]


@pytest.mark.skipif(not can_connect_to_wiktionary(), reason="need Internet")
@pytest.mark.parametrize("smoke_test_language", _SMOKE_TEST_LANGUAGES)
def test_smoke_test_scrape(smoke_test_language):
    """A smoke test for scrape()."""
    n = 10  # number of word-pron pairs to scrape
    config = config_factory(
        key=smoke_test_language.key, **smoke_test_language.config_params
    )
    assert config.language == smoke_test_language.wik_name
    pairs = []
    for i, (word, pron) in enumerate(scrape(config)):
        if i >= n:
            break
        pairs.append((word, pron))
    assert len(pairs) == n
    assert all(word and pron for (word, pron) in pairs)


def test_special_languages_covered_by_smoke_test():
    """All languages handled by wikipron.extract must have a smoke test."""
    special_languages = {lang for lang in EXTRACTION_FUNCTIONS.keys()}
    smoke_test_languages = {lang.wik_name for lang in _SMOKE_TEST_LANGUAGES}
    assert special_languages.issubset(smoke_test_languages), (
        "These languages must also be included in the smoke test: "
        f"{special_languages - smoke_test_languages}"
    )


@pytest.mark.parametrize(
    "word, no_skip_spaces, expected",
    [
        ("foobar", False, False),
        ("a phrase", False, True),
        ("hyphen-ated", False, True),
        ("prefix-", False, True),
        ("-suffix", False, True),
        ("hasdigit2", False, True),
        ("a phrase", True, False),
        ("foobar", True, False),
    ],
)
def test__skip_word(word, no_skip_spaces, expected):
    assert _skip_word(word, no_skip_spaces) == expected


@pytest.mark.parametrize(
    "date_from_word, cut_off_date, expected",
    [
        ("2019-10-15", "2019-10-20", False),
        ("2019-10-20", "2019-10-20", False),
        ("2019-10-25", "2019-10-20", True),
    ],
)
def test__skip_date(date_from_word, cut_off_date, expected):
    assert _skip_date(date_from_word, cut_off_date) == expected
<|endoftext|>"
},
{
"prompt": "fn test_root() -> Path

fn get_demo_config()

fn app_sqlite_target(app_name: str) -> str

fn create_lookup_db(app_name: str)

fn configure_logging()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
import sqlite3

from create_lookups import app_sqlite_target

from haiku_node.blockchain_helpers.eos import eosio_account
from haiku_node.lookup.eos_lookup import UnificationLookup, default_db


@pytest.mark.parametrize("app_name", ['app1', 'app2', 'app3'])
def test_validate_lookup_dbs(app_name):
    db_name = app_sqlite_target(app_name)
    conn = sqlite3.connect(db_name)
    c = conn.cursor()

    t = ('user2',)
    c.execute('SELECT native_id FROM lookup WHERE eos_account=?', t)
    res = c.fetchone()[0]
    conn.close()
    assert res == '2'


@pytest.mark.parametrize("app_name", ['app1'])
def test_user_lookup(app_name):
    db_name = app_sqlite_target(app_name)
    ul = UnificationLookup(db_name)
    user_lookup(ul)


def user_lookup(ul: UnificationLookup):
    u1name = 15426359793685626880
    u1name_str = '15426359793685626880'
    u1_str = 'user1'
    u1_n_id = 1

    print(f"eosio_account.name_to_string({u1name}) =",
          eosio_account.name_to_string(u1name))
    print(f"eosio_account.name_to_string('{u1name_str}') =",
          eosio_account.name_to_string(u1name_str))
    print(f"eosio_account.string_to_name('{u1_str}') =",
          eosio_account.string_to_name(u1_str))

    assert (eosio_account.name_to_string(u1name) == u1_str) is True
    assert (eosio_account.name_to_string(u1name_str) == u1_str) is True
    assert (eosio_account.string_to_name(u1_str) == u1name) is True

    print(f"Lookup account as readable string: '{u1_str}'")
    n_id = ul.get_native_user_id(u1_str)
    print(f"{u1_str} native ID:", n_id)

    assert (int(n_id) == u1_n_id) is True

    print(f"Lookup account as account_name int: {u1name}")
    n_id = ul.get_native_user_id(u1name)
    print(f"{u1_str} native ID:", n_id)

    assert (int(n_id) == u1_n_id) is True

    print(f"Lookup account as account_name string: '{u1name_str}'")
    n_id = ul.get_native_user_id(u1name_str)
    print(f"{u1_str} native ID:", n_id)

    assert (int(n_id) == u1_n_id) is True

    print(f"Lookup native UID {u1_n_id} as EOS Account")
    eos_acc = ul.get_eos_account(u1_n_id)
    print(f"UID {u1_n_id} EOS account:", eos_acc)

    assert (eos_acc == u1_str) is True

    meta_data = ul.get_native_user_meta()
    print("Meta data:")
    print(meta_data)

    schema_map = ul.get_schema_map('0')
    print("Schema map for app1 schema_id 0:")

    print(schema_map)

    real_table = ul.get_real_table_info('0', 'data_1')
    print("real table data for app1 schema_id 0, data_1:")
    print(real_table)


if __name__ == "__main__":
    user_lookup(UnificationLookup(default_db()))
<|endoftext|>"
},
{
"prompt": """"Backchannel Abstract Base Class.

Pluggable Backchannels should implement each of the methods relevant to
their agent and testing scenario."""
class Backchannel(ABC):


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """" Test Suite fixture definitions.

    These fixtures define the core functionality of the testing agent.

    For more information on how pytest fixtures work, see
    https://docs.pytest.org/en/latest/fixture.html#fixture
"""

import asyncio
import json
import os
from importlib import import_module
from contextlib import suppress

import pytest
from aiohttp import web

from . import Suite
from .backchannel import SuiteConnectionInfo

# pylint: disable=redefined-outer-name


@pytest.fixture(scope='session')
def event_loop():
    """ Create a session scoped event loop.

        pytest.asyncio plugin provides a default function scoped event loop
        which cannot be used as a dependency to session scoped fixtures.
    """
    return asyncio.get_event_loop()


@pytest.fixture(scope='session')
def config(pytestconfig):
    """ Get suite configuration.
    """
    yield pytestconfig.suite_config


@pytest.fixture(scope='session')
def suite():
    """Get channel manager for test suite."""
    yield Suite()


@pytest.fixture(scope='session')
async def http_endpoint(config, suite):
    """Create http server task."""

    async def handle(request):
        """aiohttp handle POST."""
        response = []
        with suite.reply(response.append):
            await suite.handle(await request.read())

        if response:
            return web.Response(body=response.pop())

        raise web.HTTPAccepted()

    app = web.Application()
    app.router.add_post('/', handle)
    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, config['host'], config['port'])
    server_task = asyncio.ensure_future(site.start())
    yield
    server_task.cancel()
    with suppress(asyncio.CancelledError):
        await server_task
    await runner.cleanup()


@pytest.fixture(scope='session')
async def backchannel(config, http_endpoint, suite):
    """Get backchannel to test subject."""
    if 'backchannel' in config and config['backchannel']:
        path_parts = config['backchannel'].split('.')
        mod_path, class_name = '.'.join(path_parts[:-1]), path_parts[-1]
        mod = import_module(mod_path)
        backchannel_class = getattr(mod, class_name)
    else:
        from default import ManualBackchannel
        backchannel_class = ManualBackchannel

    suite.set_backchannel(backchannel_class())
    await suite.backchannel.setup(config, suite)
    yield suite.backchannel
    await suite.backchannel.close()


@pytest.fixture(scope='session')
async def provider(config, suite):
    """Get provider to test subject."""
    if not 'provider' in config and not config['provider']:
        raise "No 'provider' was specified in the config file"
    path_parts = config['provider'].split('.')
    mod_path, class_name = '.'.join(path_parts[:-1]), path_parts[-1]
    mod = import_module(mod_path)
    provider_class = getattr(mod, class_name)
    suite.set_provider(provider_class())
    await suite.provider.setup(config)
    yield suite.provider


@pytest.fixture(scope='session')
def temporary_channel(http_endpoint, suite):
    """Get contextmanager for using a temporary channel."""
    yield suite.temporary_channel


@pytest.fixture
async def connection(config, temporary_channel, backchannel):
    """Fixture for active connection"""
    with temporary_channel() as conn:
        info = SuiteConnectionInfo(
            conn.did,
            conn.verkey_b58,
            'test-suite',
            config['endpoint']
        )
        their_info = await backchannel.new_connection(info)
        conn.update(**their_info._asdict())
        yield conn
<|endoftext|>"
},
{
"prompt": "fn atp_get_original_url(safe_url)

fn proofpoint_get_original_url(safe_url)

fn unescape_url(escaped_url)

fn get_fqdn(the_input)

fn extract_fqdn(the_input)

fn main()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# -*- coding: utf-8 -*-
import demistomock as demisto
from ExtractDomainAndFQDNFromUrlAndEmail import extract_fqdn, main
import pytest


@pytest.mark.parametrize('input,fqdn', [  # noqa: E501 disable-secrets-detection
    ('www.static.attackiqtes.com', 'www.static.attackiqtes.com'),
    ('http:www.static.attackiqtes.com', 'www.static.attackiqtes.com'),
    ('attackiqtes.co.il', 'attackiqtes.co.il'),
    ('ftp://www.test.com/test2/dev', 'www.test.com'),
    ('http://www.test.com/test2/dev', 'www.test.com'),
    ('www.test.fake', ''),
    ('www[.]demisto[.]com', 'www.demisto.com'),
    ('www[.]demisto[.]test2.com', 'www.demisto.test2.com'),
    ('test.zip', ''),
    ('https%3A%2F%2Fdulunggakada40[.]com', 'dulunggakada40.com'),
    ('https%3A%2F%2Fpath.test.com', 'path.test.com'),
    ('https://urldefense.com/v3/__http://survey.lavulcamktg.cl/index.php/783758', 'survey.lavulcamktg.cl'),
    ('this.is.test.com', 'this.is.test.com'),
    ('caseapi.phishlabs.com', 'caseapi.phishlabs.com'),
    ('www.bücher.de', 'www.bücher.de'),
    ('https://urldefense.proofpoint.com/v2/url?u=http-3A__go.getpostman.com_y4wULsdG0h0DDMY0Dv00100&d=DwMFaQ&c'
     '=ywDJJevdGcjv4rm9P3FcNg&r=s5kA2oIAQRXsacJiBKmTORIWyRN39ZKhobje2GyRgNs&m'
     '=vN1dVSiZvEoM9oExtQqEptm9Dbvq9tnjACDZzrBLaWI&s=zroN7KQdBCPBOfhOmv5SP1DDzZKZ1y9I3x4STS5PbHA&e=',
     'go.getpostman.com'),  # noqa: E501
    ('www[.]demisto[.]com', 'www.demisto.com'),
    ('hxxp://www[.]demisto[.]com', 'www.demisto.com'),
    ('www[.]demisto.test[.]com', 'www.demisto.test.com'),
    ('https://emea01.safelinks.protection.outlook.com/?url=https%3A%2F%2Ftwitter.com%2FPhilipsBeLux&data=02|01'
     '||cb2462dc8640484baf7608d638d2a698|1a407a2d76754d178692b3ac285306e4|0|0|636758874714819880&sdata'
     '=dnJiphWFhnAKsk5Ps0bj0p%2FvXVo8TpidtGZcW6t8lDQ%3D&reserved=0%3E%5bcid:image003.gif@01CF4D7F.1DF62650%5d'
     '%3C', 'twitter.com'),  # noqa: E501 disable-secrets-detection
])  # noqa: E124
def test_extract_fqdn_or_domain(input, fqdn):
    extracted_fqdn = extract_fqdn(input)
    # extracted_domain = extract_fqdn_or_domain(input, is_domain=True)

    assert extracted_fqdn == fqdn
    # assert extracted_domain == domain


def test_extract_fqdn_or_domain_empty_indicators(mocker):
    mocker.patch.object(demisto, 'args', return_value={'input': '1Ab.Vt'})
    mocker.patch.object(demisto, 'results')

    main()
    results = demisto.results.call_args[0]

    assert results[0] == [{'Contents': [], 'ContentsFormat': 'json', 'Type': 1}]
<|endoftext|>"
},
{
"prompt": """"Sets the RPO to ASYNC_TURBO, enabling the turbo replication feature"""
fn set_rpo_async_turbo(bucket_name)

"""Creates dual-region bucket with turbo replication enabled."""
fn create_bucket_turbo_replication(bucket_name)

"""Sets the RPO to DEFAULT, disabling the turbo replication feature"""
fn set_rpo_default(bucket_name)

"""Gets the RPO of the bucket"""
fn get_rpo(bucket_name)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import uuid

from google.cloud import storage
import pytest

import storage_create_bucket_turbo_replication
import storage_get_rpo
import storage_set_rpo_async_turbo
import storage_set_rpo_default


@pytest.fixture
def dual_region_bucket():
    """Yields a dual region bucket that is deleted after the test completes."""
    bucket = None
    while bucket is None or bucket.exists():
        bucket_name = "bucket-lock-{}".format(uuid.uuid4())
        bucket = storage.Client().bucket(bucket_name)
        bucket.location = "NAM4"
    bucket.create()
    yield bucket
    bucket.delete(force=True)


def test_get_rpo(dual_region_bucket, capsys):
    storage_get_rpo.get_rpo(dual_region_bucket.name)
    out, _ = capsys.readouterr()
    assert f"RPO for {dual_region_bucket.name} is DEFAULT." in out


def test_set_rpo_async_turbo(dual_region_bucket, capsys):
    storage_set_rpo_async_turbo.set_rpo_async_turbo(dual_region_bucket.name)
    out, _ = capsys.readouterr()
    assert f"RPO is ASYNC_TURBO for {dual_region_bucket.name}." in out


def test_set_rpo_default(dual_region_bucket, capsys):
    storage_set_rpo_default.set_rpo_default(dual_region_bucket.name)
    out, _ = capsys.readouterr()
    assert f"RPO is DEFAULT for {dual_region_bucket.name}." in out


def test_create_bucket_turbo_replication(capsys):
    bucket_name = "test-rpo-{}".format(uuid.uuid4())
    storage_create_bucket_turbo_replication.create_bucket_turbo_replication(bucket_name)
    out, _ = capsys.readouterr()
    assert f"{bucket_name} created with RPO ASYNC_TURBO in NAM4." in out
<|endoftext|>"
},
{
"prompt": """"Fixed response as replacement for app.db.crud.observations.get_actuals_left_outer_join_with_predictions
    """
fn get_actuals_left_outer_join_with_predictions()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """" Test machine learning code - collecting data, learning from data, and predicting a bias adjusted
result.
"""
from datetime import datetime
from typing import List
import json
import pytest
from pytest_bdd import scenario, given, then, when
from app.db.models import PredictionModel, PredictionModelGridSubset
from app.weather_models import machine_learning
from app.tests.weather_models.crud import get_actuals_left_outer_join_with_predictions


@pytest.fixture()
def mock_get_actuals_left_outer_join_with_predictions(monkeypatch):
    """ Mock out call to DB returning actuals macthed with predictions """
    monkeypatch.setattr(machine_learning, 'get_actuals_left_outer_join_with_predictions',
                        get_actuals_left_outer_join_with_predictions)


@pytest.mark.usefixtures('mock_get_actuals_left_outer_join_with_predictions')
@scenario("test_machine_learning.feature", "Learn weather",
          example_converters=dict(coordinate=json.loads,
                                  points=json.loads,
                                  model_temp=float,
                                  model_rh=float,
                                  timestamp=datetime.fromisoformat,
                                  bias_adjusted_temp=lambda value: None if value == 'None' else float(
                                      value),
                                  bias_adjusted_rh=lambda value: None if value == 'None' else float(value)))
def test_machine_learning():
    """ BDD Scenario for predictions """


@given("An instance of StationMachineLearning for <coordinate> within <points>", target_fixture='instance')
def given_an_instance(coordinate: List, points: List):
    """ Bind the data variable """
    return machine_learning.StationMachineLearning(
        session=None,
        model=PredictionModel(id=1),
        grid=PredictionModelGridSubset(id=1),
        points=points,
        target_coordinate=coordinate,
        station_code=None,
        max_learn_date=datetime.now())


@when('The machine learns')
def learn(instance: machine_learning.StationMachineLearning):
    """ Train the machine learning model """
    instance.learn()


@then('The <model_temp> for <timestamp> results in <bias_adjusted_temp>')
def assert_temperature(
        instance: machine_learning.StationMachineLearning,
        model_temp: float, timestamp: datetime, bias_adjusted_temp: float):
    """ Assert that the ML algorithm predicts the temperature correctly """
    result = instance.predict_temperature(model_temp, timestamp)
    assert result == bias_adjusted_temp


@then('The <model_rh> for <timestamp> results in <bias_adjusted_rh>')
def assert_rh(instance: machine_learning.StationMachineLearning,
              model_rh: float, timestamp: datetime, bias_adjusted_rh: float):
    """ Assert that the ML algorithm predicts the relative humidity correctly """
    result = instance.predict_rh(model_rh, timestamp)
    assert result == bias_adjusted_rh
<|endoftext|>"
},
{
"prompt": """"Delete a dataset."""
fn delete_dataset(project_id, dataset_id)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import datetime
import os

from google.cloud import automl
import pytest

import delete_dataset

PROJECT_ID = os.environ["AUTOML_PROJECT_ID"]
BUCKET_ID = "{}-lcm".format(PROJECT_ID)


@pytest.fixture(scope="function")
def dataset_id():
    client = automl.AutoMlClient()
    project_location = f"projects/{PROJECT_ID}/locations/us-central1"
    display_name = "test_" + datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    metadata = automl.TextExtractionDatasetMetadata()
    dataset = automl.Dataset(
        display_name=display_name, text_extraction_dataset_metadata=metadata
    )
    response = client.create_dataset(parent=project_location, dataset=dataset)
    dataset_id = response.result().name.split("/")[-1]

    yield dataset_id


def test_delete_dataset(capsys, dataset_id):
    # delete dataset
    delete_dataset.delete_dataset(PROJECT_ID, dataset_id)
    out, _ = capsys.readouterr()
    assert "Dataset deleted." in out
<|endoftext|>"
},
{
"prompt": "class FlickrDataset:
	fn __init__(self, images_dir_path: str, texts_path: str)
	"""Creates a dictionary that holds:
	
	Key: The full path to the image.
	Value: A list of lists where each token in the inner list is a word. The number
	of sublists is 5.
	
	Args:
	    texts_path: Path where the text doc with the descriptions is.
	
	Returns:
	    A dictionary that represents what is explained above."""
	fn parse_captions_filenames(texts_path: str) -> Dict[(str, List[str])]
	"""Returns the image paths, the captions and the lengths of the captions.
	
	Args:
	    imgs_file_path: A path to a file where all the images belonging to the
	    validation part of the dataset are listed.
	    img_path_caption: Image name to list of captions dict.
	    images_dir_path: A path where all the images are located.
	
	Returns:
	    Image paths, captions and lengths."""
	fn get_data_wrapper(imgs_file_path: str, img_path_caption: Dict[(str, List[str])], images_dir_path: str)


class FlickrDatasetTrain(FlickrDataset, TorchDataset):
	fn __init__(self, images_dir_path: str, texts_path: str, train_images_file_path: str)
	fn __len__(self)
	fn __getitem__(self, idx: int)


class FlickrDatasetValTest(FlickrDataset, TorchDataset):
	fn __init__(self, images_dir_path: str, texts_path: str, val_images_file_path: str)
	fn __len__(self)
	fn __getitem__(self, idx: int)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from utils.datasets import FlickrDataset


@pytest.fixture
def flickr_texts_path():
    return "data/testing_assets/flickr_tokens.txt"


@pytest.fixture
def flickr_images_path():
    return "data/testing_assets/flickr_images/"


@pytest.fixture
def flickr_train_path():
    return "data/testing_assets/flickr_train.txt"


@pytest.fixture
def flickr_val_path():
    return "data/testing_assets/flickr_val.txt"


def test_flickr_parse_captions_filenames(flickr_texts_path):
    img_path_caption = FlickrDataset.parse_captions_filenames(flickr_texts_path)
    unique_img_paths = set()
    for img_path in img_path_caption.keys():
        assert len(img_path_caption[img_path]) == 5
        unique_img_paths.add(img_path)
    assert len(unique_img_paths) == 5
<|endoftext|>"
},
{
"prompt": "class Connection:
	fn __init__(self, dsn: str, host: str, port: int, database: str, user: str, password: str, cursor_cls, echo, stack_track)
	fn __repr__(self)
	fn connected(self)
	fn host(self)
	fn port(self)
	fn user(self)
	fn password(self)
	fn database(self)
	fn echo(self)
	fn cursor(self, cursor: Type[Cursor]) -> Cursor
	"""Return a client configured from the given URL.
	
	For example::
	
	    clickhouse://[user:password]@localhost:9000/default
	    clickhouses://[user:password]@localhost:9440/default
	
	Three URL schemes are supported:
	    clickhouse:// creates a normal TCP socket connection
	    clickhouses:// creates a SSL wrapped TCP socket connection
	
	Any additional querystring arguments will be passed along to
	the Connection class's initializer."""
	fn _parse_dsn(self, url)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from asynch.connection import Connection


@pytest.mark.asyncio
async def test_pool(pool):
    assert pool.minsize == 1
    assert pool.maxsize == 10
    assert pool.size == 1
    assert pool.freesize == 1


@pytest.mark.asyncio
async def test_pool_cursor(pool):
    async with pool.acquire() as conn:
        async with conn.cursor() as cursor:
            await cursor.execute("SELECT 1")
            ret = cursor.fetchone()
            assert ret == (1,)


@pytest.mark.asyncio
async def test_acquire(pool):
    conn = await pool.acquire()
    assert isinstance(conn, Connection)
    assert pool.freesize == 0
    assert pool.size == 1
    assert conn.connected
    await pool.release(conn)
    assert pool.freesize == 1
    assert pool.size == 1
<|endoftext|>"
},
{
"prompt": "class UserFactory(DjangoModelFactory):
	username = Faker('user_name')
	email = Faker('email')
	name = Faker('name')
	fn password(self, create: bool, extracted: Sequence[Any])


class UserChangeForm(forms.UserChangeForm):


class UserCreationForm(forms.UserCreationForm):
	error_message = forms.UserCreationForm.error_messages.update({'duplicate_username': _('This username has already been taken.')})
	fn clean_username(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from itscsapp.users.forms import UserCreationForm
from itscsapp.users.tests.factories import UserFactory

pytestmark = pytest.mark.django_db


class TestUserCreationForm:

    def test_clean_username(self):
        # A user with proto_user params does not exist yet.
        proto_user = UserFactory.build()

        form = UserCreationForm(
            {
                "username": proto_user.username,
                "password1": proto_user._password,
                "password2": proto_user._password,
            }
        )

        assert form.is_valid()
        assert form.clean_username() == proto_user.username

        # Creating a user.
        form.save()

        # The user with proto_user params already exists,
        # hence cannot be created.
        form = UserCreationForm(
            {
                "username": proto_user.username,
                "password1": proto_user._password,
                "password2": proto_user._password,
            }
        )

        assert not form.is_valid()
        assert len(form.errors) == 1
        assert "username" in form.errors
<|endoftext|>"
},
{
"prompt": """"Default user for restaurant."""
class User(AbstractUser):
	name = CharField(_('Name of User'), blank=True, max_length=255)
	first_name = None
	last_name = None
	"""Get url for user's detail view.
	
	Returns:
	    str: URL for user detail."""
	fn get_absolute_url(self) -> str


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from restaurant.users.models import User

pytestmark = pytest.mark.django_db


def test_user_get_absolute_url(user: User):
    assert user.get_absolute_url() == f"/users/{user.username}/"
<|endoftext|>"
},
{
"prompt": "class Res1D:
	"""Read the res1d file"""
	fn __read(file_path)
	fn _get_time(file)
	fn _get_values(dataItemTypes, file, indices, queries, reachNums)
	fn _get_data(self, file, queries, dataItemTypes, indices, reachNums)
	fn format_string(s)
	fn find_items(self, file, queries, chainage_tolerance)
	fn read(self, file_path, queries)


class ExtractionPoint:
	"""Variable Type (eg. WaterLevel or Discharge)"""
	fn VariableType(VariableType)
	"""Name of the Branch"""
	fn BranchName(BranchName)
	"""Chainage number along branch"""
	fn Chainage(Chainage)
	fn __str__(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from mikeio.res1d import Res1D, ExtractionPoint


def get_test_query():
    query = ExtractionPoint()
    query.BranchName = "104l1"
    query.Chainage = 34.4131
    query.VariableType = "WaterLevel"

    return query


def test_file_does_not_exist():
    file = "tests/testdata/not_a_file.res1d"

    query = get_test_query()

    with pytest.raises(FileExistsError):
        assert Res1D().read(file, [query])


def test_read_single_item():
    file = "tests/testdata/Exam6Base.res1d"
    query = get_test_query()
    ts = Res1D().read(file, [query])

    assert len(ts) == 110
<|endoftext|>"
},
{
"prompt": "fn slow_double(x, sleep_dur)

"""1. Launch a few apps and write the checkpoint once a few have completed
    """
fn test_initial_checkpoint_write(n)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import argparse
import os
import time

import pytest

import parsl
from parsl.app.app import App
from parsl.tests.test_checkpointing.test_python_checkpoint_1 import \
    test_initial_checkpoint_write
from parsl.tests.configs.local_threads_checkpoint import config

rundir = test_initial_checkpoint_write()
config.checkpoint_files = [os.path.join(rundir, 'checkpoint')]

parsl.clear()
parsl.load(config)


@App('python', cache=True)
def slow_double(x, sleep_dur=1):
    import time
    time.sleep(sleep_dur)
    return x * 2


@pytest.mark.local
def test_loading_checkpoint(n=2):
    """2. Load the memoization table from previous checkpoint
    """

    d = {}

    start = time.time()
    print("Launching : ", n)
    for i in range(0, n):
        d[i] = slow_double(i)
    print("Done launching")

    for i in range(0, n):
        d[i].result()
    print("Done sleeping")

    delta = time.time() - start
    assert delta < 1, "Took longer than a second, restore from checkpoint failed"


if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument("-c", "--count", default="10",
                        help="Count of apps to launch")
    parser.add_argument("-d", "--debug", action='store_true',
                        help="Count of apps to launch")
    args = parser.parse_args()

    if args.debug:
        parsl.set_stream_logger()

    x = test_initial_checkpoint_write(n=4)
<|endoftext|>"
},
{
"prompt": "fn skip_upload(response, skip_existing, package)

fn upload(upload_settings, dists)

fn main(args)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Copyright 2013 Donald Stufft
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from __future__ import absolute_import, division, print_function
from __future__ import unicode_literals

import pretend
import pytest

from twine import cli
import twine.commands.upload


def test_dispatch_to_subcommand(monkeypatch):
    replaced_main = pretend.call_recorder(lambda args: None)
    monkeypatch.setattr(twine.commands.upload, "main", replaced_main)

    cli.dispatch(["upload", "path/to/file"])

    assert replaced_main.calls == [pretend.call(["path/to/file"])]


def test_catches_enoent():
    with pytest.raises(SystemExit):
        cli.dispatch(["non-existant-command"])
<|endoftext|>"
},
{
"prompt": """"Samples elements randomly, with replacement,
always in blocks all elements of the dataset.
Only the remainder will be sampled with less elements.

Arguments:
    data_source (Dataset): dataset to sample from
    nsamples (int): number of total samples"""
class NSamplesRandomSampler(Sampler):
	fn __init__(self, data_source, nsamples)
	fn __iter__(self)
	fn __len__(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from memcnn.data.sampling import NSamplesRandomSampler
import torch.utils.data as data
import numpy as np


@pytest.mark.parametrize('nsamples,data_samples', [(1, 1), (14, 10), (10, 14), (5, 1), (1, 5), (0, 10)])
def test_random_sampler(nsamples, data_samples):

    class TestDataset(data.Dataset):
        def __init__(self, elements):
            self.elements = elements

        def __getitem__(self, idx):
            return idx, idx

        def __len__(self):
            return self.elements

    datasrc = TestDataset(data_samples)
    sampler = NSamplesRandomSampler(datasrc, nsamples=nsamples)
    count = 0
    elements = []
    for e in sampler:
        elements.append(e)
        count += 1
        if count % data_samples == 0:
            assert len(np.unique(elements)) == len(elements)
            elements = []
    assert count == nsamples
    assert len(sampler) == nsamples
    assert sampler.__len__() == nsamples
<|endoftext|>"
},
{
"prompt": """"This is the decorator which performs recursive AST substitution of
functions, and optional JIT-compilation using `numba`_.

This must only be used on functions with positional parameters
defined; this must not be used on functions with keyword parameters.

This decorator modifies the original function (and any nested
function calls) by replacing any functions passed in using keyword
arguments. It replaces them in the AST and returns a new function
object with a new code object that calls the replacement functions
instead.

For example, a function hierarchy such as:

>>> def calculate(x):
...     return x * x
>>> def my_func(x):
...     a = calculate(x)
...     return a / 2

will take the input variable `x`, square it, and then halve the
result:

>>> my_func(6)
18.0

Six squared is 36, divided by two is 18.

If you wanted to replace the `calculate` function to
return a different calculation, you could use this `@fs`
decorator:

>>> @fs
... def my_func(x):
...     a = calculate(x)
...     return a / 2

Now the `my_func` callable is able to accept keyword arguments,
which it will replace recursively throughout its hierarchy.

If you wanted to change the `calculate` in this function to:

>>> def cube(x):
...     return x * x * x

then after applying the `@fs` decorator you can do this:

>>> my_func(6, calculate=cube)
108.0

Six cubed is 216, divided by two is 108.0.

This parametrisation can be decided at runtime - every time a
new keyword argument is passed in, it generates a new function
object with a new code object.

To store the new function object instead of executing it, pass
`return_callable=True` to the decorated function:

>>> new_func = my_func(6, calculate=cube, return_callable=True)
>>> # At this point the new function has **not** been called.
>>> new_func(6)
108.0"""
fn fs(func)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "
import numpy as np

from fastats.core.decorator import fs


def value(x, val_in, val_out, state):  # pragma: no cover
    return val_in, state


@fs
def windowed_stateful_pass(x, win):
    """
    Performs a *stateful* rolling (windowed)
    iteration over the first dimension of `x`.

    This allows to improve performance by avoiding
    re-calculating values from scratch for each
    window.

    This has the exact same properties as
    :func:`windowed_pass`, but expects the
    ``value`` function be "stateful" and have this
    return signature:
    ``(window_arr, val_in, val_out, state_arr)``

    ``val_in`` and ``val_out`` are the values
    coming "in" to the window and already thrown
    "out" of the window in each iteration.

    The state starts off as an empty array, then
    the first return from the ``value`` function
    sets the state by returning it along with the
    window's value.  The function is free to
    modify the state array in each iteration.

    In the first iteration, you should pre-allocate
    the state array to fit all the state variables
    you will need in subsequent iterations.  Due
    to optimizations made here, only first state
    return will be respected. All subsequent
    iterations can modify the array, but their
    state return value will be ignored.

    Example
    -------
    >>> import numpy as np
    >>> def rolling_sum(x, val_in, val_out, state):
    ...     if state.size == 0:   # state is empty
    ...         state = np.array([np.sum(x)], dtype=x.dtype)
    ...     else:
    ...         state[0] += val_in - val_out
    ...     return state[0], state
    >>> x = np.arange(7, dtype='float')
    >>> result = windowed_stateful_pass(x, 4, value=rolling_sum)
    >>> result
    array([ nan,  nan,  nan,   6.,  10.,  14.,  18.])
    """
    result = np.full_like(x, np.nan)
    state = np.empty(0, dtype=result.dtype)

    val_in = x[win-1]
    val_out = np.nan
    # must call the value func to set the state
    result[win-1], state = value(x[0:win], val_in, val_out, state)

    # subsequent iterations ignore state returns
    # (without this, numba won't optimize out the assignment
    #  and this will be slow)
    for i in range(win+1, x.shape[0]+1):
        start = i - win
        in_idx = i - 1

        val_in = x[in_idx]
        val_out = x[start-1]
        result[in_idx], _ = value(x[start:i], val_in, val_out, state)

    return result


if __name__ == '__main__':
    import pytest
    pytest.main([__file__])
<|endoftext|>"
},
{
"prompt": """"Helper function to construct decorated arguments

This works only with positional and likely positional arguments
strongly keyword arguments are in **kwargs. It constructs kwargs'
from positional values"""
fn _rebind(decorator, func)

"""Helper function to construct decorator from a simple function

arg: after means that decorated check should be run after the
target function

This is a 'decorate' meta function that wraps new decorator around
target function and merges decorated arguments with target arguments
convention: each new decorator should have a 'response' argument,
which is an output of a target function"""
fn decorate(after)

fn response_does_not_contain_write_only_properties(resource_client, response)

fn response_contains_resource_model_equal_current_model(response, current_resource_model)

fn response_contains_resource_model_equal_updated_model(response, current_resource_model, update_resource_model)

fn response_contains_primary_identifier(resource_client, response)

fn response_contains_unchanged_primary_identifier(resource_client, response, current_resource_model)

fn skip_not_writable_identifier(resource_client)

fn failed_event(error_code, msg)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# fixture and parameter have the same name
# pylint: disable=redefined-outer-name

import pytest

# WARNING: contract tests should use fully qualified imports to avoid issues
# when being loaded by pytest
from rpdk.core.contract.interface import Action, HandlerErrorCode, OperationStatus
from rpdk.core.contract.suite.contract_asserts import failed_event


@pytest.mark.update
@failed_event(error_code=(HandlerErrorCode.NotUpdatable, HandlerErrorCode.NotFound))
def contract_update_create_only_property(resource_client):

    if resource_client.create_only_paths:
        create_request = resource_client.generate_create_example()
        try:
            _status, response, _error = resource_client.call_and_assert(
                Action.CREATE, OperationStatus.SUCCESS, create_request
            )
            created_model = response["resourceModel"]
            update_request = resource_client.generate_invalid_update_example(
                created_model
            )
            _status, response, _error_code = resource_client.call_and_assert(
                Action.UPDATE, OperationStatus.FAILED, update_request, created_model
            )
            assert response["message"]
        finally:
            resource_client.call_and_assert(
                Action.DELETE, OperationStatus.SUCCESS, created_model
            )
    else:
        pytest.skip("No createOnly Properties. Skipping test.")


@pytest.mark.update
@failed_event(
    error_code=HandlerErrorCode.NotFound,
    msg="cannot update a resource which does not exist",
)
def contract_update_non_existent_resource(resource_client):
    create_request = resource_client.generate_invalid_create_example()
    update_request = resource_client.generate_update_example(create_request)
    _status, response, _error = resource_client.call_and_assert(
        Action.UPDATE, OperationStatus.FAILED, update_request, create_request
    )
    assert response["message"]
    return _error
<|endoftext|>"
},
{
"prompt": """"Apply f(x,theta) to x array and theta in samples.

Parameters
----------
f: function
    list of functions :math:`f(x;\theta)`  with dependent variable
    :math:`x`, parameterised by :math:`\theta`.

x: 1D array-like
    x values to evaluate :math:`f(x;\theta)` at.

samples: 2D array-like
    list of theta samples to evaluate :math:`f(x;\theta)` at.
    `shape = (nfunc, nsamples, npars)`

parallel, tqdm_kwargs: optional
    see docstring for :func:`fgivenx.parallel.parallel_apply`

cache: str, optional
    File root for saving previous calculations for re-use
    default None

Returns
-------
2D numpy.array:
    samples at each x. `shape=(len(x),len(samples),)`"""
fn compute_samples(f: function, x: 1D array-like, samples: 2D array-like) -> 

"""Extract samples and weights from getdist chains.

Parameters
----------
params: list(str)
    Names of parameters to be supplied to second argument of f(x|theta).

file_root: str, optional
    Root name for getdist chains files. This variable automatically
    defines:
    - chains_file = file_root.txt
    - paramnames_file = file_root.paramnames
    but can be overidden by chains_file or paramnames_file.

latex: bool, optional
    Also return an array of latex strings for those paramnames.

Returns
-------
samples: numpy.array
    2D Array of samples. `shape=(len(samples), len(params))`

weights: numpy.array
    Array of weights. `shape = (len(params),)`

latex: list(str), optional
    list of latex strings for each parameter
    (if latex is provided as an argument)"""
fn samples_from_getdist_chains(params: list(str), file_root: str, latex: bool) -> list(str), optional

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import numpy
import pytest
import os
from shutil import rmtree
from numpy.testing import assert_allclose, assert_array_equal
from fgivenx.samples import compute_samples, samples_from_getdist_chains

try:
    import getdist

    def test_samples_from_getdist_chains():

        # Set up getdist chains
        file_root = './.chains/test'
        labels = [r'\alpha', r'\beta', r'\gamma']
        names = ['a', 'b', 'g']
        nsamples = 1000
        params = ['a', 'g']
        i = [names.index(p) for p in params]
        numpy.random.seed(0)
        samples_ = numpy.random.rand(nsamples, len(names))
        weights_ = numpy.random.rand(nsamples)
        samples = getdist.mcsamples.MCSamples(samples=samples_, labels=labels,
                                              names=names, weights=weights_)
        samples.saveAsText(file_root, make_dirs=True)

        samples, weights = samples_from_getdist_chains(params, file_root)
        assert_allclose(samples, samples_[:, i])
        assert_allclose(weights, weights_)

        samples, weights, latex = samples_from_getdist_chains(params,
                                                              file_root,
                                                              latex=True)
        assert_allclose(weights, weights_)
        assert_array_equal(latex, numpy.array(labels)[i])

        rmtree('./.chains')

except ImportError:
    pass


def test_compute_samples():

    with pytest.raises(TypeError):
        compute_samples(None, None, None, wrong_argument=None)

    cache = '.test_cache/test'
    numpy.random.seed(0)
    nsamp = 5000
    a, b, e, f = 0, 1, 0, 1
    m = numpy.random.normal(a, b, nsamp)
    c = numpy.random.normal(e, f, nsamp)
    samples = numpy.array([list(_) for _ in zip(m, c)])
    nx = 100
    x = numpy.linspace(-1, 1, nx)
    fsamps_ = numpy.outer(x, m) + c

    def f(x, theta):
        m, c = theta
        return m*x+c

    assert(not os.path.isfile(cache + '_fsamples.pkl'))
    fsamps = compute_samples([f], x, [samples], cache=cache)
    assert(os.path.isfile(cache + '_fsamples.pkl'))

    assert_allclose(fsamps_, fsamps,)

    fsamps = compute_samples([f], x, [samples], cache=cache)
    assert_allclose(fsamps_, fsamps,)

    rmtree('.test_cache')
<|endoftext|>"
},
{
"prompt": "class Properties:
	logger = logging.getLogger(__name__)
	fn __init__(self, env)
	fn __buildPropertiesFromFile(self)
	fn __buildPropertiesFromGCPMetadata(self)
	fn __getMetadataAttribute(self, attributeName)
	fn __outputCsccKey(self, csccKey)
	fn __log_properties(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import json
import pytest
import os
from automate.automate_service import AutomateService
from cscc.cscc_service import CsccService
from properties import Properties
from datetime import datetime, timedelta
from google.protobuf import empty_pb2, struct_pb2, timestamp_pb2

class TestCsccService:
    # Tests that findings built from an Automate Report equals the number of Failed Controls
    def testBuildFindings(self):
        #SETUP
        properties = Properties()
        testSecuritySource = 'organizations/827482578277/sources/9233151395087538604'
        cscc = CsccService(properties, testSecuritySource)
        automate = AutomateService(properties)
        # GIVEN a report with Failing Controls
        reports = []
        path = f"{os.getcwd()}/test_resources"
        with open(os.path.join(path, 'mock_report.json')) as r:
            report = json.load(r)
        reports.append(report)

        # WHEN failed controls are extracted
        failedControls = automate.getFailedControls(reports)

        # THEN cscc findings can be built
        findings = cscc.buildFindings(failedControls)
        assert len(findings) == 23

    # Tests that a finding is successfully created in Google Cloud Security Centre
    @pytest.mark.skip(reason="Creates a new finding on cscc. Don't run until deletion is enabled.")
    def testCreateFindings(self):
        #SETUP
        properties = Properties()
        testSecuritySource = 'organizations/827482578277/sources/9233151395087538604'
        cscc = CsccService(properties, testSecuritySource)

        # GIVEN a mock finding
        finding = {'name': 'TESTFINDING123', 'parent': 'organizations/827482578277/sources/9233151395087538604', 
        'resource_name': 'organizations/827482578277/projects/test-project', 'state': 'INACTIVE', 'category': 'TEST-CATEGORY', 
        'external_uri': 'https://35.197.241.246/compliance/reporting/nodes/d35f7363-4d0a-40a3-b52d-d92ff1050c33', 
        'source_properties': {'control_id': struct_pb2.Value(string_value="cis-gcp-benchmark-vms-4.6"),
        'control_title': struct_pb2.Value(string_value='Title'), 'code_description': struct_pb2.Value(string_value="Instance chef-automate should have disks encrypted with csek"),
        'code_message': struct_pb2.Value(string_value="expected #has_disks_encrypted_with_csek? to return true, got false"),
        'summary': struct_pb2.Value(string_value="CIS Google Cloud Platform Foundation Benchmark Level 2"),
        'status': struct_pb2.Value(string_value='Fail')}, 'security_marks': {}, 'event_time': cscc.timestamp(datetime.now()),
        'create_time': cscc.timestamp(datetime.now())}

        # WHEN create finding is called
        cscc.createFinding(finding)

        # THEN findings are sent to cscc
        findingList = cscc.getAllFindings()
        assert len(findingList) >=1 <|endoftext|>"
},
{
"prompt": "class UserChangeForm(admin_forms.UserChangeForm):


class UserCreationForm(admin_forms.UserCreationForm):


"""Default user for trycookie."""
class User(AbstractUser):
	name = CharField(_('Name of User'), blank=True, max_length=255)
	first_name = None
	last_name = None
	"""Get url for user's detail view.
	
	Returns:
	    str: URL for user detail."""
	fn get_absolute_url(self) -> str


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """"
Module for all Form Tests.
"""
import pytest
from django.utils.translation import gettext_lazy as _

from trycookie.users.forms import UserCreationForm
from trycookie.users.models import User

pytestmark = pytest.mark.django_db


class TestUserCreationForm:
    """
    Test class for all tests related to the UserCreationForm
    """

    def test_username_validation_error_msg(self, user: User):
        """
        Tests UserCreation Form's unique validator functions correctly by testing:
            1) A new user with an existing username cannot be added.
            2) Only 1 error is raised by the UserCreation Form
            3) The desired error message is raised
        """

        # The user already exists,
        # hence cannot be created.
        form = UserCreationForm(
            {
                "username": user.username,
                "password1": user.password,
                "password2": user.password,
            }
        )

        assert not form.is_valid()
        assert len(form.errors) == 1
        assert "username" in form.errors
        assert form.errors["username"][0] == _("This username has already been taken.")
<|endoftext|>"
},
{
"prompt": """"Test a database self.driver for DB API 2.0 compatibility.
This implementation tests Gadfly, but the TestCase
is structured so that other self.drivers can subclass this 
test case to ensure compiliance with the DB-API. It is 
expected that this TestCase may be expanded in the future
if ambiguities or edge conditions are discovered.

The 'Optional Extensions' are not yet being tested.

self.drivers should subclass this test, overriding setUp, tearDown,
self.driver, connect_args and connect_kw_args. Class specification
should be as follows:

import dbapi20 
class mytest(dbapi20.DatabaseAPI20Test):
   [...] 

Don't 'import DatabaseAPI20Test from dbapi20', or you will
confuse the unit tester - just 'import dbapi20'."""
class DatabaseAPI20Test(unittest.TestCase):
	driver = None
	connect_args = ()
	connect_kw_args = {}
	table_prefix = 'dbapi20test_'
	ddl1 = 'create table %sbooze (name varchar(20))' % table_prefix
	ddl2 = 'create table %sbarflys (name varchar(20))' % table_prefix
	xddl1 = 'drop table %sbooze' % table_prefix
	xddl2 = 'drop table %sbarflys' % table_prefix
	lowerfunc = 'lower'
	fn executeDDL1(self, cursor)
	fn executeDDL2(self, cursor)
	"""self.drivers should override this method to perform required setup
	if any is necessary, such as creating the database."""
	fn setUp(self)
	"""self.drivers should override this method to perform required cleanup
	if any is necessary, such as deleting the test database.
	The default drops the tables that may be created."""
	fn tearDown(self)
	fn _connect(self)
	fn test_connect(self)
	fn test_apilevel(self)
	fn test_threadsafety(self)
	fn test_paramstyle(self)
	fn test_Exceptions(self)
	fn test_ExceptionsAsConnectionAttributes(self)
	fn test_commit(self)
	fn test_rollback(self)
	fn test_cursor(self)
	fn test_cursor_isolation(self)
	fn test_description(self)
	fn test_rowcount(self)
	lower_func = 'lower'
	fn test_callproc(self)
	fn test_close(self)
	fn test_execute(self)
	fn _paraminsert(self, cur)
	fn test_executemany(self)
	fn test_fetchone(self)
	samples = ['Carlton Cold', 'Carlton Draft', 'Mountain Goat', 'Redback', 'Victoria Bitter', 'XXXX']
	"""Return a list of sql commands to setup the DB for the fetch
	tests."""
	fn _populate(self)
	fn test_fetchmany(self)
	fn test_fetchall(self)
	fn test_mixedfetch(self)
	"""Should create a procedure called deleteme
	that returns two result sets, first the 
	number of rows in booze then "name from booze""""
	fn help_nextset_setUp(self, cur)
	"""If cleaning up is needed after nextSetTest"""
	fn help_nextset_tearDown(self, cur)
	fn test_nextset(self)
	fn test_nextset(self)
	fn test_arraysize(self)
	fn test_setinputsizes(self)
	fn test_setoutputsize_basic(self)
	fn test_setoutputsize(self)
	fn test_None(self)
	fn test_Date(self)
	fn test_Time(self)
	fn test_Timestamp(self)
	fn test_Binary(self)
	fn test_STRING(self)
	fn test_BINARY(self)
	fn test_NUMBER(self)
	fn test_DATETIME(self)
	fn test_ROWID(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import cyanodbc
import dbapi20
from distro import linux_distribution
import pytest

class CyanodbcDBApiTest(dbapi20.DatabaseAPI20Test):
    driver = cyanodbc
    connect_args = ("Driver={SQLite3 ODBC Driver};Database="
    "example.db;Timeout=1000;", )
    ""
    def test_setoutputsize(self):
        pass

    def test_nextset(self):
        pass # for sqlite no nextset()

    @pytest.mark.skipif(linux_distribution()[2]=="xenial",
            reason = "Strange behavior seen in Xenial")
    def test_rowcount(self):
        super().test_rowcount()
<|endoftext|>"
},
{
"prompt": "class UserDetailView(LoginRequiredMixin, DetailView):
	model = User
	slug_field = 'username'
	slug_url_kwarg = 'username'


class UserUpdateView(LoginRequiredMixin, UpdateView):
	model = User
	fields = ['name']
	fn get_success_url(self)
	fn get_object(self)
	fn form_valid(self, form)


class UserRedirectView(LoginRequiredMixin, RedirectView):
	permanent = False
	fn get_redirect_url(self)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from django.conf import settings
from django.test import RequestFactory

from plouffe.users.views import UserRedirectView, UserUpdateView

pytestmark = pytest.mark.django_db


class TestUserUpdateView:
    """
    TODO:
        extracting view initialization code as class-scoped fixture
        would be great if only pytest-django supported non-function-scoped
        fixture db access -- this is a work-in-progress for now:
        https://github.com/pytest-dev/pytest-django/pull/258
    """

    def test_get_success_url(
        self, user: settings.AUTH_USER_MODEL, request_factory: RequestFactory
    ):
        view = UserUpdateView()
        request = request_factory.get("/fake-url/")
        request.user = user

        view.request = request

        assert view.get_success_url() == f"/users/{user.username}/"

    def test_get_object(
        self, user: settings.AUTH_USER_MODEL, request_factory: RequestFactory
    ):
        view = UserUpdateView()
        request = request_factory.get("/fake-url/")
        request.user = user

        view.request = request

        assert view.get_object() == user


class TestUserRedirectView:
    def test_get_redirect_url(
        self, user: settings.AUTH_USER_MODEL, request_factory: RequestFactory
    ):
        view = UserRedirectView()
        request = request_factory.get("/fake-url")
        request.user = user

        view.request = request

        assert view.get_redirect_url() == f"/users/{user.username}/"
<|endoftext|>"
},
{
"prompt": """"base class for interactive templar tests"""
class BaseClass:
	UPDATE_FIXTURES = False
	TEST_FOR_MODE: Optional[str] = None
	"""Return a new tmux session.
	
	The EDITOR is set here such that vim will not create swap files."""
	fn fixture_tmux_session(request)
	"""test interactive and ``stdout`` mode ``config``"""
	fn test(self, request, tmux_session, step)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """"Tests for templar from CLI, interactive, without an EE.
"""
import pytest

from ..._interactions import Command
from ..._interactions import UiTestStep
from ..._interactions import add_indices
from ..._interactions import step_id
from .base import BaseClass
from .base import base_steps
from .base import inventory_path
from .base import playbook_path


cmdline = f"{playbook_path} -i {inventory_path}"
CLI = Command(subcommand="run", cmdline=cmdline, execution_environment=False).join()

initial_steps = (
    UiTestStep(
        user_input=CLI,
        comment="ansible-navigator run playbook",
        search_within_response=["COMPLETE", "SUCCESSFUL"],
    ),
)

steps = add_indices(initial_steps + base_steps)


@pytest.mark.parametrize("step", steps, ids=step_id)
class Test(BaseClass):
    """Run the tests for templar from CLI, interactive, without an EE."""

    UPDATE_FIXTURES = False
<|endoftext|>"
},
{
"prompt": """"A single physical button on the keyboard """
class Button:
	__slots__ = ('width', 'isMarked', 'i', 'scancode')
	_idToName: Dict[int, Text] = {}
	_nameToId: Dict[Text, int] = {}
	_nextNameId = 0
	serializedName = 'standard'
	fn __init__(self, name: Text, width: float, isMarked: bool, scancode)
	fn __repr__(self)
	fn __eq__(self, other)
	fn __hash__(self)
	fn name(self)
	fn deserialize(self, data: Dict)
	fn serialize(self)


"""A letter, number or symbol button, but not special keys like modifier, tab,
…"""
class LetterButton(Button):
	serializedName = 'letter'
	fn __init__(self, name, width, isMarked, scancode)
	fn __repr__(self)


"""A button spanning multiple rows, like the return button on european
keyboards"""
class MultiRowButton(Button):
	__slots__ = ('span',)
	serializedName = 'multi'
	fn __init__(self, name, span, width, isMarked, scancode)
	fn __repr__(self)
	fn serialize(self)


class PhysicalKeyboard:
	__slots__ = ('name', 'description', 'rows', '_buttonToRow')
	fn __init__(self, name: Text, description: Text, rows)
	fn __iter__(self)
	fn __repr__(self)
	fn __len__(self)
	"""Find button by name """
	fn __getitem__(self, name: Text) -> Button
	"""Iterate over all keys """
	fn keys(self) -> Iterator[Button]
	fn find(self, name: Text) -> Button
	fn getRow(self, btn: Button)
	fn deserialize(cls, data: Dict)
	fn serialize(self)


"""Limit the number of items drawn from iterable l to n. """
fn limit(l, n)

"""Simple YAML loader that searches the current path and the package’s
resources (for defaults)"""
class YamlLoader:
	__slots__ = ('defaultDir', 'deserialize')
	fn __init__(self, defaultDir, deserialize)
	fn __getitem__(self, k, onlyRes)
	fn __iter__(self)


"""Convert text into a string that is always renderable without combining,
control or invisible characters """
fn displayText(text)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Copyright (c) 2020 lulua contributors
# 
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
# 
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
# 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.

import pytest

from .keyboard import defaultKeyboards, Button, dataDirectory
from .util import YamlLoader

def test_defaults ():
    k = defaultKeyboards['ibmpc105']
    assert k.name == 'ibmpc105'

    with pytest.raises (KeyError):
        k = defaultKeyboards['nonexistent']

    assert len (list (defaultKeyboards)) > 0

def test_keys_unique ():
    for kbd in defaultKeyboards:
        # both, ids and names must be unique
        havei = set ()
        havename = set ()
        for btn in kbd.keys ():
            assert btn.i not in havei
            havei.add (btn.i)

            assert btn.name not in havename
            havename.add (btn.name)

def test_keyboard_getRow ():
    k = defaultKeyboards['ibmpc105']
    for btn, expect in [(k['Bl1'], 0), (k['Cr1'], 1), (k['Dr1'], 2)]:
        assert k.getRow (btn) == expect
    
def test_keyboard_getattr ():
    k = defaultKeyboards['ibmpc105']
    assert k['Dr1'] == k.find ('Dr1')
    assert k['CD_ret'] == k.find ('CD_ret')
    assert k['Cr1'] != k.find ('El1')

    with pytest.raises (KeyError):
        k['nonexistent_button']

def test_button_uniqname ():
    a = Button ('a')
    assert a.name == 'a'

    b = Button ('b')
    assert b.name == 'b'

    assert a != b

    c = Button ('a')
    assert c.name == 'a'

    assert a == c
    assert b != c

    d = dict ()
    d[a] = 1
    assert a in d
    assert b not in d
    assert c in d
    d[b] = 2
    assert b in d

    # make sure we can only compare to Buttons
    assert a != 'hello'
    assert a != 1
    assert a != dict ()

def test_serialize ():
    """ Make sure serialize (deserialize (x)) of keyboards is identity """

    rawKeyboards = YamlLoader (dataDirectory, lambda x: x)
    name = 'ibmpc105'
    assert defaultKeyboards[name].serialize () == rawKeyboards[name]

<|endoftext|>"
},
{
"prompt": "fn normal_log_prob_blobs(params)

fn normal_log_prob(params)

fn uniform_log_prob(params)

fn _test_normal(proposal, ndim, nwalkers, nsteps, seed, check_acceptance, pool, blobs)

fn _test_uniform(proposal, nwalkers, nsteps, seed)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# -*- coding: utf-8 -*-

from __future__ import division, print_function

import pytest
from emcee import moves
from .test_proposal import _test_normal, _test_uniform

__all__ = ["test_normal_stretch", "test_uniform_stretch",
           "test_nsplits_stretch"]


@pytest.mark.parametrize("blobs", [True, False])
def test_normal_stretch(blobs, **kwargs):
    kwargs["blobs"] = blobs
    _test_normal(moves.StretchMove(), **kwargs)


def test_uniform_stretch(**kwargs):
    _test_uniform(moves.StretchMove(), **kwargs)


def test_nsplits_stretch(**kwargs):
    _test_normal(moves.StretchMove(nsplits=5), **kwargs)


def test_randomize_stretch(**kwargs):
    _test_normal(moves.StretchMove(randomize_split=True), **kwargs)
<|endoftext|>"
},
{
"prompt": """"An event emitted by a service."""
class ServiceEvent:
	fn __init__(self, service: Service)


"""This event is emitted every time a new service has been initialized."""
class ServiceInitializedEvent(ServiceEvent):
	fn __init__(self, name: str, service: Service)


"""This event is emitted every time a service has been shut down."""
class ServiceShutdownEvent(ServiceEvent):
	fn __init__(self, name: str, service: Service)


"""This event is emitted from the application when it has been shut
down."""
class ApplicationShutdown(ServiceEvent):


"""The session is emitting this event when it's ready to shut down."""
class SessionCreated(ServiceEvent):
	fn __init__(self, applicaton: Service, session: Service, filename: Optional[str], template: Optional[str])


"""Event emitted when a session becomes the active session."""
class ActiveSessionChanged(ServiceEvent):


"""When the application is asked to terminate, it will inform all sessions.

The user can then save his/her work."""
class SessionShutdownRequested(ServiceEvent):


"""The session is emitting this event when it's ready to shut down."""
class SessionShutdown(ServiceEvent):


class ModelLoaded:
	fn __init__(self, service, filename)


class ModelSaved:
	fn __init__(self, service, filename)


"""This event denotes the beginning of a transaction.

Nested (sub-) transactions should not emit this signal."""
class TransactionBegin:


"""This event is emitted when a transaction (toplevel) is successfully
committed."""
class TransactionCommit:


"""If a set of operations fail (e.i.

due to an exception) the transaction should be marked for rollback.
This event is emitted to tell the operation has failed."""
class TransactionRollback:


"""Signal if an action can be activated or not."""
class ActionEnabled:
	fn __init__(self, action_name: str, enabled: bool) -> None


"""Inform the user about important events."""
class Notification:
	fn __init__(self, message)


"""The distribution metadata for Gaphor."""
fn distribution()

class NotInitializedError(Exception):


"""The Gaphor application is started from the gaphor.ui module.

This application instance is used to maintain application wide references
to services and sessions (opened models). It behaves like a singleton in many ways.

The Application is responsible for loading services and plugins. Services
are registered in the "component_registry" service."""
class Application(Service, ActionProvider):
	fn __init__(self)
	fn get_service(self, name)
	fn sessions(self)
	fn active_session(self)
	fn new_session(self)
	"""Initialize an application session."""
	fn _new_session(self, filename, template, services)
	fn has_sessions(self)
	fn has_session(self, filename)
	fn shutdown_session(self, session)
	"""Forcibly shut down all sessions. No questions asked.
	
	This is mainly for testing purposes."""
	fn shutdown(self)
	"""The user's application Quit command."""
	fn quit(self)
	fn all(self, base: type[T]) -> Iterator[tuple[(str, T)]]
	fn _transaction_proxy(self, event)


"""A user context is a set of services (including UI services) that define
a window with loaded model."""
class Session(Service):
	"""Initialize the application."""
	fn __init__(self, services)
	fn get_service(self, name)
	fn filename(self)
	fn foreground(self)
	fn shutdown(self)
	fn shutdown_service(self, name, srv)
	fn on_filename_changed(self, event)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from gaphor.application import Application
from gaphor.event import ModelLoaded, ModelSaved


@pytest.fixture
def application():
    application = Application()
    yield application
    application.shutdown()


def test_service_load(application):
    """Test loading services and querying utilities."""

    session = application.new_session()

    assert (
        session.get_service("undo_manager") is not None
    ), "Failed to load the undo manager service"

    assert (
        session.get_service("file_manager") is not None
    ), "Failed to load the file manager service"


def test_model_loaded(application):
    session = application.new_session()
    session.event_manager.handle(ModelLoaded(None, "some_file_name"))

    assert session.filename == "some_file_name"


def test_model_saved(application):
    session = application.new_session()
    session.event_manager.handle(ModelSaved(None, "some_file_name"))

    assert session.filename == "some_file_name"


def test_new_session_from_template(application, test_models):
    with (test_models / "test-model.gaphor").open() as model:
        session = application.new_session(template=model)

    assert any(session.get_service("element_factory").select())
<|endoftext|>"
},
{
"prompt": "class JSONEncoder(object):
	fn __init__(self, obj)
	fn encode(self)
	fn _get_method(self, obj)
	fn encode_dict(self, val_dict)
	fn encode_list(self, val_ls)
	fn encode_bool(self, val_b, _bm)
	fn encode_int(self, val_num)
	encode_float = encode_int
	fn encode_NoneType(self, val_none)
	fn encode_str(self, val_s)
	encode_unicode = encode_str
	encode_bytes = encode_str


fn dump(obj, file, encoder)

fn dumps(obj, encoder)

fn to_utf8(value)

fn to_unicode(value)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# -*- coding: utf-8 -*-

import pytest
from djson.encoder import JSONEncoder
from djson.utils import to_utf8


@pytest.fixture(scope='module')
def encoder():
    return JSONEncoder(None)


def test_encode_none(encoder):
    assert encoder.encode_NoneType(None) == 'null'


def test_encode_number(encoder):
    ni = 123325
    nf = 123.456
    assert encoder.encode_int(ni) == '{0}'.format(ni)
    assert encoder.encode_float(nf) == '{0}'.format(nf)


def test_encode_bool(encoder):
    assert encoder.encode_bool(True) == 'true'
    assert encoder.encode_bool(False) == 'false'


def test_encode_str(encoder):
    s = "12334324324"
    assert encoder.encode_str(s) == '"{0}"'.format(s)


def test_encode_list(encoder):
    lobj = [1, 2.4, "qwer", '34', -456, True, None, False]
    assert (sorted(encoder.encode_list(lobj)) ==
            sorted('[1, 2.4, "qwer", "34", -456, true, null, false]'))


def test_encode_dict(encoder):
    dobj = {"1": 2, 3: "4", True: "hhe", "ww": None, False: [1, -2]}
    assert (sorted(encoder.encode_dict(dobj)) ==
            sorted('{"1": 2, 3: "4", true: "hhe", "ww": null, false: [1, -2]}'))


class Custom(object):
    def hahaha(self):
        return 'hahahaha'


class MyEncoder(JSONEncoder):
    def __init__(self, *args, **kwargs):
        super(self.__class__, self).__init__(*args, **kwargs)

    def encode_Custom(self, obj):
        return '"{0}"'.format(obj.hahaha())


def test_encode_custom_obj():
    dobj = {"1": Custom(), 3: "4", True: "hhe", "ww": None, False: [1, -2]}
    encoder = MyEncoder(dobj)
    assert (sorted(encoder.encode()) ==
                  sorted(to_utf8('{"1": "hahahaha", true: "hhe", 3: "4",'
                                 ' "ww": null, false: [1, -2]}')))
<|endoftext|>"
},
{
"prompt": "fn agent_disabled_checks() -> Generator[(List[str], None, None)]

fn log_span_fmt() -> Generator[(str, None, None)]

fn snapshot_dir(tmp_path: Path) -> Generator[(Path, None, None)]

fn snapshot_ci_mode() -> Generator[(bool, None, None)]

fn snapshot_ignored_attrs() -> Generator[(Set[str], None, None)]

fn agent_url() -> Generator[(str, None, None)]

fn v04_reference_http_trace_payload_data_raw()

fn v04_reference_http_trace_payload_data(v04_reference_http_trace_payload_data_raw)

fn v04_reference_http_trace_payload_headers() -> Generator[(Dict[(str, str)], None, None)]

fn v04_trace(agent, traces: List[Trace], encoding: Literal[(msgpack, json)], token: Optional[str])

fn do_reference_v04_http_trace(agent, v04_reference_http_trace_payload_headers, v04_reference_http_trace_payload_data)

fn v06_reference_http_stats_payload_headers()

fn v06_reference_http_stats_payload_data_raw()

fn v06_reference_http_stats_payload_data(v06_reference_http_stats_payload_data_raw)

fn do_reference_v06_http_stats(agent, v06_reference_http_stats_payload_headers, v06_reference_http_stats_payload_data)

fn span(rnd: Random) -> Span

"""Return a randomly generated trace tree with `n` spans.

https://en.wikipedia.org/wiki/Pr%C3%BCfer_sequence"""
fn _prufers_trace(n: int, rnd: Random) -> Trace

fn random_trace(nspans: int, rng: Random) -> Trace

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from .conftest import v04_trace
from .trace_utils import span


async def test_reference(
    agent,
    v04_reference_http_trace_payload_headers,
    v04_reference_http_trace_payload_data,
):
    resp = await agent.put(
        "/v0.4/traces",
        headers=v04_reference_http_trace_payload_headers,
        data=v04_reference_http_trace_payload_data,
    )
    assert resp.status == 200, await resp.text()


@pytest.mark.parametrize("agent_disabled_checks", [[], ["trace_count_header"]])
async def test_trace_count_header(
    agent,
    v04_reference_http_trace_payload_headers,
    v04_reference_http_trace_payload_data,
    agent_disabled_checks,
):
    del v04_reference_http_trace_payload_headers["X-Datadog-Trace-Count"]
    resp = await agent.put(
        "/v0.4/traces",
        headers=v04_reference_http_trace_payload_headers,
        data=v04_reference_http_trace_payload_data,
    )
    if "trace_count_header" in agent_disabled_checks:
        assert resp.status == 200, await resp.text()
    else:
        assert resp.status == 400, await resp.text()
        assert "Check 'trace_count_header' failed" in await resp.text()


async def test_trace_count_header_mismatch(
    agent,
    v04_reference_http_trace_payload_headers,
    v04_reference_http_trace_payload_data,
):
    v04_reference_http_trace_payload_headers["X-Datadog-Trace-Count"] += "1"
    resp = await agent.put(
        "/v0.4/traces",
        headers=v04_reference_http_trace_payload_headers,
        data=v04_reference_http_trace_payload_data,
    )
    assert resp.status == 400, await resp.text()
    assert "Check 'trace_count_header' failed" in await resp.text()


@pytest.mark.parametrize("agent_disabled_checks", [[], ["meta_tracer_version_header"]])
async def test_meta_tracer_version_header(
    agent,
    v04_reference_http_trace_payload_headers,
    v04_reference_http_trace_payload_data,
    agent_disabled_checks,
):
    del v04_reference_http_trace_payload_headers["Datadog-Meta-Tracer-Version"]
    resp = await agent.put(
        "/v0.4/traces",
        headers=v04_reference_http_trace_payload_headers,
        data=v04_reference_http_trace_payload_data,
    )
    if "meta_tracer_version_header" in agent_disabled_checks:
        assert resp.status == 200, await resp.text()
    else:
        assert resp.status == 400, await resp.text()
        assert "Check 'meta_tracer_version_header' failed" in await resp.text()


async def test_trace_content_length(agent):
    # Assume a trace will be at least 100 bytes each
    s = span()
    trace = [s for _ in range(int(5e7 / 100))]
    resp = await v04_trace(agent, [trace], "msgpack")
    assert resp.status == 400, await resp.text()
    assert "Check 'trace_content_length' failed: content length" in await resp.text()
<|endoftext|>"
},
{
"prompt": "fn run_validator_for_test_file(filename: str, max_expression_complexity: int, ignore_django_orm_queries: bool)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import sys

import pytest
from conftest import run_validator_for_test_file


def test_fails():
    errors = run_validator_for_test_file('long_expressions.py', max_expression_complexity=3)
    assert len(errors) == 5


@pytest.mark.skipif(sys.version_info < (3, 8), reason='runs only for python 3.8+')
def test_walrus():
    errors = run_validator_for_test_file('walrus.py', max_expression_complexity=1)
    assert len(errors) == 1
<|endoftext|>"
},
{
"prompt": "fn corp_list()

fn test_find_by_corp_name(corp_list)

fn test_find_by_corp_code(corp_list)

fn test_find_by_stock_code(corp_list)

fn test_corp_to_dict(corp_list)

fn test_corp_load(corp_list)

fn test_corp_get_major_shareholder(corp_list)

fn test_corp_get_executive_shareholder(corp_list)

fn test_corp_search_filings(corp_list)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from dart_fss.fs.extract import find_all_columns

from .test_case.crp_case import test_crp_list
from .test_corp import corp_list

@pytest.mark.parametrize("corp", test_crp_list)
def test_crp_financial_statement(corp):
    corp.run_test()


@pytest.fixture(scope='session')
def fs_report(corp_list):
    # 00164779: SK하이닉스
    skhynix = corp_list.find_by_corp_code('00164779')
    return skhynix.extract_fs(bgn_de='20180101')


def test_fs_class_false(fs_report):
    df = fs_report.show('bs', show_class=False)
    columns = find_all_columns(df, 'class')
    actual = len(columns)
    expected = 0
    assert actual == expected


def test_fs_concept_false(fs_report):
    df = fs_report.show('bs', show_concept=False)
    columns = find_all_columns(df, 'concept')
    actual = len(columns)
    expected = 0
    assert actual == expected


def test_fs_show_depth(fs_report):
    df = fs_report.show('bs', show_depth=1)
    columns = find_all_columns(df, 'class')
    actual = len(columns)
    expected = 2
    assert actual == expected


def test_fs_to_dict(fs_report):
    info = fs_report.to_dict()
    actual = info['corp_code']
    expected = '00164779'
    assert actual == expected


def test_fs_to_save(fs_report):
    import os
    import tempfile
    with tempfile.TemporaryDirectory() as path:
        file_path = fs_report.save(path=path)
        actual = os.path.isfile(file_path)
    expected = True
    assert actual == expected
<|endoftext|>"
},
{
"prompt": """"Generates a nice random name from the dictionaries in words

:param max_length: max length for the name.
:type max_length: int, optional
:param entropy: how unique th name will be, currently entropy - 1 adjectives are joined with one noun.
:type entropy: int, optional
:param sep: seperator between name parts.
:type sep: str, optional

:return: the generated name
:rtype: str

:raises ValueError: if ``param2`` is equal to ``param1``."""
fn random_nice_name(max_length: int, optional, entropy: int, optional, sep: str, optional) -> str

fn random_string(length, charset)

fn random_filename(length)

fn random_buf(size)

fn perchance(probabilty)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
import random

from easypy.random import random_nice_name, random_filename


def test_random_nice_name():
    for _ in range(20):
        length = random.randint(64, 85)
        entropy = random.randint(1, 3)
        sep = random.choice(['_', '..'])
        name = random_nice_name(max_length=length, entropy=entropy, sep=sep)
        assert len(name) <= length


def test_random_nice_name_raises():
    with pytest.raises(ValueError):
        random_nice_name(max_length=10, entropy=3)


def test_random_filename():
    fn = random_filename(10)
    assert len(fn) == 10

    fn = random_filename((10, 11))
    assert 10 <= len(fn) <= 11

    fn = random_filename((11, 11))
    assert len(fn) == 11
<|endoftext|>"
},
{
"prompt": "

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """"Test irrigation_unlimited timing operations."""
import pytest
import glob
import homeassistant.core as ha
from homeassistant.config import (
    load_yaml_config_file,
    async_process_ha_core_config,
)
from tests.iu_test_support import (
    begin_test,
    finish_test,
    quiet_mode,
    test_config_dir,
    check_summary,
)
from custom_components.irrigation_unlimited.irrigation_unlimited import (
    IUCoordinator,
)
from custom_components.irrigation_unlimited.const import DOMAIN
from custom_components.irrigation_unlimited.__init__ import CONFIG_SCHEMA

quiet_mode()


async def test_timings(hass: ha.HomeAssistant, skip_setup):
    """Test timings. Process all the configuration files in the
    test_config directory matching timing_*.yaml and check the results."""

    for fname in glob.glob(test_config_dir + "timing_*.yaml"):
        print(f"Processing: {fname}")
        config = CONFIG_SCHEMA(load_yaml_config_file(fname))
        if ha.DOMAIN in config:
            await async_process_ha_core_config(hass, config[ha.DOMAIN])
        coordinator = IUCoordinator(hass).load(config[DOMAIN])

        for t in range(coordinator.tester.total_tests):
            next_time = await begin_test(t + 1, coordinator)
            await finish_test(hass, coordinator, next_time, True)

        check_summary(fname, coordinator)
<|endoftext|>"
},
{
"prompt": "class CallbackCounter(object):
	fn __init__(self, return_value)
	fn __call__(self)


fn small_buffer(ds, size)

fn webserver()

fn df_server(ds_trimmed)

fn df_server_huge()

"""Don't close event loop at the end of every function decorated by
@pytest.mark.asyncio"""
fn event_loop()

fn tornado_client(webserver, df_server, df_server_huge, event_loop)

fn dummy_client(df_server, df_server_huge)

fn client(request, dummy_client, tornado_client)

fn ds_remote(client)

fn df_remote(ds_remote)

fn ds_filtered(df_filtered)

fn df_filtered_cached()

fn df_filtered(df_filtered_cached)

fn ds_half_cache()

fn ds_half(ds_half_cache)

fn ds_trimmed_cache()

fn ds_trimmed(ds_trimmed_cache)

fn df_trimmed(ds_trimmed)

fn df_concat_cache(ds_trimmed_cache)

fn df_concat(df_concat_cache)

fn df_executor(request, df_trimmed, df_remote)

fn ds(request, ds_filtered, ds_half, ds_trimmed, ds_remote, df_concat, df_arrow)

fn df(ds)

fn ds_local(request, ds_filtered, ds_half, ds_trimmed, df_concat, df_arrow)

fn df_local_non_arrow(request, ds_filtered, ds_half, ds_trimmed, df_concat)

fn df_local(ds_local)

fn df_arrow(df_arrow_cache)

fn df_arrow_cache(df_arrow_raw)

fn df_arrow_raw(df_filtered)

fn ds_no_filter(request)

fn create_filtered()

fn create_base_ds()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from common import *
import collections
import numpy as np
import vaex
import pytest

def test_cat_string():
    ds0 = vaex.from_arrays(colors=['red', 'green', 'blue', 'green'])
    ds = ds0.ordinal_encode('colors')#, ['red', 'green'], inplace=True)
    assert ds.is_category('colors')
    assert ds.limits('colors', shape=128) == ([-0.5, 2.5], 3)

    ds = ds0.ordinal_encode('colors', values=['red', 'green'])
    assert ds.is_category('colors')
    assert ds.limits('colors', shape=128) == ([-0.5, 1.5], 2)
    assert ds.data.colors.tolist() == [0, 1, None, 1]

    assert ds.copy().is_category(ds.colors)

    # with pytest.raises(ValueError):
    # 	assert ds.is_category('colors', values=['red', 'orange'])

def test_count_cat():
    ds0 = vaex.from_arrays(colors=['red', 'green', 'blue', 'green'], counts=[1, 2, 3, 4])
    ds0 = vaex.from_arrays(colors=['red', 'green', 'blue', 'green'], names=['apple', 'apple', 'berry', 'apple'])
    ds = ds0.ordinal_encode(ds0.colors)
    ds = ds0.ordinal_encode(ds0.names)

    ds = ds0.ordinal_encode('colors', ['red', 'green', 'blue'])
    assert ds.count(binby=ds.colors).tolist() == [1, 2, 1]
    ds = ds0.ordinal_encode('colors', ['red', 'blue', 'green', ], inplace=True)
    assert ds.count(binby=ds.colors).tolist() == [1, 1, 2]


def test_categorize():
    ds0 = vaex.from_arrays(c=[0, 1, 1, 3])
    ds0.categorize('c', labels=['a', 'b', 'c', 'd'], inplace=True)
    assert ds0.is_category(ds0.c)
    assert ds0.category_labels(ds0.c) == ['a', 'b', 'c', 'd']
    assert ds0.category_count(ds0.c) == 4

def test_cat_missing_values():
    colors = ['red', 'green', 'blue', 'green', 'MISSING']
    mask   = [False, False,   False,   False,  True]
    colors = np.ma.array(colors, mask=mask)
    ds0 = vaex.from_arrays(colors=colors)
    ds = ds0.ordinal_encode('colors', ['red', 'green', 'blue'])
    assert ds.count(binby=ds.colors, edges=True).tolist() == [1, 0, 1, 2, 1, 0]

    # if we want missing values and non-categorized values to be reported seperately
    # the following is expected
    # ds = ds0.ordinal_encode('colors', ['red', 'green'])
    # assert ds.count(binby=ds.colors, edges=True).tolist() == [1, 0, 1, 2, 0, 1]


def test_categorize_integers():
    df = vaex.from_arrays(x=range(5, 15))
    df.categorize('x', min_value=5, labels=range(5, 15), inplace=True)
    assert df.count(binby='x').tolist() == [1] * 10
    assert df.binby('x', 'count').data.tolist() == [1] * 10

    df = vaex.from_arrays(x=range(5, 15))
    df.categorize('x', inplace=True)  # same, but calculated from data
    assert df.count(binby='x').tolist() == [1] * 10
    assert df.binby('x', 'count').data.tolist() == [1] * 10
<|endoftext|>"
},
{
"prompt": "fn dd_environment(instance_e2e, mock_local_tls_dns)

fn mock_local_tls_dns()

fn certs(dd_environment)

fn instance_local_no_server_hostname()

fn instance_local_not_found()

fn instance_local_ok(certs)

fn instance_local_ok_der(certs)

fn instance_local_hostname(certs)

fn instance_local_hostname_mismatch(certs)

fn instance_local_cert_bad()

fn instance_local_cert_expired(certs)

fn instance_local_cert_critical_days(certs)

fn instance_local_cert_critical_seconds(certs)

fn instance_local_cert_warning_days(certs)

fn instance_local_cert_warning_seconds(certs)

fn instance_remote_no_server()

fn instance_remote_ok()

fn instance_e2e()

fn instance_remote_ok_ip()

fn instance_remote_ok_udp()

fn instance_remote_no_resolve()

fn instance_remote_no_connect()

fn instance_remote_no_connect_port_in_host()

fn instance_remote_version_default_1_1()

fn instance_remote_version_default_1_2()

fn instance_remote_version_default_1_3()

fn instance_remote_hostname_mismatch()

fn instance_remote_self_signed_ok()

fn instance_remote_cert_expired()

fn instance_remote_fetch_intermediate_certs()

fn instance_remote_cert_critical_days()

fn instance_remote_cert_critical_seconds()

fn instance_remote_cert_warning_days()

fn instance_remote_cert_warning_seconds()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# (C) Datadog, Inc. 2019-present
# All rights reserved
# Licensed under a 3-clause BSD style license (see LICENSE)
import pytest

from datadog_checks.tls import TLSCheck

from .conftest import CA_CERT, PRIVATE_KEY


def test_tags_local():
    instance = {'name': 'foo', 'local_cert_path': 'cert.pem'}
    c = TLSCheck('tls', {}, [instance])

    assert c._tags == ['name:foo']


def test_tags_local_hostname():
    instance = {'name': 'foo', 'local_cert_path': 'cert.pem', 'server_hostname': 'www.google.com'}
    c = TLSCheck('tls', {}, [instance])

    assert c._tags == ['name:foo', 'server_hostname:www.google.com']


def test_tags_local_hostname_no_validation():
    instance = {
        'name': 'foo',
        'local_cert_path': 'cert.pem',
        'server_hostname': 'www.google.com',
        'tls_validate_hostname': False,
    }
    c = TLSCheck('tls', {}, [instance])

    assert c._tags == ['name:foo']


# This is a special edge case where both supported `validate_hostname` configs are used with differing values.
# This case should realistically never happen, but theoretically can happen
# If either `tls_validate_hostname` or `validate_hostname` is false, then `tls_validate_hostname` is False
def test_tags_local_hostname_no_validation_legacy_edge_case():
    instance = {
        'name': 'foo',
        'local_cert_path': 'cert.pem',
        'server_hostname': 'www.google.com',
        'validate_hostname': False,
        'tls_validate_hostname': True,
    }
    c = TLSCheck('tls', {}, [instance])

    assert c._tags == ['name:foo']
    assert c._tls_validate_hostname is False


def test_tags_remote():
    instance = {'name': 'foo', 'server': 'https://www.google.com'}
    c = TLSCheck('tls', {}, [instance])

    assert c._tags == ['name:foo', 'server_hostname:www.google.com', 'server:www.google.com', 'port:443']


def test_validation_data():
    c = TLSCheck('tls', {}, [{}])

    assert c._validation_data is None
    assert c.validation_data == c._validation_data
    assert isinstance(c.validation_data, tuple)


@pytest.mark.parametrize(
    'extra_config, expected_http_kwargs',
    [
        pytest.param({'ca_cert': CA_CERT}, {'tls_ca_cert': CA_CERT}, id='legacy ca_cert param'),
        pytest.param({'private_key': PRIVATE_KEY}, {'tls_private_key': PRIVATE_KEY}, id='legacy private_key param'),
        pytest.param(
            {'validate_hostname': False}, {'tls_validate_hostname': False}, id='legacy validate_hostname param'
        ),
        pytest.param({'validate_cert': False}, {'tls_verify': False}, id='legacy validate_cert param'),
    ],
)
def test_config(extra_config, expected_http_kwargs):
    instance = {
        'name': 'foo',
    }
    instance.update(extra_config)
    c = TLSCheck('tls', {}, [instance])
    c.get_tls_context()  # need to call this for config values to be saved by _tls_context_wrapper
    actual_options = {k: v for k, v in c._tls_context_wrapper.config.items() if k in expected_http_kwargs}
    assert expected_http_kwargs == actual_options
<|endoftext|>"
},
{
"prompt": """"Raised when an unexpected error happens in the watch-stream API."""
class WatchingError(Exception):


"""Special marks sent in the stream among raw events. """
class Bookmark(enum.Enum):
	LISTED = enum.auto()


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import asyncio
import json
import logging

import aiohttp.web
import pytest

from kopf.clients.watching import Bookmark, continuous_watch


async def test_listed_is_inbetween(
        settings, resource, namespace, hostname, aresponses):

    # Resource version is used as a continutation for the watch-queries.
    list_data = {'metadata': {'resourceVersion': '123'}, 'items': [
        {'spec': 'a'},
        {'spec': 'b'},
    ]}
    list_resp = aiohttp.web.json_response(list_data)
    list_url = resource.get_url(namespace=namespace)

    # The same as in the `stream` fixture. But here, we also mock lists.
    stream_data = [
        {'type': 'ADDED', 'object': {'spec': 'c'}},  # stream.feed()
        {'type': 'ADDED', 'object': {'spec': 'd'}},  # stream.feed()
        {'type': 'ERROR', 'object': {'code': 410}},  # stream.close()
    ]
    stream_text = '\n'.join(json.dumps(event) for event in stream_data)
    stream_resp = aresponses.Response(text=stream_text)
    stream_query = {'watch': 'true', 'resourceVersion': '123'}
    stream_url = resource.get_url(namespace=namespace, params=stream_query)

    aresponses.add(hostname, list_url, 'get', list_resp, match_querystring=True)
    aresponses.add(hostname, stream_url, 'get', stream_resp, match_querystring=True)

    events = []
    async for event in continuous_watch(settings=settings,
                                        resource=resource,
                                        namespace=namespace,
                                        operator_pause_waiter=asyncio.Future()):
        events.append(event)

    assert len(events) == 5
    assert events[0]['object']['spec'] == 'a'
    assert events[1]['object']['spec'] == 'b'
    assert events[2] == Bookmark.LISTED
    assert events[3]['object']['spec'] == 'c'
    assert events[4]['object']['spec'] == 'd'
<|endoftext|>"
},
{
"prompt": "fn validate_leaner(data_generator, leaner, fit_kwargs, estimate_kwargs, check_fitted, check_effect, check_effect_nji)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from ylearn.estimator_model.causal_tree import CausalTree
from . import _dgp
from ._common import validate_leaner

_test_settings = {
    # data_generator: options
    _dgp.generate_data_x1b_y1_w0v5: dict(min_samples_leaf=3, max_depth=5),
    _dgp.generate_data_x2b_y1_w0v5: dict(min_samples_leaf=3,min_samples_split=3, max_depth=5),
    _dgp.generate_data_x1m_y1_w0v5: dict(min_samples_leaf=3, max_depth=5),
}


@pytest.mark.parametrize('dg', _test_settings.keys())
def test_causal_tree(dg):
    options = _test_settings[dg]
    dr = CausalTree(**options)
    validate_leaner(dg, dr, check_effect=True)
<|endoftext|>"
},
{
"prompt": "class Termination(SystemExit):


class TimeoutExit(BaseException):


fn _terminate(signum, frame)

fn isUp(server_addr)

fn _timeout(signum, frame)

fn pytest_sessionstart(session)

fn pytest_sessionfinish(session, exitstatus)

fn pytest_addoption(parser)

"""Get the address of the Neptune server

Args:
    config (Config): Pytest config object containing the options

Returns:
    str: Address of the Neptune server"""
fn get_server_addr(config: Config) -> str

"""Get the address of the Neptune's websocket server

Args:
    config (Config): Pytest config object containing the options

Returns:
    str: Address of Neptune's websocket server"""
fn get_ws_addr(config: Config) -> str

fn pytest_configure(config)

fn pytest_runtest_protocol(item, nextitem)

fn pytest_collection_modifyitems(config, items)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Copyright 2019 Xilinx Inc.
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import logging
import pytest
import requests
import threading
import time

from neptune.neptune_api import NeptuneService, NeptuneServiceArgs
from neptune.tests.conftest import get_server_addr

@pytest.mark.timeout(15)
def test_ping(request):
    """
    Gets n pings from Neptune using the ping service

    Args:
        request (fixture): get the cmdline options
    """
    server_addr = get_server_addr(request.config)
    service = NeptuneService(server_addr, 'ping')

    service.start()

    r = requests.get('%s/serve/ping' % server_addr)
    assert r.status_code == 200

    response = r.json()
    # verify both the id and pong keys are present in the response
    assert type(response) is dict
    assert 'pong' in response
    assert 'id' in response

    float(response['pong']) # check if pong is a float

    service.stop()

@pytest.fixture(params=[
    0, # don't spawn processes
    1,
    8
])
def args_ping(request):
    if request.param is None:
        return None
    else:
        args = NeptuneServiceArgs()
        args.insert_add_arg('ping', 'processes', request.param)
        return args

def _run_test_ping_benchmark(server_addr, iterations):
    for i in range(iterations):
        r = requests.get('%s/serve/ping' % server_addr)
        assert r.status_code == 200

        response = r.json()

        # verify both the id and pong keys are present in the response
        assert type(response) is dict
        assert 'pong' in response
        assert 'id' in response

        float(response['pong']) # check if pong is a float

        # optional delay between pings
        # time.sleep(0.1)

@pytest.mark.timeout(30)
@pytest.mark.benchmark
def test_ping_benchmark(request, args_ping):
    """
    Benchmarks ping times using args_ping to configure the ping service

    Args:
        request (fixture): get the cmdline options
        args_ping (NeptuneServiceArgs): a Neptune service args object
    """
    server_addr = get_server_addr(request.config)
    service = NeptuneService(server_addr, 'ping')
    ITERATION_COUNT = 1
    THREAD_COUNT = 8

    service.start(args_ping)
    processes_count = args_ping.get_add_arg('ping', 'processes')
    threads = []

    for i in range(THREAD_COUNT):
        thread = threading.Thread(
            target=_run_test_ping_benchmark,
            args=(server_addr, ITERATION_COUNT)
        )
        threads.append(thread)

    start_time = time.time()
    for thread in threads:
        thread.start()

    for thread in threads:
        thread.join()

    end_time = time.time()

    logging.info("ping_benchmark: %d pings took %fs with %d processes" % \
        (ITERATION_COUNT*THREAD_COUNT, (end_time - start_time), processes_count))

    service.stop()
<|endoftext|>"
},
{
"prompt": "class Path(object):
	GLOB_CHARACTER = '*'
	"""Args:
	    path (Union(str, None)): A path to perform matches against"""
	fn __init__(self, path)
	"""Args:
	    filename (Union(str, Path)): A path in the repository that we're matching against
	
	Returns:
	    Path: A path object that correlates to the matched file.
	      If there isn't a match returns a :class:`Path` with the
	      value ``None``."""
	fn match(self, filename: Union(str, Path)) -> Path
	fn _match_glob(self, filename)
	fn __eq__(self, other)
	fn __hash__(self)
	fn __str__(self)
	fn __repr__(self)
	fn __bool__(self)
	fn __nonzero__(self)
	fn startswith(self, needle)
	fn __add__(self, other)
	fn __radd__(self, other)
	fn __lt__(self, other)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from __future__ import unicode_literals

import pytest

from radish.path import Path


class TestPath(object):
    def test_casts_to_boolean_when_a_path(self):
        assert Path('/')
        assert not Path(None)

    def test_boolean_for_both_py2_and_py3(self):
        p = Path('/')
        assert p.__bool__() and p.__nonzero__()

    def test_return_false_when_file_doesnt_match(self):
        path = Path('extensions/cool-extension')

        assert not path.match('src/a.py')

    def test_matches_simple_file(self):
        path = Path('extensions/cool-extension/')

        assert path.match('extensions/cool-extension/src/a.py')

    def test_matches_against_glob_expressions(self):
        path = Path('extensions/*/')

        assert path.match('extensions/cool-extension/src/a.py')

    def test_match_against_path_object(self):
        path = Path('extensions/*/')

        assert path.match(Path('extensions/cool-extension/'))

    def test_doesnt_match_against_glob_when_its_a_file(self):
        path = Path('extensions/*/')

        assert not path.match('extensions/cool-extension')

    def test_the_same_path_is_equal(self):
        assert Path('extension/') == Path('extension/')

    def test_path_is_same_as_path_which_would_match_glob(self):
        assert Path('extension/cool-extension/') == Path('extension/*/')

    def test_path_can_compare_to_a_string(self):
        assert Path('extension/cool-extension/') == 'extension/cool-extension/'

    def test_expanded_path(self):
        path = Path('extension/*/')

        assert path.match('extension/cool-extension/') == 'extension/cool-extension/'

    def test_assert_none_paths_are_equal(self):
        assert Path(None) == Path(None)

    def test__str__is_the_path_passed_in(self):
        assert str(Path('hello/')) == 'hello/'

    def test__repr__has_a_marker_for_none_when_its_none(self):
        assert repr(Path(None)) == '<Path: <None>>'

    def test_responds_to_startswith(self):
        assert Path('/root').startswith('/')
        assert not Path('root').startswith('/')

    def test_path_can_be_concatenated_with_a_string(self):
        assert (Path('/') + 'root') == '/root'
        assert ('/' + Path('root')) == '/root'

    def test_path_can_be_concatenated_with_another_path(self):
        assert (Path('/') + Path('root')) == '/root'

    class TestSorting(object):
        def test_sorts_with_other_paths(self):
            assert sorted([Path('/b'), Path('/a')]) == [Path('/a'), Path('/b')]

        def test_sorts_against_strings(self):
            assert sorted(['/b', Path('/a')]) == [Path('/a'), '/b']

        def test_raises_not_implemented_when_sorting_for_others(self):
            with pytest.raises(NotImplementedError):
                sorted([1, Path('/a')])
<|endoftext|>"
},
{
"prompt": "class ContactFactory(factory.django.DjangoModelFactory):
	hourly_rate = Decimal('20.00')
	user = factory.SubFactory(UserFactory)
	fn address_1(n)
	fn company_name(n)
	fn slug(n)


class UserContactFactory(factory.django.DjangoModelFactory):
	contact = factory.SubFactory(ContactFactory)
	user = factory.SubFactory(UserFactory)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# -*- encoding: utf-8 -*-
import pytest

from search.tests.helper import check_search_methods
from .factories import ContactFactory


@pytest.mark.django_db
def test_search_methods():
    contact = ContactFactory()
    check_search_methods(contact)


@pytest.mark.django_db
def test_str():
    contact = ContactFactory()
    str(contact)
<|endoftext|>"
},
{
"prompt": "fn helper(vm, opstr)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from .utils import helper

from hera.data import DEFAULT_DATA_START
from hera.vm import VirtualMachine


@pytest.fixture
def vm():
    return VirtualMachine()


def test_INTEGER_fills_in_memory(vm):
    helper(vm, "INTEGER(42)")

    assert vm.memory[DEFAULT_DATA_START] == 42


def test_INTEGER_increments_data_counter(vm):
    helper(vm, "INTEGER(42)")

    assert vm.dc == DEFAULT_DATA_START + 1


def test_INTEGER_does_not_increment_pc(vm):
    helper(vm, "INTEGER(42)")

    assert vm.pc == 0


def test_DSKIP_increments_data_counter(vm):
    helper(vm, "DSKIP(10)")

    assert vm.dc == DEFAULT_DATA_START + 10


def test_DSKIP_does_not_increment_pc(vm):
    helper(vm, "DSKIP(10)")

    assert vm.pc == 0


def test_LP_STRING_fills_in_memory(vm):
    helper(vm, 'LP_STRING("hello")')

    assert vm.memory[DEFAULT_DATA_START] == 5
    assert vm.memory[DEFAULT_DATA_START + 1] == ord("h")
    assert vm.memory[DEFAULT_DATA_START + 2] == ord("e")
    assert vm.memory[DEFAULT_DATA_START + 3] == ord("l")
    assert vm.memory[DEFAULT_DATA_START + 4] == ord("l")
    assert vm.memory[DEFAULT_DATA_START + 5] == ord("o")


def test_LP_STRING_increments_data_counter(vm):
    helper(vm, 'LP_STRING("hello")')

    assert vm.dc == DEFAULT_DATA_START + 6


def test_LP_STRING_does_not_increment_pc(vm):
    helper(vm, 'LP_STRING("hello")')

    assert vm.pc == 0
<|endoftext|>"
},
{
"prompt": "fn matches(actual, expected)

fn assert_meta(envelope)

fn assert_stacktrace(envelope, inside_exception, check_size)

fn assert_breadcrumb(envelope)

fn assert_attachment(envelope)

fn assert_minidump(envelope)

fn assert_timestamp(ts, now)

fn assert_event(envelope)

fn assert_crash(envelope)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
import subprocess
import sys
import os
from . import cmake, check_output, run, Envelope
from .conditions import has_inproc, has_breakpad, is_android
from .assertions import (
    assert_attachment,
    assert_meta,
    assert_breadcrumb,
    assert_stacktrace,
    assert_event,
    assert_crash,
    assert_minidump,
    assert_timestamp,
)


def test_capture_stdout(tmp_path):
    # backend does not matter, but we want to keep compile times down
    cmake(
        tmp_path,
        ["sentry_example"],
        {
            "SENTRY_BACKEND": "none",
            "SENTRY_TRANSPORT": "none",
            "BUILD_SHARED_LIBS": "OFF",
        },
    )

    # on linux we can use `ldd` to check that we don’t link to `libsentry.so`
    if sys.platform == "linux":
        output = subprocess.check_output("ldd sentry_example", cwd=tmp_path, shell=True)
        assert b"libsentry.so" not in output

    # on windows, we use `sigcheck` to check that the exe is compiled correctly
    if sys.platform == "win32":
        output = subprocess.run(
            "sigcheck sentry_example.exe",
            cwd=tmp_path,
            shell=True,
            stdout=subprocess.PIPE,
        ).stdout
        assert (b"32-bit" if os.environ.get("TEST_X86") else b"64-bit") in output
    # similarly, we use `file` on linux
    if sys.platform == "linux":
        output = subprocess.check_output(
            "file sentry_example", cwd=tmp_path, shell=True
        )
        assert (
            b"ELF 32-bit" if os.environ.get("TEST_X86") else b"ELF 64-bit"
        ) in output

    output = check_output(
        tmp_path,
        "sentry_example",
        ["stdout", "attachment", "capture-event", "add-stacktrace"],
    )
    envelope = Envelope.deserialize(output)

    assert_meta(envelope)
    assert_breadcrumb(envelope)
    assert_attachment(envelope)
    assert_stacktrace(envelope)

    assert_event(envelope)


@pytest.mark.skipif(not has_inproc, reason="test needs inproc backend")
def test_inproc_crash_stdout(tmp_path):
    cmake(
        tmp_path,
        ["sentry_example"],
        {"SENTRY_BACKEND": "inproc", "SENTRY_TRANSPORT": "none"},
    )

    child = run(tmp_path, "sentry_example", ["attachment", "crash"])
    assert child.returncode  # well, its a crash after all

    output = check_output(tmp_path, "sentry_example", ["stdout", "no-setup"])
    envelope = Envelope.deserialize(output)

    # The crash file should survive a `sentry_init` and should still be there
    # even after restarts. On Android, we can’t look inside the emulator.
    if not is_android:
        with open("{}/.sentry-native/last_crash".format(tmp_path)) as f:
            crash_timestamp = f.read()
        assert_timestamp(crash_timestamp)

    assert_meta(envelope)
    assert_breadcrumb(envelope)
    assert_attachment(envelope)

    assert_crash(envelope)


@pytest.mark.skipif(not has_breakpad, reason="test needs breakpad backend")
def test_breakpad_crash_stdout(tmp_path):
    cmake(
        tmp_path,
        ["sentry_example"],
        {"SENTRY_BACKEND": "breakpad", "SENTRY_TRANSPORT": "none"},
    )

    child = run(tmp_path, "sentry_example", ["attachment", "crash"])
    assert child.returncode  # well, its a crash after all

    with open("{}/.sentry-native/last_crash".format(tmp_path)) as f:
        crash_timestamp = f.read()
    assert_timestamp(crash_timestamp)

    output = check_output(tmp_path, "sentry_example", ["stdout", "no-setup"])
    envelope = Envelope.deserialize(output)

    assert_meta(envelope)
    assert_breadcrumb(envelope)
    assert_attachment(envelope)

    assert_minidump(envelope)
<|endoftext|>"
},
{
"prompt": """"Parse the provided Python source string and return an (typed AST, source) tuple."""
fn parse_source(src: str) -> t.Tuple[(ast.Module, t.List[str])]

"""Helper for generating linting errors from the provided source code."""
fn check_source(src: str, suppress_none_returns: bool, suppress_dummy_args: bool, allow_untyped_defs: bool, allow_untyped_nested: bool, mypy_init_return: bool, allow_star_arg_any: bool, dispatch_decorators: t.AbstractSet[str], overload_decorators: t.AbstractSet[str]) -> t.Generator[(FORMATTED_ERROR, None, None)]

"""Helper for obtaining a list of Function objects from the provided source code."""
fn functions_from_source(src: str) -> t.List[Function]

"""Iterate over a list of Function objects & find the first matching named function.

Due to the definition of the test cases, this should always return something, but there is no
protection if a match isn't found & will raise an `IndexError`."""
fn find_matching_function(func_list: t.Iterable[Function], match_name: str) -> Function

"""Check whether the input sequence is empty."""
fn check_is_empty(in_sequence: t.Sequence, msg: str) -> None

"""Check whether the input sequence is not empty."""
fn check_is_not_empty(in_sequence: t.Sequence, msg: str) -> None

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from itertools import zip_longest
from typing import List, Tuple

import pytest
import pytest_check as check

from flake8_annotations import Argument, FunctionVisitor
from testing.helpers import parse_source
from testing.test_cases.type_comment_test_cases import parser_test_cases

ARG_FIXTURE_TYPE = Tuple[List[Argument], List[Argument], str]


class TestArgumentParsing:
    """Test for proper argument parsing from source."""

    @pytest.fixture(params=parser_test_cases.items())
    def argument_lists(self, request) -> ARG_FIXTURE_TYPE:  # noqa: ANN001
        """
        Build a pair of lists of arguments to compare and return as a (truth, parsed) tuple.

        `parser_test_cases` is a dictionary of ParserTestCase named tuples, which provide the
        following:
            * `src` - Source code for the test case to be parsed
            * `args` - A list of Argument objects to be used as the truth values
            * `should_yield_ANN301` - Boolean flag indicating whether the source should yield ANN301

        A list of parsed Argument objects is taken from the class-level source parser

        The function name is also returned in order to provide a more verbose message for a failed
        assertion
        """
        test_case_name, test_case = request.param

        tree, lines = parse_source(test_case.src)
        visitor = FunctionVisitor(lines)
        visitor.visit(tree)
        parsed_arguments = visitor.function_definitions[0].args

        return test_case.args, parsed_arguments, test_case_name

    def test_argument_parsing(self, argument_lists: ARG_FIXTURE_TYPE) -> None:
        """
        Test argument parsing of the testing source code.

        Argument objects are provided as a tuple of (truth, source) lists
        """
        for truth_arg, parsed_arg in zip_longest(*argument_lists[:2]):
            failure_msg = (
                f"Comparison check failed for arg '{parsed_arg.argname}' in '{argument_lists[2]}'"
            )
            check.is_true(self._is_same_arg(truth_arg, parsed_arg), msg=failure_msg)

    @staticmethod
    def _is_same_arg(arg_a: Argument, arg_b: Argument) -> bool:
        """
        Compare two Argument objects for "equality".

        Because we are testing argument parsing in another test, we can make this comparison less
        fragile by ignoring line & column indices and instead comparing only the following:
          * argname
          * has_type_annotation
          * has_3107_annotation
          * has_type_comment
        """
        return all(
            (
                arg_a.argname == arg_b.argname,
                arg_a.has_type_annotation == arg_b.has_type_annotation,
                arg_a.has_3107_annotation == arg_b.has_3107_annotation,
                arg_a.has_type_comment == arg_b.has_type_comment,
            )
        )
<|endoftext|>"
},
{
"prompt": "

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from django.urls import reverse

from cursodjango.django_assertions import assert_contains


@pytest.fixture
def resp(client):
    return client.get(reverse('aperitivos:video', args=('motivacao',)))


def test_status_code(resp):
    assert resp.status_code == 200


def test_title_video(resp):
    assert_contains(resp, 'Video Aperitivo: Motivação</h1>')


def test_conteudo_video(resp):
    assert_contains(resp, '<iframe width="560" height="315" src="https://www.youtube.com/embed/gtgI7PC_TJk" '
                          'frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; '
                          'picture-in-picture" allowfullscreen></iframe>')
<|endoftext|>"
},
{
"prompt": "fn create_dataset_sample(project: str, display_name: str, metadata_schema_uri: str, location: str, api_endpoint: str, timeout: int)

fn get_name(out, key)

fn get_state(out)

"""Waits until the Job state of provided resource name is a particular state.

Args:
    get_job_method: Callable[[str], "proto.Message"]
        Required. The GAPIC getter method to poll. Takes 'name' parameter
        and has a 'state' attribute in its response.
    name (str):
        Required. Complete uCAIP resource name to pass to get_job_method
    expected_state (str):
        The state at which this method will stop waiting.
        Default is "CANCELLED".
    timeout (int):
        Maximum number of seconds to wait for expected_state. If the job
        state is not expected_state within timeout, a TimeoutError will be raised.
        Default is 90 seconds.
    freq (float):
        Number of seconds between calls to get_job_method.
        Default is 1.5 seconds."""
fn wait_for_job_state(get_job_method: Callable[([str], proto.Message)], name: str, expected_state: str, timeout: int, freq: float) -> None

fn delete_dataset_sample(project: str, dataset_id: str, location: str, api_endpoint: str, timeout: int)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
from uuid import uuid4

import pytest

import create_dataset_sample
import delete_dataset_sample
import helpers

PROJECT_ID = os.getenv("BUILD_SPECIFIC_GCLOUD_PROJECT")
IMAGE_METADATA_SCHEMA_URI = (
    "gs://google-cloud-aiplatform/schema/dataset/metadata/image_1.0.0.yaml"
)


@pytest.fixture
def shared_state():
    shared_state = {}
    yield shared_state


@pytest.fixture(scope="function", autouse=True)
def teardown(shared_state):
    yield

    assert "/" in shared_state["dataset_name"]

    dataset_id = shared_state["dataset_name"].split("/")[-1]

    # Delete the created dataset
    delete_dataset_sample.delete_dataset_sample(
        project=PROJECT_ID, dataset_id=dataset_id
    )


def test_ucaip_generated_create_dataset_sample_vision(capsys, shared_state):
    create_dataset_sample.create_dataset_sample(
        display_name=f"temp_create_dataset_test_{uuid4()}",
        metadata_schema_uri=IMAGE_METADATA_SCHEMA_URI,
        project=PROJECT_ID,
    )
    out, _ = capsys.readouterr()
    assert "create_dataset_response" in out

    shared_state["dataset_name"] = helpers.get_name(out)
<|endoftext|>"
},
{
"prompt": "fn __null_line_strip(line)

fn __line_strip(line)

fn split_line_list(line_list, re_block_separator, is_include_match_line, is_strip)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """"
.. codeauthor:: Tsuyoshi Hombashi <tsuyoshi.hombashi@gmail.com>
"""


import re

import pytest

from tcconfig._split_line_list import split_line_list


class Test_split_line_list:
    @pytest.mark.parametrize(
        ["value", "separator", "is_include_matched_line", "is_strip", "expected"],
        [
            [
                ["abcdefg", "ABCDEFG", "1234"],
                re.compile("DEFG$"),
                False,
                True,
                [["abcdefg"], ["1234"]],
            ],
            [
                ["abcdefg", "ABCDEFG", "ABCDEFG", "1234"],
                re.compile("DEFG$"),
                False,
                True,
                [["abcdefg"], ["1234"]],
            ],
            [
                ["ABCDEFG", "abcdefg", "ABCDEFG", "1234", "ABCDEFG"],
                re.compile("DEFG$"),
                False,
                True,
                [["abcdefg"], ["1234"]],
            ],
            [
                ["abcdefg", "ABCDEFG", "1234"],
                re.compile("DEFG$"),
                True,
                True,
                [["abcdefg"], ["ABCDEFG", "1234"]],
            ],
            [["a", "  ", "b", "c"], re.compile("^$"), False, True, [["a"], ["b", "c"]]],
            [["a", "  ", "b", "c"], re.compile("^$"), False, False, [["a", "  ", "b", "c"]]],
            [["a", "b", "c"], None, False, True, [["a", "b", "c"]]],
        ],
    )
    def test_normal(self, value, separator, is_include_matched_line, is_strip, expected):
        assert split_line_list(value, separator, is_include_matched_line, is_strip) == expected

    @pytest.mark.parametrize(
        ["value", "separator", "is_include_matched_line", "is_strip", "expected"],
        [
            [None, "", False, True, TypeError],
            [[1, 2, 3], re.compile(""), False, True, AttributeError],
            [[1, 2, 3], re.compile(""), False, False, TypeError],
        ],
    )
    def test_exception(self, value, separator, is_include_matched_line, is_strip, expected):
        with pytest.raises(expected):
            split_line_list(value, separator, is_include_matched_line, is_strip)
<|endoftext|>"
},
{
"prompt": "class TestBaseClass:
	TEST_UNDEFINED_PARAMETER = 'this is an undefined parameter to work around pytest limitations'
	fn setup_class(cls)
	fn __pytest_empty_object_fixture(self, _input, default)
	fn _create_tempfile(self, _file, _input)
	fn _create_args(self, input, extraopts)
	fn _create_args_fix(self, input, extraopts)
	fn fix_and_check(self, args, id)
	fn _create_args_parser(self)
	fn check_for_id(self, args, id, occurrences)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import os
import sys

import pytest

sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))

from base import TestBaseClass


class TestClassOelintVarsDoubleModify(TestBaseClass):

    @pytest.mark.parametrize('id', ['oelint.vars.doublemodify'])
    @pytest.mark.parametrize('occurrence', [1])
    @pytest.mark.parametrize('input', 
        [
            {
            'oelint_adv_test.bb':
            '''
            A_append_prepend_remove += "1"
            '''
            },
            {
            'oelint_adv_test.bb':
            '''
            A_append_prepend += "1"
            '''
            },
            {
            'oelint_adv_test.bb':
            '''
            A_append += "1"
            '''
            },
            {
            'oelint_adv_test.bb':
            '''
            A_prepend_remove += "1"
            '''
            },
            {
            'oelint_adv_test.bb':
            '''
            A_append_remove += "1"
            '''
            },
            {
            'oelint_adv_test.bb':
            '''
            A_remove += "1"
            '''
            },
            {
            'oelint_adv_test.bb':
            '''
            A_append_prepend_remove = " 1 "
            '''
            },
            {
            'oelint_adv_test.bb':
            '''
            A_append_remove = " 1 "
            '''
            },
            {
            'oelint_adv_test.bb':
            '''
            A_prepend_remove = " 1 "
            '''
            },
        ],
    )
    def test_bad(self, input, id, occurrence):
        self.check_for_id(self._create_args(input), id, occurrence)

    @pytest.mark.parametrize('id', ['oelint.vars.doublemodify'])
    @pytest.mark.parametrize('occurrence', [0])
    @pytest.mark.parametrize('input', 
        [
            {
            'oelint_adv_test.bb':
            '''
            A_append = " a"
            '''
            },
            {
            'oelint_adv_test.bb':
            '''
            A_prepend = "a "
            '''
            },
            {
            'oelint_adv_test.bb':
            '''
            A_remove = "a"
            '''
            },
            {
            'oelint_adv_test.bb':
            '''
            A += "a"
            '''
            },
            {
            'oelint_adv_test.bb':
            '''
            A =+ "a"
            '''
            },
        ],
    )
    def test_good(self, input, id, occurrence):
        self.check_for_id(self._create_args(input), id, occurrence)
<|endoftext|>"
},
{
"prompt": "fn skip_if_no_reference_system(func)

"""Counts number of CallNode(s) in the IRModule"""
fn count_num_calls(mod)

"""if KCompiler attribute is missing, this function raises an assertion.

Parameters
----------
orig_mod : IRModule
    Pre-partitioning module
cmsisnn_mod : IRModule
    Post-partitioning module
is_num_calls_same: bool
    Are number of CallNode(s) before and after partitioning expected to be the same"""
fn assert_partitioned_function(orig_mod: IRModule, cmsisnn_mod: IRModule, expected_ops_unchanged)

fn assert_no_external_function(mod)

"""Creates IRModule from Function"""
fn make_module(func)

"""Provides CMSIS-NN padding when output dim == input dim.
This is TFLu's "SAME" padding case."""
fn get_same_padding(in_shape, kernel, dilation, stride)

"""Returns (kernel_dtype, bias_dtype) based on input's dtype."""
fn get_kernel_bias_dtype(input_dtype)

"""Calculate the output quantization parameters for convolution based on the input and
kernel quantization paramters and the data types.

Parameters
----------
kernel_shape : List[int]
    shape of the kernel
input_scale : float
    scale of the input tensor
input_zp : int
    zero point of the input tensor
kernel_scale : Union[float, List[float]]
    scale(s) of the kernel tensor
kernel_zp : int
    zero point of the kernel tensor
is_depthwise : bool
    whether it is a depthwise convolution
input_dtype : str
    data type of the input tensor
kernel_dtype : str
    data type of the kernel tensor
output_dtype : str
    data type of the output tensor

Returns
-------
output_scale : float
    scale of the output tensor
output_zp : int
    zero point of the output tensor"""
fn get_conv2d_qnn_params(kernel_shape: List[int], input_scale: float, input_zp: int, kernel_scale: Union[(float, List[float])], kernel_zp: int, input_dtype: str, kernel_dtype: str, output_dtype: str, is_depthwise: bool) -> Tuple[(float, int)]

"""Mimics convert_qnn_fused_activation_function from TFLite frontend"""
fn make_qnn_relu(expr, fused_activation_fn, scale, zero_point, dtype)

"""Provides method to test number of pads present inside the function being visited."""
class CheckForPadsWithinCompositeFunc(tvm.relay.ExprVisitor):
	fn __init__(self)
	fn visit_call(self, call)
	fn assert_no_pads_within_func(self)
	fn assert_pads_within_func(self)


"""Creates AOT test runner for CMSIS-NN tests.

Parameters
----------
compiler_cpu : str
   Equivalent of gcc option mcpu
   Options:  cortex-m55, cortex-m7
cpu_flags: str
    Disable Arm(R) Cortex(R)-M profile vector extension (mve)
    Options:
    Arm(R) Cortex(R)-M55: when null +mve is set by default.
        +nomve disables vector extensions.
    Arm(R) Cortex(R)-M7 does not support mve."""
fn create_test_runner(compiler_cpu: str, cpu_flags: str)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

"""CMSIS-NN integration tests: Tests invalid graphs"""
import itertools
import numpy as np
import pytest
import tvm
from tvm import relay

from tvm.testing.aot import AOTTestModel, compile_and_run, generate_ref_data
from tvm.micro.testing.aot_test_utils import (
    AOT_USMP_CORSTONE300_RUNNER,
)
from utils import (
    skip_if_no_reference_system,
    get_range_for_dtype_str,
)


@skip_if_no_reference_system
@tvm.testing.requires_cmsisnn
def test_empty_function():
    ORIGINAL_MODEL = """
#[version = "0.0.5"]
def @main(%data : Tensor[(16, 29), int8]) -> Tensor[(16, 29), int8] {
    add(%data, %data)
}
"""
    CMSISNN_MODEL = """
#[version = "0.0.5"]
def @tvmgen_default_cmsis_nn_main_1(%i1: Tensor[(16, 29), int8], Inline=1, Compiler="cmsis-nn", global_symbol="tvmgen_default_cmsis_nn_main_1", Primitive=1) -> Tensor[(16, 29), int8] {
  add(%i1, %i1)
}
def @main(%data : Tensor[(16, 29), int8]) -> Tensor[(16, 29), int8] {
  %1 = @tvmgen_default_cmsis_nn_main_1(%data) /* ty=Tensor[(16, 29), int8] */;
  %1
}
"""
    orig_mod = tvm.parser.fromtext(ORIGINAL_MODEL)
    cmsisnn_mod = tvm.parser.fromtext(CMSISNN_MODEL)
    params = {}

    # validate the output
    interface_api = "c"
    use_unpacked_api = True
    test_runner = AOT_USMP_CORSTONE300_RUNNER
    dtype = "int8"
    in_min, in_max = get_range_for_dtype_str(dtype)
    rng = np.random.default_rng(12345)
    inputs = {"data": rng.integers(in_min, high=in_max, size=(16, 29), dtype=dtype)}
    outputs = generate_ref_data(orig_mod["main"], inputs, params)
    compile_and_run(
        AOTTestModel(
            module=cmsisnn_mod,
            inputs=inputs,
            outputs=outputs,
            params=params,
            output_tolerance=0,
        ),
        test_runner,
        interface_api,
        use_unpacked_api,
    )
<|endoftext|>"
},
{
"prompt": """"Find state abbrevations from a long-lat bbox.

Examples
--------
gssurgo.state_by_bbox(fpath = "tifs", ext = "tif",  xmax = -88.34945, \
                      xmin = -88.35470, ymin = 38.70095, ymax = 38.70498)

Notes
-----
download:
http://www2.census.gov/geo/tiger/GENZ2017/shp/cb_2017_us_state_500k.zip
`ogr2ogr -f GPKG states.gpkg cb_2017_us_state_50k.shp`"""
fn state_by_bbox(fpath, ext, xmax, xmin, ymin, ymax)

"""Crop original raster to bounding box so we can read in memory.

Examples
--------
gssurgo.aoi(in_raster_path = "tifs", out_raster = "path/to/aoi.tif", \
            xmax = -88.34945, xmin = -88.35470, ymin = 38.70095, \
            ymax = 38.70498)

Notes
-----
https://gis.stackexchange.com/a/237412/32531"""
fn aoi(in_raster_path, out_raster, xmin, ymax, xmax, ymin, src_tif)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from gssurgo.aoi import (
    state_by_bbox
)


def test_query_gpkg():
    assert state_by_bbox(fpath = "tests/tifs", ext = "tif",
                         xmax = -88.34945, xmin = -88.35470,
                         ymin = 38.70095, ymax = 38.70498) == \
        ['tests/tifs/IL.tif']
<|endoftext|>"
},
{
"prompt": """"ISIC4 categories enum."""
class Category(_Enum):
	SECTION = 1
	DIVISION = 2
	GROUP = 3
	CLASS = 4
	SUBCLASS = 5


"""Industry classification standards enum."""
class Standards(_Enum):
	ISIC3 = 'ISIC3'
	ISIC31 = 'ISIC31'
	ISIC4 = 'ISIC4'
	NACE1 = 'NACE1'
	NACE2 = 'NACE2'
	NAICS2017 = 'NAICS2017'
	TSIC2552 = 'TSIC2552'
	JSIC13 = 'JSIC13'
	KSIC10 = 'KSIC10'
	SKD2002 = 'SKD2002'
	SKD2008 = 'SKD2008'
	SKIS2010 = 'SKIS2010'
	CNAE2 = 'CNAE2'
	NACEBEL2003 = 'NACEBEL2003'
	NACEBEL2008 = 'NACEBEL2008'
	NAF1 = 'NAF1'
	NAF2 = 'NAF2'
	GCED2011 = 'GCED2011'
	SCIAN2018 = 'SCIAN2018'
	CCNAE2021 = 'CCNAE2021'
	CAEM2005 = 'CAEM2005'
	CAEM2009 = 'CAEM2009'
	SBI2008 = 'SBI2008'
	SIC = 'SIC'
	SSIC2020 = 'SSIC2020'


"""Classification base object for use across standards.

Examples:
    The NAICS2017 class "Space Research and Technology" can be represented as

    >>> Classification(
    ...     code="927110",
    ...     description="Space Research and Technology"
    ... )
    Classification(code='927110', description='Space Research and Technology', category=None)"""
class Classification:
	code: str
	description: str = None
	category: Category = None
	"""Return self as dict."""
	fn to_dict(self)


class Standard(_OrderedDict):
	"""Industry classification standard.
	
	A Standard defines all classes (i.e., classifications) belonging to a
	standard.
	
	Args:
	    standard: enum to reference standard
	    classes: all included classes (i.e., codes) defined in the standard
	
	Examples:
	    A subset of the NAICS2017 standard can be represented as
	
	    >>> Standard(
	    ...     standard=Standards.NAICS2017,
	    ...     classes=[
	    ...         Classification("111110", "Soybean Farming"),
	    ...         Classification("111120", "Oilseed (except Soybean) Farming"),
	    ...         Classification("111130", "Dry Pea and Bean Farming"),
	    ...     ]
	    ... )"""
	fn __init__(self, standard: Standards, classes: _List[Classification])
	"""Return repr(self)."""
	fn __repr__(self) -> str


class Concordance(_nx.DiGraph):
	"""Industry classification concordance.
	
	Concordances define direct relationships between classification
	systems. The relationships are directed (from => to) from classes
	in a source standard to classes in a destination standard. Accordingly,
	Concordances are implemented as a directed graph.
	
	Given source and destination classification standards, all defined
	classes are added as nodes in the graph. Concordances are added as
	relationships between nodes in the graph.
	
	Concordant classes are discovered by returning all descendants of a
	given node.
	
	Note:
	    Concordances must be directed acyclic graphs.
	
	Args:
	    src: source standard to map concordances from
	    dst: destination standard to map concordances to
	    concordances: directed relationships between classes in the source
	        and destination standards"""
	fn __init__(self, src: Standard, dst: Standard, concordances: _List[tuple])
	"""Return repr(self)."""
	fn __repr__(self) -> str
	"""Return set of concordant industry classifications (i.e., descendants).
	
	Args:
	    code: source classification code (e.g., "927110" from NAICS2017)
	
	Returns:
	    set: concordant (i.e., descandant) nodes from the given code
	
	Examples:
	    >>> pyisic.NAICS2017_to_ISIC4.concordant("927110")
	    {(<Standards.ISIC4: 'ISIC4'>, '5120'), (<Standards.ISIC4: 'ISIC4'>, '8413')}"""
	fn concordant(self, code: str) -> set


class ComposedGraph(_nx.DiGraph):
	"""Composed graph of industry classification concordance.
	
	Args:
	    dst: destination standard
	    concordances: iterable of concordances"""
	fn __init__(self, dst: Standard, concordances: None)
	"""Return set of concordant industry classifications.
	
	Args:
	    code: source classification code (e.g., "927110" from NAICS2017)
	    src: source standard (e.g., "NAICS2017")
	
	Returns:
	    set: concordant nodes from the given source code
	
	Examples:
	    >>> pyisic.ToISIC4("927110", Standards.NAICS2017)
	    {(<Standards.ISIC4: 'ISIC4'>, '5120'), (<Standards.ISIC4: 'ISIC4'>, '8413')}"""
	fn __call__(self, code: str, src: str) -> set


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# -*- coding: utf-8 -*-
import pytest

from pyisic import JSIC13_to_ISIC4
from pyisic.types import Standards


@pytest.mark.parametrize(
    "code,expected",
    [
        ("DOESNT EXIST", set()),
        ("A", set()),
        ("0100", {(Standards.ISIC4, "7010")}),
    ],
)
def test_jsic13_to_isic4_concordance(code: str, expected: set):
    """Test JSIC13 to ISIC4 sample concordances."""
    assert JSIC13_to_ISIC4.concordant(code) == expected
<|endoftext|>"
},
{
"prompt": """"update the cluster configuration file"""
fn update_config(config_dict, config, amount, role)

"""Create a new host(s) in OpenStack which will join the cluster as a node(s)

This hosts boots with all paramerters required for it to join the cluster
with a cloud-init.

Args:
    cloud_config (``koris.cloud.openstack.OSClusterInfo``): the content
       cloud.conf file
    os_cluster_info (``koris.cloud.openstack.OSClusterInfo``)
    role (str): the host role currently only "node" is supported here.
    zone (str): the AZ in OpenStack in which the hosts are created
    amount (int): the number of instance to create
    flavour (str): the flavor in OpenStack to create
    k8s (``koris.deploy.K8S``): an instance which creates a bootstrap token.
    config_dict (dict): the koris configuration yaml as ``dict``"""
fn add_node(cloud_config, os_cluster_info, role, zone, amount, flavor, k8s, config_dict)

"""Add a new master to OpenStack and the Kubernetes cluster.

Will add a new node to OpenStack, adjust the config, bootstrap the master
and add the IP to the LoadBalancer pool.

Args:
     builder (:class:`.cloud.openstack.ControlPlaneBuilder`): A
        ControlPlanBuilder instance.
    zone (str): The AZ to add the new node in.
    flavor (str): The node flavor of the node.
    config (str): The path of the koris config.
    config_dict (dict): The parsed koris config.
    os_cluster_info (:class:`.cloud.openstack.OSClusterInfo`): A
        OSClusterInfo instance.
    k8s (:class:`.deploy.K8S`): A K8S instance."""
fn add_master(builder, zone, flavor, config, config_dict, k8s)

"""Delete a master or worker node from the cluster.

Will perform basic validity checks on the name.

Args:
    config_dict (dict): A dictionary representing the config.
    name (str): The name of the node to delete.
    conn: An OpenStack connection object.

Raises:
    ValueError if name is invalid or resources are not found."""
fn delete_node(config_dict, name)

"""The main entry point for the program. This class does the CLI parsing
and descides which action shoud be taken"""
class Koris:
	fn __init__(self)
	fn _get_version(self)
	fn _get_verbosity(self)
	"""Bootstrap a Kubernetes cluster
	
	config - configuration file"""
	fn apply(self, config)
	"""Delete the complete cluster stack"""
	fn destroy(self, config: str, force: bool)
	"""Delete a node from the cluster, or the complete cluster.
	
	config - koris configuration file.
	resource - the type of resource to delete. [node | cluster]
	name - the name of the resource to delete.
	force - Force deletion of resource."""
	fn delete(self, config: str, resource: str, name: str, force: bool)
	"""Add a worker node or master node to the cluster.
	
	config - configuration file
	flavor - the machine flavor
	zone - the availablity zone
	role - one of node or master
	amount - the number of worker nodes to add (masters are not supported)
	---
	Add a node or a master to the current active context in your KUBECONFIG.
	You can specify any other configuration file by overriding the
	KUBECONFIG environment variable.
	If you specify a name and IP address the program will only try to join
	it to the cluster without trying to create the host in the cloud first."""
	fn add(self, config: str, flavor: str, zone: str, role: str, amount: int)


"""run and execute koris"""
fn main()

fn mock_listener()

fn mock_pool()

fn mock_pool_info()

fn mock_member(nr)

"""A default LoadBalancer Object as returned from the OpenStack SDK"""
fn default_data()

fn default_project()

fn default_location()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import os
import subprocess

import pytest

from .testdata import CONFIG
from koris.koris import delete_node


def _get_clean_env():
    """
    A helper method that ensures that it seems that we do not have an RC
    file sourced.
    """
    env = {}
    for (key, val) in dict(os.environ).items():
        if not key.startswith("OS_"):
            env[key] = val
    return env


def test_help():
    """
    It should be possible to call koris --help without sourcing an
    RC file.
    """
    cmd = ['koris', '--help']
    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE, env=_get_clean_env())
    stdout, stderr = proc.communicate()
    output = stdout.decode("utf-8").strip()
    assert proc.returncode == 0
    assert "usage: koris" in output


def test_need_rc_file():
    """
    For building a cluster, we need to source the RC file.
    """
    cmd = ['koris', 'apply', 'tests/koris_test.yml']
    try:
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE,
                                stderr=subprocess.PIPE, env=_get_clean_env())
        proc.communicate()

        # We should not reach that, since an AssertionError should be thrown.
        assert False
    except AssertionError:
        pass


def test_delete_node():
    invalid_names = ["", None]

    for name in invalid_names:
        with pytest.raises(ValueError):
            delete_node(CONFIG, name)
<|endoftext|>"
},
{
"prompt": "fn load_spacy_nlp_model() -> spacy.language.Language

fn _doc_text(doc: Doc) -> str

fn get_doc(text: str) -> spacy.tokens.Doc

fn get_tokens_df(text: str) -> pd.DataFrame

fn _get_tokens_df(doc: Doc, attrs: List[str]) -> pd.DataFrame

fn annotated_names_to_keys(annotate_settings: Dict[(str, dict)], annotated_names: List[str]) -> List[str]

fn get_pos_annotation(text: str, annotated_keys: List[str], annotations_settings: Dict, colors: Dict[(str, str)], row: pd.Series, idx: int, next_idx: int) -> Union[(Tuple[(str, Any, str)], str)]

fn get_cases_annotation(text: str, annotated_keys: List[str], _: Dict, colors: Dict[(str, str)], row: pd.Series, idx: int, next_idx: int) -> Union[(Tuple[(str, Any, str)], str)]

fn merge_same_annotation_texts(annotations: list) -> list

fn get_annotated_text(what_to_annotate: str, text: str, annotated_names: List[str], colors: Dict[(str, str)]) -> list

fn split_annotated_text(annotated_text: list, delimiter: str) -> list

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pandas.api.types as ptypes
import pytest

from satzify import constants
from satzify.helpers import (
    annotated_names_to_keys,
    get_annotated_text,
    get_tokens_df,
    split_annotated_text,
)


@pytest.fixture
def texts():
    return (
        "Alles hat ein Ende, nur die Wurst hat zwei.",
        "Also morgen ist ja genau genommen schon heute.",
        "Das Jahr ist wieder irgendwie so schnell vergangen.",
        "Ich war nur kurz was zu trinken holen und als ich zurückkam, wart ihr irgendwie weg.",
        """
        Was wir alles nicht wissen wollen! Und was wir nicht alles wissen wollen!
        Das möchte ich gar nicht wissen. Und wenn man es dann doch weiß, weil der andere sein Wissen einfach nicht für sich behalten konnte, dann können wir ja nichts dafür, wir wollten es ja gar nicht wissen. Und eine Klatschtante sind wir dann auch nicht.
        """,
    )


@pytest.fixture
def cases():
    return constants.CASES


@pytest.fixture
def pos():
    return constants.POS


@pytest.fixture
def colors():
    cases_and_pos_settings = {**constants.CASES, **constants.POS}
    colors = {k: v["color"] for k, v in cases_and_pos_settings.items()}
    return colors


@pytest.fixture
def example_text():
    return "Alles hat ein Ende, nur die Wurst hat zwei."


@pytest.fixture
def example_pos_annotations(example_text):
    return [
        (["Alles "], "Pronoun", "#fea"),
        (["hat "], "Verb", "#8ef"),
        "ein ",
        (["Ende"], "Noun", "#afa"),
        ", ",
        (["nur "], "Adverb", "#d94"),
        "die ",
        (["Wurst "], "Noun", "#afa"),
        (["hat "], "Verb", "#8ef"),
        "zwei",
        ".",
    ]


@pytest.fixture
def example_cases_annotations(example_text):
    return [
        (["Alles "], "Nom+N", "#afa"),
        "hat ",
        (["ein ", "Ende"], "Acc+N", "#fea"),
        ", ",
        "nur ",
        (["die ", "Wurst "], "Nom+F", "#afa"),
        "hat ",
        "zwei",
        ".",
    ]


def test_get_tokens_df(texts):
    for text in texts:
        tokens_df = get_tokens_df(text)
        assert ptypes.is_numeric_dtype(tokens_df["idx"].apply(int))
        assert ptypes.is_string_dtype(tokens_df["text"])
        assert ptypes.is_string_dtype(tokens_df["morph"])
        assert ptypes.is_string_dtype(tokens_df["pos_"])
        morphs = tokens_df[tokens_df["morph"] != ""]["morph"].to_list()
        for morph in morphs:
            parts = morph.split("|")
            # ensure that each and every part of a morph has a key=value structure, i.e. Case=Nom
            assert all(len(part.split("=")) == 2 for part in parts)


def test_annotated_names_to_keys(pos):
    annotated_names = ["Noun", "Verb", "Adposition"]
    keys = annotated_names_to_keys(pos, annotated_names)
    assert keys == ["NOUN", "VERB", "ADP"]


def test_pos_annotations(pos, colors, example_text, example_pos_annotations):
    pos_annotated_names = [v["name"] for v in pos.values()]
    annotations = get_annotated_text("POS", example_text, pos_annotated_names, colors)
    assert annotations == example_pos_annotations


def test_cases_annotations(cases, colors, example_text, example_cases_annotations):
    pos_annotated_names = [v["name"] for v in cases.values()]
    annotations = get_annotated_text("CASES", example_text, pos_annotated_names, colors)
    assert annotations == example_cases_annotations


def test_split_annotated_text(example_pos_annotations, example_cases_annotations):
    combined_annotations = example_pos_annotations + ["\n"] + example_cases_annotations
    list_annotated_text = split_annotated_text(combined_annotations)
    assert len(list_annotated_text) == 2
    assert list_annotated_text[0] == example_pos_annotations
    assert list_annotated_text[1] == example_cases_annotations
<|endoftext|>"
},
{
"prompt": """"Compute the phase differential along a given axis

Parameters
----------
phase : np.ndarray
    Input phase (in radians)

conv: {None, 'tf', 'th', 'channels_last', 'channels_first'}
    Convolution mode

Returns
-------
dphase : np.ndarray like `phase`
    The phase differential."""
fn phase_diff(phase: np.ndarray, conv: {None, 'tf', 'th', 'channels_last', 'channels_first'}) -> np.ndarray like `phase`

"""Quantize array entries to a fixed dtype.

Parameters
----------
x : np.ndarray
    The data to quantize

ref_min : None or float

ref_max : None or float
    The reference minimum (maximum) value for quantization.
    By default, `x.min()` (`x.max()`)

dtype : np.dtype {'uint8', 'uint16'}
    The target data type.  Any unsigned int type is supported,
    but most cases will call for `uint8`.

Returns
-------
y : np.ndarray, dtype=dtype
    The values of `x` quantized to integer values"""
fn quantize(x: np.ndarray, ref_min: None or float, ref_max: None or float, dtype: np.dtype {'uint8', 'uint16'}) -> np.ndarray, dtype=dtype

"""Convert an array to a target dtype.  Quantize if integrable.

Parameters
----------
x : np.ndarray
    The input data

dtype : np.dtype or type specification
    The target dtype

Returns
-------
x_dtype : np.ndarray, dtype=dtype
    The converted data.

    If dtype is integrable, `x_dtype` will be quantized.

See Also
--------
quantize"""
fn to_dtype(x: np.ndarray, dtype: np.dtype or type specification) -> np.ndarray, dtype=dtype

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "#!/usr/bin/env python
# -*- encoding: utf-8 -*-
'''Tests for feature utility helpers'''

import pytest
import numpy as np

import pumpp
import pumpp.feature._utils


@pytest.mark.parametrize('dtype', ['uint8', np.uint8])
def test_quantize(dtype):

    # The range -5 to 5 is broken into 256 equal pieces
    # -5/3 lands at 85 (1/3)
    # 5/3 lands at 2*85 = 270
    # 5 lands at the max
    x = np.asarray([-5, -5/3, 5/3, 5])
    y = pumpp.feature._utils.quantize(x, dtype=dtype)
    assert np.allclose(y, [0, 85, 170, 255])


def test_quantize_min():
    x = np.asarray([-5, -5/3, 5/3, 5])
    y = pumpp.feature._utils.quantize(x, ref_min=0)
    assert np.allclose(y, [0, 0, 85, 255])


def test_quantize_max():
    x = np.asarray([-5, -5/3, 5/3, 5])
    y = pumpp.feature._utils.quantize(x, ref_max=0)
    assert np.allclose(y, [0, 170, 255, 255])


@pytest.mark.xfail(raises=pumpp.ParameterError)
@pytest.mark.parametrize('dtype', ['int8', 'float32'])
def test_quantize_bad_dtype(dtype):
    x = np.asarray([-5, -5/3, 5/3, 5])
    pumpp.feature._utils.quantize(x, dtype=dtype)
<|endoftext|>"
},
{
"prompt": "fn soma(a, b)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest
from Python import soma

def test():
    assert soma(5,7) == 12<|endoftext|>"
},
{
"prompt": "class ApiType(Enum):
	JENKINS = 0
	SCRIPT = 1
	MOCK = 2


fn direct_url(api_type)

fn public_url(api_type)

fn direct_cli_url(api_type)

fn public_cli_url(api_type)

fn proxied_public_cli_url(api_type)

fn script_dir()

"""speedup is used by the mock api"""
fn select_speedup(speedup)

fn speedup()

fn skip_job_delete()

fn skip_job_load()

fn skip_job_load_sh_export_str()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Copyright (c) 2012 - 2015 Lars Hupfeldt Nielsen, Hupfeldt IT
# All rights reserved. This work is under a BSD license, see LICENSE.TXT.

import pytest

from .. import jenkins_api
from .framework import api_select
from .cfg import ApiType


@pytest.mark.not_apis(ApiType.MOCK, ApiType.SCRIPT)
def test_api_class_repr_job(api_type):
    api = api_select.api(__file__, api_type, login=True)
    job = jenkins_api.ApiJob(api, {}, 'my-job')

    jrd = eval(repr(job))
    assert jrd == {'name': 'my-job', 'dct': {}}

    invocation = jenkins_api.Invocation(job, "http://dummy", 'hello')
    assert repr(invocation) == "Invocation: 'http://dummy' None None"
<|endoftext|>"
},
{
"prompt": """"the build class is the object passed to the main method of the
uranium script.

it's designed to serve as the public API to controlling the build process.

Build is designed to be executed within the sandbox
itself. Attempting to execute this outside of the sandbox could
lead to corruption of the python environment."""
class Build(object):
	URANIUM_CACHE_DIR = '.uranium'
	HISTORY_NAME = 'history.json'
	fn __init__(self, root, config, with_sandbox, cache_requests)
	""":return: a uranium.config.Config object
	
	this is a generic dict to store / retrieve config data
	that tasks may find valuable"""
	fn config(self)
	""":return: a uranium.environment_variables.EnvironmentVariables object
	
	this is an interface to the environment variables of the
	sandbox.  variables modified here will be preserved when
	executing entry points in the sandbox."""
	fn envvars(self)
	""":return: uranium.executables.Executables
	
	an interface to execute scripts"""
	fn executables(self)
	""":return: uranium.hooks.Hooks
	
	provides hooks to attach functions to be executed during
	various phases of Uranium (like initializiation and finalization)"""
	fn hooks(self)
	""":return: uranium.history.History
	
	a dictionary that can contain basic data structures, that is
	preserved across executions.
	
	ideal for storing state, such as if a file was already downloaded."""
	fn history(self)
	""":return: uranium.options.Options
	
	an interface to arguments passed into the uranium command line."""
	fn options(self)
	""":return: uranium.packages.Packages
	
	an interface to the python packages currently installed."""
	fn packages(self)
	""":return: str
	
	returns the root of the uranium build."""
	fn root(self)
	""":return: uranium.tasks.Tasks
	
	an interface to the tasks that uranium has registered,
	or has discovered in the ubuild.py"""
	fn tasks(self)
	fn as_current_build(self)
	fn run_task(self, task_name)
	"""a decorator that adds the given function as a task.
	
	e.g.
	
	@build.task
	def main(build):
	    build.packages.install("httpretty")
	
	this is useful in the case where tasks are being sourced from
	a different file, besides ubuild.py"""
	fn task(self, f)
	"""executes the script at the specified path. """
	fn include(self, script_path, cache)
	fn run(self, options)
	fn _run(self, options)
	"""override_func: if this is not None, the _run_script will
	execute this function (passing in the script object) instead
	of executing the task_name."""
	fn _run_script(self, path, task_name, override_func)
	fn _warmup(self)
	fn _finalize(self)


fn _get_formatted_public_tasks(script)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import httpretty
from uranium import get_remote_script
from uranium.build import Build
import pytest

SCRIPT_URL = "http://www.myinternalwebsite.com/uranium_base.py"

REMOTE_SCRIPT = """
def setup(build):
    build.index_urls = ["http://www.myinternalwebsite.com/python_index"]
""".strip()


def test_get_remote_script(tmpdir):
    httpretty.enable()
    httpretty.register_uri(httpretty.GET, SCRIPT_URL,
                           body=REMOTE_SCRIPT)
    script = get_remote_script(SCRIPT_URL)
    build = Build(tmpdir.strpath)
    script["setup"](build)
    httpretty.disable()
    httpretty.reset()


def test_get_remote_script_with_cache(tmpdir):
    httpretty.enable()
    httpretty.register_uri(httpretty.GET, SCRIPT_URL,
                           body=REMOTE_SCRIPT)
    get_remote_script(SCRIPT_URL, cache_dir=tmpdir.strpath)
    httpretty.disable()
    httpretty.reset()
    get_remote_script(SCRIPT_URL, cache_dir=tmpdir.strpath)


def test_build_include_cache(tmpdir):
    httpretty.enable()
    httpretty.register_uri(httpretty.GET, SCRIPT_URL,
                           body=REMOTE_SCRIPT)
    build = Build(tmpdir.strpath)
    build.URANIUM_CACHE_DIR = tmpdir.strpath
    build.include(SCRIPT_URL, cache=True)
    httpretty.disable()
    httpretty.reset()
    build.include(SCRIPT_URL, cache=True)


def test_build_include_without_cache(tmpdir):
    httpretty.enable()
    httpretty.register_uri(httpretty.GET, SCRIPT_URL,
                           body=REMOTE_SCRIPT)
    build = Build(tmpdir.strpath)
    build.URANIUM_CACHE_DIR = tmpdir.strpath
    build.include(SCRIPT_URL, cache=False)
    httpretty.disable()
    httpretty.reset()
    with pytest.raises(Exception):
        build.include(SCRIPT_URL, cache=True)
<|endoftext|>"
},
{
"prompt": "class MissingLibrariesError(Exception):
	fn __init__(self, libs)


fn printerr()

fn load_exclude_list(filename)

"""Return a dict of the form soname => path for all the dependency of
the executable"""
fn list_soname_paths(executable)

"""Return a dict of the form soname => path"""
fn parse_ldd_output(ldd_output)

"""Return the list of sonames this binary *directly* depends on"""
fn list_dependencies(binary)

fn is_excluded(dependency, exclude_list)

fn copy(dependency, destdir)

class App:
	fn __init__(self, exclude_list, destdir, dry_run, dot_fp)
	fn run(self, binary_path)
	fn _traverse_tree(self, binary)
	fn _graph_excluded_dependency(self, binary, soname)
	fn _graph_dependency(self, binary, soname)


fn main()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from copydeps import parse_ldd_output, MissingLibrariesError


def test_parse_ldd_output():
    LDD_OUTPUT = b"""linux-vdso.so.1 =>  (0x00007ffd6f3cd000)
    libcrypto.so.1.0.0 => /home/ci/build/genymotion/./libcrypto.so.1.0.0 (0x00007f5ea40b6000)
    ./libfoo.so.2 (0x0000562cf1094000)
    """
    dct = parse_ldd_output(LDD_OUTPUT)
    assert dct == {
        'libcrypto.so.1.0.0': '/home/ci/build/genymotion/./libcrypto.so.1.0.0',
        './libfoo.so.2': './libfoo.so.2',
    }


def test_parse_ldd_output_library_not_found():
    LDD_OUTPUT = b"""linux-vdso.so.1 =>  (0x00007ffd6f3cd000)
    libstdc++.so.5 => not found
    libbar.so.2 => not found
    """
    with pytest.raises(MissingLibrariesError) as excinfo:
        parse_ldd_output(LDD_OUTPUT)
    assert excinfo.value.libs == ['libstdc++.so.5', 'libbar.so.2']
<|endoftext|>"
},
{
"prompt": "fn add_point_data(mesh, dim, num_tags, seed, dtype)

fn add_cell_data(mesh, dim, num_tags, dtype)

fn add_field_data(mesh, value, dtype)

fn add_point_sets(mesh)

"""Write and read a file, and make sure the data is the same as before.
    """
fn write_read(writer, reader, input_mesh, atol, extension)

fn generic_io(filename)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import os

import numpy
import pytest

import helpers
import meshio


@pytest.mark.parametrize("mesh", [helpers.tri_mesh])
def test_wkt(mesh):
    def writer(*args, **kwargs):
        return meshio.wkt.write(*args, **kwargs)

    helpers.write_read(writer, meshio.wkt.read, mesh, 1.0e-12)


@pytest.mark.parametrize(
    "filename, ref_sum, ref_num_cells",
    [("simple.wkt", 4, 2), ("whitespaced.wkt", 3.2, 2)],
)
def test_reference_file(filename, ref_sum, ref_num_cells):
    this_dir = os.path.dirname(os.path.abspath(__file__))
    filename = os.path.join(this_dir, "meshes", "wkt", filename)
    mesh = meshio.read(filename)
    tol = 1.0e-5
    s = numpy.sum(mesh.points)
    assert abs(s - ref_sum) < tol * abs(ref_sum)
    assert mesh.cells[0].type == "triangle"
    assert len(mesh.cells[0].data) == ref_num_cells
<|endoftext|>"
},
{
"prompt": "fn get_os_type() -> str

"""Accept an argument like {'py37': 1, 'py38': 2},
or {"windows": 1, "linux": 2, "mac": 3}

Used for version-dependent and OS tests."""
fn get_value(value_dict: dict[(str, any)])

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from __future__ import annotations

import json
import msgpack
import os
import pytest
import responses

from cyberbrain import _TracerFSM, trace
from utils import python_version, get_os_type


def pytest_addoption(parser):
    parser.addoption("--debug_mode", action="store_true", default=False)


@pytest.fixture(scope="function", name="tracer")
def fixture_tracer(request):
    trace.debug_mode = request.config.getoption("--debug_mode")

    yield trace

    # Do cleanup because the trace decorator is reused across tests.
    trace.raw_frame = None
    trace.decorated_function_code_id = None
    trace.frame_logger = None
    trace.tracer_state = _TracerFSM.INITIAL


@pytest.fixture(scope="function", name="trace")
def fixture_trace(request):
    trace.debug_mode = request.config.getoption("--debug_mode")

    yield trace

    # Do cleanup because the trace decorator is reused across tests.
    trace.raw_frame = None
    trace.decorated_function_code_id = None
    trace.frame_logger = None
    trace.tracer_state = _TracerFSM.INITIAL


@pytest.fixture
def mocked_responses(request):
    with responses.RequestsMock() as resp:
        resp.add(
            responses.POST,
            f"http://localhost:{trace.rpc_client.port}/frame",
            status=200,
            content_type="application/octet-stream",
        )
        yield resp

        frame_data = msgpack.unpackb(resp.calls[0].request.body)
        frame_name = frame_data["metadata"]["frame_name"]

        # Generates golden data.
        golden_filepath = f"test/data/{python_version}/{frame_name}.json"
        directory = os.path.dirname(golden_filepath)
        if not os.path.exists(directory):
            os.makedirs(directory)

        if not os.path.exists(golden_filepath):
            with open(golden_filepath, "wt") as f:
                json.dump(frame_data, f, ensure_ascii=False, indent=4)
            return

        # Assuming run in root directory.
        with open(golden_filepath, "rt") as f:
            golden_frame_data = json.loads(f.read())

        # Don't check request body on Windows because it has a different format.
        if get_os_type() == "windows" and frame_name in {"test_numpy", "test_pandas"}:
            return

        assert frame_data == golden_frame_data, json.dumps(frame_data, indent=4)
<|endoftext|>"
},
{
"prompt": "class IncorrectPart(Exception):


class MissingConfigParser(Exception):


class ErrorConfigParsing(Exception):


class MissingVcsModule(Exception):


class VersionNotFound(Exception):


class VCSException(Exception):


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from bamp.vcs import _get_vcs_module, make_tag_name, create_tag
from bamp.exc import MissingVcsModule


def test_get_vcs_module_git():
    vcs = _get_vcs_module('git')
    assert vcs.__name__ == 'bamp.vcs.git'


def test_get_vcs_module_failed():
    with pytest.raises(MissingVcsModule):
        _get_vcs_module('Albatros')


def test_make_tag_name_no_substitution():
    tag_message = 'it\'s a tag'
    assert make_tag_name(tag_message, '0.9.0') == tag_message


def test_make_tag_name_with_substitution():
    tag_message = '{new_version}-tag'
    assert make_tag_name(tag_message, '1.1.1') == '1.1.1-tag'
<|endoftext|>"
},
{
"prompt": "fn cli(ctx)

fn unity(ctx)

fn unity_list()

fn unity_acquire(model: Union[(str, WeatherModel)], timestamp: Union[(str, datetime)], target: Union[(str, Path)])

fn dwd(ctx)

fn smith(ctx)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """"
Invoke the tests in this file using::

    pytest -vvv -m "bbox and command"
"""
import json

import pytest
from click.testing import CliRunner

from gribmagic.commands import cli


@pytest.mark.bbox
@pytest.mark.command
def test_bbox_basic_success(gm_data_path, capsys):
    runner = CliRunner()

    # Acquire data.
    result = runner.invoke(
        cli,
        [
            "dwd",
            "acquire",
            "--recipe=tests/dwd/recipe_icon_d2_vmax10m.py",
        ],
    )
    assert result.exit_code == 0, result.output

    # Run bbox on data.
    result = runner.invoke(
        cli,
        [
            "smith",
            "bbox",
            "--country=AT",
            f"{gm_data_path}/icon-d2/**/*regular-lat-lon*.grib2",
        ],
    )
    assert result.exit_code == 0, result.output

    # Check results.
    report = json.loads(result.output)
    assert len(report) == 3

    first_item = report[0]

    assert "input" in first_item
    assert "output" in first_item
    assert "plot" in first_item

    assert first_item["input"].endswith(".grib2")
    assert first_item["output"].endswith(".grib2")
    assert "bbox_" in first_item["output"]
    assert first_item["plot"] is None


@pytest.mark.bbox
@pytest.mark.command
def test_bbox_basic_no_area_failure(gm_data_path, capsys):
    runner = CliRunner()

    # Run bbox on data.
    result = runner.invoke(
        cli,
        [
            "smith",
            "bbox",
            f"{gm_data_path}/icon-d2/**/*regular-lat-lon*.grib2",
        ],
    )
    assert result.exit_code == 2, result.output
    assert (
        "Error: Missing one of the required mutually exclusive options from 'area' option group"
        in result.output
    )


@pytest.mark.bbox
@pytest.mark.command
def test_bbox_basic_input_files_failure(gm_data_path, capsys):
    runner = CliRunner()

    # Run bbox on data.
    result = runner.invoke(
        cli,
        [
            "smith",
            "bbox",
            "--country=AT",
        ],
    )
    assert result.exit_code == 2, result.output
    assert "Error: Missing argument 'INPUT...'" in result.output


@pytest.mark.bbox
@pytest.mark.command
def test_bbox_plot_success(gm_data_path, capsys):
    runner = CliRunner()

    # Acquire data.
    result = runner.invoke(
        cli,
        [
            "dwd",
            "acquire",
            "--recipe=tests/dwd/recipe_icon_d2_vmax10m.py",
        ],
    )
    assert result.exit_code == 0, result.output

    # Run bbox on data.
    result = runner.invoke(
        cli,
        [
            "smith",
            "bbox",
            "--country=AT",
            f"{gm_data_path}/icon-d2/**/*regular-lat-lon*.grib2",
            "--plot",
        ],
    )
    assert result.exit_code == 0, result.output

    # Check results.
    report = json.loads(result.output)
    assert len(report) == 3

    first_item = report[0]

    assert "input" in first_item
    assert "output" in first_item
    assert "plot" in first_item

    assert first_item["input"].endswith(".grib2")
    assert "bbox_" in first_item["output"]
    assert first_item["plot"].endswith(".png")
<|endoftext|>"
},
{
"prompt": "fn force_bytes(string, encoding, errors)

""""""
fn download_and_store(url, path)

"""Expand the ZIP file at the given path to the cache directory.
    """
fn expand(self, local)

fn base_settings()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# -*- coding: utf-8 -*-
"""
Profile: http://hl7.org/fhir/StructureDefinition/ClinicalImpression
Release: R4
Version: 4.0.1
Build ID: 9346c8cc45
Last updated: 2019-11-01T09:29:23.356+11:00
"""

import io
import json
import os
import unittest

import pytest

from .. import clinicalimpression
from ..fhirdate import FHIRDate
from .fixtures import force_bytes


@pytest.mark.usefixtures("base_settings")
class ClinicalImpressionTests(unittest.TestCase):
    def instantiate_from(self, filename):
        datadir = os.environ.get("FHIR_UNITTEST_DATADIR") or ""
        with io.open(os.path.join(datadir, filename), "r", encoding="utf-8") as handle:
            js = json.load(handle)
            self.assertEqual("ClinicalImpression", js["resourceType"])
        return clinicalimpression.ClinicalImpression(js)

    def testClinicalImpression1(self):
        inst = self.instantiate_from("clinicalimpression-example.json")
        self.assertIsNotNone(
            inst, "Must have instantiated a ClinicalImpression instance"
        )
        self.implClinicalImpression1(inst)

        js = inst.as_json()
        self.assertEqual("ClinicalImpression", js["resourceType"])
        inst2 = clinicalimpression.ClinicalImpression(js)
        self.implClinicalImpression1(inst2)

    def implClinicalImpression1(self, inst):
        self.assertEqual(inst.date.date, FHIRDate("2014-12-06T22:33:00+11:00").date)
        self.assertEqual(inst.date.as_json(), "2014-12-06T22:33:00+11:00")
        self.assertEqual(
            force_bytes(inst.description),
            force_bytes(
                "This 26 yo male patient is brought into ER by ambulance after being involved in a motor vehicle accident"
            ),
        )
        self.assertEqual(
            inst.effectivePeriod.end.date, FHIRDate("2014-12-06T22:33:00+11:00").date
        )
        self.assertEqual(
            inst.effectivePeriod.end.as_json(), "2014-12-06T22:33:00+11:00"
        )
        self.assertEqual(
            inst.effectivePeriod.start.date, FHIRDate("2014-12-06T20:00:00+11:00").date
        )
        self.assertEqual(
            inst.effectivePeriod.start.as_json(), "2014-12-06T20:00:00+11:00"
        )
        self.assertEqual(
            force_bytes(inst.finding[0].itemCodeableConcept.coding[0].code),
            force_bytes("850.0"),
        )
        self.assertEqual(
            force_bytes(inst.finding[0].itemCodeableConcept.coding[0].system),
            force_bytes("http://hl7.org/fhir/sid/icd-9"),
        )
        self.assertEqual(force_bytes(inst.id), force_bytes("example"))
        self.assertEqual(force_bytes(inst.identifier[0].value), force_bytes("12345"))
        self.assertEqual(
            force_bytes(inst.investigation[0].code.text),
            force_bytes("Initial Examination"),
        )
        self.assertEqual(force_bytes(inst.meta.tag[0].code), force_bytes("HTEST"))
        self.assertEqual(
            force_bytes(inst.meta.tag[0].display), force_bytes("test health data")
        )
        self.assertEqual(
            force_bytes(inst.meta.tag[0].system),
            force_bytes("http://terminology.hl7.org/CodeSystem/v3-ActReason"),
        )
        self.assertEqual(force_bytes(inst.status), force_bytes("completed"))
        self.assertEqual(
            force_bytes(inst.summary),
            force_bytes(
                "provisional diagnoses of laceration of head and traumatic brain injury (TBI)"
            ),
        )
        self.assertEqual(force_bytes(inst.text.status), force_bytes("generated"))
<|endoftext|>"
},
{
"prompt": """"Converts a month to an integer."""
fn month_to_int(m)

"""Converts a month to an int form, str type, with a leading zero"""
fn month_to_str_int(m)

"""Converts a day to an int form, str type, with a leading zero"""
fn day_to_str_int(d)

"""Converts years / months / days to a float, eg 2015.0818 is August
18th 2015. """
fn date_to_float(y, m, d)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from regolith.dates import (month_to_str_int, day_to_str_int)

@pytest.mark.parametrize(
    "input,expected",
    [
        (1, "01"),
        (10, "10"),
        ("Oct", "10"),
        ("Jan", "01"),
    ],
)
def test_month_to_str(input, expected):
    assert month_to_str_int(input) == expected

@pytest.mark.parametrize(
    "input,expected",
    [
        (1, "01"),
        (10, "10"),
    ],
)
def test_day_to_str(input, expected):
    assert day_to_str_int(input) == expected

<|endoftext|>"
},
{
"prompt": """"Define a smart locator property.

This behaves pretty much like a normal read-only property and the
decorated function should return a dict containing the necessary
data to build a URL for the object.

This decorator should usually be applied to a method named
``locator`` as this name is required for `get_locator` to find it
automatically when just passing the object.

If you need more than one locator, you can define it like this::

    @locator_property
    def locator(self):
        return {...}

    @locator.other
    def locator(self):
        return {...}

The ``other`` locator can then be accessed by passing
``obj.locator.other`` to the code expecting an object with
a locator."""
class locator_property:
	fn __init__(self, fn)
	fn __get__(self, obj, objtype)
	fn __getattr__(self, name)
	fn __repr__(self)


"""Retrieve the locator data from an object.

The object may be a dictionary (in case a locator is passed) or
an object with a ``locator`` property."""
fn get_locator(obj)

class _LocatorDict(UserDict):
	fn __init__(self, locator, obj)
	fn data(self)
	fn __getattr__(self, name)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# This file is part of Indico.
# Copyright (C) 2002 - 2021 CERN
#
# Indico is free software; you can redistribute it and/or
# modify it under the terms of the MIT License; see the
# LICENSE file for more details.

import pytest

from indico.util.locators import get_locator, locator_property


def test_get_locator_none():
    with pytest.raises(TypeError):
        get_locator(object())
    with pytest.raises(TypeError):
        get_locator(None)


def test_get_locator():
    class _Prop:
        @property
        def locator(self):
            return {'foo': 'bar'}

    class _Locator:
        @locator_property
        def locator(self):
            return {'foo': 'bar'}

    assert get_locator(_Prop()) == {'foo': 'bar'}
    assert get_locator(_Locator()) == {'foo': 'bar'}
    assert get_locator({'foo': 'bar'}) == {'foo': 'bar'}


def test_locator_property():
    class _Test:
        @locator_property
        def locator(self):
            return {'foo': 'bar'}

        @locator.fancy
        def locator(self):
            return dict(self.locator, awesome='magic')

    assert isinstance(_Test.locator, locator_property)
    t = _Test()
    assert get_locator(t) == {'foo': 'bar'}
    assert get_locator(t.locator) == {'foo': 'bar'}
    assert get_locator(t.locator.fancy) == {'foo': 'bar', 'awesome': 'magic'}
    with pytest.raises(AttributeError):
        t.locator.invalid


def test_locator_property_lazy():
    class _Test:
        @locator_property
        def locator(self):
            accessed.append('locator')
            return {'foo': 'bar'}

        @locator.fancy
        def locator(self):
            accessed.append('fancy')
            return {'awesome': 'magic'}

    accessed = []
    # creating the object or accessing the locator itself doesn't
    # call the underlying function
    t = _Test()
    t.locator
    assert not accessed

    # accessing the locator in any way that needs to access its data
    # obviously triggers a call to the accessor
    assert t.locator == {'foo': 'bar'}
    assert accessed == ['locator']

    # accessing a special locator only triggers that locator's accessor
    # but not the one of the default locator
    accessed = []
    t.locator.fancy
    assert accessed == ['fancy']


def test_locator_property_nested():
    with pytest.raises(AttributeError):
        class _Test:
            @locator_property
            def locator(self):
                pass

            @locator.x
            def locator(self):
                pass

            @locator.x.y
            def locator(self):
                pass
<|endoftext|>"
},
{
"prompt": """"setup any state specific to the execution of the given module."""
fn setup_module(module)

"""teardown any state that was previously setup with a setup_module
method."""
fn teardown_module(module)

"""setup any state specific to the execution of the given function."""
fn setup_function(function)

"""teardown any state that was previously setup with a setup_function
method."""
fn teardown_function(function)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import os
import os.path
import time
import pytest
import fs

from .setup import *

def test_touch_on_existing_file():

    before_time = time.ctime(os.path.getmtime(TEST_FILE))

    fs.touch(TEST_FILE)
    after_time = time.ctime(os.path.getmtime(TEST_FILE))

    assert after_time >= before_time

def test_touch_on_new_file():
    
    new_file = os.path.join(TEST_DIR, "new_file.txt")

    if (os.path.exists(new_file)):
        raise ValueError("File new_file.txt already exists!")

    fs.touch(new_file)

    assert os.path.exists(new_file) is True

@pytest.mark.skipif(os.name == "nt", reason="does not work on windows")
def test_touch_on_directory():

    before_time = time.ctime(os.path.getmtime(TEST_DIR))

    fs.touch(TEST_DIR)
    after_time = time.ctime(os.path.getmtime(TEST_DIR))

    assert after_time >= before_time
<|endoftext|>"
},
{
"prompt": "fn parse_args() -> argparse.Namespace

fn get_alphaed_image(circle_image: numpy.ndarray, working_image: numpy.ndarray) -> numpy.ndarray

fn _radius(max_circle_radius: float) -> int

fn _center(height: int, width: int) -> Tuple[(int, int)]

fn _color() -> Tuple[(int, int, int)]

fn draw_circle_on_image(image: numpy.ndarray, max_circle_radius: float) -> numpy.ndarray

fn annealing_draw(original_image: numpy.ndarray, current_image: numpy.ndarray, max_circle_radius: float, round_per_generation: int, cool_down_count: int) -> Tuple[(numpy.ndarray, float, int)]

fn download_original_image(raw_image_url: str, filename: str) -> None

fn resize_original_image(image: numpy.ndarray, max_longside_length: float) -> numpy.ndarray

fn draw_current_image(generation: int, current_image: numpy.ndarray, sub_generation: Optional[int]) -> Path

fn draw_pointillism(args: argparse.Namespace) -> None

fn generate_gif_annimation() -> None

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from unittest.mock import call

import cv2
import numpy
import pytest

from hill_climbing_study.generate import (
    _center,
    _color,
    _radius,
    draw_circle_on_image,
    resize_original_image,
)


@pytest.mark.parametrize("max_circle_radius,expect", [(0.999, 1), (1.000, 2)])
def test__radius(mocker, max_circle_radius, expect):
    mock = mocker.patch("numpy.random.randint", return_value=int(max_circle_radius + 1))
    assert _radius(max_circle_radius) == expect
    mock.has_calls([call(1, 1), call(1, 2)], any_order=False)


def test__center(mocker):
    def _side_effect(low, high):
        return high

    mock = mocker.patch("numpy.random.randint", side_effect=_side_effect)
    assert _center(300, 400) == (400, 300)
    mock.has_calls([call(400), call(300)], any_order=False)
    mock.reset_mock()


def test__color(mocker):
    mock = mocker.patch("numpy.random.randint", return_value=128)
    assert _color() == (128, 128, 128)
    mock.has_calls([call(128), call(128), call(128)], any_order=False)


def test_draw_circle_on_image(mocker):
    _radius_mock = mocker.patch("hill_climbing_study.generate._radius", return_value=110)
    _center_mock = mocker.patch("hill_climbing_study.generate._center", return_value=(200, 150))
    _color_mock = mocker.patch("hill_climbing_study.generate._color", return_value=(63, 127, 255))
    image = numpy.full((300, 400), 127, dtype=numpy.uint8)
    expect_image = cv2.circle(image.copy(), (200, 150), 110, (63, 127, 255), -1)
    assert numpy.array_equal(draw_circle_on_image(image, 110), expect_image)
    _radius_mock.assert_called_once()
    _center_mock.assert_called_once()
    _color_mock.assert_called_once()


def test_resize_original_image():
    original_image = cv2.imread("tests/data/test_image.jpeg")
    resized = resize_original_image(original_image)
    assert cv2.absdiff(resized, cv2.imread("tests/data/test_resized.jpeg")).sum() < 1250000
<|endoftext|>"
},
{
"prompt": """"Derive a permuted hetnet from an input hetnet. This method applies the
XSwap algorithm separately for each metaedge. Hence, node degree is
preserved for each type of edge. However, edges are randomized / shuffled.

Users are recommended to interrogate the reported statistics to ensure that
edges appear to be sufficiently randomized. Primarily, the number of edges
of a given metaedge that remain unchanged from the original hetnet should
have reached an assymptote. If the number of unchanged edges has not yet
stabalized, further randomization is possible with this approach.

Parameters
----------
graph : hetnetpy.hetnet.Graph
    Input hetnet to create a permuted derivative from
multiplier : int or float
    This is multiplied by the number of edges for each metaedge to
    determine the number of swaps to attempt.
seed : int
    Seed to initialize Python random number generator. When creating many
    permuted hetnets, it's recommended to increment this number, such that
    each round of permutation shuffles edges in a different order.
metaedge_to_excluded : dict (metaedge -> set)
    Edges to exclude. This argument has not been extensively used in
    practice.
log : bool
    Whether to log diagnostic INFO via python's logging module.

Returns
-------
permuted_graph : hetnetpy.hetnet.Graph
    A permuted hetnet derived from the input graph.
stats : list of dicts
    A list where each item is a dictionary of permutation statistics at a
    checkpoint for a specific metaedge. These statistics allow tracking the
    progress of the permutation as the number of attempted swaps increases."""
fn permute_graph(graph: hetnetpy.hetnet.Graph, multiplier: int or float, seed: int, metaedge_to_excluded: dict (metaedge -> set), log: bool) -> list of dicts

"""Permute edges (of a single type) in a graph according to the XSwap function
described in https://doi.org/f3mn58. This method selects two edges and
attempts to swap their endpoints. If the swap would result in a valid edge,
the swap proceeds. Otherwise, the swap is skipped. The end result is that
node degree is preserved, but edges are shuffled, thereby losing their
original meaning.

Parameters
----------
pair_list : list of tuples
    List of edges to permute. Each edge is represented as a (source,
    target) tuple. source and target represent nodes and can be any Python
    objects that define __eq__. In other words, this function does not
    assume any specific format for nodes. If the edges are from a bipartite
    or directed graph, then all tuples must have the same alignment. For
    example, if the edges represent the bipartite Compound-binds-Gene
    relationship, all tuples should be of the form (compound, gene) and not
    intermixed with (gene, compound) tuples. The only instance where order
    of the source and target is not important is for an undirected edge
    type where the source and target nodes are of the same type, such as
    Gene-interacts-Gene.
directed : bool
    Whether the edge should be considered directed. If False, a swap that
    creates an a-b edge will be invalid if a b-a edge already exists.
multiplier : int or float
    This is multiplied by the number of edges in pair_list to determine the
    number of swaps to attempt.
excluded_pair_set : set of tuples:
    Set of possible edges to forbid. If a swap would create an edge in this
    set, it would be considered invalid and hence skipped.
seed : int
    Seed to initialize Python random number generator.
log : bool
    Whether to log diagnostic INFO via python's logging module.
inplace : bool
    Whether to modify the edge list in place.

Returns
-------
pair_list : list of tuples
    The permuted edges, derived from the input pair_list.
stats : list of dicts
    A list where each item is a dictionary of permutation statistics at a
    checkpoint. Statistics are collected at 10 checkpoints, spaced evenly
    by the number of attempts."""
fn permute_pair_list(pair_list: list of tuples, directed: bool, multiplier: int or float, excluded_pair_set: set of tuples:, seed: int, log: bool, inplace: bool) -> list of dicts

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

import hetnetpy.permute


@pytest.mark.parametrize(
    "edges,inplace",
    [
        ([(0, 0), (1, 1), (1, 2), (2, 3)], True),
        ([(0, 0), (1, 1), (1, 2), (2, 3)], False),
    ],
)
def test_permute_inplace(edges, inplace):
    old_edges = edges.copy()
    new_edges, stats = hetnetpy.permute.permute_pair_list(edges, inplace=inplace)
    assert old_edges != new_edges

    if inplace:
        assert edges == new_edges
    else:
        assert edges != new_edges
<|endoftext|>"
},
{
"prompt": "fn _fix_subparsers(subparsers)

class DeepBGCParser(argparse.ArgumentParser):
	fn parse_args(self, args, namespace)
	fn error(self, message)


fn run(argv)

fn main(argv)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from deepbgc.main import run
import pytest


def test_unit_main_help():
    with pytest.raises(SystemExit) as excinfo:
        run(['--help'])
    assert excinfo.value.code == 0


def test_unit_pipeline_help():
    with pytest.raises(SystemExit) as excinfo:
        run(['pipeline', '--help'])
    assert excinfo.value.code == 0


def test_unit_prepare_help():
    with pytest.raises(SystemExit) as excinfo:
        run(['prepare', '--help'])
    assert excinfo.value.code == 0


def test_unit_train_help():
    with pytest.raises(SystemExit) as excinfo:
        run(['train', '--help'])
    assert excinfo.value.code == 0


def test_unit_main_invalid_command():
    with pytest.raises(SystemExit) as excinfo:
        run(['invalid'])
    assert excinfo.value.code == 2<|endoftext|>"
},
{
"prompt": "fn cleanup_third_party_modules_output()

fn get_output_filenames()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Copyright 1999-2021 Alibaba Group Holding Ltd.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import pytest

from ....tests.core import require_ray
from ....utils import lazy_import
from ..ray import new_cluster
from ..tests import test_local
from .modules.utils import (  # noqa: F401; pylint: disable=unused-variable
    cleanup_third_party_modules_output,
    get_output_filenames,
)

ray = lazy_import("ray")


@pytest.fixture
async def speculative_cluster():
    client = await new_cluster(
        "test_cluster",
        worker_num=5,
        worker_cpu=2,
        worker_mem=512 * 1024**2,
        supervisor_mem=100 * 1024**2,
        config={
            # coloring based fusion will make subtask too heterogeneous such that the speculative scheduler can't
            # get enough homogeneous subtasks to calculate statistics
            "task": {"default_config": {"fuse_enabled": False}},
            "scheduling": {
                "speculation": {
                    "enabled": True,
                    "interval": 0.5,
                    "threshold": 0.2,
                    "min_task_runtime": 2,
                    "multiplier": 1.5,
                },
                # used to kill hanged subtask to release slot.
                "subtask_cancel_timeout": 0.1,
            },
        },
    )
    async with client:
        yield client


@pytest.mark.parametrize("ray_large_cluster", [{"num_nodes": 2}], indirect=True)
@pytest.mark.timeout(timeout=1000)
@require_ray
@pytest.mark.asyncio
async def test_task_speculation_execution(ray_large_cluster, speculative_cluster):
    await test_local.test_task_speculation_execution(speculative_cluster)
<|endoftext|>"
},
{
"prompt": """"Base class for profile event handler hparams."""
class ProfilerEventHandlerHparams(hp.Hparams, abc.ABC):
	"""Constructs and returns an instance of the :class:`ProfilerEventHandler`.
	
	Returns:
	    ProfilerEventHandler: The event handler."""
	fn initialize_object(self) -> ProfilerEventHandler


""":class:`.JSONTraceHandler` hyperparameters.

See :class:`.JSONTraceHandler` for documentation.

Example usage with :class:`.TrainerHparams`\:

.. code-block:: yaml

    prof_event_handlers:
        - json:
            flush_every_n_batches: 100
            buffering: -1
            output_directory: profiler_traces"""
class JSONTraceHandlerHparams(ProfilerEventHandlerHparams):
	flush_every_n_batches: int = hp.optional('Interval at which to flush the logfile.', default=100)
	buffering: int = hp.optional('Buffering parameter passed to :meth:`open` when opening the logfile.', default=-1)
	output_directory: str = hp.optional('Directory, relative to the run directory, to store traces.', default='composer_profiler')
	fn initialize_object(self) -> JSONTraceHandler


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "# Copyright 2021 MosaicML. All Rights Reserved.

import json
import os
import pathlib

import pytest

from composer.profiler.profiler_hparams import CyclicProfilerScheduleHparams, JSONTraceHparams
from composer.trainer import TrainerHparams


@pytest.mark.timeout(10)
def test_json_trace_profiler_handler(composer_trainer_hparams: TrainerHparams, tmpdir: pathlib.Path):
    profiler_file = os.path.join(tmpdir, 'trace.json')
    json_trace_handler_params = JSONTraceHparams(folder=str(tmpdir), merged_trace_filename='trace.json')

    composer_trainer_hparams.prof_trace_handlers = [json_trace_handler_params]
    composer_trainer_hparams.prof_schedule = CyclicProfilerScheduleHparams(
        wait=0,
        warmup=0,
        active=1000,
        repeat=0,
    )
    composer_trainer_hparams.max_duration = "2ep"
    composer_trainer_hparams.sys_prof_cpu = False
    composer_trainer_hparams.sys_prof_net = False
    composer_trainer_hparams.sys_prof_disk = False
    composer_trainer_hparams.sys_prof_memory = False

    trainer = composer_trainer_hparams.initialize_object()
    trainer.fit()

    with open(profiler_file, "r") as f:
        trace_json = json.load(f)
        has_epoch_start_event = False
        has_epoch_end_event = False
        for event in trace_json:
            if event["name"] == "event/epoch" and event["ph"] == "B":
                has_epoch_start_event = True
            if event["name"] == "event/epoch" and event["ph"] == "E":
                has_epoch_end_event = True
        assert has_epoch_start_event
        assert has_epoch_end_event
<|endoftext|>"
},
{
"prompt": "fn max_atoms()

fn num_data()

fn property_spec()

fn empty_asedata(tmpdir, max_atoms, property_spec)

fn example_data(max_atoms, num_data)

fn example_asedata(tmpdir, max_atoms, property_spec, example_data)

fn test_add_and_read(empty_asedata, example_data)

fn test_empty_subset_of_subset(empty_asedata, example_data)

fn partition_names(request)

fn test_merging(tmpdir, example_asedata, partition_names)

fn batch_size(request)

fn test_loader(example_asedata, batch_size)

fn test_dataset(qm9_dbpath, qm9_avlailable_properties)

fn test_extension_check()

fn h2o()

fn o2()

fn test_get_center(h2o, o2)

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

import schnetpack.data
import schnetpack.representation.hdnn as rep
from .test_data import batch_size


@pytest.mark.usefixtures("example_asedata", "batch_size")
def test_triples_exception(example_asedata, batch_size):
    loader = schnetpack.data.AtomsLoader(example_asedata, batch_size)

    reps = rep.BehlerSFBlock(n_radial=22, n_angular=5, elements=frozenset(range(100)))

    for batch in loader:
        with pytest.raises(rep.HDNNException):
            reps(batch)
        break
<|endoftext|>"
},
{
"prompt": "fn compare_arrays(arr, res, precision)

fn check_encode_decode(arr, codec, precision)

fn check_encode_decode_partial(arr, codec, precision)

fn assert_array_items_equal(res, arr)

fn check_encode_decode_array(arr, codec)

fn check_config(codec)

fn check_repr(stmt)

fn check_backwards_compatibility(codec_id, arrays, codecs, precision, prefix)

fn check_err_decode_object_buffer(compressor)

fn check_err_encode_object_buffer(compressor)

fn check_max_buffer_size(codec)

"""Codec providing gzip compression using zlib via the Python standard library.

Parameters
----------
level : int
    Compression level."""
class GZip(Codec):
	codec_id = 'gzip'
	fn __init__(self, level)
	fn encode(self, buf)
	fn decode(self, buf, out)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import itertools


import numpy as np
import pytest


from numcodecs.gzip import GZip
from numcodecs.tests.common import (check_encode_decode, check_config, check_repr,
                                    check_backwards_compatibility,
                                    check_err_decode_object_buffer,
                                    check_err_encode_object_buffer)


codecs = [
    GZip(),
    GZip(level=-1),
    GZip(level=0),
    GZip(level=1),
    GZip(level=5),
    GZip(level=9),
]


# mix of dtypes: integer, float, bool, string
# mix of shapes: 1D, 2D, 3D
# mix of orders: C, F
arrays = [
    np.arange(1000, dtype='i4'),
    np.linspace(1000, 1001, 1000, dtype='f8'),
    np.random.normal(loc=1000, scale=1, size=(100, 10)),
    np.random.randint(0, 2, size=1000, dtype=bool).reshape(100, 10, order='F'),
    np.random.choice([b'a', b'bb', b'ccc'], size=1000).reshape(10, 10, 10),
    np.random.randint(0, 2**60, size=1000, dtype='u8').view('M8[ns]'),
    np.random.randint(0, 2**60, size=1000, dtype='u8').view('m8[ns]'),
    np.random.randint(0, 2**25, size=1000, dtype='u8').view('M8[m]'),
    np.random.randint(0, 2**25, size=1000, dtype='u8').view('m8[m]'),
    np.random.randint(-2**63, -2**63 + 20, size=1000, dtype='i8').view('M8[ns]'),
    np.random.randint(-2**63, -2**63 + 20, size=1000, dtype='i8').view('m8[ns]'),
    np.random.randint(-2**63, -2**63 + 20, size=1000, dtype='i8').view('M8[m]'),
    np.random.randint(-2**63, -2**63 + 20, size=1000, dtype='i8').view('m8[m]'),
]


def test_encode_decode():
    for arr, codec in itertools.product(arrays, codecs):
        check_encode_decode(arr, codec)


def test_config():
    codec = GZip(level=3)
    check_config(codec)


def test_repr():
    check_repr("GZip(level=3)")


def test_eq():
    assert GZip() == GZip()
    assert not GZip() != GZip()
    assert GZip(1) == GZip(1)
    assert GZip(1) != GZip(9)
    assert GZip() != 'foo'
    assert 'foo' != GZip()
    assert not GZip() == 'foo'


def test_backwards_compatibility():
    check_backwards_compatibility(GZip.codec_id, arrays, codecs)


def test_err_decode_object_buffer():
    check_err_decode_object_buffer(GZip())


def test_err_encode_object_buffer():
    check_err_encode_object_buffer(GZip())


def test_err_encode_list():
    data = ['foo', 'bar', 'baz']
    for codec in codecs:
        with pytest.raises(TypeError):
            codec.encode(data)


def test_err_encode_non_contiguous():
    # non-contiguous memory
    arr = np.arange(1000, dtype='i4')[::2]
    for codec in codecs:
        with pytest.raises(ValueError):
            codec.encode(arr)


def test_err_out_too_small():
    arr = np.arange(10, dtype='i4')
    out = np.empty_like(arr)[:-1]
    for codec in codecs:
        with pytest.raises(ValueError):
            codec.decode(codec.encode(arr), out)


def test_out_too_large():
    out = np.empty((10,), dtype='i4')
    arr = out[:-1]
    arr[:] = 5
    for codec in codecs:
        codec.decode(codec.encode(arr), out)
<|endoftext|>"
},
{
"prompt": "fn test_get_seeds_different_under_mp_pool()

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from bw2calc.errors import MultipleValues
from matrix_utils.errors import AllArraysEmpty
from bw2calc.single_value_diagonal_matrix import SingleValueDiagonalMatrix as SVDM
from bw2calc.utils import get_datapackage
from pathlib import Path
import numpy as np
import pytest

fixture_dir = Path(__file__).resolve().parent / "fixtures"


def test_svdm_missing_dimension_kwarg():
    with pytest.raises(TypeError):
        SVDM(packages=["something"], matrix="something")


def test_svdm_no_data():
    with pytest.raises(AllArraysEmpty):
        SVDM(
            packages=[get_datapackage(fixture_dir / "basic_fixture.zip")],
            matrix="weighting_matrix",
            dimension=5,
        )


def test_svdm_multiple_dataresources():
    with pytest.raises(MultipleValues):
        SVDM(
            packages=[
                get_datapackage(fixture_dir / "svdm.zip"),
                get_datapackage(fixture_dir / "svdm2.zip"),
            ],
            matrix="weighting_matrix",
            dimension=500,
        )


def test_svdm_multiple_values():
    with pytest.raises(MultipleValues):
        SVDM(
            packages=[get_datapackage(fixture_dir / "basic_fixture.zip")],
            matrix="technosphere_matrix",
            dimension=500,
        )


def test_svdm_basic():
    obj = SVDM(
        packages=[
            get_datapackage(fixture_dir / "svdm.zip"),
        ],
        matrix="weighting_matrix",
        use_arrays=False,
        dimension=500,
    )
    assert obj.matrix.shape == (500, 500)
    assert obj.matrix.sum() == 500 * 42
    assert np.allclose(obj.matrix.tocoo().row, np.arange(500))
    assert np.allclose(obj.matrix.tocoo().col, np.arange(500))


def test_svdm_iteration():
    obj = SVDM(
        packages=[
            get_datapackage(fixture_dir / "svdm.zip"),
        ],
        matrix="weighting_matrix",
        use_arrays=False,
        dimension=500,
    )
    assert obj.matrix.shape == (500, 500)
    assert obj.matrix.sum() == 500 * 42
    next(obj)
    assert obj.matrix.shape == (500, 500)
    assert obj.matrix.sum() == 500 * 42


def test_svdm_distributions():
    obj = SVDM(
        packages=[
            get_datapackage(fixture_dir / "svdm.zip"),
        ],
        matrix="weighting_matrix",
        use_distributions=True,
        use_arrays=False,
        dimension=500,
    )
    assert obj.matrix.shape == (500, 500)
    total = obj.matrix.sum()
    assert total
    next(obj)
    assert total != obj.matrix.sum()
    next(obj)
    assert total != obj.matrix.sum()


def test_svdm_arrays():
    obj = SVDM(
        packages=[
            get_datapackage(fixture_dir / "svdm.zip"),
        ],
        matrix="weighting_matrix",
        use_vectors=False,
        use_arrays=True,
        dimension=500,
    )
    assert obj.matrix.shape == (500, 500)
    assert obj.matrix.sum() == 500 * 1
    next(obj)
    assert obj.matrix.shape == (500, 500)
    assert obj.matrix.sum() == 500 * 2
    next(obj)
    assert obj.matrix.shape == (500, 500)
    assert obj.matrix.sum() == 500 * 3
<|endoftext|>"
},
{
"prompt": "class OnlineStandardizingLayer(nn.Module):
	"""Accumulate mean and variance online using the "parallel algorithm" algorithm from [1].
	
	Args:
	    shape: shape of mean, variance, and std array. do not include batch dimension!
	    stable: (optional) compute using the stable version of the algorithm [1]
	    epsilon: (optional) added to the computation of the standard deviation for numerical stability.
	    use_average_std: (optional) ``True`` to normalize using std averaged over the whole observation, ``False`` to normalize using std of each component of the observation.
	
	References:
	    [1] https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm"""
	fn __init__(self, shape: Tuple[(int, ...)], stable: bool, epsilon: float, use_average_std: bool) -> None
	fn _parallel_algorithm(self, x: torch.Tensor) -> Tuple[(torch.Tensor, torch.Tensor, torch.Tensor)]
	fn forward(self, x: torch.Tensor) -> torch.Tensor
	fn mean(self) -> torch.Tensor
	fn var(self) -> torch.Tensor
	fn std(self) -> torch.Tensor


class OnlineDictStandardizingLayer(nn.Module):
	fn __init__(self, shapes: Dict[(Hashable, Tuple[(int, ...)])], stable: bool, epsilon: float, use_average_std: bool) -> None
	fn forward(self, x: Dict[(Hashable, torch.Tensor)]) -> torch.Tensor


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from itertools import product

import pytest
import torch

from swyft.networks.standardization import OnlineStandardizingLayer


class TestStandardizationLayer:
    bss = [1, 128]
    shapes = [(1,), (5,), (10, 5, 2, 1), (1, 1, 1)]
    means = [2, 139]
    stds = [1, 80]
    stables = [True, False]

    @pytest.mark.parametrize(
        "bs, shape, mean, std, stable", product(bss, shapes, means, stds, stables)
    )
    def test_online_standardization_layer_update(self, bs, shape, mean, std, stable):
        osl = OnlineStandardizingLayer(shape, stable=stable)
        osl.train()
        old_stats = osl.n.clone(), osl.mean.clone(), osl.var.clone(), osl.std.clone()

        data = torch.randn(bs, *shape) * std + mean
        _ = osl(data)

        new_stats = osl.n.clone(), osl._mean.clone(), osl.var.clone(), osl.std.clone()

        assert old_stats != new_stats

    @pytest.mark.parametrize(
        "bs, shape, mean, std, stable", product(bss, shapes, means, stds, stables)
    )
    def test_online_standardization_layer_mean(self, bs, shape, mean, std, stable):
        torch.manual_seed(0)

        osl = OnlineStandardizingLayer(shape, stable=stable)
        osl.train()

        data = torch.randn(bs, *shape) * std + mean
        _ = osl(data)

        assert torch.allclose(osl.mean, data.mean(0))

    @pytest.mark.parametrize(
        "bs, shape, mean, std, stable", product(bss, shapes, means, stds, stables)
    )
    def test_online_standardization_layer_std(self, bs, shape, mean, std, stable):
        torch.manual_seed(0)

        osl = OnlineStandardizingLayer(shape, stable=stable)
        osl.train()

        data = torch.randn(bs, *shape) * std + mean
        _ = osl(data)

        if torch.isnan(data.std(0)).all():
            replacement_std = torch.sqrt(torch.ones_like(osl.std) * osl.epsilon)
            assert torch.allclose(osl.std, replacement_std)
        else:
            assert torch.allclose(osl.std, data.std(0))

    @pytest.mark.parametrize(
        "bs, shape, mean, std, stable", product(bss, shapes, means, stds, stables)
    )
    def test_online_standardization_layer_std_average(
        self, bs, shape, mean, std, stable
    ):
        torch.manual_seed(0)

        osl = OnlineStandardizingLayer(shape, stable=stable, use_average_std=True)
        osl.train()

        data = torch.randn(bs, *shape) * std + mean
        _ = osl(data)

        if torch.isnan(data.std(0)).all():
            replacement_std = torch.sqrt(torch.ones_like(osl.std) * osl.epsilon)
            assert torch.allclose(osl.std, replacement_std.mean())
        else:
            assert torch.allclose(osl.std, data.std(0).mean())


if __name__ == "__main__":
    pass
<|endoftext|>"
},
{
"prompt": "class LockfileCorruptedError(DvcException):
	fn __init__(self, path)


fn is_valid_filename(path)

fn is_dvc_file(path)

fn check_dvc_filename(path)

class FileMixin:
	SCHEMA = None
	fn __init__(self, repo, path)
	fn __repr__(self)
	fn __hash__(self)
	fn __eq__(self, other)
	fn __str__(self)
	fn relpath(self)
	fn exists(self)
	fn _load(self)
	fn validate(cls, d, fname)
	fn remove_with_prompt(self, force)
	fn remove(self, force)
	fn dump(self, stage)


class SingleStageFile(FileMixin):
	fn __init__(self, repo, path)
	fn stage(self)
	fn stages(self)
	"""Dumps given stage appropriately in the dvcfile."""
	fn dump(self, stage)
	fn remove_with_prompt(self, force)


"""Abstraction for pipelines file, .yaml + .lock combined."""
class PipelineFile(FileMixin):
	fn _lockfile(self)
	"""Dumps given stage appropriately in the dvcfile."""
	fn dump(self, stage, update_pipeline)
	fn _dump_lockfile(self, stage)
	fn _dump_pipeline_file(self, stage)
	fn stage(self)
	fn stages(self)
	fn remove(self, force)


class Lockfile(FileMixin):
	fn load(self)
	fn dump(self, stage)


class Dvcfile:
	fn __new__(cls, repo, path)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from dvc.stage import PipelineStage
from dvc.dvcfile import Lockfile, LockfileCorruptedError
import yaml
import pytest


def test_stage_dump_no_outs_deps(tmp_dir, dvc):
    stage = PipelineStage(name="s1", repo=dvc, path="path", cmd="command")
    lockfile = Lockfile(dvc, "path.lock")
    lockfile.dump(stage)
    assert lockfile.load() == {"s1": {"cmd": "command"}}


def test_stage_dump_when_already_exists(tmp_dir, dvc):
    data = {"s1": {"cmd": "command", "deps": [], "outs": []}}
    with open("path.lock", "w+") as f:
        yaml.dump(data, f)
    stage = PipelineStage(name="s2", repo=dvc, path="path", cmd="command2")
    lockfile = Lockfile(dvc, "path.lock")
    lockfile.dump(stage)
    assert lockfile.load() == {
        **data,
        "s2": {"cmd": "command2"},
    }


def test_stage_dump_with_deps_and_outs(tmp_dir, dvc):
    data = {
        "s1": {
            "cmd": "command",
            "deps": [{"md5": "1.txt", "path": "checksum"}],
            "outs": [{"md5": "2.txt", "path": "checksum"}],
        }
    }
    with open("path.lock", "w+") as f:
        yaml.dump(data, f)

    lockfile = Lockfile(dvc, "path.lock")
    stage = PipelineStage(name="s2", repo=dvc, path="path", cmd="command2")
    lockfile.dump(stage)
    assert lockfile.load() == {
        **data,
        "s2": {"cmd": "command2"},
    }


def test_stage_overwrites_if_already_exists(tmp_dir, dvc):
    lockfile = Lockfile(dvc, "path.lock",)
    stage = PipelineStage(name="s2", repo=dvc, path="path", cmd="command2")
    lockfile.dump(stage)
    stage = PipelineStage(name="s2", repo=dvc, path="path", cmd="command3")
    lockfile.dump(stage)
    assert lockfile.load() == {
        "s2": {"cmd": "command3"},
    }


def test_load_when_lockfile_does_not_exist(tmp_dir, dvc):
    assert {} == Lockfile(dvc, "pipelines.lock").load()


@pytest.mark.parametrize(
    "corrupt_data",
    [
        {"s1": {"outs": []}},
        {"s1": {}},
        {
            "s1": {
                "cmd": "command",
                "outs": [
                    {"md5": "checksum", "path": "path", "random": "value"}
                ],
            }
        },
        {"s1": {"cmd": "command", "deps": [{"md5": "checksum"}]}},
    ],
)
def test_load_when_lockfile_is_corrupted(tmp_dir, dvc, corrupt_data):
    with open("Dvcfile.lock", "w+") as f:
        yaml.dump(corrupt_data, f)
    lockfile = Lockfile(dvc, "Dvcfile.lock")
    with pytest.raises(LockfileCorruptedError) as exc_info:
        lockfile.load()
    assert "Dvcfile.lock" in str(exc_info.value)
<|endoftext|>"
},
{
"prompt": "class TestUtils:
	fn save_and_load(sol, ocp, test_solve_of_loaded)
	fn deep_assert(first_elem, second_elem)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """"
Test for file IO
"""
import importlib.util
from pathlib import Path

import pytest
import numpy as np

from biorbd_optim import Data, OdeSolver
from .utils import TestUtils

# Load align_segment_on_rt
PROJECT_FOLDER = Path(__file__).parent / ".."
spec = importlib.util.spec_from_file_location(
    "align_segment_on_rt", str(PROJECT_FOLDER) + "/examples/align/align_segment_on_rt.py"
)
align_segment_on_rt = importlib.util.module_from_spec(spec)
spec.loader.exec_module(align_segment_on_rt)


@pytest.mark.parametrize("ode_solver", [OdeSolver.RK])
def test_align_segment_on_rt(ode_solver):
    ocp = align_segment_on_rt.prepare_ocp(
        biorbd_model_path=str(PROJECT_FOLDER) + "/examples/align/cube_and_line.bioMod",
        final_time=0.5,
        number_shooting_points=8,
        ode_solver=ode_solver,
    )
    sol = ocp.solve()

    # Check objective function value
    f = np.array(sol["f"])
    np.testing.assert_equal(f.shape, (1, 1))
    np.testing.assert_almost_equal(f[0, 0], 12320.059717265229)

    # Check constraints
    g = np.array(sol["g"])
    np.testing.assert_equal(g.shape, (91, 1))
    np.testing.assert_almost_equal(g, np.zeros((91, 1)))

    # Check some of the results
    states, controls = Data.get_data(ocp, sol["x"])
    q, qdot, tau = states["q"], states["q_dot"], controls["tau"]

    # initial and final position
    np.testing.assert_almost_equal(q[:, 0], np.array([0.305837645, 6.07684988e-18, -1.57, -1.57]))
    np.testing.assert_almost_equal(q[:, -1], np.array([0.305837645, 2.34331392e-17, 1.57, 1.57]))
    # initial and final velocities
    np.testing.assert_almost_equal(qdot[:, 0], np.array((0, 0, 0, 0)))
    np.testing.assert_almost_equal(qdot[:, -1], np.array((0, 0, 0, 0)))
    # initial and final controls
    np.testing.assert_almost_equal(tau[:, 0], np.array([1.09038782e-23, 9.81, 66.9866667, 66.9866667]))
    np.testing.assert_almost_equal(tau[:, -1], np.array([-1.61910771e-23, 9.81, -66.9866667, -66.9866667]))

    # save and load
    TestUtils.save_and_load(sol, ocp, False)


# Load align_marker_on_segment
PROJECT_FOLDER = Path(__file__).parent / ".."
spec = importlib.util.spec_from_file_location(
    "align_marker_on_segment", str(PROJECT_FOLDER) + "/examples/align/align_marker_on_segment.py"
)
align_marker_on_segment = importlib.util.module_from_spec(spec)
spec.loader.exec_module(align_marker_on_segment)


@pytest.mark.parametrize("ode_solver", [OdeSolver.RK])
def test_align_marker_on_segment(ode_solver):
    ocp = align_marker_on_segment.prepare_ocp(
        biorbd_model_path=str(PROJECT_FOLDER) + "/examples/align/cube_and_line.bioMod",
        final_time=0.5,
        number_shooting_points=8,
        ode_solver=ode_solver,
        initialize_near_solution=True,
    )
    sol = ocp.solve()

    # Check objective function value
    f = np.array(sol["f"])
    np.testing.assert_equal(f.shape, (1, 1))
    np.testing.assert_almost_equal(f[0, 0], 2632.9408063562532)

    # Check constraints
    g = np.array(sol["g"])
    np.testing.assert_equal(g.shape, (88, 1))
    np.testing.assert_almost_equal(g, np.zeros((88, 1)))

    # Check some of the results
    states, controls = Data.get_data(ocp, sol["x"])
    q, qdot, tau = states["q"], states["q_dot"], controls["tau"]

    # initial and final position
    np.testing.assert_almost_equal(q[:, 0], np.array([1, 0, 0, 0.46364761]))
    np.testing.assert_almost_equal(q[:, -1], np.array([2, 0, 1.57, 0.78539785]))
    # initial and final velocities
    np.testing.assert_almost_equal(qdot[:, 0], np.array((0, 0, 0, 0)))
    np.testing.assert_almost_equal(qdot[:, -1], np.array((0, 0, 0, 0)))
    # initial and final controls
    np.testing.assert_almost_equal(tau[:, 0], np.array([23.6216484, 12.25908094, 31.52069129, 12.94722946]))
    np.testing.assert_almost_equal(tau[:, -1], np.array([-16.65951028, 14.58725991, -36.10090439, 4.41782565]))

    # save and load
    TestUtils.save_and_load(sol, ocp, False)
<|endoftext|>"
},
{
"prompt": """"Set up a component and all its dependencies."""
fn setup_component(opp: core.OpenPeerPower, domain: str, config: Dict) -> bool

"""Call a method when a component is setup."""
fn async_when_setup(opp: core.OpenPeerPower, component: str, when_setup_cb: Callable[([core.OpenPeerPower, str], Awaitable[None])]) -> None

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """"Test the default_config init."""
from unittest.mock import patch

import pytest

from openpeerpower.setup import async_setup_component

from tests.common import MockDependency, mock_coro


@pytest.fixture(autouse=True)
def zeroconf_mock():
    """Mock zeroconf."""
    with MockDependency("zeroconf") as mocked_zeroconf:
        mocked_zeroconf.Zeroconf.return_value.register_service.return_value = mock_coro(
            True
        )
        yield


@pytest.fixture(autouse=True)
def netdisco_mock():
    """Mock netdisco."""
    with MockDependency("netdisco", "discovery"):
        yield


@pytest.fixture(autouse=True)
def recorder_url_mock():
    """Mock recorder url."""
    with patch("openpeerpower.components.recorder.DEFAULT_URL", "sqlite://"):
        yield


async def test_setup(opp):
    """Test setup."""
    assert await async_setup_component(opp, "default_config", {})
<|endoftext|>"
},
{
"prompt": """"Handle encrypted files for tarfile library."""
class SecureTarFile:
	"""Initialize encryption handler."""
	fn __init__(self, name: Path, mode: str, key: Optional[bytes], gzip: bool) -> None
	"""Start context manager tarfile."""
	fn __enter__(self) -> tarfile.TarFile
	"""Close file."""
	fn __exit__(self, exc_type, exc_value, traceback) -> None
	"""Write data."""
	fn write(self, data: bytes) -> None
	"""Read data."""
	fn read(self, size: int) -> bytes
	"""Return path object of tarfile."""
	fn path(self) -> Path
	"""Return backup size."""
	fn size(self) -> float


"""Generate an iv from data."""
fn _generate_iv(key: bytes, salt: bytes) -> bytes

"""Security safe check of path.

Prevent ../ or absolut paths"""
fn secure_path(tar: tarfile.TarFile) -> Generator[(tarfile.TarInfo, None, None)]

"""Filter to filter excludes."""
fn _is_excluded_by_filter(path: PurePath, exclude_list: list[str]) -> bool

"""Append directories and/or files to the TarFile if excludes wont filter."""
fn atomic_contents_add(tar_file: tarfile.TarFile, origin_path: Path, excludes: list[str], arcname: str) -> None

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """"Test Tarfile functions."""

import attr
import pytest

from supervisor.utils.tar import secure_path, exclude_filter


@attr.s
class TarInfo:
    """Fake TarInfo"""

    name: str = attr.ib()


def test_secure_path():
    """Test Secure Path."""
    test_list = [
        TarInfo("test.txt"),
        TarInfo("data/xy.blob"),
        TarInfo("bla/blu/ble"),
        TarInfo("data/../xy.blob"),
    ]
    assert test_list == list(secure_path(test_list))


def test_not_secure_path():
    """Test Not secure path."""
    test_list = [
        TarInfo("/test.txt"),
        TarInfo("data/../../xy.blob"),
        TarInfo("/bla/blu/ble"),
    ]
    assert [] == list(secure_path(test_list))


def test_exclude_filter_good():
    """Test exclude filter."""
    filter_funct = exclude_filter(["not/match", "/dev/xy"])
    test_list = [
        TarInfo("test.txt"),
        TarInfo("data/xy.blob"),
        TarInfo("bla/blu/ble"),
        TarInfo("data/../xy.blob"),
    ]

    assert test_list == [filter_funct(result) for result in test_list]


def test_exclude_filter_bad():
    """Test exclude filter."""
    filter_funct = exclude_filter(["*.txt", "data/*", "bla/blu/ble"])
    test_list = [
        TarInfo("test.txt"),
        TarInfo("data/xy.blob"),
        TarInfo("bla/blu/ble"),
        TarInfo("data/test_files/kk.txt"),
    ]

    for info in [filter_funct(result) for result in test_list]:
        assert info is None
<|endoftext|>"
},
{
"prompt": "

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from urllib import urlencode

import pytest
from mock import patch

from grouper.constants import USER_METADATA_SHELL_KEY
from grouper.models.user import User
from grouper.user_metadata import get_user_metadata_by_key
from tests.fixtures import (  # noqa: F401
    fe_app as app,
    graph,
    groups,
    permissions,
    service_accounts,
    session,
    standard_graph,
    users,
)
from tests.url_util import url


@pytest.mark.gen_test
def test_shell(session, users, http_client, base_url):  # noqa: F811
    with patch("grouper.fe.handlers.user_shell.settings") as mock_settings:
        mock_settings.shell = [["/bin/bash", "bash"], ["/bin/zsh", "zsh"]]

        user = users["zorkian@a.co"]
        assert not get_user_metadata_by_key(session, user.id, USER_METADATA_SHELL_KEY)

        user = User.get(session, name=user.username)
        fe_url = url(base_url, "/users/{}/shell".format(user.username))
        resp = yield http_client.fetch(
            fe_url,
            method="POST",
            body=urlencode({"shell": "/bin/bash"}),
            headers={"X-Grouper-User": user.username},
        )
        assert resp.code == 200

        user = User.get(session, name=user.username)

        assert (
            get_user_metadata_by_key(session, user.id, USER_METADATA_SHELL_KEY) is not None
        ), "The user should have shell metadata"
        assert (
            get_user_metadata_by_key(session, user.id, USER_METADATA_SHELL_KEY).data_value
            == "/bin/bash"
        )

        fe_url = url(base_url, "/users/{}/shell".format(user.username))
        resp = yield http_client.fetch(
            fe_url,
            method="POST",
            body=urlencode({"shell": "/bin/fish"}),
            headers={"X-Grouper-User": user.username},
        )
        assert resp.code == 200

        user = User.get(session, name=user.username)

        assert (
            get_user_metadata_by_key(session, user.id, USER_METADATA_SHELL_KEY) is not None
        ), "The user should have shell metadata"
        assert (
            get_user_metadata_by_key(session, user.id, USER_METADATA_SHELL_KEY).data_value
            == "/bin/bash"
        )

        fe_url = url(base_url, "/users/{}/shell".format(user.username))
        resp = yield http_client.fetch(
            fe_url,
            method="POST",
            body=urlencode({"shell": "/bin/zsh"}),
            headers={"X-Grouper-User": user.username},
        )
        assert resp.code == 200

        user = User.get(session, name=user.username)

        assert (
            get_user_metadata_by_key(session, user.id, USER_METADATA_SHELL_KEY) is not None
        ), "The user should have shell metadata"
        assert (
            get_user_metadata_by_key(session, user.id, USER_METADATA_SHELL_KEY).data_value
            == "/bin/zsh"
        )
<|endoftext|>"
},
{
"prompt": "fn read_file(filename: str) -> str

fn separate_comma_imports(xmldoc: lxml.etree.Element) -> str

fn remove_duplicated_imports(xmldoc: lxml.etree.Element) -> str

fn apply_import_ordering(xmldoc: lxml.etree.Element) -> str

fn fix_whitespace_after_imports(xmldoc: lxml.etree.Element) -> str

"""Performs a step of the transformation.

:param text file_contents: Contends of the cheetah template
:param function step: Function taking xmldoc and returning new contents
:returns: new contents of the file."""
fn perform_step(file_contents: str, step: Callable[([lxml.etree.Element], str)]) -> str

fn main(argv: Sequence[str] | None) -> int

### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import os.path

import pytest

from cheetah_lint.reorder_imports import main
from cheetah_lint.reorder_imports import perform_step
from cheetah_lint.reorder_imports import STEPS
from cheetah_lint.util import read_file


INPUT_DIRECTORY = 'tests/inputs'
OUTPUT_DIRECTORY = 'tests/outputs'


TESTS = tuple(
    template for template in os.listdir(INPUT_DIRECTORY)
    if not template.startswith('_')
)


def get_input_output(template):
    return tuple(
        read_file(os.path.join(base, template))
        for base in (INPUT_DIRECTORY, OUTPUT_DIRECTORY)
    )


@pytest.mark.parametrize('template', TESTS)
def test_integration_template(template):
    contents, expected = get_input_output(template)
    for step in STEPS:
        contents = perform_step(contents, step)
    assert expected == contents


@pytest.mark.parametrize('template', TESTS)
def test_integration_calls_main(tmpdir, template):
    contents, expected = get_input_output(template)
    template_path = os.path.join(tmpdir.strpath, 'tmp.tmpl')

    with open(template_path, 'w') as template_file:
        template_file.write(contents)

    main([template_path])

    end_contents = read_file(template_path)
    assert expected == end_contents
<|endoftext|>"
},
{
"prompt": """"URLValidator that also validates about: and chrome:// urls"""
class EnhancedURLValidator(URLValidator):
	fn __call__(self, value)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "from django.core.exceptions import ValidationError

import pytest

from fjord.base.validators import EnhancedURLValidator


class TestEnhancedURLValidator:
    def test_valid_urls(self):
        test_data = [
            'example.com',
            'example.com:80',
            'example.com:80/foo',
            'http://example.com',
            'http://example.com/foo',
            'http://example.com:80',
            'http://example.com:80/foo',
            'https://example.com',
            'https://example.com/foo',
            'https://example.com:80',
            'https://example.com:80/foo',
            'ftp://example.com',
            'about:mozilla',
            'chrome://foo',

            # Taken from Django's URLValidator test case data
            'http://www.djangoproject.com/',
            'http://localhost/',
            'http://example.com/',
            'http://www.example.com/',
            'http://www.example.com:8000/test'
            'http://valid-with-hyphens.com/',
            'http://subdomain.example.com/',
            'http://200.8.9.10/',
            'http://200.8.9.10:8000/test',
            'http://valid-----hyphens.com/',
            'http://example.com?something=value',
            'http://example.com/index.php?something=value&another=value2',
        ]

        validator = EnhancedURLValidator()

        for url in test_data:
            validator(url)

    def test_invalid_urls(self):
        test_data = [
            'foo',
            'http://',
            'http://example',
            'http://example.'
            'http://.com',
            'http://invalid-.com',
            'http://-invalid.com',
            'http://inv-.alid-.com',
            'http://inv-.-alid.com'
        ]

        validator = EnhancedURLValidator()

        for url in test_data:
            with pytest.raises(ValidationError):
                validator(url)
<|endoftext|>"
},
{
"prompt": "class Role(SurrogatePK, Model):
	__tablename__ = 'roles'
	name = Column(db.String(80), unique=True, nullable=False)
	user_id = ReferenceCol('users', nullable=True)
	user = relationship('User', backref='roles')
	fn __init__(self, name)
	fn __repr__(self)


class User(UserMixin, SurrogatePK, Model):
	__tablename__ = 'users'
	username = Column(db.String(80), unique=True, nullable=False)
	email = Column(db.String(80), unique=True, nullable=False)
	password = Column(db.String(128), nullable=True)
	created_at = Column(db.DateTime, nullable=False, default=dt.datetime.utcnow)
	first_name = Column(db.String(30), nullable=True)
	last_name = Column(db.String(30), nullable=True)
	active = Column(db.Boolean(), default=False)
	is_admin = Column(db.Boolean(), default=False)
	fn __init__(self, username, email, password)
	fn set_password(self, password)
	fn check_password(self, value)
	fn full_name(self)
	fn __repr__(self)
	fn get_auth_token()


"""An application factory, as explained here:
    http://flask.pocoo.org/docs/patterns/appfactories/

:param config_object: The configuration object to use."""
fn create_app(config_object: None)

fn register_extensions(app)

fn register_blueprints(app)

fn register_errorhandlers(app)

"""Mixin that adds convenience methods for CRUD (create, read, update, delete)
operations."""
class CRUDMixin(object):
	"""Create a new record and save it the database."""
	fn create(cls)
	"""Update specific fields of a record."""
	fn update(self, commit)
	"""Save the record."""
	fn save(self, commit)
	"""Remove the record from the database."""
	fn delete(self, commit)


"""Base model class that includes CRUD convenience methods."""
class Model(CRUDMixin, db.Model):
	__abstract__ = True


"""A mixin that adds a surrogate integer 'primary key' column named
``id`` to any declarative-mapped class."""
class SurrogatePK(object):
	__table_args__ = {'extend_existing': True}
	id = db.Column(db.Integer, primary_key=True)
	fn get_by_id(cls, id)


"""Column that adds primary key foreign key reference.

Usage: ::

    category_id = ReferenceCol('category')
    category = relationship('Category', backref='categories')"""
fn ReferenceCol(tablename, nullable, pk_name)

class Config(object):
	SECRET_KEY = os_env.get('CCFLASKTEST_SECRET', 'secret-key')
	APP_DIR = os.path.abspath(os.path.dirname(__file__))
	PROJECT_ROOT = os.path.abspath(os.path.join(APP_DIR, os.pardir))
	BCRYPT_LOG_ROUNDS = 13
	ASSETS_DEBUG = False
	DEBUG_TB_ENABLED = False
	DEBUG_TB_INTERCEPT_REDIRECTS = False
	CACHE_TYPE = 'simple'


"""Production configuration."""
class ProdConfig(Config):
	ENV = 'prod'
	DEBUG = False
	SQLALCHEMY_DATABASE_URI = 'postgresql://localhost/'
	DEBUG_TB_ENABLED = False


"""Development configuration."""
class DevConfig(Config):
	ENV = 'dev'
	DEBUG = True
	DB_NAME = 'dev.db'
	DB_PATH = os.path.join(Config.PROJECT_ROOT, DB_NAME)
	SQLALCHEMY_DATABASE_URI = 'sqlite:///{0}'.format(DB_PATH)
	DEBUG_TB_ENABLED = True
	ASSETS_DEBUG = True
	CACHE_TYPE = 'simple'


class TestConfig(Config):
	TESTING = True
	DEBUG = True
	SQLALCHEMY_DATABASE_URI = 'sqlite://'
	BCRYPT_LOG_ROUNDS = 1
	WTF_CSRF_ENABLED = False


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
from flask_script import Manager, Shell, Server
from flask_script.commands import Clean, ShowUrls
from flask_migrate import MigrateCommand
from flask.ext.login import login_required, make_secure_token, get_auth_token

from ccflasktest.app import create_app
from ccflasktest.user.models import User
from ccflasktest.settings import DevConfig, ProdConfig
from ccflasktest.database import db

if os.environ.get("CCFLASKTEST_ENV") == 'prod':
    app = create_app(ProdConfig)
else:
    app = create_app(DevConfig)

HERE = os.path.abspath(os.path.dirname(__file__))
TEST_PATH = os.path.join(HERE, 'tests')

manager = Manager(app)


def _make_context():
    """Return context dict for a shell session so you can access
    app, db, and the User model by default.
    """
    return {'app': app, 'db': db, 'User': User}


@manager.command
def test():
    """Run the tests."""
    import pytest
    exit_code = pytest.main([TEST_PATH, '--verbose'])
    return exit_code


@app.route('/token', methods=['POST'])
#  @login_required
def get_token():
    # this will make a secure token based on the current user's ID
    # cannot get a token without being first logged in...
    # maybe we want to accept username/password for every POST /token
    # and then validate the user.  If valid, then return token ?
    #  return make_secure_token(current_user.id)
    return get_auth_token()
    #  s = Serializer(app.config['SECRET_KEY'], expires_in=600)
    #  return s.dumps({'id': current_user.id})

manager.add_command('server', Server())
manager.add_command('shell', Shell(make_context=_make_context))
manager.add_command('db', MigrateCommand)
manager.add_command("urls", ShowUrls())
manager.add_command("clean", Clean())

if __name__ == '__main__':
    manager.run()
<|endoftext|>"
},
{
"prompt": """"Factory for the user model"""
class UserFactory(DjangoModelFactory):
	username = Faker('user_name')
	email = Faker('email')
	name = Faker('name')
	fn password(self, create: bool, extracted: Sequence[Any])


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import pytest

from index_auth_service.users.forms import UserCreationForm
from index_auth_service.users.tests.factories import UserFactory

pytestmark = pytest.mark.django_db


class TestUserCreationForm:
    def test_clean_username(self):
        # A user with proto_user params does not exist yet.
        proto_user = UserFactory.build()

        form = UserCreationForm(
            {
                "username": proto_user.username,
                "password1": proto_user._password,
                "password2": proto_user._password,
            }
        )

        assert form.is_valid()
        assert form.clean_username() == proto_user.username

        # Creating a user.
        form.save()

        # The user with proto_user params already exists,
        # hence cannot be created.
        form = UserCreationForm(
            {
                "username": proto_user.username,
                "password1": proto_user._password,
                "password2": proto_user._password,
            }
        )

        assert not form.is_valid()
        assert len(form.errors) == 1
        assert "username" in form.errors
<|endoftext|>"
},
{
"prompt": """"An event's location as a textual description and a list of places."""
class EventLocation:
	"""Initialize with the location description and a list of places."""
	fn __init__(self, description, places)
	"""Return the represention the event location."""
	fn __repr__(self)
	"""Return the string version of the event location, represented the
	event location solely by its description for compatibility with simple
	location strings."""
	fn __str__(self)
	"""Return true if this event location is equal the other. Otherwise
	return false. Used only for unit testing."""
	fn __eq__(self, other)
	"""Used for truth testing. Return true if the locatiion is non-blank;
	false otherwise."""
	fn __bool__(self)
	"""Make an event location from a list of places."""
	fn from_places(cls, places)
	"""Format a list of places as an English phrase."""
	fn format_place_list(places)


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": """"Test event locations."""


from meetup2xibo.updater.event_location import EventLocation
import pytest

SAMPLE_PLACES = ["Woodshop", "Classroom A"]
OTHER_PLACES = ["Conference Room 1"]


def test_repr():
    """Test that repr returns repr(description string)."""
    location = EventLocation("test", SAMPLE_PLACES)
    assert repr(location) == "EventLocation('test', ['Woodshop', 'Classroom A'])"

def test_str():
    """Test that str returns str(description string)."""
    location = EventLocation("test", SAMPLE_PLACES)
    assert str(location) == str("test")

def test_equals_same():
    """Test that two identical locations are equals."""
    location1 = EventLocation("test", SAMPLE_PLACES)
    location2 = EventLocation("test", SAMPLE_PLACES)
    assert location1 == location2

def test_equals_different_descriptions():
    """Test that two different locations are not equals."""
    location1 = EventLocation("test1", SAMPLE_PLACES)
    location2 = EventLocation("test2", SAMPLE_PLACES)
    assert location1 != location2

def test_equals_different_places():
    """Test that two different locations are not equals."""
    location1 = EventLocation("test", SAMPLE_PLACES)
    location2 = EventLocation("test", OTHER_PLACES)
    assert location1 != location2

def test_bool_true():
    """Test that bool returns true for a non-empty string."""
    location = EventLocation("test", SAMPLE_PLACES)
    assert location

def test_bool_false():
    """Test that bool returns false for an empty string."""
    location = EventLocation("", SAMPLE_PLACES)
    assert not location

test_place_lists = [
    ([], ""),
    (["abc"], "abc"),
    (["def", "abc"], "def and abc"),
    (["def", "abc", "ghi"], "def, abc, and ghi"),
    (["def", "jkl", "abc", "ghi"], "def, jkl, abc, and ghi"),
]

@pytest.mark.parametrize("place_list,expected_phrase", test_place_lists)
def test_format_place_list(place_list, expected_phrase):
    """Test formatting place lists as a phrase retaining order."""
    phrase = EventLocation.format_place_list(place_list)
    assert expected_phrase == phrase

@pytest.mark.parametrize("place_list,expected_phrase", test_place_lists)
def test_from_places(place_list, expected_phrase):
    """Test making an event location from a list of places."""
    event_location = EventLocation.from_places(place_list)
    assert event_location.description == expected_phrase
    assert event_location.places == place_list

# vim: tabstop=8 expandtab shiftwidth=4 softtabstop=4 autoindent
<|endoftext|>"
},
{
"prompt": """"An overridden ArgumentParser class which is build to accommodate mnamer's
setting patterns and delineation of parameter, directive, and positional
arguments."""
class ArgLoader(argparse.ArgumentParser):
	fn __init__(self)
	fn _add_spec(self, spec: SettingSpec)
	__iadd__ = _add_spec
	fn load(self) -> Dict[(str, Any)]
	"""Overrides ArgumentParser's format_help to dynamically generate a help
	message for use with the `--help` flag."""
	fn format_help(self) -> str


class MediaType(Enum):
	EPISODE = 'episode'
	MOVIE = 'movie'


class MessageType(Enum):
	INFO = None
	ALERT = 'yellow'
	ERROR = 'red'
	SUCCESS = 'green'
	HEADING = 'bold'


class ProviderType(Enum):
	TVDB = 'tvdb'
	TVMAZE = 'tvmaze'
	TMDB = 'tmdb'
	OMDB = 'omdb'


class SettingType(Enum):
	DIRECTIVE = 'directive'
	PARAMETER = 'parameter'
	POSITIONAL = 'positional'
	CONFIGURATION = 'configuration'


"""A dataclass which contains the argparse and metadata fields used to describe
arguments and configuration settings."""
class SettingSpec:
	group: SettingType
	dest: Optional[str] = None
	action: Optional[str] = None
	choices: Optional[List[str]] = None
	flags: Optional[List[str]] = None
	help: Optional[str] = None
	nargs: Optional[str] = None
	type: Optional[type] = None
	"""Converts ArgSpec instance into a Python dictionary."""
	fn as_dict(self) -> Dict[(str, Any)]
	__call__ = as_dict
	fn registration(self) -> Tuple[(List[str], Dict[(str, Any)])]
	fn name(self) -> Optional[str]


### Unit tests for the above files using pytest. Make sure they are concise and complete. ###

",
"completion": "import sys
from unittest.mock import patch

import pytest

from mnamer.argument import ArgLoader
from mnamer.setting_spec import SettingSpec
from mnamer.types import SettingType

pytestmark = pytest.mark.local


@pytest.mark.parametrize(
    "settings_type",
    (SettingType.DIRECTIVE, SettingType.PARAMETER, SettingType.POSITIONAL),
    ids=("directive", "parameter", "positional"),
)
def test_arg_loader__add_spec(settings_type: SettingType):
    arg_loader = ArgLoader()
    spec = SettingSpec(group=settings_type, flags=["-f"], help="foo")
    actions = getattr(arg_loader, f"_{settings_type.value}_group")
    assert len(actions._group_actions) == 0
    arg_loader += spec
    assert len(actions._group_actions) == 1


def test_arg_loader__add_spec_other():
    arg_loader = ArgLoader()
    spec = SettingSpec(group=SettingType.CONFIGURATION, flags=["-f"], help="foo")
    with pytest.raises(RuntimeError):
        arg_loader += spec


def test_arg_loader__format_help():
    arg_loader = ArgLoader()
    for spec in (
        SettingSpec(SettingType.POSITIONAL, flags=["--foo1"], help="foo1"),
        SettingSpec(SettingType.POSITIONAL, flags=["--foo2"], help="foo2"),
        SettingSpec(SettingType.POSITIONAL, flags=["--foo3"], help="foo3"),
        SettingSpec(SettingType.PARAMETER, flags=["--bar1"], help="bar1"),
        SettingSpec(SettingType.PARAMETER, flags=["--bar2"], help="bar2"),
        SettingSpec(SettingType.PARAMETER, flags=["--bar3"], help="bar3"),
        SettingSpec(SettingType.DIRECTIVE, flags=["--baz1"], help="baz1"),
        SettingSpec(SettingType.DIRECTIVE, flags=["--baz2"], help="baz2"),
        SettingSpec(SettingType.DIRECTIVE, flags=["--baz3"], help="baz3"),
    ):
        arg_loader._add_spec(spec)
    assert (
        arg_loader.format_help()
        == """
USAGE: mnamer [preferences] [directives] target [targets ...]

POSITIONAL:
  foo1
  foo2
  foo3

PARAMETERS:
  The following flags can be used to customize mnamer's behaviour. Their long
  forms may also be set in a '.mnamer-v2.json' config file, in which case cli
  arguments will take precedence.

  bar1
  bar2
  bar3

DIRECTIVES:
  Directives are one-off arguments that are used to perform secondary tasks
  like overriding media detection. They can't be used in '.mnamer-v2.json'.

  baz1
  baz2
  baz3

Visit https://github.com/jkwill87/mnamer for more information.
"""
    )


def test_arg_parser__load__valid_parameter():
    spec = SettingSpec(group=SettingType.PARAMETER, flags=["-f"], help="foo", type=int)
    arg_parser = ArgLoader(spec)
    with patch.object(sys, "argv", ["mnamer", "-f", "01"]):
        assert arg_parser.load() == {"f": 1}


def test_arg_parser__load__valid_directive():
    spec = SettingSpec(group=SettingType.DIRECTIVE, flags=["-f"], help="foo", type=int)
    arg_parser = ArgLoader(spec)
    with patch.object(sys, "argv", ["mnamer", "-f", "01"]):
        assert arg_parser.load() == {"f": 1}


def test_arg_parser__load__valid_positional():
    spec = SettingSpec(group=SettingType.POSITIONAL, flags=["f"], help="foo", type=int)
    arg_parser = ArgLoader(spec)
    with patch.object(sys, "argv", ["mnamer", "01"]):
        assert arg_parser.load() == {"f": 1}


def test_arg_parser__load__invalid_configuration():
    spec = SettingSpec(group=SettingType.CONFIGURATION, flags=["-f"], help="foo")
    arg_parser = ArgLoader(spec)
    with patch.object(sys, "argv", ["mnamer", "-f", "1"]):
        with pytest.raises(RuntimeError):
            arg_parser.load()


def test_arg_parser__missing_help():
    spec = SettingSpec(group=SettingType.DIRECTIVE, flags=["-f"])
    with pytest.raises(RuntimeError):
        ArgLoader(spec)
<|endoftext|>"
},
