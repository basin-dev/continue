[
  {
    "prompt": "\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from __future__ import division, absolute_import, print_function\n\n__copyright__ = \"Copyright (C) 2015 Andreas Kloeckner\"\n\n__license__ = \"\"\"\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\"\"\"\n\nimport sys\nimport numpy as np  # noqa\nimport numpy.linalg as la\nimport loopy as lp\nimport pyopencl as cl\nimport pyopencl.clrandom  # noqa\n\nimport logging\nlogger = logging.getLogger(__name__)\n\ntry:\n    import faulthandler\nexcept ImportError:\n    pass\nelse:\n    faulthandler.enable()\n\nfrom pyopencl.tools import pytest_generate_tests_for_pyopencl \\\n        as pytest_generate_tests\n\n__all__ = [\n        \"pytest_generate_tests\",\n        \"cl\"  # 'cl.create_some_context'\n        ]\n\n\nfrom loopy.version import LOOPY_USE_LANGUAGE_VERSION_2018_2  # noqa\n\n\ndef test_diff(ctx_factory):\n    ctx = ctx_factory()\n    queue = cl.CommandQueue(ctx)\n\n    knl = lp.make_function(\n         \"\"\"{ [i,j]: 0<=i,j<n }\"\"\",\n         \"\"\"\n         <> a = 1/(1+sinh(x[i] + y[j])**2)\n         z[i] = sum(j, exp(a * x[j]))\n         \"\"\")\n\n    knl = lp.fix_parameters(knl, n=50)\n\n    from loopy.transform.diff import diff_kernel\n    dknl, diff_map = diff_kernel(knl, \"z\", \"x\")\n    dknl = lp.make_program(dknl)\n    dknl = lp.remove_unused_arguments(dknl)\n\n    dknl = lp.add_inames_to_insn(dknl, \"diff_i0\", \"writes:a_dx or writes:a\")\n\n    print(dknl)\n\n    n = 50\n    x = np.random.randn(n)\n    y = np.random.randn(n)\n\n    dx = np.random.randn(n)\n\n    fac = 1e-1\n    h1 = 1e-4\n    h2 = h1 * fac\n\n    evt, (z0,) = knl(queue, x=x, y=y)\n    evt, (z1,) = knl(queue, x=(x + h1*dx), y=y)\n    evt, (z2,) = knl(queue, x=(x + h2*dx), y=y)\n\n    dknl = lp.set_options(dknl, write_cl=True)\n    evt, (df,) = dknl(queue, x=x, y=y)\n\n    diff1 = (z1-z0)\n    diff2 = (z2-z0)\n\n    diff1_predicted = df.dot(h1*dx)\n    diff2_predicted = df.dot(h2*dx)\n\n    err1 = la.norm(diff1 - diff1_predicted) / la.norm(diff1)\n    err2 = la.norm(diff2 - diff2_predicted) / la.norm(diff2)\n    print(err1, err2)\n\n    assert (err2 < err1 * fac * 1.1).all()\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) > 1:\n        exec(sys.argv[1])\n    else:\n        from pytest import main\n        main([__file__])\n\n# vim: foldmethod=marker\n<|endoftext|>",
    "repo": "benSepanski/loopy"
  },
  {
    "prompt": "fn get_fg_position(id, building_block)\n\nfn get_centroid(building_block)\n\nfn get_closest_point(points, point)\n\nfn get_edges(vertex)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nimport stk\n\nfrom ....case_data import CaseData\nfrom .utilities import get_centroid, get_closest_point, get_edges\n\n\n@pytest.fixture\ndef tail_1(position, flip, building_block_1):\n    point1, point2 = points = (\n        position + [-10, 0, 0],\n        position + [10, 0, 0],\n    )\n\n    def get_centroid_point(building_block):\n        return get_closest_point(\n            points=points,\n            point=get_centroid(building_block),\n        )\n\n    vertex = stk.polymer.linear.TailVertex(0, position, flip)\n    return CaseData(\n        vertex=vertex,\n        edges=(tuple(get_edges(vertex))[0], ),\n        building_block=building_block_1,\n        position=position,\n        alignment_tests={get_centroid_point: point2},\n        functional_group_edges={0: 0},\n    )\n<|endoftext|>",
    "repo": "stevenkbennett/stk"
  },
  {
    "prompt": "\"\"\"Retrieve exemplary the MNIST model to test te overall MIA Framework\"\"\"\nfn get_data()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nimport os\nimport numpy as np\n\nfrom .utils import get_data\nfrom ..wb_attack_data_generator import WBAttackGenerator\n\n\ndef test_data():\n    \"\"\"\n    The Data Generator should not crash\n    \"\"\"\n    model, x_train, x_test, y_train, y_test = get_data()\n    X = np.concatenate((x_train, x_test))\n    Y = np.concatenate((y_train, y_test))\n    wb_generator_train = WBAttackGenerator(model, X, Y,\n                                           range(0, len(x_train) // 2), range(len(x_train) // 2, len(x_train)),\n                                           10, 10, last_layer_only=True)\n    wb_generator_train.write_attack_info(f'{os.getcwd()}/libs/MIA/tests/fixtures/', \"mnist_train\")\n\n    assert os.path.exists(f'{os.getcwd()}/libs/MIA/tests/fixtures/mnist_train_data_inf.json')\n    assert os.path.exists(f'{os.getcwd()}/libs/MIA/tests/fixtures/mnist_train_target_train_attack_data.h5')\n    assert os.path.exists(f'{os.getcwd()}/libs/MIA/tests/fixtures/mnist_train_target_test_attack_data.h5')\n<|endoftext|>",
    "repo": "SAP-samples/security-research-fed-dp-mia"
  },
  {
    "prompt": "fn parse_request(request)\n\nfn conduit_connect(request, context)\n\nfn unexpected(request, context)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# encoding: utf-8\nimport json\n\nimport pytest\nimport requests_mock\n\nimport configfetcher\nimport phabricator\nimport wikibugs\nfrom tests.common import root\nfrom tests.wikibugs_network.common import parse_request, conduit_connect, unexpected\n\n\nclass WikibugsFixture:\n    def __init__(self):\n        self.events = []\n        self.wikibugs = wikibugs.Wikibugs2(\n            configfetcher.ConfigFetcher(str(root / \"config.json.example\"))\n        )\n        self.wikibugs.process_event = lambda event: self.events.append(event)\n\n    def poll(self):\n        self.wikibugs.poll()\n\n\n@pytest.fixture()\ndef bugs():\n    return WikibugsFixture()\n\n\ndef feed_query_initial(request, context):\n    content = parse_request(request)\n    assert int(content['limit']) == 1\n    assert 'before' not in content\n    return json.loads(r\"\"\"{\"result\":{\"PHID-STRY-cdxv7sji5d7wnjmiuqgv\":{\"class\":\"PhabricatorApplicationTransactionFeedStory\",\"epoch\":1577802875,\"authorPHID\":\"PHID-USER-pzp7mdlx7otgdlggnyhh\",\"chronologicalKey\":\"6776611750075855743\",\"data\":{\"objectPHID\":\"PHID-TASK-rnay3rzefpqhoaqm3guo\",\"transactionPHIDs\":{\"PHID-XACT-TASK-5esu7y3d7evlsi2\":\"PHID-XACT-TASK-5esu7y3d7evlsi2\"}}}},\"error_code\":null,\"error_info\":null}\"\"\")  # noqa\n\n\ndef feed_query_second(request, context):\n    content = parse_request(request)\n    assert content['before'] == 6776611750075855743\n    assert content['view'] == 'data'\n    return json.loads(r\"\"\"{\"result\":[],\"error_code\":null,\"error_info\":null}\"\"\")\n\n\ndef feed_query_third(request, context):\n    content = parse_request(request)\n    assert content['before'] == 6776611750075855743\n    assert content['view'] == 'data'\n    return json.loads(r\"\"\"{\"result\":{\"PHID-STRY-etrbfg7qqflcsoexaxqr\":{\"class\":\"PhabricatorApplicationTransactionFeedStory\",\"epoch\":1577804347,\"authorPHID\":\"PHID-USER-idceizaw6elwiwm5xshb\",\"chronologicalKey\":\"6776618070283272953\",\"data\":{\"objectPHID\":\"PHID-TASK-he2h6hqmwrdrav3cxqew\",\"transactionPHIDs\":{\"PHID-XACT-TASK-k6asmqpfv2t37tp\":\"PHID-XACT-TASK-k6asmqpfv2t37tp\"}}},\"PHID-STRY-x6pr64eeimmcjl3jbsay\":{\"class\":\"PhabricatorApplicationTransactionFeedStory\",\"epoch\":1577804344,\"authorPHID\":\"PHID-USER-idceizaw6elwiwm5xshb\",\"chronologicalKey\":\"6776618060350723377\",\"data\":{\"objectPHID\":\"PHID-TASK-he2h6hqmwrdrav3cxqew\",\"transactionPHIDs\":{\"PHID-XACT-TASK-ix5urhvrpvn22e2\":\"PHID-XACT-TASK-ix5urhvrpvn22e2\"}}},\"PHID-STRY-cpcsc3r3444i3vaw66bo\":{\"class\":\"PhabricatorApplicationTransactionFeedStory\",\"epoch\":1577804267,\"authorPHID\":\"PHID-USER-muirnivxp5hzppn2a3z7\",\"chronologicalKey\":\"6776617727166200626\",\"data\":{\"objectPHID\":\"PHID-TASK-dgq26etiz4wecd24gkmb\",\"transactionPHIDs\":{\"PHID-XACT-TASK-zd6b2kmmj5pnfwm\":\"PHID-XACT-TASK-zd6b2kmmj5pnfwm\"}}}},\"error_code\":null,\"error_info\":null}\"\"\")  # noqa\n\n\ndef feed_query_error_response(request, context):\n    return json.loads(r\"\"\"{\"result\":null,\"error_code\":\"ERR-CONDUIT-CORE\",\"error_info\":\"Cursor \\\"6771969043218032437\\\" does not identify a valid object in query \\\"PhabricatorFeedQuery\\\".\"}\"\"\")  # noqa\n\n\ndef test_polling(bugs):\n    with requests_mock.mock() as m:\n        m.post('/api/conduit.connect', [{'json': conduit_connect}, {'json': unexpected}])\n        m.post('/api/feed.query', [{'json': feed_query_initial}, {'json': feed_query_second}, {'json': unexpected}])\n        bugs.poll()\n        assert bugs.events == []\n\n        m.post('/api/feed.query', [{'json': feed_query_third}, {'json': unexpected}])\n        bugs.poll()\n\n        assert len(bugs.events) == 3\n\n        # TODO: add more extensive tests\n\n\ndef test_error_response(bugs):\n    with requests_mock.mock() as m:\n        m.post('/api/conduit.connect', [{'json': conduit_connect}, {'json': unexpected}])\n        m.post('/api/feed.query', [{'json': feed_query_initial}, {'json': feed_query_second}])\n        bugs.poll()\n\n        m.post('/api/feed.query', [{'json': feed_query_error_response}, {'json': unexpected}])\n        with pytest.raises(phabricator.PhabricatorException):\n            bugs.poll()\n<|endoftext|>",
    "repo": "wikimedia/labs-tools-wikibugs2"
  },
  {
    "prompt": "fn add_alpine_subscription(request)\n\nfn add_subscription(api_conf: callable)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom tests.functional.services.api.conftest import USER_API_CONFS\nfrom tests.functional import get_logger\nfrom tests.functional.services.utils.http_utils import (\n    http_post,\n    http_get,\n    RequestFailedError,\n    http_del,\n)\n\nALPINE_LATEST_SUBSCRIPTION = {\n    \"subscription_key\": \"docker.io/alpine:latest\",\n    \"subscription_type\": \"tag_update\",\n}\n_logger = get_logger(__name__)\n\n\n@pytest.fixture(scope=\"class\", params=USER_API_CONFS)\ndef add_alpine_subscription(request):\n    subscription = add_subscription(request.param)\n\n    def remove_subscription():\n        resp = http_del(\n            [\"subscriptions\", subscription.get(\"subscription_id\")], config=request.param\n        )\n        if resp.code != 200:\n            raise RequestFailedError(resp.url, resp.code, resp.body)\n\n    request.addfinalizer(remove_subscription)\n    return subscription, request.param\n\n\ndef add_subscription(api_conf: callable):\n    added_subscription = None\n    resp = http_post([\"subscriptions\"], ALPINE_LATEST_SUBSCRIPTION, config=api_conf)\n    if (\n        resp.code == 500\n        and resp.body.get(\"message\") == \"subscription already exists in DB\"\n    ):\n        # Already exists\n        resp = http_get([\"subscriptions\"], config=api_conf)\n        subscription_list = resp.body\n        for subscription in subscription_list:\n            if (\n                subscription.get(\"subscription_type\") == \"tag_update\"\n                and subscription.get(\"subscription_key\") == \"docker.io/alpine:latest\"\n            ):\n                added_subscription = subscription\n                break\n    elif resp.code != 200:\n        raise RequestFailedError(resp.url, resp.code, resp.body)\n    else:\n        added_subscription = resp.body[0]\n    return added_subscription\n<|endoftext|>",
    "repo": "anniyanvr/anchore-engine"
  },
  {
    "prompt": "fn clock(datetime)\n\nclass MemoryRepositoryAdapter(RepositoryAdapter):\n\ttype_tag = 'inmemory'\n\tfn setup(self, backups, bucket, force_clear)\n\tfn fetch_backups(self) -> List[Backup]\n\tfn cleanup_backups(self, backups: List[Backup])\n\tfn parse(cls, name: str) -> dict\n\tfn backups(self)\n\tfn backups(self, values)\n\n\nclass MemoryRepositoryLink(RepositoryLink):\n\ttype_tag_source = 'inmemory'\n\ttype_tag_target = 'inmemory'\n\tfn copy_backup(self, backup)\n\n\nclass MemoryBackupCreator(BackupCreator):\n\ttype_tag = 'inmemory'\n\tRepositoryAdapter = MemoryRepositoryAdapter\n\tfn setup(self, bucket, source_bucket, target_bucket)\n\tfn source_adapter(self)\n\tfn target_adapter(self)\n\tfn build_backup(self, backup)\n\tfn source_backups(self)\n\tfn target_backups(self)\n\n\nclass MockLocalRepository(RepositoryAdapter):\n\tfn __init__(self, directory)\n\tfn setup(self)\n\tfn fetch_backups(self)\n\tfn cleanup_backups(self, backups)\n\n\nclass MockFtpRepository(RepositoryAdapter):\n\tfn __init__(self, server)\n\tfn setup(self)\n\tfn fetch_backups(self)\n\tfn cleanup_backups(self, backups)\n\n\nclass MockMysqlToLocal(BackupCreator):\n\tfn setup(self, directory)\n\tfn target_adapter(self)\n\tfn build_backup(self)\n\n\nclass MockLocalToFtp(RepositoryLink):\n\tfn setup(self, directory, server)\n\tfn source_adapter(self)\n\tfn target_adapter(self)\n\tfn build_backup(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom easybackup.core.hook import Hook\nfrom easybackup.core.repository import Repository\nfrom easybackup.core.backup_supervisor import BackupSupervisor\nfrom .mock import MemoryBackupCreator, MemoryRepositoryAdapter, clock\n\nmockbackups = [\n    'easybackup-myproject-db-20200420_130000.tar',\n    'easybackup-myproject-db-20200420_130100.tar',\n    'easybackup-myproject-db-20200421_130000.tar',\n    'easybackup-myproject-db-20200422_130000.tar',\n]\nmemory_backup_creator = MemoryBackupCreator()\n\n\n@pytest.fixture\ndef memory_adapter():\n    return MemoryRepositoryAdapter(backups=mockbackups)\n\n\ndef test_register_hook():\n\n    obj = {'done': False}\n\n    @Hook.register('test_hook')\n    def do_something(obj):\n        obj['done'] = True\n\n    Hook.plays('test_hook', obj)\n    assert obj['done'] is True\n\n\n@clock('20200422_130000')\ndef test_emit_event_on_backup_build(memory_adapter):\n\n    rep = Repository(\n        adapter=memory_adapter\n    )\n    backup_supervisor = BackupSupervisor(\n        project='myproject',\n        volume='db',\n        creator=memory_backup_creator,\n        repository=rep\n    )\n\n    backup_list = []\n    @Hook.register('before_build_backup')\n    @Hook.register('after_build_backup')\n    def put_backup_on_list(creator, backup, repository):\n        backup_list.append(backup)\n\n    backup_supervisor.build_backup()\n    backups = rep.fetch()\n    assert backups[-1].formated_name == backup_list[0].formated_name\n    assert backups[-1].formated_name == backup_list[1].formated_name\n<|endoftext|>",
    "repo": "ln-nicolas/easybackup"
  },
  {
    "prompt": "class Map:\n\tfn __init__(self, pattern: List[str])\n\tfn check_position(self, right: int, down: int) -> str\n\tfn is_end_of_slope_reached(self, down: int) -> bool\n\n\nclass Trajectory:\n\tfn __init__(self, right: int, down: int)\n\n\nfn count_trees_on_the_slope(map: Map, trajectory: Trajectory) -> int\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom day_3_tobogan_trajectory.check_slope import count_trees_on_the_slope, \\\n    Trajectory\nfrom day_3_tobogan_trajectory.map import Map\n\n\nclass TestCheckSlock:\n    def test_should_count_no_tree_on_the_slope(self):\n        # given\n        trajectory = Trajectory(right=3, down=1)\n        map = Map(pattern=[\n            '..',\n            '..'\n        ])\n\n        # when\n        assert count_trees_on_the_slope(map, trajectory) == 0\n\n    def test_should_count_one_tree_on_the_slope(self):\n        # given\n        trajectory = Trajectory(right=3, down=1)\n        map = Map(pattern=[\n            '....',\n            '...#'\n        ])\n\n        # when\n        assert count_trees_on_the_slope(map, trajectory) == 1\n\n    def test_should_count_1_tree_on_the_slope_with_example(self):\n        # given\n        trajectory = Trajectory(right=3, down=1)\n        map = Map(pattern=[\n            '..##.......',\n            '#...#...#..',\n            '.#....#..#.',\n        ])\n\n        # when\n        assert count_trees_on_the_slope(map, trajectory) == 1\n\n    def test_should_count_2_tree_on_the_slope_with_example(self):\n        # given\n        trajectory = Trajectory(right=3, down=1)\n        map = Map(pattern=[\n            '..##.......',\n            '#...#...#..',\n            '.#....#..#.',\n            '..#.#...#.#',\n            '.#...##..#.'\n        ])\n\n        # when\n        assert count_trees_on_the_slope(map, trajectory) == 2\n\n    def test_should_count_7_tree_on_the_slope_with_full_example(self):\n        # given\n        trajectory = Trajectory(right=3, down=1)\n        map = Map(pattern=[\n            '..##.......',\n            '#...#...#..',\n            '.#....#..#.',\n            '..#.#...#.#',\n            '.#...##..#.',\n            '..#.##.....',\n            '.#.#.#....#',\n            '.#........#',\n            '#.##...#...',\n            '#...##....#',\n            '.#..#...#.#',\n        ])\n\n        # when\n        assert count_trees_on_the_slope(map, trajectory) == 7\n\n    @pytest.mark.parametrize(\"trajectory,tree_count\", [\n        (Trajectory(right=1, down=1), 2),\n        (Trajectory(right=3, down=1), 7),\n        (Trajectory(right=5, down=1), 3),\n        (Trajectory(right=7, down=1), 4),\n        (Trajectory(right=1, down=2), 2)\n    ])\n    def test_should_count_2_tree_on_different_trajectory(self, trajectory, tree_count):\n        # given\n        map = Map(pattern=[\n            '..##.......',\n            '#...#...#..',\n            '.#....#..#.',\n            '..#.#...#.#',\n            '.#...##..#.',\n            '..#.##.....',\n            '.#.#.#....#',\n            '.#........#',\n            '#.##...#...',\n            '#...##....#',\n            '.#..#...#.#',\n        ])\n\n        # when\n        assert count_trees_on_the_slope(map, trajectory) == tree_count\n<|endoftext|>",
    "repo": "Akhilian/Advent-of-code-2020"
  },
  {
    "prompt": "\"\"\"A unique, named, sentinel object.\"\"\"\nclass _SentinelObject:\n\t__slots__ = ('name',)\n\tfn __init__(self, name)\n\tfn __repr__(self)\n\tfn __reduce__(self)\n\n\n\"\"\"Access items to return a named object, usable as a sentinel.\"\"\"\nclass _Sentinel:\n\tfn __getitem__(self, name)\n\tfn __reduce__(self)\n\tfn batch(self, names)\n\n\nclass NamedMock(MagicMock):\n\tfn __name__(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import re\nimport pytest\nimport compose as cp\n\nfrom unittest.mock import Mock\n\nfrom .base import sentinel\n\n\ndef test_flip():\n    @cp.flip\n    def func(a, b):\n        return [a, b]\n\n    x, y = sentinel['x'], sentinel['y']\n    assert func(x, y) == [y, x]\n\n\ndef test_safe_apply():\n    func = Mock(name='func')\n    arg = sentinel['arg']\n    cp.safe_apply(func, arg)\n    func.assert_called_once_with(arg)\n\n\ndef test_safe_apply_error():\n    exc = RuntimeError('some runtime error')\n    func = Mock(name='func', side_effect=exc)\n    arg = sentinel['arg']\n\n    message = f\"{func!r}({arg!r}) caused error: {exc}\"\n    with pytest.raises(cp.ComposeError, match=re.escape(message)) as excinfo:\n        cp.safe_apply(func, arg)\n\n    assert excinfo.value.origin is exc\n\n\ndef test_compose_error():\n    func, arg = Mock(name='func'), Mock(name='arg')\n    exc = cp.ComposeError(func, arg)\n    exc.__cause__ = origin = ImportError('origin exception')\n\n    assert exc.func is func\n    assert exc.arg is arg\n    assert exc.origin is origin\n\n    # __str__\n    assert str(exc) == f\"{func!r}({arg!r}) caused error: {origin}\"\n\n    # __repr__\n    assert repr(exc) == f\"ComposeError(func={func!r}, arg={arg!r})\"\n<|endoftext|>",
    "repo": "kwist-sgr/compose"
  },
  {
    "prompt": "\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from django.core.exceptions import ImproperlyConfigured\n\nimport pytest\n\nfrom metroid.subscribe import match_handler_subject\n\n\ndef test_valid_pattern() -> None:\n    \"\"\"\n    Tests if a valid pattern matches  the provided subject.\n    \"\"\"\n    subject = r'^something\\/tests\\/haha.*$'\n    subject_in_message = 'something/tests/haha/asd-123'\n    is_match = match_handler_subject(subject=subject, message_subject=subject_in_message, is_regex=True)\n    assert is_match is True\n\n\ndef test_wrong_subject_match_on_pattern() -> None:\n    \"\"\"\n    Tests if the validation fails if not a matching reggex is provided.\n    \"\"\"\n    subject = r'^something\\/tests\\/haha.*$'\n    subject_in_message = 'tests/haha/asd-123'\n    is_match = match_handler_subject(subject=subject, message_subject=subject_in_message, is_regex=True)\n    assert is_match is False\n\n\ndef test_match_on_string() -> None:\n    \"\"\"\n    Tests if the pattern matches the subject provided in string format.\n    \"\"\"\n    subject = 'tests/haha.Create'\n    subject_in_message = 'tests/haha.Create'\n    is_match = match_handler_subject(subject=subject, message_subject=subject_in_message, is_regex=False)\n    assert is_match is True\n\n\ndef test_bogus_string() -> None:\n    \"\"\"\n    Tests  if a very weird string with special characters matches .\n    \"\"\"\n    subject = 'tests/haha$somethi#ngth\"atshoul,-,.dn/otbehere'\n    subject_in_message = 'tests/haha$somethi#ngth\"atshoul,-,.dn/otbehere'\n    is_match = match_handler_subject(subject=subject, message_subject=subject_in_message, is_regex=False)\n    assert is_match is True\n\n\ndef test_if_exception_is_thrown() -> None:\n    \"\"\"\n    Tests if the correct exception is thrown upon providing an invalid regex.\n    \"\"\"\n    with pytest.raises(ImproperlyConfigured) as e:\n        subject = 'tests/invalid['\n        subject_in_message = 'tests/invalid['\n        is_match = match_handler_subject(subject=subject, message_subject=subject_in_message, is_regex=True)\n    assert str(e.value) == f'Provided regex pattern: {subject} is invalid.'\n<|endoftext|>",
    "repo": "ingvaldlorentzen/metroid"
  },
  {
    "prompt": "fn _path_to_tests()\n\nfn path_to_tests()\n\nfn tmp_directory()\n\nfn add_current_to_sys_path()\n\nfn grid_search_3_params()\n\nfn grid_search_4_params()\n\nfn grid_search_2_params()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from copy import copy\nimport sys\nimport os\nimport shutil\nimport tempfile\nfrom pathlib import Path\n\nimport pytest\n\n# These are fixtures to get the same configuration that matplotlib uses\n# to run tests with pytest. Note that importing other fixtures from that\n# module leads to weird error messages (e.g. \"pd\", which patches pandas)\n# https://github.com/matplotlib/matplotlib/blob/master/lib/matplotlib/testing/conftest.py\nfrom matplotlib.testing.conftest import (mpl_test_settings,\n                                         mpl_image_comparison_parameters)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import datasets\n\n\ndef _path_to_tests():\n    return Path(__file__).absolute().parent\n\n\n@pytest.fixture(scope='session')\ndef path_to_tests():\n    return _path_to_tests()\n\n\n@pytest.fixture()\ndef tmp_directory():\n    old = os.getcwd()\n    tmp = tempfile.mkdtemp()\n    os.chdir(str(tmp))\n\n    yield tmp\n\n    shutil.rmtree(str(tmp))\n    os.chdir(old)\n\n\n@pytest.fixture\ndef add_current_to_sys_path():\n    old = copy(sys.path)\n    sys.path.insert(0, os.path.abspath('.'))\n    yield sys.path\n    sys.path = old\n\n\n@pytest.fixture(scope='session')\ndef grid_search_3_params():\n    parameters = {\n        'n_estimators': [1, 2, 5, 10],\n        'criterion': ['gini', 'entropy'],\n        'max_features': ['sqrt', 'log2'],\n    }\n\n    est = RandomForestClassifier(random_state=42)\n    clf = GridSearchCV(est, parameters, cv=5)\n\n    X, y = datasets.make_classification(200,\n                                        10,\n                                        n_informative=5,\n                                        class_sep=0.7,\n                                        random_state=42)\n    clf.fit(X, y)\n\n    return clf\n\n\n@pytest.fixture(scope='session')\ndef grid_search_4_params():\n    parameters = {\n        'n_estimators': [1, 2, 5, 10],\n        'criterion': ['gini', 'entropy'],\n        'max_features': ['sqrt', 'log2'],\n        'min_samples_split': [2],\n    }\n\n    est = RandomForestClassifier(random_state=42)\n    clf = GridSearchCV(est, parameters, cv=5)\n\n    X, y = datasets.make_classification(200,\n                                        10,\n                                        n_informative=5,\n                                        class_sep=0.7,\n                                        random_state=42)\n    clf.fit(X, y)\n\n    return clf\n\n\n@pytest.fixture(scope='session')\ndef grid_search_2_params():\n    parameters = {\n        'n_estimators': [1, 2, 5, 10],\n        'criterion': ['gini', 'entropy'],\n    }\n\n    est = RandomForestClassifier(random_state=42)\n    clf = GridSearchCV(est, parameters, cv=5)\n\n    X, y = datasets.make_classification(200,\n                                        10,\n                                        n_informative=5,\n                                        class_sep=0.7,\n                                        random_state=42)\n    clf.fit(X, y)\n\n    return clf\n<|endoftext|>",
    "repo": "abcnishant007/sklearn-evaluation"
  },
  {
    "prompt": "\"\"\"Parse a form\"\"\"\nfn batch_parse_form(project_id, input_uri, destination_uri)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific ladnguage governing permissions and\n# limitations under the License.\n\nimport os\nimport uuid\n\nfrom google.cloud import storage\nimport pytest\n\nimport batch_parse_form_beta\n\n\nBUCKET = 'document-ai-{}'.format(uuid.uuid4())\nOUTPUT_PREFIX = 'TEST_OUTPUT_{}'.format(uuid.uuid4())\nPROJECT_ID = os.environ['GCLOUD_PROJECT']\nINPUT_URI = 'gs://cloud-samples-data/documentai/invoice.pdf'\nBATCH_OUTPUT_URI = 'gs://{}/{}/'.format(BUCKET, OUTPUT_PREFIX)\n\n\n@pytest.fixture(autouse=True)\ndef setup_teardown():\n    \"\"\"Create a temporary bucket to store annotation output.\"\"\"\n    storage_client = storage.Client()\n    bucket = storage_client.create_bucket(BUCKET)\n\n    yield\n\n    bucket.delete(force=True)\n\n\ndef test_batch_parse_form(capsys):\n    batch_parse_form_beta.batch_parse_form(PROJECT_ID, INPUT_URI, BATCH_OUTPUT_URI)\n    out, _ = capsys.readouterr()\n    assert 'Output files' in out\n<|endoftext|>",
    "repo": "luxiangu/python-docs-samples"
  },
  {
    "prompt": "class CodeReportWriter:\n\tfn __init__(self, code_queryset)\n\tfn get_fields_iterator(self)\n\tfn get_report(self, format, as_response)\n\tfn format_xls_report(self, filename, as_response)\n\tfn _format_delimited_report(self, filename, as_response, field_delimiter, record_delimiter)\n\tfn format_csv_report(self, filename, as_response)\n\tfn format_tsv_report(self, filename, as_response)\n\tfn format_pdf_report(self, filename, as_response)\n\n\nfn get_code_report(format, as_response, order_by_literate_code)\n\nfn get_setting(name, default)\n\nfn get_integer_setting(name, default)\n\nfn validate_settings()\n\nfn _validate_code()\n\nfn _validate_prefixes()\n\nfn _validate_print()\n\nfn _create_test_order()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from io import BytesIO\n\nimport pytest\nfrom django.conf import settings\n\nfrom lippukala.models import Code, Order\nfrom lippukala.reports import CodeReportWriter, get_code_report\nfrom lippukala.settings import PREFIXES\n\nfrom .utils import _create_test_order\n\npytestmark = pytest.mark.django_db\n\n\ndef test_creating_order():\n    order = _create_test_order()\n    assert order.code_set.count() == 25, \"orders don't hold their codes\"\n    for code in order.code_set.all():\n        assert code.literate_code.startswith(PREFIXES[code.prefix]), \"prefixes don't work\"\n\n\ndef test_cant_create_invalid_prefix():\n    order = Order.objects.create(comment=\"Dummy\")\n\n    with pytest.raises(ValueError):\n        Code.objects.create(order=order, prefix=\"HQ\")\n\n    with pytest.raises(ValueError):\n        Code.objects.create(order=order, prefix=\"69\")\n\n    order.delete()\n\n\ndef test_double_use_code():\n    order = _create_test_order()\n    code = order.code_set.all()[:1][0]\n    assert not code.is_used, \"code is not used\"\n    code.set_used(save=True)\n    assert code.is_used, \"code is used\"\n    code = Code.objects.get(pk=code.pk)  # reload it to see if saving worked\n    assert code.is_used, \"code is used (even from the db)\"\n\n    with pytest.raises(ValueError):\n        code.set_used()\n\n\ndef test_csv_reports_have_good_stuff():\n    order = _create_test_order()\n    csv_report_data = get_code_report(\"csv\", False).decode(settings.DEFAULT_CHARSET)\n    # We don't particularly care if we have extra orders/codes at this point, just as long\n    # as the ones we just created are found\n    for code in order.code_set.all():\n        assert code.literate_code in csv_report_data, f\"code {code!r} was missing\"\n\n\ndef test_all_report_formats_seem_to_work():\n    _create_test_order()\n    formats = [n.split(\"_\")[1] for n in dir(CodeReportWriter) if n.startswith(\"format_\")]\n    for format in formats:\n        assert get_code_report(format, False, True)\n        assert get_code_report(format, True, False)\n\n\n@pytest.mark.parametrize(\"one_per_page\", (False, True))\ndef test_printing(one_per_page):\n    from lippukala.printing import OrderPrinter\n\n    printer = OrderPrinter()\n    printer.ONE_TICKET_PER_PAGE = one_per_page\n    for x in range(3):\n        order = _create_test_order()\n        printer.process_order(order)\n\n    outf = BytesIO()\n    outf.write(printer.finish())\n    assert outf.getvalue().startswith(b\"%PDF\")\n<|endoftext|>",
    "repo": "kcsry/lippukala"
  },
  {
    "prompt": "\"\"\"A Bokeh-specific ``DeprecationWarning`` subclass.\n\nUsed to selectively filter Bokeh deprecations for unconditional display.\"\"\"\nclass BokehDeprecationWarning(DeprecationWarning):\n\n\n\"\"\"A Bokeh-specific ``UserWarning`` subclass.\n\nUsed to selectively filter Bokeh warnings for unconditional display.\"\"\"\nclass BokehUserWarning(UserWarning):\n\n\n\"\"\"    \"\"\"\nfn verify_all(module, ALL)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "#-----------------------------------------------------------------------------\n# Copyright (c) 2012 - 2020, Anaconda, Inc., and Bokeh Contributors.\n# All rights reserved.\n#\n# The full license is in the file LICENSE.txt, distributed with this software.\n#-----------------------------------------------------------------------------\n\n#-----------------------------------------------------------------------------\n# Boilerplate\n#-----------------------------------------------------------------------------\nimport pytest ; pytest\n\n#-----------------------------------------------------------------------------\n# Imports\n#-----------------------------------------------------------------------------\n\n# Standard library imports\nimport warnings\n\n# Bokeh imports\nfrom bokeh._testing.util.api import verify_all\nfrom bokeh.util.warnings import BokehDeprecationWarning, BokehUserWarning\n\n# Module under test\nimport bokeh as b # isort:skip\n\n#-----------------------------------------------------------------------------\n# Setup\n#-----------------------------------------------------------------------------\n\nALL =  (\n    '__version__',\n    'license',\n    'sampledata',\n)\n\n_LICENSE = \"\"\"\\\nCopyright (c) 2012 - 2020, Anaconda, Inc., and Bokeh Contributors\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice,\nthis list of conditions and the following disclaimer.\n\nRedistributions in binary form must reproduce the above copyright notice,\nthis list of conditions and the following disclaimer in the documentation\nand/or other materials provided with the distribution.\n\nNeither the name of Anaconda nor the names of any contributors\nmay be used to endorse or promote products derived from this software\nwithout specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\nARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\nLIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\nCONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF\nTHE POSSIBILITY OF SUCH DAMAGE.\n\n\"\"\"\n\n#-----------------------------------------------------------------------------\n# General API\n#-----------------------------------------------------------------------------\n\nTest___all__ = verify_all(b, ALL)\n\ndef test___version___type() -> None:\n    assert isinstance(b.__version__, str)\n\ndef test___version___defined() -> None:\n    assert b.__version__ != 'unknown'\n\ndef test_license(capsys) -> None:\n    b.license()\n    out, err = capsys.readouterr()\n    assert out == _LICENSE\n\nclass TestWarnings:\n    @pytest.mark.parametrize('cat', (BokehDeprecationWarning, BokehUserWarning))\n    def test_bokeh_custom(self, cat) -> None:\n        r = warnings.formatwarning(\"message\", cat, \"line\", \"lineno\")\n        assert r == \"%s: %s\\n\" %(cat.__name__, \"message\")\n\n    def test_general_default(self) -> None:\n        r = warnings.formatwarning(\"message\", RuntimeWarning, \"line\", \"lineno\")\n        assert r == \"line:lineno: RuntimeWarning: message\\n\"\n\n    # TODO (bev) issue with this one test and 3.9 support PR\n    @pytest.mark.skip\n    def test_filters(self) -> None:\n        assert ('always', None, BokehUserWarning, None, 0) in warnings.filters\n        assert ('always', None, BokehDeprecationWarning, None, 0) in warnings.filters\n\n#-----------------------------------------------------------------------------\n# Dev API\n#-----------------------------------------------------------------------------\n\n#-----------------------------------------------------------------------------\n# Private API\n#-----------------------------------------------------------------------------\n\n#-----------------------------------------------------------------------------\n# Code\n#-----------------------------------------------------------------------------\n<|endoftext|>",
    "repo": "Saravji/bokeh"
  },
  {
    "prompt": "class TextAnalyticsClientPreparer(AzureMgmtPreparer):\n\tfn __init__(self, client_cls, client_kwargs)\n\tfn create_resource(self, name)\n\n\nclass TextAnalyticsTest(AzureRecordedTestCase):\n\tfn assertOpinionsEqual(self, opinion_one, opinion_two)\n\tfn validateConfidenceScores(self, confidence_scores)\n\tfn assert_healthcare_data_sources_equal(self, data_sources_a, data_sources_b)\n\tfn assert_healthcare_entities_equal(self, entity_a, entity_b)\n\tfn document_result_to_action_type(self, document_result)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# coding=utf-8\n# ------------------------------------\n# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT License.\n# ------------------------------------\n\nimport pytest\nfrom azure.ai.textanalytics.aio import TextAnalyticsClient\nfrom azure.core.credentials import AzureKeyCredential\nfrom testcase import TextAnalyticsPreparer\nfrom testcase import TextAnalyticsTest\n\n\nclass TestAuth(TextAnalyticsTest):\n\n    @pytest.mark.live_test_only\n    @TextAnalyticsPreparer()\n    async def test_active_directory_auth(self, textanalytics_test_endpoint):\n        token = self.get_credential(TextAnalyticsClient, is_async=True)\n        text_analytics = TextAnalyticsClient(textanalytics_test_endpoint, token)\n\n        docs = [{\"id\": \"1\", \"text\": \"I should take my cat to the veterinarian.\"},\n                {\"id\": \"2\", \"text\": \"Este es un document escrito en Espa\u00f1ol.\"},\n                {\"id\": \"3\", \"text\": \"\u732b\u306f\u5e78\u305b\"},\n                {\"id\": \"4\", \"text\": \"Fahrt nach Stuttgart und dann zum Hotel zu Fu.\"}]\n\n        response = await text_analytics.detect_language(docs)\n\n    @TextAnalyticsPreparer()\n    async def test_empty_credentials(self, textanalytics_test_endpoint, textanalytics_test_api_key):\n        with pytest.raises(TypeError):\n            text_analytics = TextAnalyticsClient(textanalytics_test_endpoint, \"\")\n\n    @TextAnalyticsPreparer()\n    def test_bad_type_for_credentials(self, textanalytics_test_endpoint, textanalytics_test_api_key):\n        with pytest.raises(TypeError):\n            text_analytics = TextAnalyticsClient(textanalytics_test_endpoint, [])\n\n    @TextAnalyticsPreparer()\n    def test_none_credentials(self, textanalytics_test_endpoint, textanalytics_test_api_key):\n        with pytest.raises(ValueError):\n            text_analytics = TextAnalyticsClient(textanalytics_test_endpoint, None)\n\n    @TextAnalyticsPreparer()\n    def test_none_endpoint(self, textanalytics_test_endpoint, textanalytics_test_api_key):\n        with pytest.raises(ValueError):\n            text_analytics = TextAnalyticsClient(None, AzureKeyCredential(textanalytics_test_api_key))\n<|endoftext|>",
    "repo": "cochi2/azure-sdk-for-python"
  },
  {
    "prompt": "class Migrate:\n\tupgrade_operators: List[str] = []\n\tdowngrade_operators: List[str] = []\n\t_upgrade_fk_m2m_index_operators: List[str] = []\n\t_downgrade_fk_m2m_index_operators: List[str] = []\n\t_upgrade_m2m: List[str] = []\n\t_downgrade_m2m: List[str] = []\n\t_aerich = Aerich.__name__\n\t_rename_old = []\n\t_rename_new = []\n\tddl: BaseDDL\n\t_last_version_content: Optional[dict] = None\n\tapp: str\n\tmigrate_location: str\n\tdialect: str\n\t_db_version: Optional[str] = None\n\tfn get_all_version_files(cls) -> List[str]\n\tfn _get_model(cls, model: str) -> Type[Model]\n\t\"\"\"add operator,differentiate fk because fk is order limit\n\t:param operator:\n\t:param upgrade:\n\t:param fk_m2m:\n\t:return:\"\"\"\n\tfn _add_operator(cls, operator: str, upgrade: None, fk_m2m: None)\n\t\"\"\"diff models and add operators\n\t:param old_models:\n\t:param new_models:\n\t:param upgrade:\n\t:return:\"\"\"\n\tfn diff_models(cls, old_models: Dict[(str, dict)], new_models: Dict[(str, dict)], upgrade: None)\n\tfn add_model(cls, model: Type[Model])\n\tfn drop_model(cls, table_name: str)\n\tfn create_m2m(cls, model: Type[Model], field_describe: dict, reference_table_describe: dict)\n\tfn drop_m2m(cls, table_name: str)\n\tfn _resolve_fk_fields_name(cls, model: Type[Model], fields_name: Tuple[str])\n\tfn _drop_index(cls, model: Type[Model], fields_name: Tuple[str], unique)\n\tfn _add_index(cls, model: Type[Model], fields_name: Tuple[str], unique)\n\tfn _add_field(cls, model: Type[Model], field_describe: dict, is_pk: bool)\n\tfn _alter_default(cls, model: Type[Model], field_describe: dict)\n\tfn _alter_null(cls, model: Type[Model], field_describe: dict)\n\tfn _set_comment(cls, model: Type[Model], field_describe: dict)\n\tfn _modify_field(cls, model: Type[Model], field_describe: dict)\n\tfn _drop_fk(cls, model: Type[Model], field_describe: dict, reference_table_describe: dict)\n\tfn _remove_field(cls, model: Type[Model], column_name: str)\n\tfn _rename_field(cls, model: Type[Model], old_field_name: str, new_field_name: str)\n\tfn _change_field(cls, model: Type[Model], old_field_describe: dict, new_field_describe: dict)\n\t\"\"\"add fk\n\t:param model:\n\t:param field_describe:\n\t:param reference_table_describe:\n\t:return:\"\"\"\n\tfn _add_fk(cls, model: Type[Model], field_describe: dict, reference_table_describe: dict)\n\t\"\"\"fk/m2m/index must be last when add,first when drop\n\t:return:\"\"\"\n\tfn _merge_operators(cls)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import asyncio\nimport os\n\nimport pytest\nfrom tortoise import Tortoise, expand_db_url, generate_schema_for_client\nfrom tortoise.backends.asyncpg.schema_generator import AsyncpgSchemaGenerator\nfrom tortoise.backends.mysql.schema_generator import MySQLSchemaGenerator\nfrom tortoise.backends.sqlite.schema_generator import SqliteSchemaGenerator\n\nfrom aerich.ddl.mysql import MysqlDDL\nfrom aerich.ddl.postgres import PostgresDDL\nfrom aerich.ddl.sqlite import SqliteDDL\nfrom aerich.migrate import Migrate\n\ndb_url = os.getenv(\"TEST_DB\", \"sqlite://:memory:\")\ndb_url_second = os.getenv(\"TEST_DB_SECOND\", \"sqlite://:memory:\")\ntortoise_orm = {\n    \"connections\": {\n        \"default\": expand_db_url(db_url, True),\n        \"second\": expand_db_url(db_url_second, True),\n    },\n    \"apps\": {\n        \"models\": {\n            \"models\": [\"tests.models\", \"aerich.models\"],\n            \"default_connection\": \"default\",\n        },\n        \"models_second\": {\"models\": [\"tests.models_second\"], \"default_connection\": \"second\"},\n    },\n}\n\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef reset_migrate():\n    Migrate.upgrade_operators = []\n    Migrate.downgrade_operators = []\n    Migrate._upgrade_fk_m2m_index_operators = []\n    Migrate._downgrade_fk_m2m_index_operators = []\n    Migrate._upgrade_m2m = []\n    Migrate._downgrade_m2m = []\n\n\n@pytest.fixture(scope=\"session\")\ndef event_loop():\n    policy = asyncio.get_event_loop_policy()\n    res = policy.new_event_loop()\n    asyncio.set_event_loop(res)\n    res._close = res.close\n    res.close = lambda: None\n\n    yield res\n\n    res._close()\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\nasync def initialize_tests(event_loop, request):\n    await Tortoise.init(config=tortoise_orm, _create_db=True)\n    await generate_schema_for_client(Tortoise.get_connection(\"default\"), safe=True)\n\n    client = Tortoise.get_connection(\"default\")\n    if client.schema_generator is MySQLSchemaGenerator:\n        Migrate.ddl = MysqlDDL(client)\n    elif client.schema_generator is SqliteSchemaGenerator:\n        Migrate.ddl = SqliteDDL(client)\n    elif client.schema_generator is AsyncpgSchemaGenerator:\n        Migrate.ddl = PostgresDDL(client)\n    Migrate.dialect = Migrate.ddl.DIALECT\n    request.addfinalizer(lambda: event_loop.run_until_complete(Tortoise._drop_databases()))\n<|endoftext|>",
    "repo": "personalcomputer/aerich"
  },
  {
    "prompt": "fn parity(count)\n\nclass CountRobotdocsconf(RobotdocsconfBase):\n\tfn __init__(self, count)\n\tfn robotdocsconf(self)\n\tfn _metadata_str(self)\n\n\nclass Counts:\n\tfn __init__(self, counts)\n\tfn __hash__(self)\n\tfn __repr__(self)\n\tfn __eq__(self, other)\n\tfn __len__(self)\n\tfn counts(self)\n\n\nclass NameRobotdocsconfBase:\n\t\"\"\"Return robotdocsconf dictionary.\n\t        \"\"\"\n\tfn robotdocsconf(self)\n\t\"\"\"Return name of the robotdocsconf (section)\n\t        \"\"\"\n\tfn name(self)\n\tfn __eq__(self, other)\n\tfn __repr__(self)\n\tfn __len__(self)\n\tfn __hash__(self)\n\n\nclass ExpectedRobotdocsconf(NameRobotdocsconfBase):\n\tfn __init__(self, counts)\n\tfn robotdocsconf(self)\n\tfn name(self)\n\n\nclass ActualRobotdocsconf(NameRobotdocsconfBase):\n\tfn __init__(self, name, robotdocsconf)\n\tfn robotdocsconf(self)\n\tfn name(self)\n\n\nclass RobotDoc:\n\tfn __init__(self, tmpdir, rootpaths)\n\tfn ctx(self)\n\tfn rootpath(self)\n\tfn expected_robotdocsconfs(self)\n\tfn _robotdocsconfs_gen(self)\n\tfn _get_every_second(self, start)\n\tfn _create_robotdocsconf_files(self)\n\tfn _create_to_root(self, rootpath)\n\tfn _mkdir_rootpath(self)\n\tfn _content_gen(self)\n\tfn _mkdir_args(self)\n\n\nclass CliVerifier:\n\tfn __init__(self)\n\tfn set_script_runner(self, script_runner)\n\tfn set_docspec(self, docspec)\n\tfn set_mock_crldocbuilder(self, mock_crldocbuilder)\n\tfn set_robotdoc(self, robotdoc)\n\tfn set_resource(self, resource)\n\tfn verify(self)\n\tfn _verify_valid(self)\n\tfn _assert_robotdocsconfs(self)\n\tfn _verify_invalid(self)\n\tfn _run(self)\n\tfn _args(self)\n\tfn _actual_docspec(self)\n\tfn _actual_robotdocsconfs(self)\n\tfn _actual_resource(self)\n\tfn _actual_counts(robotdocsconfs)\n\tfn _actual_arg(self)\n\tfn _expected_docspec(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import itertools\nimport logging\nimport mock\nimport pytest\nfrom .cliverifier import CliVerifier\nfrom .robotdoc import RobotDoc\n\n\n__copyright__ = \"Copyright (C) 2019, Nokia\"\n\n\ndef pytest_configure(config):  # pylint: disable=unused-argument\n    logging.basicConfig(\n        format='%(processName)s %(asctime)s.%(msecs)03d %(levelname)s %(message)s',\n        level=logging.DEBUG)\n    # TODO: remove workaround for too verbose flakes after\n    # https://github.com/tholo/pytest-flake8/issues/42 corrected\n    logging.getLogger('flake8').setLevel(logging.WARN)\n\n\n@pytest.fixture\ndef cliverifier(script_runner, docspec, mock_crldocbuilder, robotdoc, resource):\n    c = CliVerifier()\n    c.set_script_runner(script_runner)\n    c.set_docspec(docspec)\n    c.set_mock_crldocbuilder(mock_crldocbuilder)\n    c.set_robotdoc(robotdoc)\n    c.set_resource(resource)\n    return c\n\n\nDOCSPECS = ['all', 'builtin', 'api', 'crl']\nDOCSPEC_COMB = [set(c) for r in range(len(DOCSPECS))\n                for c in itertools.combinations(DOCSPECS, r)]\n\n\n@pytest.fixture(params=DOCSPEC_COMB)\ndef docspec(request):\n    return request.param\n\n\n@pytest.fixture()\ndef mock_crldocbuilder():\n    with mock.patch('crl.doc.cmdline.CRLDocBuilder') as p:\n        yield p\n\n\nROOT_PATHS = [[], [['root']], [['root0'], ['root1']], [['root'], ['parent', 'child']]]\n\n\n@pytest.fixture(params=ROOT_PATHS)\ndef robotdoc(request, tmpdir):\n    with RobotDoc(tmpdir=tmpdir, rootpaths=request.param).ctx() as r:\n        yield r\n\n\nRESOURCES = [['resource{}'.format(i) for i in range(j)] for j in range(3)]\n\n\n@pytest.fixture(params=RESOURCES)\ndef resource(request):\n    return request.param\n<|endoftext|>",
    "repo": "nokia/crl-doc"
  },
  {
    "prompt": "\"\"\"A requisite that annotates the requisitor.\"\"\"\nclass Annotate(Requisite):\n\tpriority = -1\n\t\"\"\"Args:\n\t    key (String)\n\t    value (Any)\n\t\n\tSee also:\n\t    $GraphNode\"\"\"\n\tfn __init__(self, key, value)\n\tfn satisfy(self, requisitor, workflow)\n\n\n\"\"\"A requisite that alters the requisitor (assigns some requisite to another task).\n\nIncludes the assignee into the workflow automatically.\"\"\"\nclass Assign(Requisite):\n\tpriority = -1\n\t\"\"\"Args:\n\t    task (Task) - an assignee\n\t    requisite (Requisite) - a requisite to assign\"\"\"\n\tfn __init__(self, task, requisite)\n\tfn satisfy(self, requisitor, workflow)\n\n\n\"\"\"A requisite that extends an annotation of the requisitor.\n\nAssumes the annotation values form a set.\"\"\"\nclass ExtendAnnotation(Requisite):\n\tpriority = -2\n\t\"\"\"Args:\n\t    key (String)\n\t    values (Set[Any])\"\"\"\n\tfn __init__(self, key, values)\n\tfn satisfy(self, requisitor, workflow)\n\n\n\"\"\"A requisite that adds a link from some task to the requisitor.\n\nIncludes the task into the workflow automatically.\n\nThe requisitor must not be $None.\"\"\"\nclass Follow(Requisite):\n\tpriority = -2\n\t\"\"\"Args:\n\t    task (Task)\"\"\"\n\tfn __init__(self, task)\n\tfn satisfy(self, requisitor, workflow)\n\n\n\"\"\"A requisite that includes some task into the workflow.\n\nRecursively satisfies the requisite of the new task.\nCollects all requisites along the MRO.\"\"\"\nclass Include(Requisite):\n\t\"\"\"Args:\n\t    task (Task)\"\"\"\n\tfn __init__(self, task)\n\tfn satisfy(self, requisitor, workflow)\n\n\n\"\"\"A requisite that attempts to satisfy other requisites.\n\nThis is a helper class that allows to treat requisite collections as requisites.\nIt preserves the requisitor.\"\"\"\nclass SatisfyAll(Requisite):\n\t\"\"\"Args:\n\t    requisites (Iterable[Requisite])\"\"\"\n\tfn __init__(self, requisites)\n\tfn satisfy(self, requisitor, workflow)\n\n\n\"\"\"Get the canonical form of the requisite from a shortcut.\n\nConverts:\n  - a mapping to a series of $Assign's\n        Keys - assignees, values - assignments.\n  - an iterable object to a corresponding $SatisfyAll\n  - a task to a corresponding $Follow\n        This is the most convenient way to define dependencies.\n\nArgs:\n    requisite (Any) - an object to convert to a requisite\n\nReturns:\n    Requisite\n\nRaises:\n    RequisiteConformationError if $requisite cannot be converted to a requisite\"\"\"\nfn conform(requisite)\n\n\"\"\"Create an auto-conforming requisite property.\n\nAllows you to write shorter requisite declarations.\n\nArgs:\n    evaluator (Callable[[Any], Any]) - a requisite property evaluator\n\nReturns:\n    Property - auto-conforming requisite property\"\"\"\nfn shortcut(evaluator)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom edera import Condition\nfrom edera import Task\nfrom edera.requisites import shortcut\nfrom edera.storages import InMemoryStorage\nfrom edera.testing import DefaultScenario\nfrom edera.testing import Stub\nfrom edera.testing import Test\nfrom edera.testing import TestableTask\nfrom edera.workflow import WorkflowBuilder\n\n\ndef test_testable_task_annotates_itself():\n\n    class S(DefaultScenario):\n        pass\n\n    class A(TestableTask):\n        pass\n\n    class B(TestableTask):\n\n        def execute(self):\n            pass\n\n    class C(TestableTask):\n\n        @shortcut\n        def requisite(self):\n            return [A(), B()]\n\n        @property\n        def tests(self):\n            yield S()\n\n    workflow = WorkflowBuilder().build(C())\n    assert workflow[A()][\"tests\"] == set()\n    assert workflow[B()][\"tests\"] == {DefaultScenario()}\n    assert workflow[C()][\"tests\"] == {S()}\n\n\ndef test_testing_task_has_correct_target():\n\n    class A(Task):\n        pass\n\n    class T(Test):\n\n        registry = InMemoryStorage()\n\n    test = T(scenario=DefaultScenario(), subject=A())\n    assert not test.target.check()\n    test.execute()\n    assert test.target.check()\n\n\ndef test_testing_task_runs_scenario():\n\n    class A(Task):\n\n        def execute(self):\n            raise RuntimeError\n\n    class T(Test):\n\n        registry = InMemoryStorage()\n\n    test = T(scenario=DefaultScenario(), subject=A())\n    assert not set(test.target.invariants)\n    assert not test.target.check()\n    with pytest.raises(RuntimeError):\n        test.execute()\n    assert not test.target.check()\n\n\ndef test_testing_task_postchecks_target():\n\n    class E(Condition):\n\n        def check(self):\n            raise RuntimeError\n\n    class A(Task):\n\n        target = E()\n\n    class T(Test):\n\n        registry = InMemoryStorage()\n\n    test = T(scenario=DefaultScenario(), subject=A())\n    assert (test.target >> E()) in set(test.target.invariants)\n    assert not test.target.check()\n    with pytest.raises(RuntimeError):\n        test.execute()\n    assert not test.target.check()\n\n\ndef test_stubbing_task_runs_scenario():\n\n    class F(Condition):\n\n        def check(self):\n            return False\n\n    class A(Task):\n\n        target = F()\n\n        def execute(self):\n            raise RuntimeError\n\n    stub = Stub(scenario=DefaultScenario(), subject=A())\n    assert stub.target == F()\n    with pytest.raises(RuntimeError):\n        stub.execute()\n<|endoftext|>",
    "repo": "thoughteer/edera"
  },
  {
    "prompt": "\"\"\"Adapts tunable parameters, namely step size and mass matrix, during the\nwarmup phase. This class provides lookup properties to read the latest\nvalues of ``step_size`` and ``inverse_mass_matrix``. These values are\nperiodically updated when adaptation is engaged.\"\"\"\nclass WarmupAdapter(object):\n\tfn __init__(self, step_size, adapt_step_size, target_accept_prob, adapt_mass_matrix, is_diag_mass)\n\tfn _build_adaptation_schedule(self)\n\t\"\"\"Finds a reasonable step size and resets step size adaptation scheme.\"\"\"\n\tfn reset_step_size_adaptation(self, z)\n\tfn _update_step_size(self, accept_prob)\n\tfn _update_r_dist(self)\n\tfn _end_adaptation(self)\n\t\"\"\"Model specific properties that are specified when the HMC kernel is setup.\n\t\n\t:param warmup_steps: Number of warmup steps that the sampler is initialized with.\n\t:param initial_step_size: Step size to use to initialize the Dual Averaging scheme.\n\t:param inv_mass_matrix: Initial value of the inverse mass matrix.\n\t:param find_reasonable_step_size_fn: A callable to find reasonable step size when\n\t    mass matrix is changed.\"\"\"\n\tfn configure(self, warmup_steps: None, initial_step_size: None, inv_mass_matrix: None, find_reasonable_step_size_fn: None)\n\t\"\"\"Called at each step during the warmup phase to learn tunable\n\tparameters.\n\t\n\t:param int t: time step, beginning at 0.\n\t:param dict z: latent variables.\n\t:param float accept_prob: acceptance probability of the proposal.\"\"\"\n\tfn step(self, t: int, z: dict, accept_prob: float)\n\tfn adaptation_schedule(self)\n\tfn inverse_mass_matrix(self)\n\tfn inverse_mass_matrix(self, value)\n\tfn r_dist(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nfrom pyro.infer.mcmc.adaptation import (\n    ArrowheadMassMatrix,\n    BlockMassMatrix,\n    WarmupAdapter,\n    adapt_window,\n)\nfrom tests.common import assert_close, assert_equal, skipif_rocm\n\n\n@pytest.mark.parametrize(\"adapt_step_size, adapt_mass, warmup_steps, expected\", [\n    (False, False, 100, []),\n    (False, True, 50, [(0, 6), (7, 44), (45, 49)]),\n    (True, False, 150, [(0, 74), (75, 99), (100, 149)]),\n    (True, True, 200, [(0, 74), (75, 99), (100, 149), (150, 199)]),\n    (True, True, 280, [(0, 74), (75, 99), (100, 229), (230, 279)]),\n    (True, True, 18, [(0, 17)]),\n])\n@skipif_rocm\ndef test_adaptation_schedule(adapt_step_size, adapt_mass, warmup_steps, expected):\n    adapter = WarmupAdapter(0.1,\n                            adapt_step_size=adapt_step_size,\n                            adapt_mass_matrix=adapt_mass)\n    adapter.configure(warmup_steps, mass_matrix_shape={\"z\": (5, 5)})\n    expected_schedule = [adapt_window(i, j) for i, j in expected]\n    assert_equal(adapter.adaptation_schedule, expected_schedule, prec=0)\n\n\n@pytest.mark.parametrize(\"diagonal\", [True, False])\n@skipif_rocm\ndef test_arrowhead_mass_matrix(diagonal):\n    shape = (2, 3)\n    num_samples = 1000\n\n    size = shape[0] * shape[1]\n    block_adapter = BlockMassMatrix()\n    arrowhead_adapter = ArrowheadMassMatrix()\n    mass_matrix_shape = (size,) if diagonal else (size, size)\n    block_adapter.configure({(\"z\",): mass_matrix_shape})\n    arrowhead_adapter.configure({(\"z\",): mass_matrix_shape})\n\n    cov = torch.randn(size, size)\n    cov = torch.mm(cov, cov.t())\n    if diagonal:\n        cov = cov.diag().diag()\n    z_dist = torch.distributions.MultivariateNormal(torch.zeros(size), covariance_matrix=cov)\n    g_dist = torch.distributions.MultivariateNormal(torch.zeros(size), precision_matrix=cov)\n    z_samples = z_dist.sample((num_samples,)).reshape((num_samples,) + shape)\n    g_samples = g_dist.sample((num_samples,)).reshape((num_samples,) + shape)\n\n    for i in range(num_samples):\n        block_adapter.update({\"z\": z_samples[i]}, {\"z\": g_samples[i]})\n        arrowhead_adapter.update({\"z\": z_samples[i]}, {\"z\": g_samples[i]})\n    block_adapter.end_adaptation()\n    arrowhead_adapter.end_adaptation()\n\n    assert_close(arrowhead_adapter.inverse_mass_matrix[('z',)],\n                 block_adapter.inverse_mass_matrix[('z',)],\n                 atol=0.3, rtol=0.3)\n<|endoftext|>",
    "repo": "ashishfarmer/pyro"
  },
  {
    "prompt": "fn get_first_leaf_with_value(code, value)\n\nfn _read(nb_str)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import jupytext\nimport pytest\n\nfrom testutils import exploratory, mixed, _read\nfrom soorgeon import proto\n\n# TODO: do we need roundtrip conversion? we'l only use this for static analysis\n# so i think we're fine\nmixed_expected = '1 + 1 # Cell 1\\n2 + 2 # Cell 3'\n\n\n@pytest.mark.parametrize('code, expected', [\n    [mixed, mixed_expected],\n])\ndef test_prototask_str(code, expected):\n    assert str(\n        proto.ProtoTask('name', _read(code).cells, df_format=None,\n                        py=True)) == expected\n\n\n@pytest.mark.parametrize('cells_idx, expected', [\n    [(0, 3), 'from sklearn.datasets import load_iris'],\n])\ndef test_prototask_add_imports_cell(cells_idx, expected):\n    cells = jupytext.reads(exploratory,\n                           fmt='py:light').cells[cells_idx[0]:cells_idx[1]]\n    pt = proto.ProtoTask('task', cells, df_format=None, py=True)\n    cell = pt._add_imports_cell(exploratory,\n                                add_pathlib_and_pickle=False,\n                                definitions=None,\n                                df_format=None)\n    assert cell['source'] == expected\n<|endoftext|>",
    "repo": "jramirez857/soorgeon"
  },
  {
    "prompt": "fn mock_connection(fn)\n\nclass MockObject(object):\n\tfn __init__(self)\n\n\nclass MockConnection(Connection):\n\tfn __init__(self)\n\tfn write(self, data)\n\tfn last(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import datetime\nimport re\n\nimport pytest\n\nfrom pyromancer.decorators import command\nfrom pyromancer.exceptions import CommandException\nfrom pyromancer.objects import Match, Line, Timer\nfrom pyromancer.test.decorators import mock_connection\nfrom pyromancer.test.mock_objects import MockObject\n\nMESSAGES = [\n    ('Hello world', 'Hello world'),\n    (('Hello {}', 'world'), 'Hello world'),\n    (('Hello {} and {}', 'world', 'Mars'), 'Hello world and Mars'),\n    (('Hello {} and {}', ['world', 'Mars']), 'Hello world and Mars'),\n    (('Hello {sphere}', {'sphere': 'world'}), 'Hello world'),\n    (('Hello {} and {red_one}', 'world', {'red_one': 'Mars'}),\n     'Hello world and Mars'),\n    (('Hello {}, {} and {red_one}', 'world', 'moon', {'red_one': 'Mars'}),\n     'Hello world, moon and Mars'),\n    (('Hello {}, {} and {red_one}', ['world', 'moon'], {'red_one': 'Mars'}),\n     'Hello world, moon and Mars'),\n    (('Hello {}', ['world', 'moon']), \"Hello ['world', 'moon']\"),\n]\n\n\ndef test_command_decorator_set_function():\n    instance = command(r'')\n    function = lambda m: m\n    instance(function)\n\n    assert isinstance(function.command, command)\n    assert function.command.function is function\n\n\n@mock_connection\ndef test_command_messaging_return_tuple(c):\n    line = Line(':John!JDoe@some.host PRIVMSG #Chan :Some cool message', c)\n    instance = command(r'')\n    match = Match(None, line, c)\n\n    for msg, expected in MESSAGES:\n        instance.send_messages(msg, match, [])\n        assert c.last == 'PRIVMSG #Chan :{}'.format(expected)\n\n\n@mock_connection\ndef test_command_messaging_return_list(c):\n    line = Line(':John!JDoe@some.host PRIVMSG #Chan :Some cool message', c)\n    instance = command(r'')\n    match = Match(None, line, c)\n\n    instance.send_messages([msg for msg, expected in MESSAGES], match, [])\n\n    for index, (msg, expected) in enumerate(MESSAGES):\n        assert c.outbox[index] == 'PRIVMSG #Chan :{}'.format(expected)\n\n\n@mock_connection\ndef test_command_messaging_yielding(c):\n    def mock_command():\n        for msg, expected in MESSAGES:\n            yield msg\n\n    line = Line(':John!JDoe@some.host PRIVMSG #Chan :Some cool message', c)\n    instance = command(r'')\n    match = Match(None, line, c)\n\n    instance.send_messages(mock_command(), match, [])\n\n    for index, (msg, expected) in enumerate(MESSAGES):\n        assert c.outbox[index] == 'PRIVMSG #Chan :{}'.format(expected)\n\n\ndef test_command_appends_timers():\n    instance = command(r'')\n    match = Match(None, None, None)\n    timers = []\n\n    instance.send_messages((datetime.timedelta(seconds=3), 'User', 'Hi'),\n                           match, timers)\n    assert len(timers) == 1\n    assert isinstance(timers[0], Timer)\n    assert timers[0].scheduled == datetime.timedelta(seconds=3)\n    assert timers[0].msg_tuple == ('User', 'Hi', (), {},)\n\n\n@mock_connection\ndef test_command_matches_patterns(c):\n    line = Line(':John!JDoe@some.host PRIVMSG #Chan :Some cool message', c)\n    settings = MockObject(command_prefix='!')\n\n    instance = command(r'^Some', prefix=False)\n    assert bool(instance.matches(line, settings)) is True\n\n    instance = command(r'message$', prefix=False)\n    assert bool(instance.matches(line, settings)) is True\n\n    instance = command(r'^Some cool message$', prefix=False)\n    assert bool(instance.matches(line, settings)) is True\n\n    instance = command(r'mESsagE', prefix=False, flags=re.IGNORECASE)\n    assert bool(instance.matches(line, settings)) is True\n\n    instance = command(r'Some')\n    assert bool(instance.matches(line, settings)) is False\n\n    settings = MockObject(command_prefix='S')\n\n    instance = command(r'^ome')\n    assert bool(instance.matches(line, settings)) is True\n\n    instance = command(r'cool')\n    assert bool(instance.matches(line, settings)) is True\n\n    line = Line(':irc.example.net 376 A :End of MOTD command', c)\n\n    instance = command(r'example', prefix=False)\n    assert bool(instance.matches(line, settings)) is False\n\n    instance = command(r'example', raw=True, prefix=False)\n    assert bool(instance.matches(line, settings)) is True\n\n\n@mock_connection\ndef test_command_matches_code(c):\n    with pytest.raises(CommandException):\n        command()\n\n    with pytest.raises(CommandException):\n        command(code='Foo')\n\n    settings = MockObject(command_prefix='!')\n    instance = command(code=376)\n\n    line = Line(':irc.example.net 376 A :End of MOTD command', c)\n    assert line.code == 376\n    assert bool(instance.matches(line, settings)) is True\n\n    line = Line(':irc.example.net 375 A :- irc.example.net message of the day',\n                c)\n    assert line.code == 375\n    assert bool(instance.matches(line, settings)) is False\n\n\n@mock_connection\ndef test_command_matches_command(c):\n    # A command is a 4 to 5 character all-capital string received from the\n    # server. Examples: JOIN, QUIT, NICK, etc.\n    settings = MockObject(command_prefix='!')\n    instance = command(command='PART')\n\n    line = Line(':John!JDoe@some.host PART #Chan :\"Bye !\"', c)\n    assert line.command == 'PART'\n    assert bool(instance.matches(line, settings)) is True\n\n    line = Line(':John!JDoe@some.host NICK Paul\"', c)\n    assert line.command == 'NICK'\n    assert bool(instance.matches(line, settings)) is False\n<|endoftext|>",
    "repo": "Gwildor/Pyromancer"
  },
  {
    "prompt": "\"\"\"Setup for test run.\"\"\"\nfn env_setup()\n\n\"\"\"Cleanup post test run.\"\"\"\nfn env_teardown()\n\n\"\"\"File or folder, it is deleted.\n\nArgs:\n    path: path to a file or dir\"\"\"\nfn delete_it(path: None)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"\nUsed for pytest plugins & session scoped fixtures.\n\"\"\"\nfrom __future__ import absolute_import\nimport sys\nimport mock\nimport pytest\nimport tests.common as tc\n\n\n@pytest.fixture(scope='session', autouse=True)\ndef setup_test_bed(request):\n    \"\"\"\n    Fixture sets up the testing environment for pakit as a whole.\n\n    Session scope, executes before all tests.\n    \"\"\"\n    request.addfinalizer(tc.env_teardown)\n    tc.env_setup()\n\n\n@pytest.yield_fixture()\ndef mock_print():\n    \"\"\"\n    A fixture that mocks python's print function during test.\n    \"\"\"\n    if sys.version_info < (3, 0):\n        print_mod = '__builtin__.print'\n    else:\n        print_mod = 'builtins.print'\n    with mock.patch(print_mod) as mock_obj:\n        yield mock_obj\n\n\n@pytest.yield_fixture()\ndef mock_input():\n    \"\"\"\n    A fixture that mocks python's print function during test.\n    \"\"\"\n    if sys.version_info < (3, 0):\n        input_mod = '__builtin__.raw_input'\n    else:\n        input_mod = 'builtins.input'\n    with mock.patch(input_mod) as mock_obj:\n        yield mock_obj\n\n\n@pytest.yield_fixture(scope='function', autouse=True)\ndef around_all_tests():\n    \"\"\"\n    Executes before and after EVERY test.\n\n    Can be helpful for tracking bugs impacting test bed.\n    \"\"\"\n    # before\n    yield\n    # after\n<|endoftext|>",
    "repo": "starcraftman/recipe_bot"
  },
  {
    "prompt": "fn save_cookies(driver, location)\n\nfn load_cookies(driver, url, location)\n\nfn delete_cookies(driver, domains)\n\nfn set_cookies(driver, user, password)\n\nfn admin_login(driver, user, password)\n\nfn get_host_ip()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\nimport Cookies\n\n\n@pytest.mark.usefixtures(\"setup_login\")\nclass TestLogin:\n\n    def wait_until(self, delay, condition):\n        WebDriverWait(self.driver, delay).until(condition)\n\n    def test_login(self, user=\"alice@foo.com\", password=\"testtest\"):\n        driver = self.driver\n\n        # Test invalid username\n        driver.find_element_by_id(\"identifier\").send_keys(\"wrong_alice\")\n        driver.find_element_by_id(\"password\").send_keys(password)\n        driver.find_element_by_id(\"submit\").click()\n\n        # Presence of the 'password' element implies a failed login\n        self.wait_until(30, EC.presence_of_element_located((By.ID, \"password\")))\n\n        # Test invalid password\n        driver.find_element_by_id(\"identifier\").send_keys(user)\n        driver.find_element_by_id(\"password\").send_keys(\"wrong_password\")\n        driver.find_element_by_id(\"submit\").click()\n\n        self.wait_until(30, EC.presence_of_element_located((By.ID, \"password\")))\n\n        # Test valid login\n        driver.find_element_by_id(\"identifier\").send_keys(user)\n        driver.find_element_by_id(\"password\").send_keys(password)\n        driver.find_element_by_id(\"submit\").click()\n\n        # Presence of the 'usermenu' element implies successful login\n        self.wait_until(30, EC.presence_of_element_located((By.ID, \"usermenu\")))\n\n    def test_logout(self):\n        driver = self.driver\n\n        driver.get(\"http://\" + Cookies.get_host_ip() + \":9000/conferences\")\n        self.wait_until(30, EC.visibility_of_element_located((By.XPATH, '//*[@id=\"usermenu\"]')))\n\n        driver.find_element_by_id(\"usermenu\").click()\n        driver.find_element_by_partial_link_text(\"Logout\").click()\n        assert driver.find_element_by_partial_link_text(\"Login\")\n<|endoftext|>",
    "repo": "neuroinformatics/GCA-Web"
  },
  {
    "prompt": "class BaseTest:\n\tfn initdir(self, tmpdir)\n\tfn ethane(self)\n\tfn methane(self)\n\tfn h2o(self)\n\tfn ch2(self)\n\tfn ester(self)\n\tfn ch3(self)\n\tfn c3(self)\n\tfn n4(self)\n\tfn hydrogen(self)\n\tfn betacristobalite(self)\n\tfn propyl(self)\n\tfn hexane(self, propyl)\n\tfn octane(self)\n\tfn sixpoints(self)\n\tfn benzene(self)\n\tfn rigid_benzene(self)\n\tfn benzene_from_parts(self)\n\tfn box_of_benzenes(self, benzene)\n\tfn rigid_ch(self)\n\tfn silane(self)\n\tfn chf(self)\n\tfn connect_and_reconnect(self, chf)\n\tfn copper_cell(self)\n\tfn graphene(self)\n\tfn cscl_crystal(self)\n\n\nclass DelayImportError(ImportError, SkipTest):\n\n\n\"\"\"Import a module, and issue a nice message to stderr if the module isn't installed.\n\nParameters\n----------\nmodule : str\n    The module you'd like to import, as a string\n\nReturns\n-------\nmodule : {module, object}\n    The module object\n\nExamples\n--------\n>>> # the following two lines are equivalent. the difference is that the\n>>> # second will check for an ImportError and print you a very nice\n>>> # user-facing message about what's wrong (where you can install the\n>>> # module from, etc) if the import fails\n>>> import tables\n>>> tables = import_('tables')\n\nNotes\n-----\nThe pybel/openbabel block is meant to resolve compatibility between\nopenbabel 2.x and 3.0.  There may be other breaking changes but the change\nin importing them is the major one we are aware of. For details, see\nhttps://open-babel.readthedocs.io/en/latest/UseTheLibrary/migration.html#python-module\"\"\"\nfn import_(module: str) -> {module, object}\n\n\"\"\"Get the full path to one of the reference files shipped for utils.\n\nIn the source distribution, these files are in ``mbuild/utils/reference``,\nbut on installation, they're moved to somewhere in the user's python\nsite-packages directory.\n\nParameters\n----------\nname : str\n    Name of the file to load (with respect to the reference/ folder).\"\"\"\nfn get_fn(name: str)\n\nfn run_from_ipython()\n\n\"\"\"Convert mb.Compound to Protobuf Message file\n\nParameters\n---------\ncmpd : mb.Compound\nfilename : str\nbinary: bool, default True \n    If True, will print a binary file\n    If False, will print to a text file\n    Todo: This could be more elegantly detected\n\nNotes\n----\nTodo: Handle Ports in the protocol buffer (.proto) and in this writer/reader\"\"\"\nfn write_pb2(cmpd, filename, binary)\n\n\"\"\"Convert a Protobuf Message file into mb.Compound\n\nParameters\n---------\nfilename : str\nbinary: bool, default True \n    If True, will print a binary file\n    If False, will print to a text file\n    Todo: This could be more elegantly detected\n\nReturns\n------\nroot_compound : mb.Compound\"\"\"\nfn read_pb2(filename, binary)\n\n\"\"\"Given mb.Compound, parse propertes into compound_pb2.Compound\"\"\"\nfn _mb_to_proto(cmpd, proto)\n\n\"\"\"Parse the mb.Compound bonds, add to the proto bonds\n\nParameters\n---------\ncmpd : mb.Compound\nproto : compound_pb2.Compound\"\"\"\nfn _add_proto_bonds(cmpd, proto)\n\n\"\"\"Recurisve method to look for a compound_pb2's children\n\nParameters\n---------\nproto : compound_pb2\n\nNotes\n-----\nBase Case: there are no children to the proto, just return \nRecursion: First look at proto's children and return these children (sub_proto)\n    Then make the recursive call to look at all the sub_proto's successors\nThis is similar to mb.Compound().successors()\nUnlike mb.Compound(), we need to also keep track of parents in this recursion\"\"\"\nfn _proto_successors(proto)\n\n\"\"\"Given compound_pb2.Compound, create mb.Compound \n\nParameters\n----------\nproto: compound_pb2.Compound()\"\"\"\nfn _proto_to_mb(proto: compound_pb2.Compound())\n\n\"\"\"Parse the compound_pb2.Compound bonds, add to mb.Compound\n\nParameters\n---------\nproto : compound_pb2.Compound\ncmpd : mb.Compound\nproto_to_cmpd : dict\n    keys : compound_pb2.Compound.id\n    value : mb.Compound\"\"\"\nfn _add_mb_bonds(proto, cmpd, proto_to_cmpd)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom mbuild.tests.base_test import BaseTest\nfrom mbuild.formats.protobuf import write_pb2, read_pb2\nfrom mbuild.utils.io import has_protobuf\n\nclass TestPB2(BaseTest):\n\n    @pytest.mark.skipif(not has_protobuf, reason=\"Protobuf package not installed\")\n    def test_loop(self, ethane):\n        write_pb2(ethane, 'ethane.pb2')\n        proto = read_pb2('ethane.pb2')\n        assert ethane.n_particles == proto.n_particles\n        assert ethane.n_bonds == proto.n_bonds\n        assert len(ethane.children) == len(proto.children)\n<|endoftext|>",
    "repo": "mattwthompson/mbuild"
  },
  {
    "prompt": "fn match(command)\n\nfn get_new_command(command)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom thefuck.rules.react_native_command_unrecognized import match, \\\n    get_new_command\nfrom tests.utils import Command\n\nstderr = 'Command `{}` unrecognized'.format\n\nstdout = '''\nUsage: react-native <command>\n\nCommands:\n  - start: starts the webserver\n  - bundle: builds the javascript bundle for offline use\n  - unbundle: builds javascript as \"unbundle\" for offline use\n  - new-library: generates a native library bridge\n  - android: generates an Android project for your app\n  - run-android: builds your app and starts it on a connected Android emulator or device\n  - log-android: print Android logs\n  - run-ios: builds your app and starts it on iOS simulator\n  - log-ios: print iOS logs\n  - upgrade: upgrade your app's template files to the latest version; run this after updating the react-native version in your package.json and running npm install\n  - link: link a library\n'''\n\n\n@pytest.mark.parametrize('command', [\n    Command('react-native star', stderr=stderr('star')),\n    Command('react-native android-logs', stderr=stderr('android-logs'))])\ndef test_match(command):\n    assert match(command)\n\n\n@pytest.mark.parametrize('command', [\n    Command('gradle star', stderr=stderr('star')),\n    Command('react-native start')])\ndef test_not_match(command):\n    assert not match(command)\n\n\n@pytest.mark.parametrize('command, result', [\n    (Command('react-native star', stdout, stderr('star')),\n     'react-native start'),\n    (Command('react-native logsandroid -f', stdout, stderr('logsandroid')),\n     'react-native log-android -f')])\ndef test_get_new_command(command, result):\n    assert get_new_command(command)[0] == result\n<|endoftext|>",
    "repo": "MJGrey/thefuck"
  },
  {
    "prompt": "\"\"\"world is going to be a nxn matrix\nNow we say who is owning what. conq_countries will store\nthe number of countries per player\nconq_countries[0] will correspond to player0...\"\"\"\nfn start_game(n_players)\n\nfn global_turn(world, n_players, select_grid, event, text, computer_turn)\n\nfn individual_turn(world, current_player_id)\n\nfn human_turn(text, world, select_grid, event)\n\n\"\"\"Check the 4 neighbour countries and combat them\n2 conditions\nFirst to ensure that the i or j are not out of the matrix.\nthey should be smaller than the size of the matrix -1, since python\nstarts counting on 0\"\"\"\nfn search(world, current_player_id, i, j)\n\nfn combat(attacker_pos, defender_pos)\n\nfn count_countries(x)\n\nfn rand_num()\n\nfn ranking(world, n_players)\n\nfn human_combat(select_grid, world, text)\n\nfn initialize()\n\nfn calculate_colors(n_players)\n\nfn initialize_draw()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# -*- coding: utf-8 -*-\n\n\"\"\"\nThis is to be run in the command line with\npytest *nameOfFile*\n\nTo run all\npython -m pytest\nor\npy.test\n\nif you would like to see details, use -v (verbose)\n\n\"\"\"\nfrom .context import sample\nimport helpers as h\nimport numpy as np\nimport global_variables as g\nimport pytest\n\n#We define the variables\ncurrent_player_id = 0            \nn_players = 3\ng.initialize(n_players)\nworld = np.array([[0,0,0],\n                  [1,2,1],\n                  [1,2,2]])\n\n\ndef test_rand_num():\n   assert h.rand_num() < 3\n        \ndef test_search():\n        assert h.search(world,current_player_id,2,2) == -1\n        assert h.search(world,current_player_id,0,2) == (1,2)        <|endoftext|>",
    "repo": "inimaz/Game"
  },
  {
    "prompt": "fn parse_event(event: ChannelEvent) -> ChannelEvent\n\nfn make_message(event: ChannelEvent, topic: Topic, ref: Optional[str], payload: Optional[dict[(str, Any)]]) -> ChannelMessage\n\nfn generate_reference(event: ChannelEvent) -> str\n\n\"\"\"Provides the main functionality for running a client with asynchronous handlers\n\nThis class should generally be initialised and used as an asynchronous context manager.\n\nInside the `async with` block you should register for topics using `PHXChannelsClient.register_topic_subscription`\nand register event handlers using `PHXChannelsClient.register_event_handler`.\n\nEvent handler functions can be `async` or normal functions.\n* Async functions are run in the event loop `PHXChannelsClient._loop`\n* Normal functions are run using the provided executor pool (`ThreadPoolExecutor` by default)\"\"\"\nclass PHXChannelsClient:\n\tchannel_socket_url: str\n\tlogger: Logger\n\t_client_start_event: Event\n\t_event_handler_config: dict[ChannelEvent, EventHandlerConfig]\n\t_topic_registration_status: dict[Topic, TopicRegistration]\n\t_loop: AbstractEventLoop\n\t_executor_pool: Optional[Executor]\n\t_registration_queue: Queue\n\t_topic_registration_task: Optional[Task]\n\tfn __init__(self, channel_socket_url: str, channel_auth_token: Optional[str], event_loop: Optional[AbstractEventLoop])\n\tfn _parse_message(self, socket_message: Union[(str, bytes)]) -> ChannelMessage\n\tfn shutdown(self, reason: str, websocket: Optional[client.WebSocketClientProtocol], executor_pool: Optional[Executor], wait_for_completion: bool) -> None\n\tfn register_event_handler(self, event: ChannelEvent, handlers: list[ChannelHandlerFunction], topic: Optional[Topic]) -> None\n\tfn register_topic_subscription(self, topic: Topic) -> Event\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from unittest.mock import AsyncMock\n\nfrom hypothesis import given\nimport pytest\n\nfrom phx_events import json_handler\nfrom phx_events.client import PHXChannelsClient\nfrom phx_events.utils import make_message\nfrom tests.strategy_utils import channel_event_strategy\n\n\npytestmark = pytest.mark.asyncio\n\n\nclass TestPHXChannelsClientSendMessage:\n    @given(channel_event_strategy())\n    async def test_correct_message_passed_to_websocket_send(self, event_dict):\n        mock_websocket = AsyncMock()\n        message = make_message(**event_dict)\n\n        async with PHXChannelsClient('ws://test.websocket/') as client:\n            await client._send_message(mock_websocket, message)\n\n        mock_websocket.send.assert_called_with(json_handler.dumps(message))\n        mock_websocket.send.assert_awaited()\n<|endoftext|>",
    "repo": "Flickswitch/phx_events"
  },
  {
    "prompt": "fn get_observable()\n\nfn get_observable_to_21()\n\nfn get_observable()\n\nfn get_observable_to_21()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom async_rx.observable import rx_sum\n\nfrom ..model import ObserverCounterCollector\nfrom .model import get_observable_to_21\n\n\n@pytest.mark.curio\nasync def test_rx_sum():\n\n    seeker = ObserverCounterCollector()\n    sub = await rx_sum(observable=get_observable_to_21()).subscribe(seeker)\n    await sub()\n\n    assert seeker.on_completed_count == 1\n    assert seeker.on_next_count == 1\n    assert seeker.on_error_count == 0\n    assert seeker.items == [210]\n<|endoftext|>",
    "repo": "geronimo-iia/async-rx"
  },
  {
    "prompt": "\"\"\"MutableMapping that automatically spills out dask key/value pairs to disk when\nthe total size of the stored data exceeds the target\"\"\"\nclass SpillBuffer(Buffer):\n\tspilled_by_key: dict[Hashable, int]\n\tspilled_total: int\n\tfn __init__(self, spill_directory: str, target: int)\n\t\"\"\"Key/value pairs stored in RAM. Alias of zict.Buffer.fast.\n\tFor inspection only - do not modify directly!\"\"\"\n\tfn memory(self) -> Mapping[(Hashable, Any)]\n\t\"\"\"Key/value pairs spilled out to disk. Alias of zict.Buffer.slow.\n\tFor inspection only - do not modify directly!\"\"\"\n\tfn disk(self) -> Mapping[(Hashable, Any)]\n\tfn _weight(key: Hashable, value: Any) -> int\n\tfn _on_evict(self, key: Hashable, value: Any) -> None\n\tfn _on_retrieve(self, key: Hashable, value: Any) -> None\n\tfn __setitem__(self, key: Hashable, value: Any) -> None\n\tfn __delitem__(self, key: Hashable) -> None\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom dask.sizeof import sizeof\n\nfrom distributed.spill import SpillBuffer\n\n\ndef test_spillbuffer(tmpdir):\n    buf = SpillBuffer(str(tmpdir), target=300)\n    # Convenience aliases\n    assert buf.memory is buf.fast\n    assert buf.disk is buf.slow\n\n    assert not buf.spilled_by_key\n    assert buf.spilled_total == 0\n\n    a, b, c, d = \"a\" * 100, \"b\" * 100, \"c\" * 100, \"d\" * 100\n    s = sizeof(a)\n    # Test assumption made by this test, mostly for non CPython implementations\n    assert 100 < s < 200\n\n    buf[\"a\"] = a\n    assert not buf.disk\n    assert not buf.spilled_by_key\n    assert buf.spilled_total == 0\n    assert buf[\"a\"] == a\n\n    buf[\"b\"] = b\n    assert not buf.disk\n    assert not buf.spilled_by_key\n    assert buf.spilled_total == 0\n\n    buf[\"c\"] = c\n    assert set(buf.disk) == {\"a\"}\n    assert buf.spilled_by_key == {\"a\": s}\n    assert buf.spilled_total == s\n\n    assert buf[\"a\"] == a\n    assert set(buf.disk) == {\"b\"}\n    assert buf.spilled_by_key == {\"b\": s}\n    assert buf.spilled_total == s\n\n    buf[\"d\"] = d\n    assert set(buf.disk) == {\"b\", \"c\"}\n    assert buf.spilled_by_key == {\"b\": s, \"c\": s}\n    assert buf.spilled_total == s * 2\n\n    # Deleting an in-memory key does not automatically move spilled keys back to memory\n    del buf[\"a\"]\n    assert set(buf.disk) == {\"b\", \"c\"}\n    assert buf.spilled_by_key == {\"b\": s, \"c\": s}\n    assert buf.spilled_total == s * 2\n    with pytest.raises(KeyError):\n        buf[\"a\"]\n\n    # Deleting a spilled key updates the metadata\n    del buf[\"b\"]\n    assert set(buf.disk) == {\"c\"}\n    assert buf.spilled_by_key == {\"c\": s}\n    assert buf.spilled_total == s\n    with pytest.raises(KeyError):\n        buf[\"b\"]\n\n    # Updating a spilled key moves it to the top of the LRU and to memory\n    buf[\"c\"] = c * 2\n    assert set(buf.disk) == {\"d\"}\n    assert buf.spilled_by_key == {\"d\": s}\n    assert buf.spilled_total == s\n\n    # Single key is larger than target and goes directly into slow\n    e = \"e\" * 500\n    slarge = sizeof(e)\n    buf[\"e\"] = e\n    assert set(buf.disk) == {\"d\", \"e\"}\n    assert buf.spilled_by_key == {\"d\": s, \"e\": slarge}\n    assert buf.spilled_total == s + slarge\n\n    # Updating a spilled key with another larger than target updates slow directly\n    buf[\"d\"] = \"d\" * 500\n    assert set(buf.disk) == {\"d\", \"e\"}\n    assert buf.spilled_by_key == {\"d\": slarge, \"e\": slarge}\n    assert buf.spilled_total == slarge * 2\n<|endoftext|>",
    "repo": "bzah/distributed"
  },
  {
    "prompt": "\"\"\"Check if is ResNet building block.\"\"\"\nfn is_block(modules)\n\n\"\"\"Check if is one of the norms.\"\"\"\nfn is_norm(modules)\n\n\"\"\"Check if norm layer is in correct train state.\"\"\"\nfn check_norm_state(modules, train_state)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nimport torch\n\nfrom mmdet.models.backbones import ResNeXt\nfrom mmdet.models.backbones.resnext import Bottleneck as BottleneckX\nfrom .utils import is_block\n\n\ndef test_renext_bottleneck():\n    with pytest.raises(AssertionError):\n        # Style must be in ['pytorch', 'caffe']\n        BottleneckX(64, 64, groups=32, base_width=4, style='tensorflow')\n\n    # Test ResNeXt Bottleneck structure\n    block = BottleneckX(\n        64, 64, groups=32, base_width=4, stride=2, style='pytorch')\n    assert block.conv2.stride == (2, 2)\n    assert block.conv2.groups == 32\n    assert block.conv2.out_channels == 128\n\n    # Test ResNeXt Bottleneck with DCN\n    dcn = dict(type='DCN', deform_groups=1, fallback_on_stride=False)\n    with pytest.raises(AssertionError):\n        # conv_cfg must be None if dcn is not None\n        BottleneckX(\n            64,\n            64,\n            groups=32,\n            base_width=4,\n            dcn=dcn,\n            conv_cfg=dict(type='Conv'))\n    BottleneckX(64, 64, dcn=dcn)\n\n    # Test ResNeXt Bottleneck forward\n    block = BottleneckX(64, 16, groups=32, base_width=4)\n    x = torch.randn(1, 64, 56, 56)\n    x_out = block(x)\n    assert x_out.shape == torch.Size([1, 64, 56, 56])\n\n    # Test ResNeXt Bottleneck forward with plugins\n    plugins = [\n        dict(\n            cfg=dict(\n                type='GeneralizedAttention',\n                spatial_range=-1,\n                num_heads=8,\n                attention_type='0010',\n                kv_stride=2),\n            stages=(False, False, True, True),\n            position='after_conv2')\n    ]\n    block = BottleneckX(64, 16, groups=32, base_width=4, plugins=plugins)\n    x = torch.randn(1, 64, 56, 56)\n    x_out = block(x)\n    assert x_out.shape == torch.Size([1, 64, 56, 56])\n\n\ndef test_resnext_backbone():\n    with pytest.raises(KeyError):\n        # ResNeXt depth should be in [50, 101, 152]\n        ResNeXt(depth=18)\n\n    # Test ResNeXt with group 32, base_width 4\n    model = ResNeXt(depth=50, groups=32, base_width=4)\n    for m in model.modules():\n        if is_block(m):\n            assert m.conv2.groups == 32\n    model.init_weights()\n    model.train()\n\n    imgs = torch.randn(1, 3, 224, 224)\n    feat = model(imgs)\n    assert len(feat) == 4\n    assert feat[0].shape == torch.Size([1, 256, 56, 56])\n    assert feat[1].shape == torch.Size([1, 512, 28, 28])\n    assert feat[2].shape == torch.Size([1, 1024, 14, 14])\n    assert feat[3].shape == torch.Size([1, 2048, 7, 7])\n\n\nregnet_test_data = [\n    ('regnetx_400mf',\n     dict(w0=24, wa=24.48, wm=2.54, group_w=16, depth=22,\n          bot_mul=1.0), [32, 64, 160, 384]),\n    ('regnetx_800mf',\n     dict(w0=56, wa=35.73, wm=2.28, group_w=16, depth=16,\n          bot_mul=1.0), [64, 128, 288, 672]),\n    ('regnetx_1.6gf',\n     dict(w0=80, wa=34.01, wm=2.25, group_w=24, depth=18,\n          bot_mul=1.0), [72, 168, 408, 912]),\n    ('regnetx_3.2gf',\n     dict(w0=88, wa=26.31, wm=2.25, group_w=48, depth=25,\n          bot_mul=1.0), [96, 192, 432, 1008]),\n    ('regnetx_4.0gf',\n     dict(w0=96, wa=38.65, wm=2.43, group_w=40, depth=23,\n          bot_mul=1.0), [80, 240, 560, 1360]),\n    ('regnetx_6.4gf',\n     dict(w0=184, wa=60.83, wm=2.07, group_w=56, depth=17,\n          bot_mul=1.0), [168, 392, 784, 1624]),\n    ('regnetx_8.0gf',\n     dict(w0=80, wa=49.56, wm=2.88, group_w=120, depth=23,\n          bot_mul=1.0), [80, 240, 720, 1920]),\n    ('regnetx_12gf',\n     dict(w0=168, wa=73.36, wm=2.37, group_w=112, depth=19,\n          bot_mul=1.0), [224, 448, 896, 2240]),\n]\n<|endoftext|>",
    "repo": "IuliuNovac/mmswin"
  },
  {
    "prompt": "\"\"\"Makes a data structure safe for JSON silently discarding invalid\nobjects from nested structures.  This also converts dates.\"\"\"\nfn to_safe_json(data)\n\n\"\"\"Returns a date in iso8601 format.\"\"\"\nfn format_iso8601(d)\n\n\"\"\"Parse an iso8601 date into a datetime object.  The timezone is\nnormalized to UTC.\"\"\"\nfn parse_iso8601(value)\n\nfn get_application_name()\n\n\"\"\"A property that is lazily calculated and then cached.\"\"\"\nclass cached_property(object):\n\tfn __init__(self, func, name, doc)\n\tfn __get__(self, obj, type)\n\n\nfn get_iterator_next_method(it)\n\nfn is_unicode(x)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# -*- coding: utf-8 -*-\n\"\"\"\n    test utils for logbook\n    ~~~~~~~~~~~~~~~~~~~~~~\n\n    :copyright: (c) 2010 by Armin Ronacher, Georg Brandl.\n    :license: BSD, see LICENSE for more details.\n\"\"\"\nimport functools\nimport os\nimport sys\nfrom contextlib import contextmanager\n\nimport logbook\nfrom logbook.helpers import StringIO\n\nimport pytest\n\n_missing = object()\n\nLETTERS = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n\n\ndef get_total_delta_seconds(delta):\n    \"\"\"\n    Replacement for datetime.timedelta.total_seconds() for Python 2.5, 2.6\n    and 3.1\n    \"\"\"\n    return (delta.microseconds + (delta.seconds + delta.days * 24 * 3600) * 10**6) / 10**6\n\n\nrequire_py3 = pytest.mark.skipif(\n    sys.version_info[0] < 3, reason=\"Requires Python 3\")\n\nappveyor = pytest.mark.skipif(\n    os.environ.get('APPVEYOR') != 'True', reason='AppVeyor CI test')\n\ntravis = pytest.mark.skipif(\n    os.environ.get('TRAVIS') != 'true', reason='Travis CI test')\n\n\ndef require_module(module_name):\n    found = True\n    try:\n        __import__(module_name)\n    except ImportError:\n        found = False\n\n    return pytest.mark.skipif(\n        not found, reason='Module {0} is required'.format(module_name))\n\n\ndef make_fake_mail_handler(**kwargs):\n    class FakeMailHandler(logbook.MailHandler):\n        mails = []\n\n        def get_connection(self):\n            return self\n\n        def close_connection(self, con):\n            pass\n\n        def sendmail(self, fromaddr, recipients, mail):\n            self.mails.append((fromaddr, recipients, mail))\n\n    kwargs.setdefault('level', logbook.ERROR)\n    return FakeMailHandler('foo@example.com', ['bar@example.com'], **kwargs)\n\n\ndef missing(name):\n    def decorate(f):\n        @functools.wraps(f)\n        def wrapper(*args, **kwargs):\n            old = sys.modules.get(name, _missing)\n            sys.modules[name] = None\n            try:\n                f(*args, **kwargs)\n            finally:\n                if old is _missing:\n                    del sys.modules[name]\n                else:\n                    sys.modules[name] = old\n        return wrapper\n    return decorate\n\n\ndef activate_via_with_statement(handler):\n    return handler\n\n\n@contextmanager\ndef activate_via_push_pop(handler):\n    handler.push_thread()\n    try:\n        yield handler\n    finally:\n        handler.pop_thread()\n\n\n@contextmanager\ndef capturing_stderr_context():\n    original = sys.stderr\n    sys.stderr = StringIO()\n    try:\n        yield sys.stderr\n    finally:\n        sys.stderr = original\n<|endoftext|>",
    "repo": "YoavCohen/logbook"
  },
  {
    "prompt": "\"\"\"Return a user object defined by a user_id.\"\"\"\nfn create_user(user_id: str) -> Dict[(str, Any)]\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"Unittests for file service api.\"\"\"\nimport os\nimport shutil\nfrom datetime import datetime\nfrom typing import Tuple\n\nimport pytest\nfrom nameko.testing.services import worker_factory\n\nfrom files.service import FilesService\nfrom .utils import create_user\n\nfile_service = worker_factory(FilesService)\n\n\n@pytest.mark.parametrize(\n    'ref_path',\n    ['final.txt', 'folder1/folder2/final.txt']\n)\ndef test_upload_path(user_id_folder: Tuple[str, str], tmp_folder: str, upload_file: str, ref_path: str) -> None:\n    \"\"\"Upload file to a given path.\n\n    Test a simple and a more complex path - file inside several folders.\n    \"\"\"\n    user_folder, user_id = user_id_folder\n    user = create_user(user_id)\n    tmp_path = os.path.join(tmp_folder, 'upload.txt')\n    shutil.copyfile(upload_file, tmp_path)\n    assert os.path.isfile(tmp_path)\n\n    result = file_service.upload(user=user, tmp_path=tmp_path, path=ref_path)\n    assert datetime.strptime(result['data'].pop('modified'), '%Y-%m-%dT%H:%M:%SZ')\n    assert result == {\n        'code': 200,\n        'data': {\n            'path': f'{ref_path}',\n            'size': 15\n        },\n        'status': 'success'}\n    assert os.path.isfile(os.path.join(user_folder, 'files', ref_path))\n\n\n@pytest.mark.parametrize(\n    'folders_only',\n    ['', 'folder1/folder2'])\ndef test_delete(user_id_folder: Tuple[str, str], upload_file: str, folders_only: str) -> None:\n    \"\"\"Test upload and delete of file in the main or inside another folder.\"\"\"\n    user_folder, user_id = user_id_folder\n    user = create_user(user_id)\n    folders_path = os.path.join(user_folder, 'files', folders_only)\n    filepath = os.path.join(folders_path, 'delete.txt')\n    if folders_only:\n        os.makedirs(folders_path)\n    shutil.copyfile(upload_file, filepath)\n    assert os.path.isfile(filepath)\n\n    result = file_service.delete(user=user, path=os.path.join(folders_only, 'delete.txt'))\n    assert result == {'status': 'success', 'code': 204}\n    assert not os.path.isfile(filepath)\n\n\ndef test_get_all(user_id_folder: Tuple[str, str], upload_file: str) -> None:\n    \"\"\"Upload and then retrieve several files and check the sorted response.\"\"\"\n    user_folder, user_id = user_id_folder\n    user = create_user(user_id)\n\n    shutil.copyfile(upload_file, os.path.join(user_folder, 'files', '1.txt'))\n    folders2 = os.path.join(user_folder, 'files', 'folder1', 'folder2')\n    os.makedirs(folders2)\n    shutil.copyfile(upload_file, os.path.join(folders2, '2.txt'))\n    folders3 = os.path.join(user_folder, 'files', 'folder3')\n    os.makedirs(folders3)\n    shutil.copyfile(upload_file, os.path.join(folders3, '3.txt'))\n    shutil.copyfile(upload_file, os.path.join(folders3, '4.txt'))\n\n    result = file_service.get_all(user=user)\n    for i in range(len(result['data']['files'])):\n        datetime.strptime(result['data']['files'][i].pop('modified'), '%Y-%m-%dT%H:%M:%SZ')\n    assert result == {\n        'status': 'success',\n        'code': 200,\n        'data': {\n            'files': [\n                {'path': '1.txt', 'size': 15},\n                {'path': 'folder1/folder2/2.txt', 'size': 15},\n                {'path': 'folder3/3.txt', 'size': 15},\n                {'path': 'folder3/4.txt', 'size': 15},\n            ],\n            'links': []}}\n\n\ndef test_download(user_id_folder: Tuple[str, str], upload_file: str) -> None:\n    \"\"\"Test upload and download of file.\"\"\"\n    user_folder, user_id = user_id_folder\n    user = create_user(user_id)\n\n    filepath = os.path.join(user_folder, 'files', 'download.txt')\n    shutil.copyfile(upload_file, filepath)\n    assert os.path.isfile(filepath)\n\n    result = file_service.download(user=user, path='download.txt')\n    assert result == {\n        'status': 'success',\n        'code': 200,\n        'headers': {'Content-Type': 'application/octet-stream'},\n        'file': filepath\n    }\n    assert os.path.isfile(filepath)\n<|endoftext|>",
    "repo": "Open-EO/openeo-openshift-driver"
  },
  {
    "prompt": "\"\"\"Mixin that calls a callback after each call to the `__delitem__()` function.\"\"\"\nclass DelMixin:\n\tfn __init__(self, delete_callback: Callable[([str], Any)])\n\tfn __delitem__(self, key)\n\n\n\"\"\"Behaves like a subclass of `cachetools.Cache`, but keeps a persistent copy\nof the cache on disk.\n\nThe persistent copy is lazily instantiated at the first access to an attribute\nof the underlying cache.\n\nThe persistent copy is updated after every write (add or delete item).\n\nIf items in the cache are modified without re-adding them to the dict, the\npersistent cache will not be updated.\n\nPersistency can be deactivated by providing `None` as the filename.\n\nInternally, the `shelve` library is used to implement the cache.\n\nParameters\n----------\nwrapped_cache_cls: subclass of `Cache`\n    the class of the cache that this PersistentCache should mimic.\nfilename: str or None\n    filename for the persistent cache. A file extension may be appended. See\n    `shelve.open()` for more information. If `None`, persistency is deactivated.\n*args:\n    forwarded to the init function of `wrapped_cache_cls`\n*kwargs:\n    forwarded to the init function of `wrapped_cache_cls`\"\"\"\nclass PersistentCache(MutableMapping):\n\tfn __init__(self, wrapped_cache_cls: Type[Cache], filename: str)\n\t\"\"\"Called when an item is deleted from the wrapped cache\"\"\"\n\tfn delete_callback(self, key)\n\tfn hash_key(key)\n\tfn __setitem__(self, key, value)\n\tfn setdefault(self, k, v)\n\tfn __getitem__(self, item)\n\tfn __getattr__(self, item)\n\tfn __delitem__(self, v) -> None\n\tfn __contains__(self, item)\n\tfn initialize_if_not_initialized(self)\n\tfn close(self)\n\t\"\"\"Try to tidy up.\n\t\n\tThis is just for show since we sync the dict after every change anyway.\"\"\"\n\tfn __del__(self)\n\tfn __len__(self) -> int\n\tfn __iter__(self) -> Iterator\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import os\nimport tempfile\n\nimport cachetools\nimport pytest\nfrom cachetools import LRUCache\n\nfrom shelved_cache.persistent_cache import PersistentCache\n\n\n@pytest.fixture\ndef tmpfile():\n    with tempfile.NamedTemporaryFile() as f:\n        yield f.name\n\n\n@pytest.fixture\ndef tmpdir():\n    with tempfile.TemporaryDirectory() as d:\n        yield d\n\n\ndef test_getsetitem(tmpdir):\n    filename = os.path.join(tmpdir, \"cache\")\n    pc = PersistentCache(LRUCache, filename=filename, maxsize=3)\n    pc[\"a\"] = 42\n    assert pc[\"a\"] == 42\n\n\ndef test_setdefault(tmpdir):\n    filename = os.path.join(tmpdir, \"cache\")\n    pc = PersistentCache(LRUCache, filename=filename, maxsize=3)\n\n    # insert\n    res = pc.setdefault(\"a\", 42)\n    assert res == 42\n    assert pc.persistent_dict[str(hash(\"a\"))] == (\"a\", 42)\n\n    # retrieve\n    res = pc.setdefault(\"a\", 42)\n    assert res == 42\n    assert pc.persistent_dict[str(hash(\"a\"))] == (\"a\", 42)\n\n\ndef test_eviction(tmpdir):\n    filename = os.path.join(tmpdir, \"cache\")\n    pc = PersistentCache(LRUCache, filename=filename, maxsize=2)\n    pc[\"a\"] = 42\n    pc[\"b\"] = 43\n    pc[\"c\"] = 44\n    # \"a\" should be evicted now\n    assert \"a\" not in pc\n    assert PersistentCache.hash_key(\"a\") not in pc.persistent_dict\n    assert \"b\" in pc\n    assert PersistentCache.hash_key(\"b\") in pc.persistent_dict\n\n\ndef test_non_string_keys(tmpdir):\n    filename = os.path.join(tmpdir, \"cache\")\n    pc = PersistentCache(LRUCache, filename=filename, maxsize=3)\n    pc[23] = 42\n    assert pc[23] == 42\n\n\ndef test_persistency(tmpdir):\n    filename = os.path.join(tmpdir, \"cache\")\n    pc = PersistentCache(LRUCache, filename=filename, maxsize=2)\n    pc[\"a\"] = 42\n    pc[\"b\"] = 43\n    pc[\"c\"] = 44\n    # \"a\" should be evicted now\n    assert \"a\" not in pc\n    assert \"b\" in pc\n    pc.close()\n\n    pc2 = PersistentCache(LRUCache, filename=filename, maxsize=2)\n    assert \"a\" not in pc2\n    assert pc2[\"b\"] == 43\n    assert pc2[\"c\"] == 44\n\n\ndef test_no_persistency():\n    filename = None\n    pc = PersistentCache(LRUCache, filename=filename, maxsize=2)\n    pc[\"a\"] = 42\n    pc[\"b\"] = 43\n    pc[\"c\"] = 44\n    # \"a\" should be evicted now\n    assert \"a\" not in pc\n    assert \"b\" in pc\n    pc.close()\n\n    pc2 = PersistentCache(LRUCache, filename=filename, maxsize=2)\n    assert \"a\" not in pc2\n    assert \"b\" not in pc2\n\n\ndef test_non_str_key_persistency(tmpdir):\n    filename = os.path.join(tmpdir, \"cache\")\n    pc = PersistentCache(LRUCache, filename=filename, maxsize=2)\n    pc[23] = 42\n    pc[24] = 43\n    pc.close()\n\n    pc2 = PersistentCache(LRUCache, filename=filename, maxsize=2)\n    assert pc2[23] == 42\n    assert pc2[24] == 43\n\n\ndef test_decorator(tmpdir):\n    filename = os.path.join(tmpdir, \"cache\")\n    pc = PersistentCache(LRUCache, filename, maxsize=2)\n\n    @cachetools.cached(pc)\n    def square(x):\n        print(\"called\")\n        return x * x\n\n    assert square(3) == 9\n    # outputs \"called\"\n    assert square(3) == 9\n    # no output because the cache is used\n<|endoftext|>",
    "repo": "mariushelf/shelved_cache"
  },
  {
    "prompt": "\"\"\"shortcut to instantiate a version plugin as well as a dummy repository\"\"\"\nfn instantiate_version(tmpdir, ctlr)\n\n\"\"\"shortcut to instantiate a version plugin as well as a dummy repository\"\"\"\nfn instantiate_semver2(tmpdir, ctlr)\n\nfn instantiate_test_plugin(typ, name, _ctl)\n\n\"\"\"In order to test the versioning plugin we need a dummy\nrepository plugin - so we can test that actions are properly\npropagated to a repository managed by the version plugin.\n\nThis plugin serves that purpose\"\"\"\nclass DummyRepositoryPlugin(RepositoryPlugin):\n\tfn init(self)\n\tfn uuid(self)\n\tfn is_cloned(self)\n\tfn is_clean(self)\n\tfn branch(self)\n\tfn commit(self)\n\tfn clone(self)\n\tfn pull(self)\n\tfn push(self)\n\tfn tag(self, version)\n\tfn checkout(self, branch)\n\tfn merge(self, a, b)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import os\nimport shutil\n\nimport pytest\nfrom util import instantiate_test_plugin\n\nimport ctl\nfrom ctl.plugins.changelog import ChangelogVersionMissing\n\n\ndef instantiate(tmpdir, ctlr=None, **kwargs):\n    dirpath = f\"{tmpdir}\"\n    md_file = os.path.join(dirpath, \"CHANGELOG.md\")\n    data_file = os.path.join(dirpath, \"CHANGELOG.yml\")\n    config = {\n        \"config\": {\n            \"md_file\": md_file,\n            \"data_file\": data_file,\n        }\n    }\n    config[\"config\"].update(kwargs)\n    plugin = instantiate_test_plugin(\"changelog\", \"test_changelog\", _ctl=ctlr, **config)\n    return plugin\n\n\ndef test_init():\n    ctl.plugin.get_plugin_class(\"changelog\")\n\n\ndef test_generate_clean(tmpdir, ctlr, data_changelog_generate_clean):\n    plugin = instantiate(tmpdir, ctlr)\n    data_file = plugin.get_config(\"data_file\")\n    plugin.generate_clean(data_file)\n\n    assert plugin.load(data_file) == data_changelog_generate_clean.expected\n\n    with pytest.raises(ValueError):\n        plugin.generate_clean(data_file)\n\n\ndef test_generate(tmpdir, ctlr, data_changelog_generate):\n    data_file = os.path.join(data_changelog_generate.path, \"CHANGELOG.yml\")\n    plugin = instantiate(tmpdir, ctlr, data_file=data_file)\n    md_file = plugin.get_config(\"md_file\")\n    plugin.generate(md_file, data_file)\n\n    with open(md_file) as fh:\n        content = fh.read()\n        assert content.strip() == data_changelog_generate.md.strip()\n\n\ndef test_generate_datafile(tmpdir, ctlr, data_changelog_generate_datafile):\n    md_file = os.path.join(data_changelog_generate_datafile.path, \"CHANGELOG.md\")\n    plugin = instantiate(tmpdir, ctlr, md_file=md_file)\n    data_file = plugin.get_config(\"data_file\")\n    plugin.generate_datafile(md_file, data_file)\n\n    with open(data_file) as fh:\n        content = fh.read()\n        assert content.strip() == data_changelog_generate_datafile.yml.strip()\n\n\ndef test_release(tmpdir, ctlr, data_changelog_release):\n\n    md_file_src = os.path.join(data_changelog_release.path, \"CHANGELOG.md\")\n\n    plugin = instantiate(tmpdir, ctlr)\n    md_file = plugin.get_config(\"md_file\")\n    data_file = plugin.get_config(\"data_file\")\n    shutil.copyfile(md_file_src, md_file)\n    plugin.generate_datafile(md_file, data_file)\n    plugin.release(\"1.1.0\", data_file)\n\n    with open(data_file) as fh:\n        content = fh.read()\n        print(content)\n        assert content.strip() == data_changelog_release.yml.strip()\n\n\ndef test_validate(tmpdir, ctlr, data_changelog_generate):\n\n    data_file = os.path.join(data_changelog_generate.path, \"CHANGELOG.yml\")\n    plugin = instantiate(tmpdir, ctlr, data_file=data_file)\n\n    plugin.validate(data_file, \"1.0.0\")\n\n    with pytest.raises(ChangelogVersionMissing):\n        plugin.validate(data_file, \"1.1.0\")\n<|endoftext|>",
    "repo": "grizz/ctl"
  },
  {
    "prompt": "\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import sys\nimport subprocess\nfrom pathlib import Path\nfrom typing import List\n\nimport pytest\nfrom syrupy import SnapshotAssertion\n\nfrom prisma.generator import BASE_PACKAGE_DIR\nfrom prisma.generator.utils import remove_suffix\nfrom .utils import ROOTDIR\n\n\ndef get_files_from_templates(directory: Path) -> List[str]:\n    \"\"\"Return a list of all auto-generated python modules\"\"\"\n    files: List[str] = []\n\n    for template in directory.iterdir():\n        if template.is_dir():\n            files.extend(get_files_from_templates(template))\n        elif template.name.endswith('.py.jinja') and not template.name.startswith('_'):\n            if directory.name == 'templates':\n                name = template.name\n            else:\n                name = str(template.relative_to(template.parent.parent))\n\n            files.append(remove_suffix(name, '.jinja'))\n\n    return files\n\n\nSYNC_ROOTDIR = ROOTDIR / '__prisma_sync_output__' / 'prisma'\nASYNC_ROOTDIR = ROOTDIR / '__prisma_async_output__' / 'prisma'\nFILES = get_files_from_templates(BASE_PACKAGE_DIR / 'generator' / 'templates')\n\n\n@pytest.mark.parametrize('file', FILES)\ndef test_sync(snapshot: SnapshotAssertion, file: str) -> None:\n    \"\"\"Ensure synchronous client files match\"\"\"\n    assert SYNC_ROOTDIR.joinpath(file).absolute().read_text() == snapshot\n\n\n@pytest.mark.parametrize('file', FILES)\ndef test_async(snapshot: SnapshotAssertion, file: str) -> None:\n    \"\"\"Ensure asynchronous client files match\"\"\"\n    assert ASYNC_ROOTDIR.joinpath(file).absolute().read_text() == snapshot\n\n\ndef test_sync_client_can_be_imported() -> None:\n    \"\"\"Synchronous client can be imported\"\"\"\n    proc = subprocess.run(\n        [sys.executable, '-c', 'import prisma; print(prisma.__file__)'],\n        cwd=str(SYNC_ROOTDIR.parent),\n        check=True,\n        stdout=subprocess.PIPE,\n    )\n    assert proc.stdout.decode('utf-8').rstrip('\\n') == str(SYNC_ROOTDIR / '__init__.py')\n\n\ndef test_async_client_can_be_imported() -> None:\n    \"\"\"Asynchronous client can be imported\"\"\"\n    proc = subprocess.run(\n        [sys.executable, '-c', 'import prisma; print(prisma.__file__)'],\n        cwd=str(ASYNC_ROOTDIR.parent),\n        check=True,\n        stdout=subprocess.PIPE,\n    )\n    assert proc.stdout.decode('utf-8').rstrip('\\n') == str(\n        ASYNC_ROOTDIR / '__init__.py'\n    )\n<|endoftext|>",
    "repo": "adriangb/prisma-client-py"
  },
  {
    "prompt": "fn send_simple_message(recipient)\n\nclass MainPage(webapp2.RequestHandler):\n\tfn get(self)\n\n\nclass SendEmailHandler(webapp2.RequestHandler):\n\tfn post(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright 2016 Google Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport mock\nimport pytest\nimport webtest\n\nimport main\n\n\n@pytest.fixture\ndef app():\n    return webtest.TestApp(main.app)\n\n\ndef test_get(app):\n    response = app.get('/')\n    if response.status_int != 200:\n        raise AssertionError\n\n\n@mock.patch('python_http_client.client.Client._make_request')\ndef test_post(make_request_mock, app):\n    response = mock.Mock()\n    response.getcode.return_value = 200\n    response.read.return_value = 'OK'\n    response.info.return_value = {}\n    make_request_mock.return_value = response\n\n    app.post('/send', {\n        'recipient': 'user@example.com'\n    })\n\n    if not make_request_mock.called:\n        raise AssertionError\n    request = make_request_mock.call_args[0][1]\n    if 'user@example.com' not in request.data:\n        raise AssertionError\n<|endoftext|>",
    "repo": "dlminvestments/python-docs-samples"
  },
  {
    "prompt": "fn get_test_assets()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom traitlets import TraitError\n\nfrom ipygany import PolyMesh, Warp\n\nfrom .utils import get_test_assets\n\n\ndef test_default_input():\n    vertices, triangles, data_1d, data_3d = get_test_assets()\n\n    poly = PolyMesh(vertices=vertices, triangle_indices=triangles, data=[data_1d, data_3d])\n\n    warped_mesh = Warp(poly)\n\n    assert warped_mesh.input == (('1d', 'x'), 0, 0)\n\n    poly = PolyMesh(vertices=vertices, triangle_indices=triangles, data=[data_3d])\n\n    warped_mesh = Warp(poly)\n\n    assert warped_mesh.input == '3d'\n\n\ndef test_input():\n    vertices, triangles, data_1d, data_3d = get_test_assets()\n\n    poly = PolyMesh(vertices=vertices, triangle_indices=triangles, data=[data_1d, data_3d])\n\n    warped_mesh = Warp(poly)\n\n    with pytest.raises(TraitError):\n        warped_mesh.input = (('1d', 'x'), 0)\n\n    warped_mesh.input = ('1d', 0, 0)\n    assert warped_mesh.input == (('1d', 'x'), 0, 0)\n\n    warped_mesh.input = ('1d', 0, 32)\n    assert warped_mesh.input == (('1d', 'x'), 0, 32)\n\n    warped_mesh.input = (0, 0, '1d')\n    assert warped_mesh.input == (0, 0, ('1d', 'x'))\n\n    with pytest.raises(TraitError):\n        warped_mesh.input = ('3d', 0, 0)\n\n    warped_mesh = Warp(poly, input=('1d', 0, 0))\n    assert warped_mesh.input == (('1d', 'x'), 0, 0)\n\n    warped_mesh = Warp(poly, input=(0, 0, '1d'))\n    assert warped_mesh.input == (0, 0, ('1d', 'x'))\n\n    warped_mesh = Warp(warped_mesh, input=(0, '1d', 0))\n    assert warped_mesh.input == (0, ('1d', 'x'), 0)\n<|endoftext|>",
    "repo": "stjordanis/ipygany"
  },
  {
    "prompt": "class KeyboardButton(ListMainButton):\n\tfn __init__(self, title: str)\n\n\nclass QuickReplyContentBase:\n\tfn __init__(self, text: str, caption: str)\n\n\nclass QuickReplyContentText(QuickReplyContentBase):\n\tfn __init__(self, header: str, text: str, caption: str)\n\tfn json(self) -> Dict[(str, str)]\n\n\nclass QuickReplyContentImage(QuickReplyContentBase):\n\tfn __init__(self, url: str, text: str, caption: str)\n\tfn json(self) -> Dict[(str, str)]\n\n\nclass QuickReplyContentDocument(QuickReplyContentBase):\n\tfn __init__(self, url: str, filename: str, text: str, caption: str)\n\tfn json(self) -> Dict[(str, str)]\n\n\nclass QuickReplyContentVideo(QuickReplyContentBase):\n\tfn __init__(self, url: str, text: str, caption: str)\n\tfn json(self) -> Dict[(str, str)]\n\n\nclass QuickReply:\n\tfn __init__(self, callback_data: str, content: Union[(QuickReplyContentText, QuickReplyContentImage, QuickReplyContentDocument, QuickReplyContentVideo)], options: Optional[List[KeyboardButton]])\n\tfn add(self, element: KeyboardButton) -> QuickReply\n\tfn dict(self)\n\tfn json(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom waio.keyboard.reply import QuickReply, QuickReplyContentText, KeyboardButton, QuickReplyContentImage\n\n\ndef test_reply_keyboard_text():\n    kb_content = QuickReplyContentText(\n        header=\"this is the header\",\n        text=\"this is the body\",\n        caption=\"this is the footer\"\n    )\n    kb = QuickReply(callback_data=\"callback_keyboard_123\", content=kb_content)\n\n    kb.add(KeyboardButton(title='keyboard1')).add(KeyboardButton(title='keyboard2'))\n\n    assert kb.dict() == {\n        \"type\": \"quick_reply\",\n        \"msgid\": \"callback_keyboard_123\",\n        \"content\": {\n            \"type\": \"text\",\n            \"header\": \"this is the header\",\n            \"text\": \"this is the body\",\n            \"caption\": \"this is the footer\"\n        },\n        \"options\": [\n            {\n                \"type\": \"text\",\n                \"title\": \"keyboard1\"\n            },\n            {\n                \"type\": \"text\",\n                \"title\": \"keyboard2\"\n            }\n        ]\n    }\n\n\ndef test_reply_keyboard_image():\n    kb_content = QuickReplyContentImage(\n        text=\"this is the body\",\n        caption=\"this is the footer\",\n        url=\"https://www.buildquickbots.com/whatsapp/media/sample/jpg/sample01.jpg\"\n    )\n    kb = QuickReply(callback_data=\"callback_keyboard_123_img\", content=kb_content)\n\n    kb.add(KeyboardButton(title='keyboard1')).add(KeyboardButton(title='keyboard2'))\n\n    assert kb.dict() == {\n        \"type\": \"quick_reply\",\n        \"msgid\": \"callback_keyboard_123_img\",\n        \"content\": {\n            \"type\": \"image\",\n            \"url\": \"https://www.buildquickbots.com/whatsapp/media/sample/jpg/sample01.jpg\",\n            \"text\": \"this is the body\",\n            \"caption\": \"this is the footer\"\n        },\n        \"options\": [\n            {\n                \"type\": \"text\",\n                \"title\": \"keyboard1\"\n            },\n            {\n                \"type\": \"text\",\n                \"title\": \"keyboard2\"\n            }\n        ]\n    }\n<|endoftext|>",
    "repo": "dotX12/waio"
  },
  {
    "prompt": "class ParserException(Exception):\n\tfn __init__(self, message, item)\n\tfn set_err_pos(self, lineno, col_offset)\n\tfn __str__(self)\n\n\nclass VariableDeclarationException(ParserException):\n\n\nclass StructureException(ParserException):\n\n\nclass ConstancyViolationException(ParserException):\n\n\nclass NonPayableViolationException(ParserException):\n\n\nclass InvalidLiteralException(ParserException):\n\n\nclass InvalidTypeException(ParserException):\n\n\nclass TypeMismatchException(ParserException):\n\n\nclass FunctionDeclarationException(ParserException):\n\n\nclass EventDeclarationException(ParserException):\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom pytest import raises\n\nfrom vyper import compiler\nfrom vyper.exceptions import FunctionDeclarationException\n\nfail_list = [\n    \"\"\"\n@public\ndef \u01511qwerty(i: int128) -> int128:\n    temp_var : int128 = i\n    return temp_var\n    \"\"\",\n\"\"\"\n@public\ndef int128(i: int128) -> int128:\n    temp_var : int128 = i\n    return temp_var\n    \"\"\",\n\"\"\"\n@public\ndef decimal(i: int128) -> int128:\n    temp_var : int128 = i\n    return temp_var\n    \"\"\",\n    \"\"\"\n@public\ndef wei(i: int128) -> int128:\n    temp_var : int128 = i\n    return temp_var\n    \"\"\",\n    \"\"\"\n@public\ndef false(i: int128) -> int128:\n    temp_var : int128 = i\n    return temp_var\n    \"\"\",\n]\n\n\n@pytest.mark.parametrize('bad_code', fail_list)\ndef test_varname_validity_fail(bad_code):\n    with raises(FunctionDeclarationException):\n        compiler.compile_code(bad_code)\n\n\nvalid_list = [\n    \"\"\"\n@public\ndef func(i: int128) -> int128:\n    variable : int128 = i\n    return variable\n    \"\"\",\n    \"\"\"\n@public\ndef func_to_do_math(i: int128) -> int128:\n    var_123 : int128 = i\n    return var_123\n    \"\"\",\n    \"\"\"\n@public\ndef first1(i: int128) -> int128:\n    _var123 : int128 = i\n    return _var123\n    \"\"\",\n]\n\n\n@pytest.mark.parametrize('good_code', valid_list)\ndef test_varname_validity_success(good_code):\n    assert compiler.compile_code(good_code) is not None\n<|endoftext|>",
    "repo": "ehanoc/vyper"
  },
  {
    "prompt": "fn utcnow()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from datetime import datetime\n\nfrom dateutil.tz import UTC\nfrom google.protobuf.wrappers_pb2 import StringValue\nimport pytest\n\nfrom protobuf_serialization.tests.compiled.example_pb2 import Foo, Parent, Child, Coin, Category\nfrom protobuf_serialization.utils import write_datetime_to_message\nfrom protobuf_serialization.deserialization import protobuf_to_dict\n\n\n@pytest.fixture\ndef foo():\n    foo = Foo(\n        id=123,\n        name='val',\n    )\n    foo.content.value = 'crypto'\n    foo.is_active.value = True\n    foo.count.value = 501\n    dt = datetime(2019, 1, 1, 12, 0, tzinfo=UTC)\n    write_datetime_to_message(foo.created_at, dt)\n    return foo\n\n\ndef test_protobuf_to_dict_basic():\n    foo = Foo(\n        id=123,\n        name='val',\n    )\n    foo.content.value = 'crypto'\n    foo.is_active.value = True\n    foo.count.value = 501\n\n    dt = datetime(2019, 1, 1, 12, 0, tzinfo=UTC)\n    write_datetime_to_message(foo.created_at, dt)\n\n    data = protobuf_to_dict(foo)\n\n    assert data == dict(\n        id=123,\n        name='val',\n        content='crypto',\n        is_active=True,\n        count=501,\n        created_at=dt,\n    )\n\n\ndef test_fields(foo):\n    data = protobuf_to_dict(foo, fields=('id', 'content'))\n    assert set(data.keys()) == {'id', 'content'}\n\n\ndef test_unset_wrapped_fields():\n    # Test that None is returned for Wrapped fields that are unset\n    foo = Foo(\n        id=123,\n        name='val',\n    )\n    data = protobuf_to_dict(foo)\n    assert data == dict(\n        id=123,\n        name='val',\n        content=None,\n        is_active=None,\n        count=None,\n        created_at=None,\n    )\n\n\ndef test_omit_keys_if_null():\n    # Test omit_keys_if_null option\n    foo = Foo(\n        id=123,\n        name='val',\n    )\n    data = protobuf_to_dict(foo, omit_keys_if_null=True)\n    assert data == dict(\n        id=123,\n        name='val'\n    )\n\n    foo.content.value = 'stuff'\n    data = protobuf_to_dict(foo, omit_keys_if_null=True)\n    assert data == dict(\n        id=123,\n        name='val',\n        content='stuff'\n    )\n\n\ndef test_nested_deserialization():\n    child = Child(name='child', label=StringValue(value='small'))\n    parent = Parent(name='parent', label=StringValue(value='big'), child=child)\n\n    data = protobuf_to_dict(parent)\n    assert data == dict(\n        name='parent',\n        label='big',\n        child=dict(\n            name='child',\n            label='small',\n            category=None,\n        )\n    )\n\n\ndef test_advanced_fields():\n    coin = Coin()\n    data = protobuf_to_dict(coin)\n\n    assert data == dict(\n        tags={},\n        names=[],\n        categories=[],\n    )\n\n    coin.tags['USD'] = 'TUSD'\n    coin.tags['HKD'] = 'THKD'\n    coin.names.append('alpha')\n    coin.names.append('beta')\n    cat1 = Category(name='foo', type='cool')\n    cat2 = Category(name='bar', type='lame')\n    coin.categories.extend([cat1, cat2])\n\n    data = protobuf_to_dict(coin)\n\n    assert data == dict(\n        tags=dict(USD='TUSD', HKD='THKD'),\n        names=['alpha', 'beta'],\n        categories=[\n            dict(name='foo', type='cool'),\n            dict(name='bar', type='lame'),\n        ]\n    )\n<|endoftext|>",
    "repo": "trusttoken/protobuf-serialization-py"
  },
  {
    "prompt": "\"\"\"    \"\"\"\nfn verify_all(module, ALL)\n\nclass _TestHasProps(HasProps):\n\tx = Int(12)\n\ty = String('hello')\n\tz = List(Int, [1, 2, 3])\n\tzz = Dict(String, Int)\n\ts = String(None)\n\n\nclass _TestModel(HasProps):\n\tx = Int(12)\n\ty = String('hello')\n\tz = List(Int, [1, 2, 3])\n\tzz = Dict(String, Int)\n\ts = String(None)\n\n\nclass _TestModel2(HasProps):\n\tx = Int(12)\n\ty = String('hello')\n\tz = List(Int, [1, 2, 3])\n\tzz = Dict(String, Int)\n\ts = String(None)\n\n\n\"\"\"Accepts only the string \"auto\".\n\nUseful for properties that can be configured to behave \"automatically\".\n\nExample:\n\n    This property is often most useful in conjunction with the\n    :class:`~bokeh.core.properties.Either` property.\n\n    .. code-block:: python\n\n        >>> class AutoModel(HasProps):\n        ...     prop = Either(Float, Auto)\n        ...\n\n        >>> m = AutoModel()\n\n        >>> m.prop = 10.2\n\n        >>> m.prop = \"auto\"\n\n        >>> m.prop = \"foo\"      # ValueError !!\n\n        >>> m.prop = [1, 2, 3]  # ValueError !!\"\"\"\nclass Auto(Enum):\n\tfn __init__(self)\n\tfn __str__(self)\n\tfn _sphinx_type(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "#-----------------------------------------------------------------------------\n# Copyright (c) 2012 - 2020, Anaconda, Inc., and Bokeh Contributors.\n# All rights reserved.\n#\n# The full license is in the file LICENSE.txt, distributed with this software.\n#-----------------------------------------------------------------------------\n\n#-----------------------------------------------------------------------------\n# Boilerplate\n#-----------------------------------------------------------------------------\nimport pytest ; pytest\n\n#-----------------------------------------------------------------------------\n# Imports\n#-----------------------------------------------------------------------------\n\n# Bokeh imports\nfrom _util_property import _TestHasProps, _TestModel\nfrom bokeh._testing.util.api import verify_all\n\n# Module under test\nimport bokeh.core.property.auto as bcpa # isort:skip\n\n#-----------------------------------------------------------------------------\n# Setup\n#-----------------------------------------------------------------------------\n\nALL = (\n    'Auto',\n)\n\n#-----------------------------------------------------------------------------\n# General API\n#-----------------------------------------------------------------------------\n\nclass Test_Auto(object):\n\n    def test_valid(self) -> None:\n        prop = bcpa.Auto()\n        assert prop.is_valid(None)\n        assert prop.is_valid(\"auto\")\n\n    def test_invalid(self) -> None:\n        prop = bcpa.Auto()\n        assert not prop.is_valid(False)\n        assert not prop.is_valid(True)\n        assert not prop.is_valid(0)\n        assert not prop.is_valid(1)\n        assert not prop.is_valid(0.0)\n        assert not prop.is_valid(1.0)\n        assert not prop.is_valid(1.0+1.0j)\n        assert not prop.is_valid(\"\")\n        assert not prop.is_valid(())\n        assert not prop.is_valid([])\n        assert not prop.is_valid({})\n        assert not prop.is_valid(_TestHasProps())\n        assert not prop.is_valid(_TestModel())\n\n    def test_has_ref(self) -> None:\n        prop = bcpa.Auto()\n        assert not prop.has_ref\n\n    def test_str(self) -> None:\n        prop = bcpa.Auto()\n        assert str(prop) == \"Auto\"\n\n#-----------------------------------------------------------------------------\n# Dev API\n#-----------------------------------------------------------------------------\n\n#-----------------------------------------------------------------------------\n# Private API\n#-----------------------------------------------------------------------------\n\n#-----------------------------------------------------------------------------\n# Code\n#-----------------------------------------------------------------------------\n\nTest___all__ = verify_all(bcpa, ALL)\n<|endoftext|>",
    "repo": "gmanolia/bokeh"
  },
  {
    "prompt": "class CalDAVStorage(RustStorage):\n\tstorage_name = 'caldav'\n\tfileext = '.ics'\n\t_repr_attributes = ('username', 'url')\n\tstart_date = None\n\tend_date = None\n\tfn __init__(self, url, username, password, useragent, verify_cert, auth_cert, auth_cert_password, start_date, end_date, item_types)\n\n\nclass CardDAVStorage(RustStorage):\n\tstorage_name = 'carddav'\n\tfileext = '.vcf'\n\t_repr_attributes = ('username', 'url')\n\tfn __init__(self, url, username, password, useragent, verify_cert, auth_cert, auth_cert_password)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# -*- coding: utf-8 -*-\n\nimport datetime\nfrom textwrap import dedent\n\nimport pytest\n\nfrom tests import EVENT_TEMPLATE, TASK_TEMPLATE, VCARD_TEMPLATE\n\nfrom vdirsyncer.storage.dav import CalDAVStorage\n\nfrom . import DAVStorageTests, dav_server\nfrom .. import format_item\n\n\nclass TestCalDAVStorage(DAVStorageTests):\n    storage_class = CalDAVStorage\n\n    @pytest.fixture(params=['VTODO', 'VEVENT'])\n    def item_type(self, request):\n        return request.param\n\n    def test_doesnt_accept_vcard(self, item_type, get_storage_args):\n        args = get_storage_args()\n        args['item_types'] = (item_type,)\n        s = self.storage_class(**args)\n\n        try:\n            s.upload(format_item(item_template=VCARD_TEMPLATE))\n        except Exception:\n            pass\n        assert not list(s.list())\n\n    @pytest.mark.xfail(dav_server == 'radicale',\n                       reason='Radicale doesn\\'t support timeranges.')\n    def test_timerange_correctness(self, get_storage_args):\n        start_date = datetime.datetime(2013, 9, 10)\n        end_date = datetime.datetime(2013, 9, 13)\n        args = get_storage_args()\n        args['start_date'] = start_date\n        args['end_date'] = end_date\n        s = self.storage_class(**args)\n\n        too_old_item = format_item(item_template=dedent(u'''\n            BEGIN:VCALENDAR\n            VERSION:2.0\n            PRODID:-//hacksw/handcal//NONSGML v1.0//EN\n            BEGIN:VEVENT\n            DTSTART:19970714T170000Z\n            DTEND:19970715T035959Z\n            SUMMARY:Bastille Day Party\n            X-SOMETHING:{r}\n            UID:{r}\n            END:VEVENT\n            END:VCALENDAR\n            ''').strip())\n\n        too_new_item = format_item(item_template=dedent(u'''\n            BEGIN:VCALENDAR\n            VERSION:2.0\n            PRODID:-//hacksw/handcal//NONSGML v1.0//EN\n            BEGIN:VEVENT\n            DTSTART:20150714T170000Z\n            DTEND:20150715T035959Z\n            SUMMARY:Another Bastille Day Party\n            X-SOMETHING:{r}\n            UID:{r}\n            END:VEVENT\n            END:VCALENDAR\n            ''').strip())\n\n        good_item = format_item(item_template=dedent(u'''\n            BEGIN:VCALENDAR\n            VERSION:2.0\n            PRODID:-//hacksw/handcal//NONSGML v1.0//EN\n            BEGIN:VEVENT\n            DTSTART:20130911T170000Z\n            DTEND:20130912T035959Z\n            SUMMARY:What's with all these Bastille Day Partys\n            X-SOMETHING:{r}\n            UID:{r}\n            END:VEVENT\n            END:VCALENDAR\n            ''').strip())\n\n        s.upload(too_old_item)\n        s.upload(too_new_item)\n        expected_href, _ = s.upload(good_item)\n\n        (actual_href, _), = s.list()\n        assert actual_href == expected_href\n\n    @pytest.mark.skipif(dav_server == 'icloud',\n                        reason='iCloud only accepts VEVENT')\n    def test_item_types_general(self, get_storage_args):\n        args = get_storage_args()\n        s = self.storage_class(**args)\n        event = s.upload(format_item(item_template=EVENT_TEMPLATE))[0]\n        task = s.upload(format_item(item_template=TASK_TEMPLATE))[0]\n\n        for item_types, expected_items in [\n            (('VTODO', 'VEVENT'), {event, task}),\n            (('VTODO',), {task}),\n            (('VEVENT',), {event}),\n        ]:\n            args['item_types'] = item_types\n            s = self.storage_class(**args)\n            assert set(href for href, etag in s.list()) == expected_items\n<|endoftext|>",
    "repo": "tbm/vdirsyncer"
  },
  {
    "prompt": "class UserChangeForm(admin_forms.UserChangeForm):\n\n\nclass UserCreationForm(admin_forms.UserCreationForm):\n\n\nclass UserDetailForm(forms.ModelForm):\n\tfn __init__(self)\n\n\nclass ApplicantUserForm(forms.ModelForm):\n\tfn __init__(self)\n\t\"\"\"Check if user exists in the database, if not add them and email a welcome message.\"\"\"\n\tfn add_user(self, request, email, password)\n\t\"\"\"Check if user exists in the database, if not add them and email a welcome message.\"\"\"\n\tfn authenticate_user(self, request, username, password)\n\n\n\"\"\"User leaving information to be contacted by goldengate\"\"\"\nclass AccountHolderForm(forms.Form):\n\tusername = forms.EmailField(max_length=100)\n\tphone_number = forms.RegexField(regex='^\\\\+?1?\\\\d{9,15}$', error_messages={'required': \"Phone number must be entered in the format: '+999999999'. Up to 15 digits allowed.\"}, required=False)\n\tpassword = forms.CharField(widget=forms.PasswordInput())\n\t\"\"\"Check if user exists in the database, if not add them and email a welcome message.\"\"\"\n\tfn authenticate_user(self, request, username, password)\n\n\n\"\"\"Default user for GoldenGate.\"\"\"\nclass ApplicantUser(AbstractBaseUser, PermissionsMixin):\n\tfirst_name = CharField(max_length=100)\n\tlast_name = CharField(max_length=100)\n\tuser_email = EmailField(max_length=100, unique=True)\n\tphone_number = PhoneNumberField(blank=True)\n\tcounty = CharField(max_length=100)\n\tdistrict = CharField(max_length=100)\n\tdivision = CharField(max_length=100)\n\tis_staff = BooleanField(required=True)\n\tis_active = BooleanField(required=True)\n\tUSERNAME_FIELD = 'user_email'\n\tREQUIRED_FIELDS = ['first_name', 'last_name']\n\tobjects = ApplicantUserManager()\n\tfn __str__(self)\n\t\"\"\"Get url for user's detail view.\n\tReturns:\n\t    str: URL for user detail.\"\"\"\n\tfn get_absolute_url(self) -> str\n\tfn has_perm(self, perm, obj)\n\tfn has_module_perms(self, app_label)\n\t\"\"\"Is the user a member of staff?\"\"\"\n\tfn is_staff(self)\n\t\"\"\"Is the user admin?\"\"\"\n\tfn is_admin(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"\nModule for all Form Tests.\n\"\"\"\nimport pytest\nfrom django.utils.translation import ugettext_lazy as _\n\nfrom goldengate.users.forms import UserCreationForm\nfrom goldengate.users.models import User\n\npytestmark = pytest.mark.django_db\n\n\nclass TestUserCreationForm:\n    \"\"\"\n    Test class for all tests related to the UserCreationForm\n    \"\"\"\n\n    def test_username_validation_error_msg(self, user: User):\n        \"\"\"\n        Tests UserCreation Form's unique validator functions correctly by testing:\n            1) A new user with an existing username cannot be added.\n            2) Only 1 error is raised by the UserCreation Form\n            3) The desired error message is raised\n        \"\"\"\n\n        # The user already exists,\n        # hence cannot be created.\n        form = UserCreationForm(\n            {\n                \"username\": user.username,\n                \"password1\": user.password,\n                \"password2\": user.password,\n            }\n        )\n\n        assert not form.is_valid()\n        assert len(form.errors) == 1\n        assert \"username\" in form.errors\n        assert form.errors[\"username\"][0] == _(\"This username has already been taken.\")\n<|endoftext|>",
    "repo": "knoahlr/goldengate"
  },
  {
    "prompt": "\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# ============LICENSE_START=======================================================\n#  Copyright (C) 2020 Nordix Foundation.\n# ================================================================================\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n# ============LICENSE_END=========================================================\n\nimport os\nimport socket\nimport ssl\nimport tarfile\nimport tempfile\nimport time\nfrom io import StringIO\nfrom typing import List\n\nimport docker\nimport pytest\nfrom docker.models.containers import Container\nfrom lxml import etree\nfrom ncclient.transport.ssh import MSG_DELIM\n\nimport settings\n\nHELLO_DTD = etree.DTD(StringIO(\"\"\"\n<!ELEMENT hello (capabilities, session-id)>\n<!ATTLIST hello xmlns CDATA #REQUIRED>\n<!ELEMENT capabilities (capability+)>\n<!ELEMENT capability (#PCDATA)>\n<!ELEMENT session-id (#PCDATA)>\n\"\"\"))\n\nINITIAL_CONFIG_DIR = \"data/tls_initial\"\nNEW_CONFIG_DIR = \"data/tls_new\"\n\n\nclass TestTLS:\n    container: Container\n\n    @classmethod\n    def setup_class(cls):\n        dkr = docker.from_env()\n        containers = dkr.containers.list(filters={\"ancestor\": \"netconf-pnp-simulator:latest\"})\n        assert len(containers) == 1\n        cls.container = containers[0]\n\n    def test_tls_connect(self):\n        nc_connect(INITIAL_CONFIG_DIR)\n\n    @pytest.mark.parametrize(\"round_id\", [f\"round #{i + 1}\" for i in range(6)])\n    def test_tls_reconfiguration(self, round_id):\n        # pylint: disable=W0613\n        self.reconfigure_and_check(NEW_CONFIG_DIR, INITIAL_CONFIG_DIR)\n        self.reconfigure_and_check(INITIAL_CONFIG_DIR, NEW_CONFIG_DIR)\n\n    def reconfigure_and_check(self, good_config_dir: str, bad_config_dir: str):\n        with simple_tar([f\"{good_config_dir}/{b}.pem\" for b in [\"ca\", \"server_key\", \"server_cert\"]]) as config_tar:\n            status = self.container.put_archive(f\"/config/tls\", config_tar)\n            assert status\n        test_start = int(time.time())\n        exit_code, (_, err) = self.container.exec_run(\"/opt/bin/reconfigure-tls.sh\", demux=True)\n        if exit_code != 0:\n            print(f\"reconfigure-tls.sh failed with rc={exit_code}\")\n            log_all(\"stderr\", err)\n            log_all(\"Container Logs\", self.container.logs(since=test_start))\n            assert False\n        nc_connect(good_config_dir)\n        # Exception matching must be compatible with Py36 and Py37+\n        with pytest.raises(ssl.SSLError, match=r\".*\\[SSL: CERTIFICATE_VERIFY_FAILED\\].*\"):\n            nc_connect(bad_config_dir)\n\n\ndef log_all(heading: str, lines: object):\n    print(f\"{heading}:\")\n    if isinstance(lines, bytes):\n        lines = lines.decode(\"utf-8\")\n    if isinstance(lines, str):\n        lines = lines.split(\"\\n\")\n    for line in lines:\n        print(\" \", line)\n\n\ndef simple_tar(paths: List[str]):\n    file = tempfile.NamedTemporaryFile()\n    with tarfile.open(mode=\"w\", fileobj=file) as tar:\n        for path in paths:\n            abs_path = os.path.abspath(path)\n            tar.add(abs_path, arcname=os.path.basename(path), recursive=False)\n    file.seek(0)\n    return file\n\n\ndef nc_connect(config_dir: str):\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0) as sock:\n        context = ssl.create_default_context()\n        context.load_verify_locations(f\"{config_dir}/ca.pem\")\n        context.load_cert_chain(certfile=f\"{config_dir}/client_cert.pem\", keyfile=f\"{config_dir}/client_key.pem\")\n        context.check_hostname = False\n        with context.wrap_socket(sock, server_side=False, server_hostname=settings.HOST) as conn:\n            conn.connect((settings.HOST, settings.TLS_PORT))\n            buf = nc_read_msg(conn)\n            print(f\"Received NETCONF HelloMessage:\\n{buf}\")\n            conn.close()\n            assert buf.endswith(MSG_DELIM)\n            hello_root = etree.XML(buf[:-len(MSG_DELIM)])\n            valid = HELLO_DTD.validate(hello_root)\n            if not valid:\n                log_all(\"Invalid NETCONF <hello> msg\", list(HELLO_DTD.error_log.filter_from_errors()))\n                assert False\n\n\ndef nc_read_msg(conn: ssl.SSLSocket):\n    buf = ''\n    while True:\n        data = conn.recv(4096)\n        if data:\n            buf += data.decode(encoding=\"utf-8\")\n            if buf.endswith(MSG_DELIM):\n                break\n        else:\n            break\n    return buf\n<|endoftext|>",
    "repo": "eliezio/netconf-pnp-simulator"
  },
  {
    "prompt": "fn make_fake_event() -> Type[Event]\n\nfn load_plugin(nonebug_init: None) -> Set[Plugin]\n\n\"\"\"Make a snapshot for sys.modules before initializing.\nClear nonebot and other modules not in snapshot after test case running completed.\nEnsure every test case has a clean nonebot environment.\n\nBy default, this fixture will be auto called by `nonebug_init`.\"\"\"\nfn nonebug_clear() -> Generator[(None, None, None)]\n\n\"\"\"Initialize nonebot before test case running.\nAnd clear nonebot after test case running completed.\"\"\"\nfn nonebug_init(nonebug_clear: None, request) -> None\n\n\"\"\"Get a test app provided by nonebug.\nUse app to define test cases and run them.\"\"\"\nfn nonebug_app(nonebug_init: None, monkeypatch: pytest.MonkeyPatch) -> App\n\n\"\"\"Call `nonebug_init` and return a new instance of Processor Test App.\"\"\"\nfn processor_app(nonebug_init: None, monkeypatch: pytest.MonkeyPatch) -> ProcessorApp\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom utils import make_fake_event\n\nfrom nonebug.fixture import *\nfrom nonebug import ProcessorApp\n\n\n@pytest.mark.asyncio\nasync def test_dependent(app: \"ProcessorApp\"):\n    from nonebot.adapters import Event\n    from nonebot.params import EventParam\n    from nonebot.exception import TypeMisMatch\n\n    FakeEvent = make_fake_event(test_field=(str, \"test\"))\n    FakeEvent2 = make_fake_event(test_field2=(str, \"test2\"))\n\n    def _handle(event: Event):\n        ...\n\n    def _handle_fake(event: FakeEvent):\n        ...\n\n    def _handle_return():\n        return True\n\n    async with app.test_dependent(_handle, allow_types=[EventParam]) as ctx:\n        ctx.pass_params(event=FakeEvent())\n    async with app.test_dependent(_handle_fake, allow_types=[EventParam]) as ctx:\n        ctx.pass_params(event=FakeEvent())\n\n    event = FakeEvent2()\n    try:\n        async with app.test_dependent(_handle_fake, allow_types=[EventParam]) as ctx:\n            ctx.pass_params(event=event)\n    except TypeMisMatch as e:\n        assert e.param.name == \"event\"\n        assert e.value is event\n    else:\n        assert False, \"handler should be skipped\"\n\n    try:\n        async with app.test_dependent(_handle_return) as ctx:\n            ctx.should_return(False)\n    except AssertionError as e:\n        pass\n    else:\n        assert False, \"Handler return value should be checked\"\n<|endoftext|>",
    "repo": "nonebot/nonebug"
  },
  {
    "prompt": "class Client:\n\tmsg_sender: MessageSender\n\tdisconnector: Disconnector\n\tname: Optional[str] = None\n\tfn __hash__(self) -> int\n\n\nclass Response:\n\tcode: HTTPStatus\n\tmsg: str\n\n\nclass Probe:\n\n\nclass ChatToClient:\n\tsender: str\n\tmessage: str\n\troom: Optional[str] = None\n\n\nclass MessageSender:\n\tfn __init__(self, serializer: Serializer, time_factory: TimeFactory) -> None\n\tfn send(self, msg: Message) -> None\n\tfn _convert(msg: Message) -> JSON\n\n\nclass Disconnector:\n\tfn __init__(self) -> None\n\tfn disconnect(self) -> None\n\tfn should_disconnect(self) -> bool\n\n\nfn _auth_deco(holder: AuthClientsHolder, func: _ServerMessageHandler) -> _ServerMessageHandler\n\nclass AuthClientsHolder:\n\tfn __init__(self) -> None\n\tfn add_client(self, client: Client) -> None\n\tfn remove_client(self, client: Client) -> None\n\tfn is_authed(self, client: Client) -> bool\n\tfn find_client(self, name: str) -> Optional[Client]\n\tfn all(self) -> ValuesView[Client]\n\tfn required(self) -> Callable[([_ServerMessageHandler], _ServerMessageHandler)]\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from http import HTTPStatus\nfrom unittest.mock import MagicMock\n\nimport pytest\nfrom gb_chat.common.disconnector import Disconnector\nfrom gb_chat.io.message_sender import MessageSender\nfrom gb_chat.msg.server_to_client import Response\nfrom gb_chat.server.auth_clients_holder import AuthClientsHolder\nfrom gb_chat.server.client import Client\n\n\n@pytest.fixture\ndef client():\n    return Client(MagicMock(spec_set=MessageSender), MagicMock(spec_set=Disconnector))\n\n\n@pytest.fixture\ndef sut():\n    return AuthClientsHolder()\n\n\ndef test_add_client_raises_when_no_name(sut, client):\n    with pytest.raises(ValueError):\n        sut.add_client(client)\n\n\n@pytest.fixture\ndef client_with_name():\n    return Client(\n        MagicMock(spec_set=MessageSender), MagicMock(spec_set=Disconnector), \"username\"\n    )\n\n\ndef test_add_client_with_name(sut, client_with_name):\n    sut.add_client(client_with_name)\n    assert sut.is_authed(client_with_name)\n    assert sut.find_client(client_with_name.name) is client_with_name\n    assert [x for x in sut.all] == [client_with_name]\n\n\ndef test_remove_client_raises_when_no_name(sut, client):\n    with pytest.raises(ValueError):\n        sut.remove_client(client)\n\n\ndef test_remove_client_raises_when_client_not_in_list(sut, client_with_name):\n    with pytest.raises(ValueError):\n        sut.remove_client(client_with_name)\n\n\ndef test_remove_client(sut, client_with_name):\n    sut.add_client(client_with_name)\n    sut.remove_client(client_with_name)\n    assert not sut.is_authed(client_with_name)\n    assert sut.find_client(client_with_name.name) is None\n\n\nclass Sut:\n    auth = AuthClientsHolder()\n\n    def __init__(self, mock) -> None:\n        self.mock = mock\n\n    @auth.required\n    def method(self, msg, from_client):\n        self.mock(msg, from_client)\n\n\ndef test_required_disconnect_client_when_not_authed(client_with_name):\n    mock = MagicMock()\n    sut = Sut(mock)\n    sut.method(MagicMock(), client_with_name)\n    mock.assert_not_called()\n    client_with_name.msg_sender.send.assert_called_once_with(\n        Response(HTTPStatus.UNAUTHORIZED, \"Allowed only for authed users\")\n    )\n    client_with_name.disconnector.disconnect.assert_not_called()\n\n\ndef test_required_called_when_authed(client_with_name):\n    mock = MagicMock()\n    msg = MagicMock()\n    sut = Sut(mock)\n    sut.auth.add_client(client_with_name)\n    sut.method(msg, client_with_name)\n    mock.assert_called_once_with(msg, client_with_name)\n    client_with_name.msg_sender.send.assert_not_called()\n    client_with_name.disconnector.disconnect.assert_not_called()\n<|endoftext|>",
    "repo": "Cerzon/gb_chat"
  },
  {
    "prompt": "class CustomSignupForm(SignupForm):\n\tfn __init__(self)\n\tfn clean_email(self)\n\tfn save(self, request)\n\n\n\"\"\"Provide a custom form for setting password.\n\nThis causes resetting a password to imply the email has been verified.\"\"\"\nclass CustomResetPasswordKeyForm(ResetPasswordKeyForm):\n\tfn save(self)\n\n\nclass AcceptInvitationForm(forms.Form):\n\tinvitation_id = forms.IntegerField(widget=forms.HiddenInput())\n\tfn __init__(self)\n\tfn clean_invitation_id(self)\n\n\nclass CreateInvitationForm(forms.ModelForm):\n\tfn __init__(self)\n\tfn clean_recipient(self)\n\n\nclass TeamForm(forms.ModelForm):\n\tinitial_invite_1 = forms.EmailField(required=False)\n\tinitial_invite_2 = forms.EmailField(required=False)\n\tinitial_invite_3 = forms.EmailField(required=False)\n\tfn __init__(self)\n\tfn clean(self)\n\tfn get_invites(self)\n\n\nclass CreateSubmissionForm(forms.ModelForm):\n\tfn __init__(self)\n\tfn clean_accepted_terms(self)\n\tfn clean_test_prediction_file(self)\n\tfn clean(self)\n\n\nclass ApproachForm(forms.ModelForm):\n\tfn __init__(self)\n\tfn clean_description(self)\n\tfn clean(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from datetime import timedelta\n\nfrom django.core.files.uploadedfile import SimpleUploadedFile\nfrom django.http import HttpRequest\nfrom django.utils import timezone\nimport pytest\n\nfrom stade.core.forms import CreateSubmissionForm\n\n\n@pytest.fixture\ndef limited_approach(transactional_db, approach_factory):\n    a = approach_factory(task__max_submissions_per_week=1, task__locked=False)\n    # this object must be saved because we need an id to submit the form\n    a.save()\n\n    # add user to team\n    a.team.users.add(a.team.creator)\n\n    return a\n\n\n@pytest.mark.django_db\ndef test_submission_throttling(limited_approach):\n    request = HttpRequest()\n    request.user = limited_approach.team.creator\n\n    for _ in range(limited_approach.task.max_submissions_per_week):\n        form = CreateSubmissionForm(\n            data={'accepted_terms': True},\n            files={'test_prediction_file': SimpleUploadedFile('some-file.csv', b'foo')},\n            approach_id=limited_approach.id,\n            request=request,\n        )\n        form.instance.approach = limited_approach\n        form.instance.creator = limited_approach.team.creator\n        assert form.is_valid(), form.errors\n        form.save()\n\n    form = CreateSubmissionForm(\n        data={'accepted_terms': True},\n        files={'test_prediction_file': SimpleUploadedFile('some-file.csv', b'foo')},\n        approach_id=limited_approach.id,\n        request=request,\n    )\n    form.instance.approach = limited_approach\n    form.instance.creator = limited_approach.team.creator\n    assert not form.is_valid(), form.errors\n\n\n@pytest.mark.django_db\ndef test_next_available_submission(limited_approach, submission_factory):\n    start = timezone.now()\n    submission_creation_time = start - timedelta(days=1)\n\n    submission_factory(created=submission_creation_time, approach=limited_approach)\n\n    # The approach should now be rate limited\n    assert (\n        limited_approach.task.pending_or_succeeded_submissions(limited_approach.team).count()\n        == limited_approach.task.max_submissions_per_week\n    )\n\n    # The next available submission should be in 6 days\n    assert limited_approach.task.next_available_submission(\n        limited_approach.team\n    ) == submission_creation_time + timedelta(weeks=1)\n<|endoftext|>",
    "repo": "ImageMarkup/stade"
  },
  {
    "prompt": "class TestBaseClass:\n\tTEST_UNDEFINED_PARAMETER = 'this is an undefined parameter to work around pytest limitations'\n\tfn setup_class(cls)\n\tfn __pytest_empty_object_fixture(self, _input, default)\n\tfn _create_tempfile(self, _file, _input)\n\tfn _create_args(self, input, extraopts)\n\tfn _create_args_fix(self, input, extraopts)\n\tfn fix_and_check(self, args, id)\n\tfn _create_args_parser(self)\n\tfn check_for_id(self, args, id, occurences)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import os\nimport sys\n\nimport pytest\n\nsys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))\n\nfrom base import TestBaseClass\n\nclass TestClassOelintVarRootFSCmd(TestBaseClass):\n\n    @pytest.mark.parametrize('id', ['oelint.var.rootfspostcmd'])\n    @pytest.mark.parametrize('occurance', [1])\n    @pytest.mark.parametrize('input', \n        [\n            {\n            'oelint_adv_test.bb':\n            '''\n            ROOTFS_POSTPROCESS_COMMAND += \"abc; adef ;;\"\n            '''\n            },\n        ],\n    )\n    def test_bad(self, input, id, occurance):\n        self.check_for_id(self._create_args(input), id, occurance)\n\n    @pytest.mark.parametrize('id', ['oelint.var.rootfspostcmd'])\n    @pytest.mark.parametrize('occurance', [0])\n    @pytest.mark.parametrize('input', \n        [\n            {\n            'oelint_adv_test.bb':\n            '''\n            ROOTFS_POSTPROCESS_COMMAND += \"abc; adef;;\"\n            '''\n            },\n        ],\n    )\n    def test_good(self, input, id, occurance):\n        self.check_for_id(self._create_args(input), id, occurance)<|endoftext|>",
    "repo": "Rahix/oelint-adv"
  },
  {
    "prompt": "\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom tests.fsm import TestStates\nfrom waio import Dispatcher, Bot\nfrom waio.factory.base import ResponseModel\nfrom waio.factory.factory import factory_gupshup\nfrom waio.rules import StateRule\nfrom waio.storage import RedisStorage\nfrom waio.types import Message\n\nbot = Bot(\n    apikey='FAKE_API_KEY',\n    src_name='FAKE_SRC_NAME',\n    phone_number=79289998877\n)\n\nstorage = RedisStorage(prefix_fsm='fsm', redis_url=\"redis://localhost:6379\")\ndp = Dispatcher(bot=bot, storage=storage)\n\n\n@pytest.mark.asyncio\nasync def test_check_state_filter_true():\n    _state = dp.state(user_phone=79990000000)\n    await _state.set_state(TestStates.email)\n    message_json = {\n        \"app\": \"DemoApp\",\n        \"timestamp\": 1580227766370,\n        \"version\": 2,\n        \"type\": \"message\",\n        \"payload\": {\n            \"id\": \"ABEGkYaYVSEEAhAL3SLAWwHKeKrt6s3FKB0c\",\n            \"source\": \"79289998877\",\n            \"type\": \"text\",\n            \"payload\": {\n                \"text\": \"Hi\"\n            },\n            \"sender\": {\n                \"phone\": \"79990000000\",\n                \"name\": \"Smit\",\n                \"country_code\": \"7\",\n                \"dial_code\": \"9990000000\"\n            }\n        }\n    }\n    data_load = factory_gupshup.load(message_json, ResponseModel)\n    message_model = Message(bot=dp.bot, message=data_load, state_func=dp.state)\n    rule = StateRule(state=TestStates.email)\n\n    check_filter = await rule.check(message=message_model)\n    assert check_filter is True\n\n    await _state.finish(clear_data=True)\n\n\n@pytest.mark.asyncio\nasync def test_check_state_filter_false():\n    _state = dp.state(user_phone=79990000000)\n    await _state.set_state(TestStates.email)\n\n    message_json = {\n        \"app\": \"DemoApp\",\n        \"timestamp\": 1580227766370,\n        \"version\": 2,\n        \"type\": \"message\",\n        \"payload\": {\n            \"id\": \"ABEGkYaYVSEEAhAL3SLAWwHKeKrt6s3FKB0c\",\n            \"source\": \"79289998877\",\n            \"type\": \"text\",\n            \"payload\": {\n                \"text\": \"Hi\"\n            },\n            \"sender\": {\n                \"phone\": \"79287776655\",\n                \"name\": \"Anna\",\n                \"country_code\": \"7\",\n                \"dial_code\": \"9287776655\"\n            }\n        }\n    }\n    data_load = factory_gupshup.load(message_json, ResponseModel)\n    message_model = Message(bot=dp.bot, message=data_load, state_func=dp.state)\n    rule = StateRule(state=TestStates.email)\n\n    check_filter = await rule.check(message=message_model)\n    assert check_filter is False\n\n    await _state.finish(clear_data=True)<|endoftext|>",
    "repo": "dotX12/waio"
  },
  {
    "prompt": "class UserFactory(DjangoModelFactory):\n\tusername = Faker('user_name')\n\temail = Faker('email')\n\tname = Faker('name')\n\tfn password(self, create: bool, extracted: Sequence[Any])\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom celery.result import EagerResult\n\nfrom translations_tool.users.tasks import get_users_count\nfrom translations_tool.users.tests.factories import UserFactory\n\npytestmark = pytest.mark.django_db\n\n\ndef test_user_count(settings):\n    \"\"\"A basic test to execute the get_users_count Celery task.\"\"\"\n    UserFactory.create_batch(3)\n    settings.CELERY_TASK_ALWAYS_EAGER = True\n    task_result = get_users_count.delay()\n    assert isinstance(task_result, EagerResult)\n    assert task_result.result == 3\n<|endoftext|>",
    "repo": "dzieje-khorinis/Translations-Tool"
  },
  {
    "prompt": "\"\"\"A helper for easy registering and unregistering event handlers.\"\"\"\nclass EventWatcher:\n\tfn __init__(self, element: Element, element_dispatcher: ElementDispatcher | None, default_handler: Handler | None)\n\t\"\"\"Watch a certain path of elements starting with the DiagramItem. The\n\thandler is optional and will default the default provided at\n\tconstruction time.\n\t\n\tWatches should be set in the constructor, so they can be registered\n\tand unregistered in one shot.\n\t\n\tThis interface is fluent (returns self).\"\"\"\n\tfn watch(self, path: str, handler: Handler | None) -> EventWatcher\n\t\"\"\"Unregister handlers.\n\t\n\tExtra arguments are ignored (makes connecting to destroy signals\n\tmuch easier though).\"\"\"\n\tfn unsubscribe_all(self)\n\n\n\"\"\"The Element based Dispatcher allows handlers to receive only events\nrelated to certain elements. Those elements should be registered too. A\npath should be provided, that is used to find those changes.\n\nThe handlers are registered on their property attribute. This avoids\nsubclass lookups and is pretty specific. As a result this dispatcher is\ntailored for dispatching events from the data model (ElementUpdated)\n\nFor example: if you're a TransitionItem (gaphor.core.modeling.Presentation\ninstance) and you're interested in the value of the guard attribute of the\nmodel element that's represented by this item (gaphor.UML.Transition), you\ncan register a handler like this::\n\n  dispatcher.subscribe(element,\n          'guard.specification[LiteralSpecification].value', self._handler)\n\nNote the '[' and ']'. This is because guard references ValueSpecification,\nwhich does not have a value attribute. Therefore, the default reference type\nis overruled in favour of the LiteralSpecification.\n\nThis dispatcher keeps track of the kind of events that are dispatched. The\ndispatcher table is updated accordingly (so the right handlers are fired\nevery time).\"\"\"\nclass ElementDispatcher(Service):\n\tfn __init__(self, event_manager, modeling_language)\n\tfn shutdown(self) -> None\n\tfn subscribe(self, handler: Handler, element: Element, path: str) -> None\n\t\"\"\"Unregister a handler from the registry.\"\"\"\n\tfn unsubscribe(self, handler: Handler) -> None\n\t\"\"\"Given a start element and a path, return a tuple of properties\n\t(association, attribute, etc.) representing the path.\"\"\"\n\tfn _path_to_properties(self, element, path)\n\t\"\"\"Provided an element and a path of properties (props), register the\n\thandler for each property.\"\"\"\n\tfn _add_handlers(self, element, props, handler)\n\t\"\"\"Remove the handler of the path of elements.\"\"\"\n\tfn _remove_handlers(self, element, property, handler)\n\tfn on_element_change_event(self, event)\n\tfn on_model_loaded(self, event)\n\n\n\"\"\"Mark a function/method as an event handler for a particular type of\nevent.\"\"\"\nfn event_handler()\n\n\"\"\"The Event Manager.\"\"\"\nclass EventManager(Service):\n\tfn __init__(self) -> None\n\tfn shutdown(self) -> None\n\t\"\"\"Register a handler.\n\t\n\tHandlers are triggered (executed) when specific events are\n\temitted through the handle() method.\"\"\"\n\tfn subscribe(self, handler: Handler) -> None\n\t\"\"\"Unregister a previously registered handler.\"\"\"\n\tfn unsubscribe(self, handler: Handler) -> None\n\t\"\"\"Send event notifications to registered handlers.\"\"\"\n\tfn handle(self) -> None\n\n\nclass CoreModelingLanguage(ModelingLanguage):\n\tfn name(self) -> str\n\tfn toolbox_definition(self)\n\tfn diagram_types(self)\n\tfn lookup_element(self, name)\n\n\n\"\"\"This class can be used to instantly combine modeling languages.\"\"\"\nclass MockModelingLanguage(ModelingLanguage):\n\tfn __init__(self)\n\tfn name(self) -> str\n\tfn toolbox_definition(self)\n\tfn diagram_types(self)\n\tfn lookup_element(self, name)\n\n\nclass UMLModelingLanguage(ModelingLanguage):\n\tfn name(self) -> str\n\tfn toolbox_definition(self) -> ToolboxDefinition\n\t\"\"\"Return an iterator (id, name) for each diagram type.\"\"\"\n\tfn diagram_types(self) -> Iterable[DiagramType]\n\tfn lookup_element(self, name)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"Basic test case for Gaphor tests.\n\nEverything is about services so the Case can define it's required\nservices and start off.\n\"\"\"\nfrom __future__ import annotations\n\n# isort: skip_file\n\nfrom io import StringIO\nfrom pathlib import Path\n\nimport pytest\n\n# Load gaphor.ui first, so GTK library versions are set corrently\nimport gaphor.ui\n\nfrom gaphor.core import Transaction\nfrom gaphor.core.eventmanager import EventManager\nfrom gaphor.core.modeling import Diagram, ElementFactory\nfrom gaphor.core.modeling.elementdispatcher import ElementDispatcher\nfrom gaphor.core.modeling.modelinglanguage import (\n    CoreModelingLanguage,\n    MockModelingLanguage,\n)\nfrom gaphor.storage import storage\nfrom gaphor.UML.modelinglanguage import UMLModelingLanguage\n\n\n@pytest.fixture\ndef event_manager():\n    return EventManager()\n\n\n@pytest.fixture\ndef element_factory(event_manager, modeling_language):\n    element_factory = ElementFactory(\n        event_manager, ElementDispatcher(event_manager, modeling_language)\n    )\n    yield element_factory\n    element_factory.shutdown()\n\n\n@pytest.fixture\ndef modeling_language():\n    return MockModelingLanguage(CoreModelingLanguage(), UMLModelingLanguage())\n\n\n@pytest.fixture\ndef diagram(element_factory, event_manager):\n    with Transaction(event_manager):\n        diagram = element_factory.create(Diagram)\n    yield diagram\n    with Transaction(event_manager):\n        diagram.unlink()\n\n\n@pytest.fixture\ndef create(diagram, element_factory):\n    def _create(item_class, element_class=None):\n        return diagram.create(\n            item_class,\n            subject=(element_factory.create(element_class) if element_class else None),\n        )\n\n    return _create\n\n\n@pytest.fixture\ndef kindof(element_factory):\n    def _kindof(type):\n        return element_factory.lselect(type)\n\n    return _kindof\n\n\n@pytest.fixture\ndef saver(element_factory):\n    def save():\n        \"\"\"Save diagram into string.\"\"\"\n\n        f = StringIO()\n        storage.save(f, element_factory)\n        data = f.getvalue()\n        f.close()\n\n        return data\n\n    return save\n\n\n@pytest.fixture\ndef loader(element_factory, modeling_language):\n    def load(data):\n        \"\"\"Load data from specified string.\"\"\"\n        element_factory.flush()\n        assert not list(element_factory.select())\n\n        f = StringIO(data)\n        storage.load(f, factory=element_factory, modeling_language=modeling_language)\n        f.close()\n\n    return load\n\n\n@pytest.fixture\ndef test_models():\n    \"\"\"The folder where test models can be found.\"\"\"\n    return Path(__file__).parent.parent / \"test-models\"\n\n\n@pytest.fixture\ndef models():\n    \"\"\"The folder where test models can be found.\"\"\"\n    return Path(__file__).parent.parent / \"models\"\n<|endoftext|>",
    "repo": "MartinIIOT/gaphor"
  },
  {
    "prompt": "\"\"\"select the target columns\n\nArgs:\n    source_df:\n        input dataframe.\n    column:\n        Explicitly used columns. if set `\"__all__\"`, return all columns.\n        column \u3068 exlucdes \u306f\u540c\u6642\u306b\u8a2d\u5b9a\u3067\u304d\u307e\u305b\u3093 (\u3069\u3061\u3089\u304b\u4e00\u65b9\u3060\u3051\u9078\u629e\u3067\u304d\u307e\u3059)\n    excludes:\n        Explicitly remove columns\nReturns:\"\"\"\nfn get_target_columns(source_df: pd.DataFrame, column: Union[(str, List)], excludes: Union[(None, List)]) -> Iterable\n\n\"\"\"apply feature engineering for each columns.\"\"\"\nclass ColumnWiseBlock(BaseBlock):\n\tengine = None\n\t\"\"\"Args:\n\t    name:\n\t        this block name.\n\t    column:\n\t        use columns. if set `\"__all__\"`, use all columns get from parent blocks\n\t    excludes:\n\t        if set, exclude these columns from column.\n\t        [NOTE]\n\t            when set specific column (ex. ['foo', 'bar']), excludes must be None.\n\t    **kwargs:\"\"\"\n\tfn __init__(self, name, column: Union[(str, List)], excludes: Union[(None, List)])\n\tfn create_new_engine(self, column_name: str) -> BaseEngine\n\tfn unzip(self, experiment: ExperimentBackend)\n\tfn frozen(self, experiment: ExperimentBackend)\n\tfn get_output_colname(self, column)\n\tfn fit(self, source_df, y, experiment) -> pd.DataFrame\n\tfn transform(self, source_df)\n\n\nclass FilterBlock(ColumnWiseBlock):\n\tfn fit(self, source_df, y, experiment) -> pd.DataFrame\n\tfn transform(self, source_df)\n\tfn frozen(self, experiment: ExperimentBackend)\n\tfn unzip(self, experiment: ExperimentBackend)\n\n\nclass BinningCountBlock(ColumnWiseBlock):\n\tengine = BinCountEncoder\n\tfn __init__(self, name, column, bins)\n\tfn create_new_engine(self, column_name: str)\n\tfn get_output_colname(self, column)\n\n\nclass OneHotEncodingBlock(ColumnWiseBlock):\n\tengine = OneHotEncoder\n\tfn create_new_engine(self, column_name: str) -> BaseEstimator\n\tfn transform(self, source_df)\n\n\nclass CountEncodingBlock(ColumnWiseBlock):\n\tengine = CountEncoder\n\tfn get_output_colname(self, column)\n\n\nclass FillnaBlock(BaseBlock):\n\t_save_attributes = ['fill_values_']\n\tfn fit(self, source_df: pd.DataFrame, y: Union[(None, np.ndarray)], experiment: ExperimentBackend) -> pd.DataFrame\n\tfn transform(self, source_df: pd.DataFrame) -> pd.DataFrame\n\n\nclass TunedLogisticBlock(TunerBlock):\n\tmodel_class = LogisticRegression\n\tinitial_params = {'solver': 'liblinear', 'penalty': 'l2', 'input_scaling': 'standard'}\n\tfn generate_model_class_try_params(self, trial)\n\n\nclass TunedRidgeBlock(TunerBlock):\n\tmodel_class = Ridge\n\tinitial_params = {'input_scaling': 'standard'}\n\tfn generate_model_class_try_params(self, trial)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pandas as pd\nimport pytest\n\nfrom vivid import create_runner\nfrom vivid.backends import LocalExperimentBackend\nfrom vivid.estimators.linear import TunedRidgeBlock\nfrom vivid.features.base import FillnaBlock\nfrom vivid.features.base import get_target_columns\n\n\n@pytest.fixture\ndef source_df():\n    return pd.DataFrame({\n        'a': [1, 2],\n        'b': [1, 2]\n    })\n\n\ndef test_fillna_block(regression_hasna_set, tmpdir):\n    df, y = regression_hasna_set\n    runner = create_runner(TunedRidgeBlock(name='ridge', n_trials=1))\n    with pytest.raises(ValueError):\n        runner.fit(df, y)\n\n    fillna_block = FillnaBlock('FNA')\n    ridge = TunedRidgeBlock(parent=fillna_block, name='ridge', n_trials=1)\n\n    runner = create_runner(ridge, experiment=LocalExperimentBackend(tmpdir))\n    runner.fit(df, y)\n\n    # can predict re-define blocks (check save require field to tmpdir)\n    fillna_block = FillnaBlock('FNA')\n    ridge = TunedRidgeBlock(parent=fillna_block, name='ridge', n_trials=1)\n    runner = create_runner(ridge, experiment=LocalExperimentBackend(tmpdir))\n    runner.predict(df)\n\n\ndef test_target_columns(source_df):\n    x = get_target_columns(source_df)\n    assert x == ['a', 'b']\n\n    x = get_target_columns(source_df, excludes=['a'])\n    assert x == ['b']\n\n    x = get_target_columns(source_df, column=['a'])\n    assert x == ['a']\n\n    x = get_target_columns(source_df, column=['a'], excludes=['a'])\n    assert x == []\n\n\ndef test_raise_not_exist_column(source_df):\n    with pytest.raises(ValueError, match='some specific column does not exist in source_df columns. i.e.'):\n        get_target_columns(source_df, column=['a', 'b', 'c'])\n\n\ndef test_raise_not_exist_excludes(source_df):\n    with pytest.raises(ValueError, match='some specific `excludes` columns does not exist'):\n        get_target_columns(source_df, excludes=['c'])\n<|endoftext|>",
    "repo": "nyk510/vivid"
  },
  {
    "prompt": "fn print_log(volttron_home)\n\nfn get_rand_ip_and_port()\n\nfn get_rand_port(ip)\n\nfn is_port_open(ip, port)\n\nfn get_rand_vip()\n\nfn get_rand_ipc_vip()\n\nfn build_wrapper(vip_address)\n\nfn cleanup_wrapper(wrapper)\n\nfn cleanup_wrappers(platforms)\n\nfn volttron_instance1(request)\n\nfn volttron_instance2(request)\n\nfn volttron_instance_msgdebug(request)\n\nfn volttron_instance_encrypt(request)\n\nfn volttron_instance1_web(request)\n\nfn volttron_instance2_web(request)\n\nfn volttron_instance_module_web(request)\n\n\"\"\"Fixture that returns a single instance of volttron platform for testing\n\n@param request: pytest request object\n@return: volttron platform instance\"\"\"\nfn volttron_instance(request: None)\n\n\"\"\"Fixture to get more than 1 volttron instance for test\nUse this fixture to get more than 1 volttron instance for test. This\nreturns a function object that should be called with number of instances\nas parameter to get a list of volttron instnaces. The fixture also\ntakes care of shutting down all the instances at the end\n\nExample Usage:\n\ndef test_function_that_uses_n_instances(get_volttron_instances):\n    instance1, instance2, instance3 = get_volttron_instances(3)\n\n@param request: pytest request object\n@return: function that can used to get any number of\n    volttron instances for testing.\"\"\"\nfn get_volttron_instances(request: None)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\nimport pytest\n\nfrom volttron.platform import get_services_core\nfrom .volttron_platform_fixtures import print_log\n\nPLATFORM_AGENT_CONFIG = {\n    # Agent id is used in the display on volttron central.\n    \"agentid\": \"Platform Agent\",\n\n    # Configuration parameters.\n    #\n    # The period of time to go between attempting to report status to the\n    # platform.historian.\n    #\n    # Default \"report_status_period\": 30\n    \"report_status_period\": 15\n}\n\nVC_CONFIG = {\n    # The agentid is used during display on the VOLTTRON central platform\n    # it does not need to be unique.\n    \"agentid\": \"Volttron Central\",\n\n    # By default the webroot will be relative to the installation directory\n    # of the agent when it is installed.  One can override this by specifying\n    # the root directory here.\n    # \"webroot\": \"path/to/webroot\",\n\n    # Authentication for users is handled through a naive password algorithm\n    # import hashlib\n    # hashlib.sha512(password).hexdigest() where password is the plain text password.\n    \"users\": {\n        \"reader\": {\n            \"password\": \"2d7349c51a3914cd6f5dc28e23c417ace074400d7c3e176bcf5da72fdbeb6ce7ed767ca00c6c1fb754b8df5114fc0b903960e7f3befe3a338d4a640c05dfaf2d\",\n            \"groups\": [\n                \"reader\"\n            ]\n        },\n        \"admin\": {\n            \"password\": \"c7ad44cbad762a5da0a452f9e854fdc1e0e7a52a38015f23f3eab1d80b931dd472634dfac71cd34ebc35d16ab7fb8a90c81f975113d6c7538dc69dd8de9077ec\",\n            \"groups\": [\n                \"admin\"\n            ]\n        },\n        \"dorothy\": {\n            \"password\": \"cf1b67402d648f51ef6ff8805736d588ca07cbf018a5fba404d28532d839a1c046bfcd31558dff658678b3112502f4da9494f7a655c3bdc0e4b0db3a5577b298\",\n            \"groups\": [\n                \"reader, writer\"\n            ]\n        }\n    }\n}\n\n\n@pytest.fixture\ndef vc_instance(request, volttron_instance1_web):\n    \"\"\"\n    Creates an instance of volttron with a `VolttronCentral` agent\n    already installed and started.\n\n    :returns tuple:\n        - the PlatformWrapper\n        - the uuid of the `VolttronCentral` agent.\n        - the jsonrpc address to be used for communicating with the\n          `VolttronCentral` agent.\n    \"\"\"\n    agent_uuid = volttron_instance1_web.install_agent(\n        agent_dir=get_services_core(\"VolttronCentral\"),\n        config_file=VC_CONFIG,\n        start=True\n    )\n\n    rpc_addr = volttron_instance1_web.jsonrpc_endpoint\n\n    # Allow all incoming connections that are encrypted.\n    volttron_instance1_web.allow_all_connections()\n\n    def cleanup():\n        print('Cleanup vc_instance')\n        volttron_instance1_web.remove_agent(agent_uuid)\n        print_log(volttron_instance1_web.volttron_home)\n\n    request.addfinalizer(cleanup)\n    return volttron_instance1_web, agent_uuid, rpc_addr\n\n\n@pytest.fixture\ndef pa_instance(request, volttron_instance2_web):\n    \"\"\"\n    Creates an instance of volttron with a `VolttronCentralPlatform` agent\n    already installed and started.\n\n    :returns tuple: the platformwrapper and the uuid of the agaent installed.\n    \"\"\"\n    agent_uuid = volttron_instance2_web.install_agent(\n        agent_dir=get_services_core(\"VolttronCentralPlatform\"),\n        config_file=PLATFORM_AGENT_CONFIG,\n        start=True\n    )\n\n    # Allow all incoming encrypted connections\n    volttron_instance2_web.allow_all_connections()\n\n    def cleanup():\n        print('Cleanup pa_instance')\n        volttron_instance2_web.remove_agent(agent_uuid)\n        print_log(volttron_instance2_web.volttron_home)\n\n    request.addfinalizer(cleanup)\n\n    return volttron_instance2_web, agent_uuid\n<|endoftext|>",
    "repo": "laroque/volttron"
  },
  {
    "prompt": "\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import json\nfrom io import BytesIO\n\nimport pytest\nfrom minio import Minio\n\nfrom shepherd.constants import INPUT_DIR\n\n\n@pytest.fixture()\ndef job(minio: Minio, bucket):\n    job_id = bucket\n    data = json.dumps({\"content\": \"Lorem ipsum\"}).encode()\n    minio.put_object(job_id, INPUT_DIR + \"/payload.json\", BytesIO(data), len(data))\n    yield job_id\n\n\nasync def test_get_input_success(job, aiohttp_client, app):\n    job_id = job\n    client = await aiohttp_client(app)\n    response = await client.get(\"/jobs/{}/input/payload.json\".format(job_id))\n    assert response.status == 200\n\n    data = await response.json()\n    assert \"content\" in data\n\n\nasync def test_get_input_not_found(job, aiohttp_client, app):\n    job_id = job\n    client = await aiohttp_client(app)\n\n    response = await client.get(\"/jobs/{}/input/i-dont-exist.json\".format(job_id))\n    assert response.status == 404\n<|endoftext|>",
    "repo": "iterait/shepherd"
  },
  {
    "prompt": "class UserFactory(DjangoModelFactory):\n\tusername = Faker('user_name')\n\temail = Faker('email')\n\tname = Faker('name')\n\tfn password(self, create: bool, extracted: Sequence[Any])\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom devproject.users.forms import UserCreationForm\nfrom devproject.users.tests.factories import UserFactory\n\npytestmark = pytest.mark.django_db\n\n\nclass TestUserCreationForm:\n    def test_clean_username(self):\n        # A user with proto_user params does not exist yet.\n        proto_user = UserFactory.build()\n\n        form = UserCreationForm(\n            {\n                \"username\": proto_user.username,\n                \"password1\": proto_user._password,\n                \"password2\": proto_user._password,\n            }\n        )\n\n        assert form.is_valid()\n        assert form.clean_username() == proto_user.username\n\n        # Creating a user.\n        form.save()\n\n        # The user with proto_user params already exists,\n        # hence cannot be created.\n        form = UserCreationForm(\n            {\n                \"username\": proto_user.username,\n                \"password1\": proto_user._password,\n                \"password2\": proto_user._password,\n            }\n        )\n\n        assert not form.is_valid()\n        assert len(form.errors) == 1\n        assert \"username\" in form.errors\n<|endoftext|>",
    "repo": "francisbrito/take-home-assignment"
  },
  {
    "prompt": "fn pathway(id_1: str, id_2: str, path_length: int)\n\nfn health_check()\n\nfn get_random_image_id(excluding)\n\n\"\"\"returns n evenly spaced points between start_coord and end_coord\"\"\"\nfn get_ideal_coords(start_coord, end_coord, n)\n\n\"\"\"Return the indexes of the points which most closely match an ideal path\"\"\"\nfn get_path_indexes(closest_indexes, start_index, end_index)\n\nfn get_pathway(id_1, id_2, n_nodes, ids, feature_index)\n\nfn id_to_url(image_id)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import numpy as np\nimport time\n\nimport pytest\nimport requests\nfrom starlette.testclient import TestClient\n\nfrom .api import app\nfrom .utils import ids\n\nclient = TestClient(app)\n\n\ndef test_health_check():\n    response = client.get('/image_pathways/health_check')\n    assert response.status_code == 200\n    assert response.json() == {'status': 'healthy'}\n\n\ndef test_docs():\n    response = client.get('/image_pathways/docs')\n    assert response.status_code == 200\n\n\ndef test_redoc():\n    response = client.get('/image_pathways/redoc')\n    assert response.status_code == 200\n\n\ndef test_response_time():\n    n, times = 100, []\n    for _ in range(n):\n        start_time = time.time()\n        client.get('/image_pathways')\n        times.append(time.time() - start_time)\n    assert np.mean(times) <= 0.5\n\n\ndef test_reponse_contains_all_elements():\n    response_elements = [\n        'id_path',\n        'image_url_path'\n    ]\n    response = client.get('/image_pathways').json()\n    for element in response_elements:\n        assert element in response\n\n\ndef test_response_length():\n    for path_length in range(3, 25):\n        response = client.get(\n            '/image_pathways?path_length=' + str(path_length)\n        ).json()\n        assert len(response['id_path']) == path_length\n        assert len(response['image_url_path']) == path_length\n\n\ndef test_response_accepts_image_id_1():\n    for image_id in np.random.choice(ids, size=10):\n        response = client.get('/image_pathways?id_1=' + image_id)\n        assert response.status_code == 200\n\n\ndef test_response_accepts_image_id_2():\n    for image_id in np.random.choice(ids, size=10):\n        response = client.get('/image_pathways?id_2=' + image_id)\n        assert response.status_code == 200\n\n\ndef test_does_not_accept_invalid_image_id_1():\n    response = client.get('/image_pathways?id_1=some_rubbish')\n    assert response.status_code == 404\n    assert response.json() == {'detail': 'Invalid image_id'}\n\n\ndef test_does_not_accept_invalid_image_id_2():\n    response = client.get('/image_pathways?id_2=some_rubbish')\n    assert response.status_code == 404\n    assert response.json() == {'detail': 'Invalid image_id'}\n\n\ndef test_does_not_accept_invalid_n():\n    response = client.get('/image_pathways?path_length=some_rubbish')\n    assert response.status_code == 422\n<|endoftext|>",
    "repo": "wellcomecollection/data-science"
  },
  {
    "prompt": "\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nimport xyzservices.providers as xyz\nimport requests\nimport mercantile\n\nflat_free = xyz.filter(requires_token=False).flatten()\n\n\ndef check_provider(provider):\n    for key in [\"attribution\", \"name\"]:\n        assert key in provider.keys()\n    assert provider.url.startswith(\"http\")\n    for option in [\"{z}\", \"{y}\", \"{x}\"]:\n        assert option in provider.url\n\n\ndef get_tile(provider):\n    bounds = provider.get(\"bounds\", [[-180, -90], [180, 90]])\n    lat = (bounds[0][0] + bounds[1][0]) / 2\n    lon = (bounds[0][1] + bounds[1][1]) / 2\n    zoom = (provider.get(\"min_zoom\", 0) + provider.get(\"max_zoom\", 20)) // 2\n    tile = mercantile.tile(lon, lat, zoom)\n    z = tile.z\n    x = tile.x\n    y = tile.y\n    return (z, x, y)\n\n\ndef get_response(url):\n    s = requests.Session()\n    a = requests.adapters.HTTPAdapter(max_retries=3)\n    s.mount(\"http://\", a)\n    s.mount(\"https://\", a)\n    r = s.get(url)\n    return r.status_code\n\n\n@pytest.mark.parametrize(\"provider_name\", xyz.flatten())\ndef test_minimal_provider_metadata(provider_name):\n    provider = xyz.flatten()[provider_name]\n    check_provider(provider)\n\n\n@pytest.mark.parametrize(\"name\", flat_free)\ndef test_free_providers(name):\n    provider = flat_free[name]\n\n    if provider.get(\"status\"):\n        pytest.xfail(\"Provider is known to be broken.\")\n\n    z, x, y = get_tile(provider)\n\n    try:\n        r = get_response(provider.build_url(z=z, x=x, y=y))\n        assert r == requests.codes.ok\n    except AssertionError as e:\n        if r == 403:\n            pytest.xfail(\"Provider not available due to API restrictions (Error 403).\")\n\n        elif r == 503:\n            pytest.xfail(\"Service temporarily unavailable (Error 503).\")\n\n        elif r == 502:\n            pytest.xfail(\"Bad Gateway (Error 502).\")\n\n        # check another tiles\n        elif r == 404:\n            # in some cases, the computed tile is not availble. trying known tiles.\n            options = [(12, 2154, 1363), (6, 13, 21), (16, 33149, 22973)]\n            results = []\n            for o in options:\n                z, x, y = o\n                r = get_response(provider.build_url(z=z, x=x, y=y))\n                results.append(r)\n            if not any([x == requests.codes.ok for x in results]):\n                raise ValueError(f\"Response code: {r}\")\n        else:\n            raise ValueError(f\"Response code: {r}\")\n<|endoftext|>",
    "repo": "drnextgis/xyzservices"
  },
  {
    "prompt": "\"\"\"Get absolute path for ``fname``.\n\n:param fname: Filename.\n:type fname: str or pathlib.Path\n:rtype: str\"\"\"\nfn get_reference_file(fname: str or pathlib.Path) -> str\n\n\"\"\":param fname: Filename.\n:type fname: str or pathlib.Path\"\"\"\nfn get_reference_image(fname: str or pathlib.Path)\n\n\"\"\":param fname: Filename of the font.\n:type fname: str or pathlib.Path\"\"\"\nfn get_reference_font(fname: str or pathlib.Path, fsize)\n\n\"\"\"Load :py:class:`PIL.ImageFont` type font from provided fname\n\n:param fname: The name of the file that contains the PIL.ImageFont\n:type fname: str\n:rtype: :py:class:`PIL.ImageFont`\"\"\"\nfn get_reference_pillow_font(fname: str) -> :py:class:`PIL.ImageFont`\n\nfn get_spidev()\n\n\"\"\":param img_path: Location of image.\n:type img_path: str\"\"\"\nfn assert_identical_image(reference, target, img_path: str)\n\nfn i2c_error(path_name, err_no)\n\nfn fib(n)\n\n\"\"\"Create a mock \"open\" that will mock open multiple files in sequence\nArgs:\n    *file_contents ([str]): a list of file contents to be returned by open\nReturns:\n    (MagicMock) a mock opener that will return the contents of the first\n        file when opened the first time, the second file when opened the\n        second time, etc.\"\"\"\nfn multi_mock_open()\n\nfn skip_unsupported_platform(err)\n\nfn _positional_args_list(mock)\n\nfn assert_only_cleans_whats_setup(gpio)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Copyright (c) 2019-2020 Richard Hull and contributors\n# See LICENSE.rst for details.\n\n\"\"\"\nTests for the :py:class:`luma.core.interface.serial.ftdi_i2c` class.\n\"\"\"\n\nimport pytest\nfrom unittest.mock import Mock, patch\nfrom luma.core.interface.serial import ftdi_i2c\nfrom helpers import fib\nimport luma.core.error\n\n\n@patch('pyftdi.i2c.I2cController')\ndef test_init(mock_controller):\n    instance = Mock()\n    instance.get_port = Mock()\n    mock_controller.side_effect = [instance]\n\n    ftdi_i2c(device='ftdi://dummy', address='0xFF')\n    mock_controller.assert_called_with()\n    instance.configure.assert_called_with('ftdi://dummy')\n    instance.get_port.assert_called_with(0xFF)\n\n\n@patch('pyftdi.i2c.I2cController')\ndef test_command(mock_controller):\n    cmds = [3, 1, 4, 2]\n    port = Mock()\n    instance = Mock()\n    instance.get_port = Mock(return_value=port)\n    mock_controller.side_effect = [instance]\n\n    serial = ftdi_i2c(device='ftdi://dummy', address=0x3C)\n    serial.command(*cmds)\n    port.write_to.assert_called_once_with(0x00, cmds)\n\n\n@patch('pyftdi.i2c.I2cController')\ndef test_data(mock_controller):\n    data = list(fib(100))\n    port = Mock()\n    instance = Mock()\n    instance.get_port = Mock(return_value=port)\n    mock_controller.side_effect = [instance]\n\n    serial = ftdi_i2c(device='ftdi://dummy', address=0x3C)\n    serial.data(data)\n    port.write_to.assert_called_once_with(0x40, data)\n\n\n@patch('pyftdi.i2c.I2cController')\ndef test_cleanup(mock_controller):\n    port = Mock()\n    instance = Mock()\n    instance.get_port = Mock(return_value=port)\n    mock_controller.side_effect = [instance]\n\n    serial = ftdi_i2c(device='ftdi://dummy', address=0x3C)\n    serial.cleanup()\n    instance.terminate.assert_called_once_with()\n\n\n@patch('pyftdi.i2c.I2cController')\ndef test_init_device_address_error(mock_controller):\n    address = 'foo'\n    with pytest.raises(luma.core.error.DeviceAddressError) as ex:\n        ftdi_i2c(device='ftdi://dummy', address=address)\n    assert str(ex.value) == f'I2C device address invalid: {address}'\n<|endoftext|>",
    "repo": "chaosAD/fake_luma.core"
  },
  {
    "prompt": "fn tmp_h5_file(tmp_path)\n\nfn h5_context(tmp_path)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import contextlib\nfrom pathlib import Path\n\nimport h5py\nimport pytest\nfrom bluesky.tests.conftest import RE  # noqa\nfrom ophyd.tests.conftest import hw  # noqa\n\n# from suitcase.utils.tests.conftest import (  # noqa\n#     example_data,\n#     generate_data,\n#     plan_type,\n#     detector_list,\n#     event_type,\n# )  # noqa\n\n\n@pytest.fixture\ndef tmp_h5_file(tmp_path):\n    return h5py.File(tmp_path / Path(\"test.h5\"), \"w\")\n\n\n@pytest.fixture\ndef h5_context(tmp_path):\n    @contextlib.contextmanager\n    def make_context():\n        # print(\"  entering\")\n        try:\n            f = h5py.File(tmp_path / Path(\"test.h5\"), \"w\")\n            yield f\n        except RuntimeError as err:\n            print(\"  ERROR:\", err)\n        finally:\n            # print(\"  exiting\")\n            f.close()\n\n    return make_context\n<|endoftext|>",
    "repo": "jklynch/suitcase-nxsas"
  },
  {
    "prompt": "class Context:\n\tfn __init__(self)\n\t\"\"\"Logs a message to stderr.\"\"\"\n\tfn log(self, msg)\n\t\"\"\"Logs a message to stderr only if verbose is enabled.\"\"\"\n\tfn vlog(self, msg)\n\n\nclass ComplexCLI(click.MultiCommand):\n\tfn list_commands(self, ctx)\n\tfn get_command(self, ctx, cmd_name)\n\n\n\"\"\"A complex command line interface.\"\"\"\nfn cli(ctx, verbose)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import os\nimport shutil\nimport uuid\nfrom pathlib import Path\nimport pytest\nfrom click.testing import CliRunner\nfrom git.repo import Repo\nfrom envadmin.cli import cli\nfrom tests.utilities import constants\n\n\nPATH = os.path.join(Path.home(), \"temp\", \"envadmin_sandbox\", str(uuid.uuid4())[:8])\n\n\n@pytest.fixture()\ndef runner():\n    yield CliRunner()\n\n\n@pytest.fixture()\ndef temp_folder():\n    # Deletes and recreates PATH\n    if os.path.isdir(PATH):\n        shutil.rmtree(PATH)\n\n    os.mkdir(PATH)\n\n    yield PATH\n\n    # Clear files at end.\n    shutil.rmtree(PATH)\n\n\n@pytest.fixture()\ndef temp_git_folder(temp_folder):\n    repo = Repo.init(temp_folder)\n    repo.create_remote(name=\"origin\", url=\"git@gitlab.com:temp/sandbox.git\")\n    yield temp_folder\n\n\n@pytest.fixture()\ndef temp_envadmin_folder(runner, temp_git_folder):\n    runner.invoke(cli, [\"-v\", \"init\",\n                        \"-p\", temp_git_folder,\n                        \"-c\", temp_git_folder,\n                        \"-e\", \"test@test.com\",\n                        \"--no-push\"])\n\n    runner.invoke(cli, [\"namespace\",\n                        \"-c\", temp_git_folder,\n                        \"create\",\n                        \"-n\", constants.NAMESPACE1])\n<|endoftext|>",
    "repo": "ivanklee86/envadmin"
  },
  {
    "prompt": "\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import inspect\nimport os\nimport pytest\nimport shutil\nfrom unittest import mock\n\nfrom hoverxref.translators import HoverXRefHTMLTranslatorMixin\n\nfrom .utils import srcdir\n\n\n@pytest.mark.sphinx(\n    srcdir=srcdir,\n    buildername='latex',\n    confoverrides={\n        'hoverxref_project': 'myproject',\n        'hoverxref_version': 'myversion',\n    },\n)\ndef test_dont_override_translator_non_html_builder(app, status, warning):\n    app.build()\n    path = app.outdir / 'test.tex'\n    assert path.exists() is True\n    content = open(path).read()\n\n    assert app.builder.format == 'latex'\n    for name, klass in app.registry.translators.items():\n        assert not issubclass(klass, HoverXRefHTMLTranslatorMixin)\n\n\n@pytest.mark.sphinx(\n    srcdir=srcdir,\n    buildername='html',\n    confoverrides={\n        'hoverxref_project': 'myproject',\n        'hoverxref_version': 'myversion',\n    },\n)\ndef test_override_translator_non_html_builder(app, status, warning):\n    app.build()\n    path = app.outdir / 'index.html'\n    assert path.exists() is True\n    content = open(path).read()\n\n    assert app.builder.format == 'html'\n    for name, klass in app.registry.translators.items():\n        assert issubclass(klass, HoverXRefHTMLTranslatorMixin)\n\n\n@pytest.mark.sphinx(\n    srcdir=srcdir,\n    buildername='latex',\n    confoverrides={\n        'hoverxref_project': 'myproject',\n        'hoverxref_version': 'myversion',\n        'hoverxref_auto_ref': True,\n    },\n)\ndef test_dont_fail_non_html_builder(app, status, warning):\n    \"\"\"\n    Test our resolver is not used by non-HTML builder.\n\n    When running the build with ``latex`` as builder and\n    ``hoverxref_auto_ref=True`` it should not fail with\n\n    def _get_docpath(self, builder, docname):\n        docpath = builder.get_outfilename(docname)\n        AttributeError: 'LaTeXBuilder' object has no attribute 'get_outfilename'\n\n    LaTeXBuilder should never use our resolver.\n    \"\"\"\n\n    with mock.patch('hoverxref.domains.HoverXRefBaseDomain._get_docpath') as _get_docpath:\n        app.build()\n        assert not _get_docpath.called\n    path = app.outdir / 'test.tex'\n    assert path.exists() is True\n    content = open(path).read()\n\n    assert app.builder.format == 'latex'\n<|endoftext|>",
    "repo": "Daltz333/sphinx-hoverxref"
  },
  {
    "prompt": "fn redirectTest(response, route)\n\nfn redTestResponses(responses, route)\n\nfn checkRendered(response)\n\nfn basicResponses(route, client)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nimport os\nfrom helpers import redirectTest, checkRendered\n\ndef test_home(client, app):\n    route = \"/home\"\n    if \"PLACES_KEY\" in app.config:\n        del app.config['PLACES_KEY']\n    response = client.get(route)\n    # there will be no key at this point should redirect to setKey page\n    redirectTest(response, '/setKey')\n    app.config.update(PLACES_KEY = 'test')\n    response = client.get(route)\n    #now there is a key there should be no redirect, check that the status code is 200 and\n    #response contains html\n    checkRendered(response)\n\n<|endoftext|>",
    "repo": "TomBennett-Lloyd/SurfLodger_Flask"
  },
  {
    "prompt": "class BaseFactory(SQLAlchemyModelFactory):\n\n\nclass UserFactory(BaseFactory):\n\tusername = Sequence(lambda n: 'user{0}'.format(n))\n\temail = Sequence(lambda n: 'user{0}@example.com'.format(n))\n\tpassword = PostGenerationMethodCall('set_password', 'example')\n\tactive = True\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# -*- coding: utf-8 -*-\n\"\"\"Model unit tests.\"\"\"\nimport datetime as dt\n\nimport pytest\n\nfrom erdos.models.user import User, Role\nfrom .factories import UserFactory\n\n\n@pytest.mark.usefixtures('db')\nclass TestUser:\n\n    def test_get_by_id(self):\n        user = User('foo', 'foo@bar.com')\n        user.save()\n\n        retrieved = User.get_by_id(user.id)\n        assert retrieved == user\n\n    def test_created_at_defaults_to_datetime(self):\n        user = User(username='foo', email='foo@bar.com')\n        user.save()\n        assert bool(user.created_at)\n        assert isinstance(user.created_at, dt.datetime)\n\n    def test_password_is_nullable(self):\n        user = User(username='foo', email='foo@bar.com')\n        user.save()\n        assert user.password is None\n\n    def test_factory(self, db):\n        user = UserFactory(password=\"myprecious\")\n        db.session.commit()\n        assert bool(user.username)\n        assert bool(user.email)\n        assert bool(user.created_at)\n        assert user.is_admin is False\n        assert user.active is True\n        assert user.check_password('myprecious')\n\n    def test_check_password(self):\n        user = User.create(username=\"foo\", email=\"foo@bar.com\",\n                    password=\"foobarbaz123\")\n        assert user.check_password('foobarbaz123') is True\n        assert user.check_password(\"barfoobaz\") is False\n\n    def test_full_name(self):\n        user = UserFactory(first_name=\"Foo\", last_name=\"Bar\")\n        assert user.full_name == \"Foo Bar\"\n\n    def test_roles(self):\n        role = Role(name='admin')\n        role.save()\n        u = UserFactory()\n        u.roles.append(role)\n        u.save()\n        assert role in u.roles\n<|endoftext|>",
    "repo": "gabrielwebgurus/flask-elasticsearch-api"
  },
  {
    "prompt": "\"\"\"Load npy file as numpy array.\"\"\"\nfn _load_npy(handle)\n\n\"\"\"Load image file as numpy array.\"\"\"\nfn _load_img(handle, target_dtype, size)\n\n\"\"\"Load json file as python object.\"\"\"\nfn _load_json(handle)\n\n\"\"\"Load and decode a string.\"\"\"\nfn _load_text(handle, split, encoding)\n\n\"\"\"Load GraphDef from a binary proto file.\"\"\"\nfn _load_graphdef_protobuf(handle)\n\n\"\"\"Load a file.\n\nFile format is inferred from url. File retrieval strategy is inferred from\nURL. Returned object type is inferred from url extension.\n\nArgs:\n  url_or_handle: a (reachable) URL, or an already open file handle\n\nRaises:\n  RuntimeError: If file extension or URL is not supported.\"\"\"\nfn load(url_or_handle: None, cache)\n\nfn load_using_loader(url_or_handle, loader, cache)\n\nfn is_handle(url_or_handle)\n\nfn get_extension(url_or_handle)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# -*- coding: UTF-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport pytest\n\nimport numpy as np\nfrom lucid.misc.io.loading import load\nimport io\n\n\ndef test_load_json():\n  path = \"./tests/fixtures/dictionary.json\"\n  dictionary = load(path)\n  assert \"key\" in dictionary\n\n\ndef test_load_text():\n  path = \"./tests/fixtures/string.txt\"\n  string = load(path)\n  assert u\"\ud83d\udc15\" in string\n\n\ndef test_load_multiline_text_as_list():\n  path = \"./tests/fixtures/multiline.txt\"\n  string_list = load(path, split=True)\n  assert isinstance(string_list, list)\n  assert all(isinstance(string, (\"\".__class__, u\"\".__class__)) for string in string_list)\n\n\ndef test_load_npy():\n  path = \"./tests/fixtures/array.npy\"\n  array = load(path)\n  assert array.shape is not None\n\n\ndef test_load_npz():\n  path = \"./tests/fixtures/arrays.npz\"\n  arrays = load(path)\n  assert isinstance(arrays, np.lib.npyio.NpzFile)\n\n\n@pytest.mark.parametrize(\"path\", [\n  \"./tests/fixtures/rgbeye.png\",\n  \"./tests/fixtures/noise_uppercase.PNG\",\n  \"./tests/fixtures/rgbeye.jpg\",\n  \"./tests/fixtures/noise.jpeg\",\n  \"./tests/fixtures/image.xyz\",\n])\ndef test_load_image(path):\n  image = load(path)\n  assert image.shape is not None\n  assert all(dimension > 2 for dimension in image.shape)\n\n\n@pytest.mark.parametrize(\"path\", [\n  \"./tests/fixtures/rgbeye.png\",\n  \"./tests/fixtures/noise.jpeg\",\n])\ndef test_load_image_resized(path):\n  image = load(path)\n  assert image.shape is not None\n  assert all(dimension > 2 for dimension in image.shape)\n\n\ndef test_load_garbage_with_unknown_extension():\n  path = \"./tests/fixtures/string.XYZ\"\n  with pytest.raises(RuntimeError):\n    load(path)\n\n\ndef test_load_json_with_file_handle():\n  path = \"./tests/fixtures/dictionary.json\"\n  with io.open(path, 'r') as handle:\n    dictionary = load(handle)\n  assert \"key\" in dictionary\n<|endoftext|>",
    "repo": "fabricatedmath/lucid"
  },
  {
    "prompt": "class UserFactory(DjangoModelFactory):\n\tusername = Faker('user_name')\n\temail = Faker('email')\n\tname = Faker('name')\n\tfn password(self, create: bool, extracted: Sequence[Any])\n\n\nclass ConsumerFactory(UserFactory):\n\n\nclass CoordinatorFactory(UserFactory):\n\n\nclass VendorFactory(UserFactory):\n\n\nclass InstructorFactory(UserFactory):\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom django.conf import settings\nfrom django.contrib import messages\nfrom django.contrib.auth.models import AnonymousUser\nfrom django.contrib.messages.middleware import MessageMiddleware\nfrom django.contrib.sessions.middleware import SessionMiddleware\nfrom django.http import HttpRequest\nfrom django.test import RequestFactory\nfrom django.urls import reverse\nfrom rest_framework import status\n\nfrom server.users.forms import UserChangeForm\nfrom server.users.models import User\nfrom server.users.tests.factories import UserFactory\nfrom server.users.views import UserRedirectView, UserUpdateView, user_detail_view\n\npytestmark = pytest.mark.django_db\n\n\nclass TestUserUpdateView:\n    \"\"\"\n    TODO:\n        extracting view initialization code as class-scoped fixture\n        would be great if only pytest-django supported non-function-scoped\n        fixture db access -- this is a work-in-progress for now:\n        https://github.com/pytest-dev/pytest-django/pull/258\n    \"\"\"\n\n    def dummy_get_response(self, request: HttpRequest):\n        return None\n\n    def test_get_success_url(self, user: User, rf: RequestFactory):\n        view = UserUpdateView()\n        request = rf.get(\"/fake-url/\")\n        request.user = user\n\n        view.request = request\n\n        assert view.get_success_url() == f\"/users/{user.username}/\"\n\n    def test_get_object(self, user: User, rf: RequestFactory):\n        view = UserUpdateView()\n        request = rf.get(\"/fake-url/\")\n        request.user = user\n\n        view.request = request\n\n        assert view.get_object() == user\n\n    def test_form_valid(self, user: User, rf: RequestFactory):\n        view = UserUpdateView()\n        request = rf.get(\"/fake-url/\")\n\n        # Add the session/message middleware to the request\n        SessionMiddleware(self.dummy_get_response).process_request(request)\n        MessageMiddleware(self.dummy_get_response).process_request(request)\n        request.user = user\n\n        view.request = request\n\n        # Initialize the form\n        form = UserChangeForm()\n        form.cleaned_data = []\n        view.form_valid(form)\n\n        messages_sent = [m.message for m in messages.get_messages(request)]\n        assert messages_sent == [\"Information successfully updated\"]\n\n\nclass TestUserRedirectView:\n    def test_get_redirect_url(self, user: User, rf: RequestFactory):\n        view = UserRedirectView()\n        request = rf.get(\"/fake-url\")\n        request.user = user\n\n        view.request = request\n\n        assert view.get_redirect_url() == f\"/users/{user.username}/\"\n\n\nclass TestUserDetailView:\n    def test_authenticated(self, user: User, rf: RequestFactory):\n        request = rf.get(\"/fake-url/\")\n        request.user = UserFactory()\n\n        response = user_detail_view(request, username=user.username)\n\n        assert response.status_code == status.HTTP_200_OK\n\n    def test_not_authenticated(self, user: User, rf: RequestFactory):\n        request = rf.get(\"/fake-url/\")\n        request.user = AnonymousUser()\n\n        response = user_detail_view(request, username=user.username)\n        login_url = reverse(settings.LOGIN_URL)\n\n        assert response.status_code == status.HTTP_302_FOUND\n        assert response.url == f\"{login_url}?next=/fake-url/\"\n<|endoftext|>",
    "repo": "omert-visiblerisk/connective"
  },
  {
    "prompt": "class UserFactory(DjangoModelFactory):\n\tusername = Faker('user_name')\n\temail = Faker('email')\n\tname = Faker('name')\n\tfn password(self, create: bool, extracted: Sequence[Any])\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom django.contrib.auth.models import AnonymousUser\nfrom django.http.response import Http404\nfrom django.test import RequestFactory\n\nfrom bonds.users.models import User\nfrom bonds.users.tests.factories import UserFactory\nfrom bonds.users.views import (\n    UserRedirectView,\n    UserUpdateView,\n    user_detail_view,\n)\n\npytestmark = pytest.mark.django_db\n\n\nclass TestUserUpdateView:\n    \"\"\"\n    TODO:\n        extracting view initialization code as class-scoped fixture\n        would be great if only pytest-django supported non-function-scoped\n        fixture db access -- this is a work-in-progress for now:\n        https://github.com/pytest-dev/pytest-django/pull/258\n    \"\"\"\n\n    def test_get_success_url(self, user: User, rf: RequestFactory):\n        view = UserUpdateView()\n        request = rf.get(\"/fake-url/\")\n        request.user = user\n\n        view.request = request\n\n        assert view.get_success_url() == f\"/users/{user.username}/\"\n\n    def test_get_object(self, user: User, rf: RequestFactory):\n        view = UserUpdateView()\n        request = rf.get(\"/fake-url/\")\n        request.user = user\n\n        view.request = request\n\n        assert view.get_object() == user\n\n\nclass TestUserRedirectView:\n    def test_get_redirect_url(self, user: User, rf: RequestFactory):\n        view = UserRedirectView()\n        request = rf.get(\"/fake-url\")\n        request.user = user\n\n        view.request = request\n\n        assert view.get_redirect_url() == f\"/users/{user.username}/\"\n\n\nclass TestUserDetailView:\n    def test_authenticated(self, user: User, rf: RequestFactory):\n        request = rf.get(\"/fake-url/\")\n        request.user = UserFactory()\n\n        response = user_detail_view(request, username=user.username)\n\n        assert response.status_code == 200\n\n    def test_not_authenticated(self, user: User, rf: RequestFactory):\n        request = rf.get(\"/fake-url/\")\n        request.user = AnonymousUser()  # type: ignore\n\n        response = user_detail_view(request, username=user.username)\n\n        assert response.status_code == 302\n        assert response.url == \"/accounts/login/?next=/fake-url/\"\n\n    def test_case_sensitivity(self, rf: RequestFactory):\n        request = rf.get(\"/fake-url/\")\n        request.user = UserFactory(username=\"UserName\")\n\n        with pytest.raises(Http404):\n            user_detail_view(request, username=\"username\")\n<|endoftext|>",
    "repo": "ghostforpy/bonds-docker"
  },
  {
    "prompt": "\"\"\"Replacement for the standard library deepcopy.\n\nThe stdlib's copy.deepcopy is notoriously slow. After benchmarking the REST Proxy produce API\nthe deepcopy was identified as a bottleneck. Currently it is faster to encode the objects as\njson and decode the result instead of using copy.deepcopy.\"\"\"\nfn deepcopy(obj: Any) -> Any\n\n\"\"\"Return datetime to ISO 8601 variant suitable for users.\nAssume UTC for datetime objects without a timezone, always use\nthe Z timezone designator.\"\"\"\nfn isoformat(datetime_obj)\n\nfn default_json_serialization(obj)\n\nfn json_encode(obj)\n\nfn assert_never(value: NoReturn) -> NoReturn\n\nclass Result:\n\tfn __init__(self, status, json_result, headers)\n\tfn json(self)\n\tfn __repr__(self)\n\tfn ok(self)\n\n\nclass Client:\n\tfn __init__(self, server_uri, client_factory, server_ca: Optional[str])\n\n\nfn convert_to_int(object_: dict, key: str, content_type: str)\n\nclass KarapaceKafkaClient(KafkaClient):\n\tfn __init__(self)\n\tfn close_invalid_connections(self)\n\tfn _poll(self, timeout)\n\t\"\"\"Lifted from the parent class with the caveat that the node id will always belong to the bootstrap node,\n\tthus ensuring we do not end up in a stale metadata loop\"\"\"\n\tfn _maybe_refresh_metadata(self, wakeup)\n\n\nclass KarapaceBrokerConnection(BrokerConnection):\n\tfn __init__(self, host, port, afi)\n\tfn close(self, error)\n\tfn ns_blackout(self)\n\tfn blacked_out(self)\n\n\nfn trim_margin(s: str) -> str\n\nclass KotlinRange:\n\tminimum: int\n\tmaximum: int\n\tfn __str__(self) -> str\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"\nkarapace - schema tests\n\nCopyright (c) 2019 Aiven Ltd\nSee LICENSE for details\n\"\"\"\nfrom karapace.protobuf.kotlin_wrapper import trim_margin\nfrom karapace.utils import Client\nfrom tests.utils import create_subject_name_factory\n\nimport logging\nimport pytest\n\nbaseurl = \"http://localhost:8081\"\n\n\ndef add_slashes(text: str) -> str:\n    escape_dict = {\n        \"\\a\": \"\\\\a\",\n        \"\\b\": \"\\\\b\",\n        \"\\f\": \"\\\\f\",\n        \"\\n\": \"\\\\n\",\n        \"\\r\": \"\\\\r\",\n        \"\\t\": \"\\\\t\",\n        \"\\v\": \"\\\\v\",\n        \"'\": \"\\\\'\",\n        '\"': '\\\\\"',\n        \"\\\\\": \"\\\\\\\\\",\n    }\n    trans_table = str.maketrans(escape_dict)\n    return text.translate(trans_table)\n\n\nlog = logging.getLogger(__name__)\n\n\n@pytest.mark.parametrize(\"trail\", [\"\", \"/\"])\nasync def test_protobuf_schema_compatibility(registry_async_client: Client, trail: str) -> None:\n    subject = create_subject_name_factory(f\"test_protobuf_schema_compatibility-{trail}\")()\n\n    res = await registry_async_client.put(f\"config/{subject}{trail}\", json={\"compatibility\": \"BACKWARD\"})\n    assert res.status == 200\n\n    original_schema = \"\"\"\n            |syntax = \"proto3\";\n            |package a1;\n            |message TestMessage {\n            |    message Value {\n            |        string str2 = 1;\n            |        int32 x = 2;\n            |    }\n            |    string test = 1;\n            |    .a1.TestMessage.Value val = 2;\n            |}\n            |\"\"\"\n\n    original_schema = trim_margin(original_schema)\n\n    res = await registry_async_client.post(\n        f\"subjects/{subject}/versions{trail}\", json={\"schemaType\": \"PROTOBUF\", \"schema\": original_schema}\n    )\n    assert res.status == 200\n    assert \"id\" in res.json()\n\n    evolved_schema = \"\"\"\n            |syntax = \"proto3\";\n            |package a1;\n            |message TestMessage {\n            |    message Value {\n            |        string str2 = 1;\n            |        Enu x = 2;\n            |    }\n            |    string test = 1;\n            |    .a1.TestMessage.Value val = 2;\n            |    enum Enu {\n            |        A = 0;\n            |        B = 1;\n            |    }\n            |}\n            |\"\"\"\n    evolved_schema = trim_margin(evolved_schema)\n\n    res = await registry_async_client.post(\n        f\"compatibility/subjects/{subject}/versions/latest{trail}\",\n        json={\"schemaType\": \"PROTOBUF\", \"schema\": evolved_schema},\n    )\n    assert res.status == 200\n    assert res.json() == {\"is_compatible\": True}\n\n    res = await registry_async_client.post(\n        f\"subjects/{subject}/versions{trail}\", json={\"schemaType\": \"PROTOBUF\", \"schema\": evolved_schema}\n    )\n    assert res.status == 200\n    assert \"id\" in res.json()\n\n    res = await registry_async_client.post(\n        f\"compatibility/subjects/{subject}/versions/latest{trail}\",\n        json={\"schemaType\": \"PROTOBUF\", \"schema\": original_schema},\n    )\n    assert res.json() == {\"is_compatible\": True}\n    assert res.status == 200\n    res = await registry_async_client.post(\n        f\"subjects/{subject}/versions{trail}\", json={\"schemaType\": \"PROTOBUF\", \"schema\": original_schema}\n    )\n    assert res.status == 200\n    assert \"id\" in res.json()\n<|endoftext|>",
    "repo": "dswiecki/karapace"
  },
  {
    "prompt": "fn decode_timeout(value: str) -> float\n\nfn encode_timeout(timeout: float) -> str\n\n\"\"\"Represents request's deadline - fixed point in time\n    \"\"\"\nclass Deadline:\n\tfn __init__(self) -> None\n\tfn __lt__(self, other: object) -> bool\n\tfn __eq__(self, other: object) -> bool\n\tfn from_headers(cls, headers: _Headers) -> Optional[Deadline]\n\tfn from_timeout(cls, timeout: float) -> Deadline\n\t\"\"\"Calculates remaining time for the current request completion\n\t\n\tThis function returns time in seconds as a floating point number,\n\tgreater or equal to zero.\"\"\"\n\tfn time_remaining(self) -> float\n\n\nfn encode_grpc_message(message: str) -> str\n\nfn decode_grpc_message(value: str) -> str\n\nfn decode_bin_value(value: bytes) -> bytes\n\nfn decode_metadata(headers: _Headers) -> _Metadata\n\nfn encode_bin_value(value: bytes) -> bytes\n\nfn encode_metadata(metadata: _MetadataLike) -> _Headers\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom multidict import MultiDict\n\nfrom grpclib.metadata import Deadline\nfrom grpclib.metadata import encode_timeout, decode_timeout\nfrom grpclib.metadata import encode_metadata, decode_metadata\nfrom grpclib.metadata import encode_grpc_message, decode_grpc_message\n\n\n@pytest.mark.parametrize('value, expected', [\n    (100, '100S'),\n    (15, '15S'),\n    (7, '7000m'),\n    (0.02, '20m'),\n    (0.001, '1000u'),\n    (0.00002, '20u'),\n    (0.000001, '1000n'),\n    (0.00000002, '20n'),\n])\ndef test_encode_timeout(value, expected):\n    assert encode_timeout(value) == expected\n\n\n@pytest.mark.parametrize('value, expected', [\n    ('5H', 5 * 3600),\n    ('4M', 4 * 60),\n    ('3S', 3),\n    ('200m', pytest.approx(0.2)),\n    ('100u', pytest.approx(0.0001)),\n    ('50n', pytest.approx(0.00000005)),\n])\ndef test_decode_timeout(value, expected):\n    assert decode_timeout(value) == expected\n\n\ndef test_deadline():\n    assert Deadline.from_timeout(1) < Deadline.from_timeout(2)\n\n    with pytest.raises(TypeError) as err:\n        Deadline.from_timeout(1) < 'invalid'\n    err.match('comparison is not supported between instances '\n              'of \\'Deadline\\' and \\'str\\'')\n\n    assert Deadline(_timestamp=1) == Deadline(_timestamp=1)\n\n    assert Deadline.from_timeout(1) != 'invalid'\n\n\n@pytest.mark.parametrize('value, output', [\n    ('\u0430^2+\u0431^2=\u0446^2', '%D0%B0^2+%D0%B1^2=%D1%86^2'),\n])\ndef test_grpc_message_encoding(value, output):\n    assert encode_grpc_message(value) == output\n    encode_grpc_message(value).encode('ascii')\n    assert decode_grpc_message(output) == value\n\n\ndef test_grpc_message_decode_safe():\n    # 0xFF is invalid byte in utf-8:\n    # https://www.cl.cam.ac.uk/~mgk25/ucs/examples/UTF-8-test.txt\n    assert (decode_grpc_message('%FF^2+%FF^2=%FF^2')\n            == '\\ufffd^2+\\ufffd^2=\\ufffd^2')\n\n\n@pytest.mark.parametrize('value, output', [\n    ({'regular': 'value'}, [('regular', 'value')]),  # dict-like\n    ([('regular', 'value')], [('regular', 'value')]),  # list of pairs\n    ({'binary-bin': b'value'}, [('binary-bin', 'dmFsdWU')])\n])\ndef test_encode_metadata(value, output):\n    assert encode_metadata(value) == output\n\n\ndef test_encode_metadata_errors():\n    with pytest.raises(TypeError) as e1:\n        encode_metadata({'regular': b'invalid'})\n    e1.match('Invalid metadata value type, str expected')\n\n    with pytest.raises(TypeError) as e2:\n        encode_metadata({'binary-bin': 'invalid'})\n    e2.match('Invalid metadata value type, bytes expected')\n\n\n@pytest.mark.parametrize('key', [\n    'grpc-internal',\n    'Upper-Case',\n    'invalid~character',\n    ' spaces ',\n])\ndef test_encode_metadata_invalid_key(key):\n    with pytest.raises(ValueError) as err:\n        encode_metadata({key: 'anything'})\n    err.match('Invalid metadata key')\n\n\ndef test_decode_metadata_empty():\n    metadata = decode_metadata([\n        (':method', 'POST'),\n        ('te', 'trailers'),\n        ('content-type', 'application/grpc'),\n        ('user-agent', 'Test'),\n        ('grpc-timeout', '100m'),\n    ])\n    assert metadata == MultiDict()\n\n\n@pytest.mark.parametrize('key, value, expected', [\n    ('regular', 'value', 'value'),\n    ('binary-bin', 'dmFsdWU', b'value'),\n])\ndef test_decode_metadata_regular(key, value, expected):\n    metadata = decode_metadata([\n        (':method', 'POST'),\n        ('te', 'trailers'),\n        ('content-type', 'application/grpc'),\n        ('user-agent', 'Test'),\n        ('grpc-timeout', '100m'),\n        (key, value),\n    ])\n    assert metadata == MultiDict({key: expected})\n    assert type(metadata[key]) is type(expected)\n<|endoftext|>",
    "repo": "linw1995/grpclib"
  },
  {
    "prompt": "\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"Tests for ClimaCell const.\"\"\"\nimport pytest\n\nfrom homeassistant.components.climacell.const import ClimaCellSensorEntityDescription\nfrom homeassistant.const import TEMP_FAHRENHEIT\n\n\nasync def test_post_init():\n    \"\"\"Test post initialization check for ClimaCellSensorEntityDescription.\"\"\"\n\n    with pytest.raises(RuntimeError):\n        ClimaCellSensorEntityDescription(\n            key=\"a\", name=\"b\", unit_imperial=TEMP_FAHRENHEIT\n        )\n<|endoftext|>",
    "repo": "nemesis03/core"
  },
  {
    "prompt": "\"\"\"Just a BytesIO with an async read method.\"\"\"\nclass AsyncBuffer(BytesIO):\n\treadexactly = read\n\t\"\"\"Append data to the buffer without changing the current position.\"\"\"\n\tfn append(self, data)\n\n\nfn inc_nonce(nonce)\n\n\"\"\"Split sequence in equal-sized chunks.\nThe last chunk is not padded.\"\"\"\nfn split_chunks(seq, n)\n\nfn b(s)\n\n\"\"\"long_to_bytes(n:long, blocksize:int) : string\nConvert a long integer to a byte string.\nIf optional blocksize is given and greater than zero, pad the front of the\nbyte string with binary zeros so that the length is a multiple of\nblocksize.\"\"\"\nfn long_to_bytes(n, blocksize)\n\n\"\"\"bytes_to_long(string) : long\nConvert a byte string to a long integer.\nThis is (essentially) the inverse of long_to_bytes().\"\"\"\nfn bytes_to_long(s)\n\nfn server()\n\nfn client()\n\nfn test_handshake(client, server)\n\n\"\"\"Return a tuple with `(unbox_stream, box_stream)` (reader/writer).\n\n:return: (:class:`secret_handshake.boxstream.UnboxStream`,\n          :class:`secret_handshake.boxstream.BoxStream`) \"\"\"\nfn get_stream_pair(reader, writer)\n\nclass UnboxStream(object):\n\tfn __init__(self, reader, key, nonce)\n\n\nclass BoxStream(object):\n\tfn __init__(self, writer, key, nonce)\n\tfn write(self, data)\n\tfn close(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright (c) 2017 PySecretHandshake contributors (see AUTHORS for more details)\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n\nimport pytest\n\nfrom secret_handshake.boxstream import HEADER_LENGTH, BoxStream, UnboxStream\nfrom secret_handshake.util import AsyncBuffer, async_comprehend\n\nfrom .test_crypto import CLIENT_ENCRYPT_KEY, CLIENT_ENCRYPT_NONCE\n\nMESSAGE_1 = (b'\\xcev\\xedE\\x06l\\x02\\x13\\xc8\\x17V\\xfa\\x8bZ?\\x88B%O\\xb0L\\x9f\\x8e\\x8c0y\\x1dv\\xc0\\xc9\\xf6\\x9d\\xc2\\xdf\\xdb'\n             b'\\xee\\x9d')\nMESSAGE_2 = b\"\\x141\\xd63\\x13d\\xd1\\xecZ\\x9b\\xd0\\xd4\\x03\\xcdR?'\\xaa.\\x89I\\x92I\\xf9guL\\xaa\\x06?\\xea\\xca/}\\x88*\\xb2\"\nMESSAGE_3 = (b'\\xcbYY\\xf1\\x0f\\xa5O\\x13r\\xa6\"\\x15\\xc5\\x9d\\r.*\\x0b\\x92\\x10m\\xa6(\\x0c\\x0c\\xc61\\x80j\\x81)\\x800\\xed\\xda'\n             b'\\xad\\xa1')\nMESSAGE_CLOSED = b'\\xb1\\x14hU\\'\\xb5M\\xa6\"\\x03\\x9duy\\xa1\\xd4evW,\\xdcE\\x18\\xe4+ C4\\xe8h\\x96\\xed\\xc5\\x94\\x80'\n\n\n@pytest.mark.asyncio\nasync def test_boxstream():\n    buffer = AsyncBuffer()\n    box_stream = BoxStream(buffer, CLIENT_ENCRYPT_KEY, CLIENT_ENCRYPT_NONCE)\n    box_stream.write(b'foo')\n    buffer.seek(0)\n    assert await buffer.read() == MESSAGE_1\n\n    pos = buffer.tell()\n    box_stream.write(b'foo')\n    buffer.seek(pos)\n    assert await buffer.read() == MESSAGE_2\n\n    pos = buffer.tell()\n    box_stream.write(b'bar')\n    buffer.seek(pos)\n    assert await buffer.read() == MESSAGE_3\n\n    pos = buffer.tell()\n    box_stream.close()\n    buffer.seek(pos)\n    assert await buffer.read() == MESSAGE_CLOSED\n\n\n@pytest.mark.asyncio\nasync def test_unboxstream():\n    buffer = AsyncBuffer(MESSAGE_1 + MESSAGE_2 + MESSAGE_3 + MESSAGE_CLOSED)\n    buffer.seek(0)\n\n    unbox_stream = UnboxStream(buffer, CLIENT_ENCRYPT_KEY, CLIENT_ENCRYPT_NONCE)\n    assert not unbox_stream.closed\n    assert (await async_comprehend(unbox_stream)) == [b'foo', b'foo', b'bar']\n    assert unbox_stream.closed\n\n\n@pytest.mark.asyncio\nasync def test_long_packets():\n    data_size = 6 * 1024\n    data = bytes(n % 256 for n in range(data_size))\n\n    # box 6K buffer\n    buffer = AsyncBuffer()\n    box_stream = BoxStream(buffer, CLIENT_ENCRYPT_KEY, CLIENT_ENCRYPT_NONCE)\n    box_stream.write(data)\n    # the size overhead corresponds to the two packet headers\n    assert buffer.tell() == data_size + (HEADER_LENGTH * 2)\n    buffer.seek(0)\n\n    # now let's unbox it and check whether it's OK\n    unbox_stream = UnboxStream(buffer, CLIENT_ENCRYPT_KEY, CLIENT_ENCRYPT_NONCE)\n    first_packet = await unbox_stream.read()\n    assert first_packet == data[:4096]\n    second_packet = await unbox_stream.read()\n    assert second_packet == data[4096:]\n<|endoftext|>",
    "repo": "cn-uofbasel/ssbdrv"
  },
  {
    "prompt": "\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# (C) Datadog, Inc. 2018\n# All rights reserved\n# Licensed under a 3-clause BSD style license (see LICENSE)\n\nimport pytest\nfrom requests.exceptions import ConnectionError\n\nfrom datadog_checks.gitlab_runner import GitlabRunnerCheck\n\nfrom .common import ALLOWED_METRICS, BAD_CONFIG, CONFIG, CUSTOM_TAGS, GITLAB_RUNNER_TAGS, HOST\n\n\ndef test_check(aggregator):\n    \"\"\"\n    Basic Test for gitlab integration.\n    \"\"\"\n    gitlab_runner = GitlabRunnerCheck('gitlab_runner', CONFIG['init_config'], {}, instances=CONFIG['instances'])\n\n    gitlab_runner.check(CONFIG['instances'][0])\n\n    aggregator.assert_service_check(\n        GitlabRunnerCheck.MASTER_SERVICE_CHECK_NAME,\n        status=GitlabRunnerCheck.OK,\n        tags=GITLAB_RUNNER_TAGS + CUSTOM_TAGS,\n        count=1,\n    )\n\n    aggregator.assert_service_check(\n        GitlabRunnerCheck.PROMETHEUS_SERVICE_CHECK_NAME, status=GitlabRunnerCheck.OK, tags=CUSTOM_TAGS, count=1\n    )\n\n    for metric in ALLOWED_METRICS:\n        if metric.startswith('ci_runner'):\n            aggregator.assert_metric(\"gitlab_runner.{}\".format(metric))\n        else:\n            aggregator.assert_metric(\"gitlab_runner.{}\".format(metric), tags=CUSTOM_TAGS, count=1)\n\n\ndef test_connection_failure(aggregator):\n    \"\"\"\n    Make sure we're failing when the URL isn't right\n    \"\"\"\n\n    gitlab_runner = GitlabRunnerCheck('gitlab', BAD_CONFIG['init_config'], {}, instances=BAD_CONFIG['instances'])\n\n    with pytest.raises(ConnectionError):\n        gitlab_runner.check(BAD_CONFIG['instances'][0])\n\n    # We should get two failed service checks\n    aggregator.assert_service_check(\n        GitlabRunnerCheck.MASTER_SERVICE_CHECK_NAME,\n        status=GitlabRunnerCheck.CRITICAL,\n        tags=['gitlab_host:{}'.format(HOST), 'gitlab_port:1234'] + CUSTOM_TAGS,\n        count=1,\n    )\n\n    aggregator.assert_service_check(\n        GitlabRunnerCheck.PROMETHEUS_SERVICE_CHECK_NAME, status=GitlabRunnerCheck.CRITICAL, tags=CUSTOM_TAGS, count=1\n    )\n<|endoftext|>",
    "repo": "int-tt/integrations-core"
  },
  {
    "prompt": "\"\"\"Base factory.\"\"\"\nclass BaseFactory(SQLAlchemyModelFactory):\n\n\n\"\"\"User factory.\"\"\"\nclass UserFactory(BaseFactory):\n\tusername = Sequence(lambda n: f'user{n}')\n\tpassword = PostGenerationMethodCall('set_password', 'example')\n\tactive = True\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# -*- coding: utf-8 -*-\n\"\"\"Model unit tests.\"\"\"\nimport datetime as dt\n\nimport pytest\n\nfrom radio_chaser.user.models import Role, User\n\nfrom .factories import UserFactory\n\n\n@pytest.mark.usefixtures(\"db\")\nclass TestUser:\n    \"\"\"User tests.\"\"\"\n\n    def test_get_by_id(self):\n        \"\"\"Get user by ID.\"\"\"\n        user = User(\"foo\", \"foo@bar.com\")\n        user.save()\n\n        retrieved = User.get_by_id(user.id)\n        assert retrieved == user\n\n    def test_created_at_defaults_to_datetime(self):\n        \"\"\"Test creation date.\"\"\"\n        user = User(username=\"foo\")\n        user.save()\n        assert bool(user.created_at)\n        assert isinstance(user.created_at, dt.datetime)\n\n    def test_password_is_nullable(self):\n        \"\"\"Test null password.\"\"\"\n        user = User(username=\"foo\")\n        user.save()\n        assert user.password is None\n\n    def test_factory(self, db):\n        \"\"\"Test user factory.\"\"\"\n        user = UserFactory(password=\"myprecious\")\n        db.session.commit()\n        assert bool(user.username)\n        assert bool(user.created_at)\n        assert user.is_admin is False\n        assert user.active is True\n        assert user.check_password(\"myprecious\")\n\n    def test_check_password(self):\n        \"\"\"Check password.\"\"\"\n        user = User.create(username=\"foo\", password=\"foobarbaz123\")\n        assert user.check_password(\"foobarbaz123\") is True\n        assert user.check_password(\"barfoobaz\") is False\n\n    def test_full_name(self):\n        \"\"\"User full name.\"\"\"\n        user = UserFactory(first_name=\"Foo\", last_name=\"Bar\")\n        assert user.full_name == \"Foo Bar\"\n\n    def test_roles(self):\n        \"\"\"Add a role to a user.\"\"\"\n        role = Role(name=\"admin\")\n        role.save()\n        user = UserFactory()\n        user.roles.append(role)\n        user.save()\n        assert role in user.roles\n<|endoftext|>",
    "repo": "SeattleDSA/radio-chaser"
  },
  {
    "prompt": "class UserFactory(DjangoModelFactory):\n\tusername = Faker('user_name')\n\temail = Faker('email')\n\tname = Faker('name')\n\tfn password(self, create: bool, extracted: Sequence[Any])\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom celery.result import EagerResult\n\nfrom great_url_shortener.users.tasks import get_users_count\nfrom great_url_shortener.users.tests.factories import UserFactory\n\npytestmark = pytest.mark.django_db\n\n\ndef test_user_count(settings):\n    \"\"\"A basic test to execute the get_users_count Celery task.\"\"\"\n    UserFactory.create_batch(3)\n    settings.CELERY_TASK_ALWAYS_EAGER = True\n    task_result = get_users_count.delay()\n    assert isinstance(task_result, EagerResult)\n    assert task_result.result == 3\n<|endoftext|>",
    "repo": "theodor85/url-shortener"
  },
  {
    "prompt": "class cq_directive(Directive):\n\thas_content = True\n\trequired_arguments = 0\n\toptional_arguments = 2\n\toption_spec = {'height': directives.length_or_unitless, 'width': directives.length_or_percentage_or_unitless, 'align': directives.unchanged}\n\tfn run(self)\n\n\nfn setup(app)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom glob import glob\nfrom itertools import chain, count\n\nfrom docutils.parsers.rst import directives, Directive\nfrom docutils.core import publish_doctree\nfrom docutils.utils import Reporter\n\nimport cadquery as cq\nfrom cadquery import cqgi\nfrom cadquery.cq_directive import cq_directive\n\n\ndef find_examples(pattern=\"examples/*.py\"):\n\n    for p in glob(pattern):\n        with open(p, encoding=\"UTF-8\") as f:\n            code = f.read()\n\n        yield code\n\n\ndef find_examples_in_docs(pattern=\"doc/*.rst\"):\n\n    # dummy CQ directive for code\n    class dummy_cq_directive(cq_directive):\n\n        codes = []\n\n        def run(self):\n\n            self.codes.append(\"\\n\".join(self.content))\n\n            return []\n\n    directives.register_directive(\"cadquery\", dummy_cq_directive)\n\n    # read and parse all rst files\n    for p in glob(pattern):\n        with open(p, encoding=\"UTF-8\") as f:\n            doc = f.read()\n\n        publish_doctree(\n            doc, settings_overrides={\"report_level\": Reporter.SEVERE_LEVEL + 1}\n        )\n\n    # yield all code snippets\n    for c in dummy_cq_directive.codes:\n\n        yield c\n\n\n@pytest.mark.parametrize(\n    \"code\", chain(find_examples(), find_examples_in_docs()), ids=count(0)\n)\ndef test_example(code):\n\n    # build\n    res = cqgi.parse(code).build()\n\n    assert res.exception is None\n\n    # check if the resulting objects are valid\n    for r in res.results:\n        r = r.shape\n        if isinstance(r, cq.Workplane):\n            for v in r.vals():\n                if isinstance(v, cq.Shape):\n                    assert v.isValid()\n        elif isinstance(r, cq.Shape):\n            assert r.isValid()\n<|endoftext|>",
    "repo": "shimwell/cadquery"
  },
  {
    "prompt": "class Blocked(Exception):\n\n\nclass TestRun:\n\tdut = None\n\texecutor = None\n\tLOGGER: Log = None\n\tplugin_manager = None\n\tfn step(cls, message)\n\tfn group(cls, message)\n\tfn iteration(cls, iterable, group_name)\n\tfn fail(cls, message)\n\tfn block(cls, message)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "#\n# Copyright(c) 2020 Intel Corporation\n# SPDX-License-Identifier: BSD-3-Clause-Clear\n#\n\nimport pytest\nimport sys\nimport importlib\nfrom core.test_run import TestRun\n\n\nclass PluginManager:\n    def __init__(self, item, config):\n        if 'plugins_dir' in config:\n            sys.path.append(config['plugins_dir'])\n        self.plugins = {}\n\n        self.plugins_config = config.get('plugins', {})\n\n        self.req_plugins = config.get('req_plugins', {})\n        self.opt_plugins = config.get('opt_plugins', {})\n\n        self.req_plugins.update(dict(map(lambda mark: (mark.args[0], mark.kwargs),\n                                item.iter_markers(name=\"require_plugin\"))))\n\n        req_plugin_mod = {}\n        opt_plugin_mod = {}\n\n        for name in self.req_plugins:\n            try:\n                req_plugin_mod[name] = self.__import_plugin(name)\n            except ModuleNotFoundError:\n                pytest.skip(\"Unable to find requested plugin!\")\n\n        for name in self.opt_plugins:\n            try:\n                opt_plugin_mod[name] = self.__import_plugin(name)\n            except ModuleNotFoundError as e:\n                TestRun.LOGGER.debug(\n                    f\"Failed to import '{name}' - optional plugin. \" f\"Reason: {e}\"\n                )\n                continue\n\n        for name, mod in req_plugin_mod.items():\n            try:\n                self.plugins[name] = mod.plugin_class(\n                    self.req_plugins[name],\n                    self.plugins_config.get(name, {}).get(\"config\", {}))\n            except Exception:\n                pytest.skip(f\"Unable to initialize plugin '{name}'\")\n\n        for name, mod in opt_plugin_mod.items():\n            try:\n                self.plugins[name] = mod.plugin_class(\n                    self.opt_plugins[name],\n                    self.plugins_config.get(name, {}).get(\"config\", {}))\n            except Exception as e:\n                TestRun.LOGGER.debug(\n                    f\"Failed to initialize '{name}' - optional plugin. \" f\"Reason: {e}\"\n                )\n                continue\n\n    def __import_plugin(self, name):\n        provided_by = self.plugins_config.get(name, {}).get(\"provided_by\")\n        if provided_by:\n            return importlib.import_module(provided_by)\n\n        try:\n            return importlib.import_module(f\"internal_plugins.{name}\")\n        except ModuleNotFoundError:\n            pass\n\n        return importlib.import_module(f\"external_plugins.{name}\")\n\n    def hook_pre_setup(self):\n        for plugin in self.plugins.values():\n            plugin.pre_setup()\n\n    def hook_post_setup(self):\n        for plugin in self.plugins.values():\n            plugin.post_setup()\n\n    def hook_teardown(self):\n        for plugin in self.plugins.values():\n            plugin.teardown()\n\n    def get_plugin(self, name):\n        if name not in self.plugins:\n            raise KeyError(\"Requested plugin does not exist\")\n        return self.plugins[name]\n<|endoftext|>",
    "repo": "josehu07/open-cas-linux-mf"
  },
  {
    "prompt": "\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom icevision.all import *\nfrom icevision.models.ultralytics.yolov5.backbones import *\n\n\ndef _test_preds(preds):\n    assert len(preds) == 3\n    assert isinstance(preds[0].detection.bboxes[0], BBox)\n    assert len(preds[0].detection.scores) == len(preds[0].detection.labels)\n    assert len(preds[2].detection.scores) == len(preds[2].detection.labels)\n\n\n@pytest.mark.parametrize(\n    \"backbone\",\n    [small, medium, large],\n)\ndef test_yolo_predict(fridge_ds, backbone):\n    _, valid_ds = fridge_ds\n    model = models.ultralytics.yolov5.model(\n        num_classes=5, img_size=384, backbone=backbone(pretrained=True)\n    )\n    preds = models.ultralytics.yolov5.predict(model, valid_ds, detection_threshold=0.0)\n    _test_preds(preds)\n\n\n@pytest.mark.parametrize(\n    \"backbone\",\n    [small, medium, large],\n)\ndef test_yolo_predict_from_dl(fridge_ds, backbone):\n    _, valid_ds = fridge_ds\n    infer_dl = models.ultralytics.yolov5.infer_dl(valid_ds, batch_size=1, shuffle=False)\n    model = models.ultralytics.yolov5.model(\n        num_classes=5, img_size=384, backbone=backbone(pretrained=True)\n    )\n    preds = models.ultralytics.yolov5.predict_from_dl(\n        model, infer_dl, detection_threshold=0.0\n    )\n    _test_preds(preds)\n<|endoftext|>",
    "repo": "Toucan-Systems/icevision"
  },
  {
    "prompt": "class ErrorHandler(list):\n\tfn append(self, error)\n\n\nfn process(input, filepath)\n\nfn process_all(input, filepath, error_handler)\n\nfn assert_like(d1, d2)\n\nfn outdent(doc)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import glob\nimport os\n\nimport pytest\n\nfrom .utils import assert_like, process_all\n\n\ndef simple_name(filename):\n    return os.path.splitext(os.path.basename(filename))[0]\n\n\npath = os.path.dirname(os.path.realpath(__file__))\nfiles = [\n    pytest.param(f, id=simple_name(f))\n    for f in glob.glob(path + \"/samples/**/*.yml\", recursive=True)\n]\n\nos.environ[\"YGLU_ENABLE_ENV\"] = \"true\"\n\n\n@pytest.mark.parametrize(\"filename\", files)\ndef test_sample(filename):\n    with open(filename) as file_handle:\n        (input, output) = process_all(file_handle, filename, FailingErrorHandler())\n        assert_like(input, output)\n\nclass FailingErrorHandler(list):\n    def __init__(self):\n        self.nodes = set()\n\n    def append(self, error):\n        raise error\n<|endoftext|>",
    "repo": "lbovet/yglu"
  },
  {
    "prompt": "class CounterWrapper:\n\tfn __init__(self, func)\n\tfn __call__(self)\n\tfn count(self, integrator)\n\n\nfn valid_args_id(args)\n\nfn valid_args(request)\n\nfn invalid_args_id(args)\n\nfn invalid_args(request)\n\nfn _test_moves_id(args)\n\nfn _test_moves_args(_valid_args)\n\nfn test_moves_args(request)\n\nfn _cpp_args(_valid_args)\n\nfn cpp_args_id(args)\n\nfn cpp_args(request)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright (c) 2009-2022 The Regents of the University of Michigan.\n# Part of HOOMD-blue, released under the BSD 3-Clause License.\n\n\"\"\"Test hoomd.hpmc.nec.integrate.Sphere.\"\"\"\n\nimport hoomd\nimport hoomd.hpmc.tune\nfrom hoomd.conftest import operation_pickling_check\n\nimport pytest\nimport math\n\n# from test_sphere_eos.py for hoomd-2.9\n# see for example Guang-Wen Wu and Richard J. Sadus, doi:10.1002.aic10233\n_phi_p_ref = [(0.29054, 0.1), (0.91912, 0.2), (2.2768, 0.3), (5.29102, 0.4),\n              (8.06553, 0.45), (9.98979, 0.475)]\nrel_err_cs = 0.0015\n\n\n@pytest.mark.parametrize(\"betap,phi\", _phi_p_ref)\n@pytest.mark.serial\n@pytest.mark.validate\ndef test_sphere_eos_nec(betap, phi, simulation_factory,\n                        lattice_snapshot_factory):\n    \"\"\"Test that NEC runs and computes the pressure correctly.\"\"\"\n    n = 7\n\n    v_particle = 4 / 3 * math.pi * (0.5)**3\n    lattice_length = (v_particle / phi)**(1. / 3)\n    snap = lattice_snapshot_factory(n=n, a=lattice_length)\n\n    sim = simulation_factory(snap)\n    sim.seed = 123456\n    sim.state.thermalize_particle_momenta(hoomd.filter.All(), kT=1)\n\n    mc = hoomd.hpmc.nec.integrate.Sphere(default_d=0.05, update_fraction=0.05)\n    mc.shape['A'] = dict(diameter=1)\n    mc.chain_time = 0.05\n    sim.operations.integrator = mc\n\n    triggerTune = hoomd.trigger.Periodic(50, 0)\n    tune_nec_d = hoomd.hpmc.tune.MoveSize.scale_solver(\n        triggerTune,\n        moves=['d'],\n        target=0.10,\n        tol=0.001,\n        max_translation_move=0.15)\n    sim.operations.tuners.append(tune_nec_d)\n\n    tune_nec_ct = hoomd.hpmc.nec.tune.ChainTime.scale_solver(triggerTune,\n                                                             target=20.0,\n                                                             tol=1.0,\n                                                             gamma=20.0)\n    sim.operations.tuners.append(tune_nec_ct)\n\n    # equilibrate\n    sim.run(1000)\n    sim.operations.tuners.clear()\n\n    pressures = []\n    mc.nselect = 50\n    for i in range(100):\n        sim.run(1)\n        pressures.append(mc.virial_pressure)\n\n    mean = sum(pressures) / len(pressures)\n    variance = sum(p**2 for p in pressures) / len(pressures) - mean**2\n    std_dev = variance**0.5\n    error = std_dev / (len(pressures) - 1)**0.5\n\n    # confidence interval, 0.95 quantile of the normal distribution\n    ci = 1.96\n\n    assert math.isclose(betap, mean, abs_tol=ci * (error + rel_err_cs * betap))\n    assert error < 1\n\n    mcCounts = mc.counters\n    necCounts = mc.nec_counters\n\n    assert mcCounts.overlap_errors == 0\n    assert necCounts.overlap_errors == 0\n\n\nnec_test_parameters = [\n    (\n        hoomd.hpmc.nec.integrate.Sphere,\n        dict(default_d=0.2, chain_time=0.75, update_fraction=0.125, nselect=2),\n        dict(diameter=1),\n    ),\n    (\n        hoomd.hpmc.nec.integrate.ConvexPolyhedron,\n        dict(default_d=0.2,\n             default_a=0.1,\n             chain_probability=0.5,\n             chain_time=0.75,\n             update_fraction=0.125,\n             nselect=2),\n        dict(vertices=[[1, 1, 1], [1, 1, -1], [1, -1, 1], [1, -1, -1],\n                       [-1, 1, 1], [-1, 1, -1], [-1, -1, 1], [-1, -1, -1]]),\n    ),\n]\n\n\n@pytest.mark.serial\n@pytest.mark.parametrize('integrator_cls, integrator_args, shape',\n                         nec_test_parameters)\ndef test_pickling(integrator_cls, integrator_args, shape, simulation_factory,\n                  two_particle_snapshot_factory):\n    mc = integrator_cls(**integrator_args)\n    mc.shape[\"A\"] = shape\n    sim = simulation_factory(two_particle_snapshot_factory(L=1000))\n    operation_pickling_check(mc, sim)\n<|endoftext|>",
    "repo": "ianrgraham/hoomd-blue"
  },
  {
    "prompt": "class Persistence:\n\tfn __get__(self, keyring, type)\n\t\"\"\"Set the persistence value on the Keyring. Value may be\n\tone of the win32cred.CRED_PERSIST_* constants or a\n\tstring representing one of those constants. For example,\n\t'local machine' or 'session'.\"\"\"\n\tfn __set__(self, keyring, value)\n\n\n\"\"\"WinVaultKeyring stores encrypted passwords using the Windows Credential\nManager.\n\nRequires pywin32\n\nThis backend does some gymnastics to simulate multi-user support,\nwhich WinVault doesn't support natively. See\nhttps://bitbucket.org/kang/python-keyring-lib/issue/47/winvaultkeyring-only-ever-returns-last#comment-731977\nfor details on the implementation, but here's the gist:\n\nPasswords are stored under the service name unless there is a collision\n(another password with the same service name but different user name),\nin which case the previous password is moved into a compound name:\n{username}@{service}\"\"\"\nclass WinVaultKeyring(KeyringBackend):\n\tpersist = Persistence()\n\t\"\"\"If available, the preferred backend on Windows.\"\"\"\n\tfn priority(cls)\n\tfn _compound_name(username, service)\n\tfn get_password(self, service, username)\n\tfn _get_password(self, target)\n\tfn set_password(self, service, username, password)\n\tfn _set_password(self, target, username, password)\n\tfn delete_password(self, service, username)\n\tfn _delete_password(self, target)\n\tfn get_credential(self, service, username)\n\n\n\"\"\"A compatibility wrapper for old PyWin32 errors, such as reported in\nhttps://bitbucket.org/kang/python-keyring-lib/issue/140/\"\"\"\nclass OldPywinError:\n\tfn __init__(self, orig)\n\tfn funcname(self)\n\tfn winerror(self)\n\tfn wrap(cls, orig_err)\n\n\nfn is_ascii_printable(s)\n\n\"\"\"Test for the keyring's basic functions. password_set and password_get\n    \"\"\"\nclass BackendBasicTests:\n\tDIFFICULT_CHARS = string.whitespace + string.punctuation\n\tfn _init_properties(self, request)\n\tfn cleanup(self)\n\tfn set_password(self, service, username, password)\n\tfn check_set_get(self, service, username, password)\n\tfn test_password_set_get(self)\n\tfn test_difficult_chars(self)\n\tfn test_delete_present(self)\n\tfn test_delete_not_present(self)\n\tfn test_delete_one_in_group(self)\n\tfn test_name_property(self)\n\tfn test_unicode_chars(self)\n\tfn test_unicode_and_ascii_chars(self)\n\t\"\"\"Issue #47 reports that WinVault isn't storing passwords for\n\tmultiple users. This test exercises that test for each of the\n\tbackends.\"\"\"\n\tfn test_different_user(self)\n\tfn test_credential(self)\n\tfn test_set_properties(self, monkeypatch)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import sys\n\nimport pytest\n\nimport keyring.backends.Windows\nfrom keyring.testing.backend import BackendBasicTests\n\n\n@pytest.mark.skipif(\n    not keyring.backends.Windows.WinVaultKeyring.viable, reason=\"Needs Windows\"\n)\nclass WinVaultKeyringTestCase(BackendBasicTests):\n    def tearDown(self):\n        # clean up any credentials created\n        for cred in self.credentials_created:\n            try:\n                self.keyring.delete_password(*cred)\n            except Exception as e:\n                print(e, file=sys.stderr)\n\n    def init_keyring(self):\n        return keyring.backends.Windows.WinVaultKeyring()\n\n\n@pytest.mark.skipif('sys.platform != \"win32\"')\ndef test_winvault_always_viable():\n    \"\"\"\n    The WinVault backend should always be viable on Windows.\n    \"\"\"\n    assert keyring.backends.Windows.WinVaultKeyring.viable\n<|endoftext|>",
    "repo": "ddstreet/keyring"
  },
  {
    "prompt": "fn calculate_price_without_vat(price_with_vat)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from decimal import Decimal\n\nimport pytest\n\nfrom parking_permits_app.pricing.value_added_tax import calculate_price_without_vat\n\n\n@pytest.mark.parametrize(\n    \"price_with_vat, price_without_vat\",\n    [\n        (Decimal(\"124\"), Decimal(\"100\")),\n        (Decimal(\"60\"), Decimal(\"48.39\")),\n        (Decimal(\"10000\"), Decimal(\"8064.52\")),\n        (Decimal(\"32.75\"), Decimal(\"26.41\")),\n        (Decimal(\"0\"), Decimal(\"0\")),\n    ],\n)\ndef test_calculate_price_without_vat_function_returns_correct_result(\n    price_with_vat, price_without_vat\n):\n    assert calculate_price_without_vat(price_with_vat) == price_without_vat\n<|endoftext|>",
    "repo": "mingfeng/parking-permits"
  },
  {
    "prompt": "\"\"\"Function to load sample datasets.\n\nOrder of read:\n(1) Tries to read dataset from local folder first.\n(2) Then tries to read dataset from folder in GitHub \"address\" (see below)\n(3) Then tries to read from sktime (if installed)\n(4) Raises error if none exist\n\nList of available datasets on GitHub can be checked using\n(1) ``get_data('index')`` or\n(2) ``get_data('index', folder='time_series/seasonal)``\n(see available \"folder\" options below)\n\n\nExample\n-------\n>>> from pycaret.datasets import get_data\n>>> all_datasets = get_data('index')\n>>> juice = get_data('juice')\n\n\ndataset: str, default = 'index'\n    Index value of dataset.\n\n\nfolder: Optional[str], default = None\n    The folder from which to get the data.\n    If 'None', gets it from the \"common\" folder. Other options are:\n        - time_series/seasonal\n        - time_series/random_walk\n        - time_series/white_noise\n\n\nsave_copy: bool, default = False\n    When set to true, it saves a copy in current working directory.\n\n\nprofile: bool, default = False\n    When set to true, an interactive EDA report is displayed.\n\n\nverbose: bool, default = True\n    When set to False, head of data is not displayed.\n\naddress: string, default = None\n    Download url of dataset. Defaults to None which fetches the dataset from\n    \"https://raw.githubusercontent.com/pycaret/datasets/main/\". For people\n    having difficulty linking to github, they can change the default address\n    to their own\n    (e.g. \"https://gitee.com/IncubatorShokuhou/pycaret/raw/master/datasets/\")\n\n\nReturns:\n    pandas.DataFrame\n\n\nWarnings\n--------\n- Use of ``get_data`` requires internet connection.\n\n\nRaises\n------\nImportError\n    (1) When trying to import time series datasets that require sktime,\n    but sktime has not been installed.\n    (2) If the data does not exist\"\"\"\nfn get_data(dataset: str, folder: Optional[str], save_copy: bool, profile: bool, verbose: bool, address: Optional[str])\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import os, sys\n\nsys.path.insert(0, os.path.abspath(\"..\"))\n\nimport pandas as pd\nimport numpy as np\nimport pytest\nimport pycaret.classification\nimport pycaret.datasets\nfrom IPython.display import display\n\ndef test():\n\n    # loading dataset\n    data = pycaret.datasets.get_data(\"blood\")\n\n    # initialize setup\n    clf1 = pycaret.classification.setup(\n        data,\n        target=\"Class\",\n        silent=True,\n        html=False,\n        n_jobs=1,\n    )\n\n    # EDA\n    pycaret.classification.eda(display_format = 'svg')\n\n    # assert\n    assert 1 == 1\n\nif __name__ == \"__main__\":\n    test()\n<|endoftext|>",
    "repo": "daikikatsuragawa/pycaret"
  },
  {
    "prompt": "class UserFactory(DjangoModelFactory):\n\tusername = Faker('user_name')\n\temail = Faker('email')\n\tname = Faker('name')\n\tfn password(self, create: bool, extracted: Sequence[Any])\n\n\n\"\"\"Default user for Qiubit Test.\"\"\"\nclass User(AbstractUser):\n\tname = CharField(_('Name of User'), blank=True, max_length=255)\n\tfirst_name = None\n\tlast_name = None\n\t\"\"\"Get url for user's detail view.\n\t\n\tReturns:\n\t    str: URL for user detail.\"\"\"\n\tfn get_absolute_url(self) -> str\n\n\nclass UserDetailView(LoginRequiredMixin, DetailView):\n\tmodel = User\n\tslug_field = 'username'\n\tslug_url_kwarg = 'username'\n\n\nclass UserUpdateView(LoginRequiredMixin, SuccessMessageMixin, UpdateView):\n\tmodel = User\n\tfields = ['name']\n\tsuccess_message = _('Information successfully updated')\n\tfn get_success_url(self)\n\tfn get_object(self)\n\n\nclass UserRedirectView(LoginRequiredMixin, RedirectView):\n\tpermanent = False\n\tfn get_redirect_url(self)\n\n\nclass UserChangeForm(admin_forms.UserChangeForm):\n\n\nclass UserCreationForm(admin_forms.UserCreationForm):\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom django.conf import settings\nfrom django.contrib import messages\nfrom django.contrib.auth.models import AnonymousUser\nfrom django.contrib.messages.middleware import MessageMiddleware\nfrom django.contrib.sessions.middleware import SessionMiddleware\nfrom django.test import RequestFactory\nfrom django.urls import reverse\n\nfrom qiubit.users.forms import UserChangeForm\nfrom qiubit.users.models import User\nfrom qiubit.users.tests.factories import UserFactory\nfrom qiubit.users.views import (\n    UserRedirectView,\n    UserUpdateView,\n    user_detail_view,\n)\n\npytestmark = pytest.mark.django_db\n\n\nclass TestUserUpdateView:\n    \"\"\"\n    TODO:\n        extracting view initialization code as class-scoped fixture\n        would be great if only pytest-django supported non-function-scoped\n        fixture db access -- this is a work-in-progress for now:\n        https://github.com/pytest-dev/pytest-django/pull/258\n    \"\"\"\n\n    def test_get_success_url(self, user: User, rf: RequestFactory):\n        view = UserUpdateView()\n        request = rf.get(\"/fake-url/\")\n        request.user = user\n\n        view.request = request\n\n        assert view.get_success_url() == f\"/users/{user.username}/\"\n\n    def test_get_object(self, user: User, rf: RequestFactory):\n        view = UserUpdateView()\n        request = rf.get(\"/fake-url/\")\n        request.user = user\n\n        view.request = request\n\n        assert view.get_object() == user\n\n    def test_form_valid(self, user: User, rf: RequestFactory):\n        view = UserUpdateView()\n        request = rf.get(\"/fake-url/\")\n\n        # Add the session/message middleware to the request\n        SessionMiddleware().process_request(request)\n        MessageMiddleware().process_request(request)\n        request.user = user\n\n        view.request = request\n\n        # Initialize the form\n        form = UserChangeForm()\n        form.cleaned_data = []\n        view.form_valid(form)\n\n        messages_sent = [m.message for m in messages.get_messages(request)]\n        assert messages_sent == [\"Information successfully updated\"]\n\n\nclass TestUserRedirectView:\n    def test_get_redirect_url(self, user: User, rf: RequestFactory):\n        view = UserRedirectView()\n        request = rf.get(\"/fake-url\")\n        request.user = user\n\n        view.request = request\n\n        assert view.get_redirect_url() == f\"/users/{user.username}/\"\n\n\nclass TestUserDetailView:\n    def test_authenticated(self, user: User, rf: RequestFactory):\n        request = rf.get(\"/fake-url/\")\n        request.user = UserFactory()\n\n        response = user_detail_view(request, username=user.username)\n\n        assert response.status_code == 200\n\n    def test_not_authenticated(self, user: User, rf: RequestFactory):\n        request = rf.get(\"/fake-url/\")\n        request.user = AnonymousUser()\n\n        response = user_detail_view(request, username=user.username)\n        login_url = reverse(settings.LOGIN_URL)\n\n        assert response.status_code == 302\n        assert response.url == f\"{login_url}?next=/fake-url/\"\n<|endoftext|>",
    "repo": "willmateusav/qiubit"
  },
  {
    "prompt": "fn somar(x, y)\n\nfn multiplicar(x, y)\n\nfn subtrair(x, y)\n\nfn divisao(x, y)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom Principal import somar\nfrom principal import multiplicar\nfrom principal import subtrair\nfrom principal import divisao\n\n\ndef teste_somar():\n    assert somar(2,4)==6\n\ndef teste_multiplicar():\n    assert multiplicar(6,12)==72\n\ndef teste_subtrair():\n    assert subtrair(290-800)==610\n\ndef teste_divisao():\n    assert divisao(6,2)== 3<|endoftext|>",
    "repo": "dom8891/DevOps_revisao"
  },
  {
    "prompt": "fn dynamoGetMap(DiningCommon, metric)\n\nfn doesNotHaveMeal(diningCommon, mealTime)\n\nfn hours(diningCommon, mealTime)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "#test_hours.py\r\n\r\nimport boto3\r\nimport json\r\nimport os\r\nimport pytest\r\nimport decimal\r\nfrom boto3.dynamodb.conditions import Key, Attr\r\n\r\nfrom hoursFunctions import *\r\n\r\nclient = boto3.resource('dynamodb',aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'], aws_secret_access_key=os.environ[\r\n'AWS_SECRET_ACCESS_KEY'], region_name='us-east-1')\r\ntable = client.Table('GauchoEats')\r\n\r\ndef test_hours():\r\n    isWeekend = True if (str(dynamoGetMap(\"dlg\", \"isWeekend\")) == \"True\") else False\r\n    print(\"testing dlg breakfast: \")\r\n    dlgBreakfast = hours(\"dlg\", \"breakfast\")\r\n    assert dlgBreakfast == \"Dlg doesn't have breakfast.\"\r\n    if isWeekend == False:\r\n        print(\"testing dlg lunch: \")\r\n        assert hours(\"dlg\", \"lunch\") == \"D.L.G. is open from 11:00 AM to 2:30 PM for lunch\"\r\n        print(\"testing dlg dinner: \")\r\n        assert hours(\"dlg\", \"dinner\") == \"D.L.G. is open from 5:00 PM to 8:00 PM for dinner\"\r\n        print(\"testing dlg late-night: \")\r\n        assert hours(\"dlg\", \"late-night\") == \"D.L.G. is open from 9:00 PM to 12:30 AM for late-night\"\r\n        print(\"testing carrillo breakfast:\")\r\n        assert hours(\"carrillo\", \"breakfast\") == \"Carrillo is open from 7:15 AM to 10:00 AM for breakfast\"\r\n        print(\"testing carrillo lunch\")\r\n        assert hours(\"carrillo\", \"lunch\") == \"Carrillo is open from 11:00 AM to 2:30 PM for lunch\"\r\n        print(\"testing carrillo dinner:\")\r\n        assert hours(\"carrillo\", \"dinner\") == \"Carrillo is open from 5:00 PM to 8:00 PM for dinner\"\r\n        print(\"testing ortega breakfast:\")\r\n        assert hours(\"ortega\", \"breakfast\") == \"Ortega is open from 7:15 AM to 10:00 AM for breakfast\"\r\n        print(\"testing ortega lunch:\")\r\n        assert hours(\"ortega\", \"lunch\") == \"Ortega is open from 11:00 AM to 2:30 PM for lunch\"\r\n        print(\"testing ortega dinner:\")\r\n        assert hours(\"ortega\", \"dinner\") == \"Ortega is open from 5:00 PM to 8:00 PM for dinner\"\r\n    else:\r\n        print(\"testing carrillo brunch:\")\r\n        assert hours(\"carrillo\", \"brunch\") == \"Carrillo is open from 10:30 AM to 2:00 PM for brunch\"\r\n        print(\"testing carrillo dinner:\")\r\n        assert hours(\"carrillo\", \"dinner\") == \"Carrillo is open from 5:00 PM to 8:00 PM for dinner\"\r\n        print(\"testing dlg brunch:\")\r\n        assert hours(\"dlg\", \"brunch\") == \"D.L.G. is open from 10:30 AM to 2:00 PM for brunch\"\r\n        print(\"testing dlg dinner: \")\r\n        assert hours(\"dlg\", \"dinner\") == \"D.L.G. is open from 5:00 PM to 8:00 PM for dinner\"\r\n<|endoftext|>",
    "repo": "darhsu/DiningTime"
  },
  {
    "prompt": "\"\"\"GFGenome class\"\"\"\nclass GFGenome(pyensembl.Genome):\n\tfn __init__(self, reference_name: object, annotation_name: object, annotation_version: object, gtf_path_or_url: object, transcript_fasta_paths_or_urls: object, protein_fasta_paths_or_urls: object, decompress_on_download: object, copy_local_files_to_cache: object, require_ensembl_ids: object, cache_directory_path: object, copy_genome)\n\tfn check_fasta_dictionary(self)\n\t\"\"\"Get GFTranscript objects. Optionally restrict to a particular chromosome and strand\n\t\n\t:param contig: optional, chromosome transcripts for which are required\n\t:type contig: int\n\t:param strand: optional, chromosome strand transcripts for which are required\n\t:type strand: str\n\t:return: list of gfeat.transcript.GFTranscript\"\"\"\n\tfn transcripts(self, contig: int, strand: str)\n\t\"\"\"Get GFTranscript objects by the specified transcript name\n\t\n\t:param transcript_name: name of the transcripts\n\t:type transcript_name: str\n\t:return: gfeat.transcript.GFTranscript object\"\"\"\n\tfn transcripts_by_name(self, transcript_name: str)\n\t\"\"\"Get GFTranscript objects by the specified protein id\n\t\n\t:param protein_id: protein id transcript for which is required\n\t:type protein_id: str\n\t:return: gfeat.transcript.GFTranscript object\"\"\"\n\tfn transcript_by_protein_id(self, protein_id: str)\n\t\"\"\"Get GFTranscript object by the transcript id\n\t\n\t:param transcript_id: id of the transcript\n\t:type transcript_id: str\n\t:return: gfeat.transcript.GFTranscript object\"\"\"\n\tfn gftranscript_by_id(self, transcript_id: str)\n\t\"\"\"Get the consensus Kozak sequence (Kozak sequence which is present more times than all other for this being)\n\t\n\t:param seq: True \u2013 to return the letter representation of the sequence\n\t\n\t    False \u2013 to return the digit representation of the sequence: 0 \u2013 A, 1 \u2013 C, 2 \u2013 G, 3 \u2013 T\n\t\n\t:type seq: bool\n\t:return: string, consensus Kozak sequence constituting of either digits or letters depending on the specified\n\t        seq parameter\"\"\"\n\tfn get_consensus_Kozak_seq(self, seq: bool)\n\t\"\"\"Get Kozak matrix with all transcripts for this being. Consensus columns are removed\n\t\n\t:return: pandas.DataFrame,\n\t\n\t        column names \u2013 first number corresponds to the base position in the Kozak sequence,\n\t        e.g. 0A means base A 5 bases upstream from the start codon\n\t\n\t        rows \u2013 1 if it has the corresponding base, 0 otherwise\n\t\n\tNOTE: columns corresponding to the most frequent bases at each position are removed\"\"\"\n\tfn get_Kozak_matrix(self)\n\t\"\"\"Get the consensus stop codon context sequence (stop codon context sequence which is present more times than\n\tall other for this being)\n\t\n\t:param seq: True \u2013 to return the letter representation of the sequence\n\t\n\t                  False \u2013 to return the digit representation of the sequence: 0 \u2013 A, 1 \u2013 C, 2 \u2013 G, 3 \u2013 T\n\t:type seq: bool\n\t:return: string, consensus stop codon context sequence constituting of either digits or letters\n\t        depending on the specified seq parameter\"\"\"\n\tfn get_consensus_stop_codon_context(self, seq: bool)\n\t\"\"\"Get stop codon context matrix with all transcripts for this being. Consensus columns are removed\n\t\n\t:return: pandas.DataFrame,\n\t\n\t        column names \u2013 first number corresponds to the base position in the stop codon\n\t        context sequence, e.g. 0A means base A 5 bases upstream from the start codon\n\t\n\t        rows \u2013 1 if it has the corresponding base, 0 otherwise\n\t\n\tNOTE: columns corresponding to the most frequent bases at each position are removed\"\"\"\n\tfn get_stop_codon_context_matrix(self)\n\t\"\"\"Get truncated PCA of the codon frequency matrix with 2 principal components\n\t\n\t:return: pandas.DataFrame, truncated PCA of the codon frequency matrix with 2 principal components\"\"\"\n\tfn get_codon_pair_bias(self)\n\t\"\"\"Get a table which shows whether a certain nucleobase in Kozak sequence or stop codon context was mutated or not.\n\t\n\t:param vcf: path to the vcf.gz or file opened using cyvcf2\n\t:type vcf: string or an \"opened\" file\n\t:return: pd.DataFrame,\n\t\n\t        column names \u2013 K_i \u2013 where i shows position in Kozak sequence;\n\t        S_i \u2013 where i shows position in stop codon context; gene_id; transcript_id\n\t\n\t        rows \u2013 NaN \u2013 no variant, 1 \u2013 heterozygous variant, 2 \u2013 homozygous variant\"\"\"\n\tfn get_nucleobase_mutation_table(self, vcf: string or an \"opened\" file)\n\n\nclass GFGene(pyensembl.Gene):\n\tfn from_pyensembl(cls, obj)\n\tfn iter_all(cls, genome)\n\t\"\"\"Calculate how many times the given codon is present in all transcript sequences corresponding to this gene\n\t:param codon: string, codon to be found in the sequence\n\t:return: how many times the given codon is present in the sequence\"\"\"\n\tfn coding_sequence_codon_count(self, codon: None)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom gfeat.genome import GFGenome\nfrom gfeat.gene import GFGene\n\n\n@pytest.fixture()\ndef gene():\n    data1 = GFGenome(reference_name='hg38_test',\n                     annotation_name='hg38_chr22_test',\n                     gtf_path_or_url=\"./tests/data/gencode.v24.annotation_chr22.gtf\",\n                     transcript_fasta_paths_or_urls=\"./tests/data/hg38_chr22.fa\",\n                     )\n    test_gene = GFGene('ENSG00000008735.13', 'MAPK8IP2', '22', 50600685, 50613981, '+', None, data1)\n    return test_gene\n<|endoftext|>",
    "repo": "gagneurlab/gfeat"
  },
  {
    "prompt": "class Node:\n\tfn __init__(self, val)\n\tfn __repr__(self)\n\tfn __str__(self)\n\n\nclass BST:\n\tfn __init__(self, iter)\n\tfn __repr__(self)\n\tfn __str__(self)\n\tfn in_order(self, operation)\n\tfn pre_order(self, operation)\n\tfn post_order(self, operation)\n\tfn insert(self, val)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom bst import BST as bst\n\n@pytest.fixture\ndef empty_bst():\n    \"\"\" \n    returns empty tree\n    \"\"\"\n    return bst()\n\n@pytest.fixture\ndef full_bst():\n    \"\"\" \n    creates a tree with values to check\n    \"\"\"\n    return bst([3, 5, 8, 15, 30, 12, 1, 21])\n\n\n<|endoftext|>",
    "repo": "bhold6160/data-structures-and-algorithms"
  },
  {
    "prompt": "class HTTPClient:\n\tfn __init__(self, base_url: str, session: requests.Session)\n\tfn get_html(self, params: dict, url_path: str) -> str\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nimport requests\nfrom mockito import mock\nfrom requests import Response\nfrom io import BytesIO as BufferIO\n\nfrom projectx1.http_client import HTTPClient\n\n\n@pytest.fixture\ndef session_factory(when):\n    def factory(return_value):\n        session = mock(requests.Session)\n        if isinstance(return_value, requests.exceptions.RequestException):\n            when(session).get(...).thenRaise(return_value)\n        else:\n            when(session).get(...).thenReturn(return_value)\n        \n        return session\n\n    return factory\n\n\ndef test_http_client_get_html_successful_connection_to_server(session_factory):\n    expected_page_html = '<html>website</html>'\n    \n    expected_response = Response()\n    expected_response.status_code = 200\n    expected_response.encoding = 'utf-8'\n    expected_response.raw = BufferIO(expected_page_html.encode('utf-8'))\n\n    session = session_factory(return_value=expected_response)\n    http_client = HTTPClient(base_url='https://www.example.com', session=session)\n    page_html = http_client.get_html(params={'page': 0})\n    \n    assert expected_page_html == page_html\n\n\ndef test_http_client_get_html_request_exception_occurred(session_factory):\n    expected_page_html = ''\n\n    session = session_factory(return_value=requests.exceptions.ConnectionError())\n    http_client = HTTPClient(base_url='https://www.example.com', session=session)\n    page_html = http_client.get_html(params={'page': 0})\n\n    assert expected_page_html == page_html\n<|endoftext|>",
    "repo": "jideobs/projectx1"
  },
  {
    "prompt": "\"\"\"Ask a yes/no question via raw_input() and return their answer.\n\nArgs:\n    question (str): Question hat is presented to the user.\n    default (str): The presumed answer, if the user just hits <Enter>.\n        It must be \"yes\" (the default), \"no\" or None (meaning\n        an answer is required of the user).\n\nReturns (boolean):\n    True, if 'yes', False otherwise.\"\"\"\nfn query_yes_no(question: str, default: str)\n\n\"\"\"Ask for user input.\n\nThis asks a yes/no question with a preset default.\nYou can bypass the user-input and fetch the default answer, if\nyou set\n\nArgs:\n    question: The question to ask on stdout.\n    default_answer: The default value to return.\n    default_answer_str:\n        The default answer string that we present to the user.\"\"\"\nfn ask(question: str, default_answer: bool, default_answer_str: str) -> bool\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"Test user interface utility methods.\"\"\"\nimport os\n\nimport pytest\n\nfrom benchbuild.utils.user_interface import ask\n\n\n@pytest.fixture\ndef env():\n    os.putenv(\"TEST\", \"yes\")\n\n\ndef test_ask(env):\n    assert ask(\"Test?\", default_answer=True) == True\n    assert ask(\"Test?\", default_answer=False) == False\n<|endoftext|>",
    "repo": "Ramdambo/benchbuild"
  },
  {
    "prompt": "fn select(value: Union[(Selector, Condition, str)]) -> Selector\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import marshmallow\nimport pytest\n\nfrom aiosnow import fields\nfrom aiosnow.query.utils import select\n\n\ndef test_fields_boolean_type():\n    assert isinstance(fields.Boolean(), fields.Boolean)\n\n\ndef test_fields_boolean_deserialize_plain_valid(mock_boolean_field):\n    f_bool = mock_boolean_field(\"test_bool\")\n\n    assert f_bool.deserialize(True) is True\n    assert f_bool.deserialize(1) is True\n    assert f_bool.deserialize(False) is False\n    assert f_bool.deserialize(0) is False\n\n\ndef test_fields_boolean_deserialize_plain_invalid(mock_boolean_field):\n    with pytest.raises(marshmallow.ValidationError):\n        mock_boolean_field(\"test_bool\").deserialize(3)\n\n\ndef test_fields_boolean_select_falsy(mock_boolean_field):\n    assert (\n        select(mock_boolean_field(\"test_bool\").is_falsy()).sysparms == \"test_bool!=true\"\n    )\n\n\ndef test_fields_boolean_select_true(mock_boolean_field):\n    assert (\n        select(mock_boolean_field(\"test_bool\").is_true()).sysparms == \"test_bool=true\"\n    )\n<|endoftext|>",
    "repo": "michaeldcanady/aiosnow"
  },
  {
    "prompt": "\"\"\"Validation for the essential parameters of step class\"\"\"\nclass EssentialParameters(object):\n\t\"\"\"Args:\n\t    cls_name: class name which has validation target parameters\n\t    param_list: list of validation target parameters\"\"\"\n\tfn __init__(self, cls_name, param_list)\n\tfn __call__(self)\n\n\n\"\"\"Validation for the table of sqlite\"\"\"\nclass SqliteTableExistence(object):\n\t\"\"\"Args:\n\t    dbname: database name\n\t    tblname: table name\n\t    returns_bool: return bool or not\"\"\"\n\tfn __init__(self, dbname, tblname, returns_bool)\n\tfn __call__(self)\n\n\n\"\"\"Validation for io: input\"\"\"\nclass IOInput(object):\n\tfn __init__(self, io)\n\tfn __call__(self)\n\n\n\"\"\"Validation for io: input\"\"\"\nclass IOOutput(object):\n\tfn __init__(self, io)\n\tfn __call__(self)\n\n\nclass CliboaException(Exception):\n\n\nclass DirStructureInvalid(CliboaException):\n\n\nclass FileNotFound(CliboaException):\n\n\nclass InvalidFileCount(CliboaException):\n\n\nclass InvalidParameter(CliboaException):\n\n\nclass InvalidCount(CliboaException):\n\n\nclass InvalidFormat(CliboaException):\n\n\nclass ScenarioFileInvalid(CliboaException):\n\n\nclass StepExecutionFailed(CliboaException):\n\n\nclass SqliteInvalid(CliboaException):\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "#\n# Copyright 2019 BrainPad Inc. All Rights Reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n#\nimport os\nimport shutil\nimport sqlite3\n\nimport pytest\n\nfrom cliboa.conf import env\nfrom cliboa.scenario.validator import EssentialParameters, SqliteTableExistence\nfrom cliboa.util.exception import CliboaException, SqliteInvalid\n\n\nclass TestEssentialParameters(object):\n    def test_essential_parameters_ng(self):\n        \"\"\"\n        EssentialParameters invalid case\n        \"\"\"\n        with pytest.raises(CliboaException) as excinfo:\n            valid = EssentialParameters(\"DummyClass\", [\"\"])\n            valid()\n        assert \"is not specified\" in str(excinfo.value)\n\n\nclass TestSqliteTableExistence(object):\n    def setup_method(self, method):\n        self.__db_dir = os.path.join(env.BASE_DIR, \"db\")\n\n    def test_table_existence_ng_with_exc(self):\n        \"\"\"\n        SqliteTableExistende invalid case\n        \"\"\"\n        # create test db and insert dummy data\n        os.makedirs(self.__db_dir)\n        db_file = os.path.join(self.__db_dir, \"spam.db\")\n        conn = sqlite3.connect(db_file)\n        conn.execute(\"create table spam_table (id, name, age);\")\n        conn.execute(\"insert into spam_table (id, name, age) values(1,1,1);\")\n        conn.commit()\n        conn.close()\n\n        with pytest.raises(SqliteInvalid) as excinfo:\n            valid = SqliteTableExistence(db_file, \"spam_table2\")\n            valid()\n        shutil.rmtree(self.__db_dir)\n        assert \"not found\" in str(excinfo.value)\n\n    def test_table_existence_ng_with_bool(self):\n        \"\"\"\n        SqliteTableExistende invalid case\n        \"\"\"\n        # create test db and insert dummy data\n        os.makedirs(self.__db_dir)\n        db_file = os.path.join(self.__db_dir, \"spam.db\")\n        conn = sqlite3.connect(db_file)\n        conn.execute(\"create table spam_table (id, name, age);\")\n        conn.execute(\"insert into spam_table (id, name, age) values(1,1,1);\")\n        conn.commit()\n        conn.close()\n\n        valid = SqliteTableExistence(db_file, \"spam_table2\", True)\n        exists_tbl = valid()\n        shutil.rmtree(self.__db_dir)\n        assert exists_tbl is False\n\n    def test_table_existence_ok_with_bool(self):\n        \"\"\"\n        SqliteTableExistende invalid case\n        \"\"\"\n        # create test db and insert dummy data\n        os.makedirs(self.__db_dir)\n        db_file = os.path.join(self.__db_dir, \"spam.db\")\n        conn = sqlite3.connect(db_file)\n        conn.execute(\"create table spam_table (id, name, age);\")\n        conn.execute(\"insert into spam_table (id, name, age) values(1,1,1);\")\n        conn.commit()\n        conn.close()\n\n        valid = SqliteTableExistence(db_file, \"spam_table\", True)\n        exists_tbl = valid()\n        shutil.rmtree(self.__db_dir)\n        assert exists_tbl is True\n<|endoftext|>",
    "repo": "kyoujuro/cliboa"
  },
  {
    "prompt": "\"\"\"Client to use in the SQL databases integration. Overrides BaseClient Overrides BaseClient\nmakes the connection to the DB server\"\"\"\nclass Client:\n\tfn __init__(self, dialect: str, host: str, username: str, password: str, port: str, database: str, connect_parameters: str)\n\t\"\"\"Converting a dialect to the correct string needed in order to connect the wanted dialect\n\t:param dialect: the SQL db\n\t:return: a key string needed for the connection\"\"\"\n\tfn _convert_dialect_to_module(dialect: str) -> str\n\t\"\"\"Creating and engine according to the instance preferences and connecting\n\t:return: a connection object that will be used in order to execute SQL queries\"\"\"\n\tfn _create_engine_and_connect(self) -> sqlalchemy.engine.base.Connection\n\t\"\"\"Execute query in DB via engine\n\t:param bind_vars: in case there are names and values - a bind_var dict, in case there are only values - list\n\t:param sql_query: the SQL query\n\t:return: results of query, table headers\"\"\"\n\tfn sql_query_execute_request(self, sql_query: str, bind_vars: Any) -> Tuple[(Dict, List)]\n\n\n\"\"\"In case no port was chosen, a default port will be chosen according to the SQL db type\n:param dialect: sql db type\n:return: default port needed for connection\"\"\"\nfn generate_default_port_by_dialect(dialect: str) -> str\n\n\"\"\"The bind variables can be given in 2 legal ways: as 2 lists - names and values, or only values\nany way defines a different executing way, therefore there are 2 legal return types\n:param bind_variables_names: the names of the bind variables, must be in the length of the values list\n:param bind_variables_values: the values of the bind variables, can be in the length of the names list\n        or in case there is no name lists - at any length\n:return: a dict or lists of the bind variables\"\"\"\nfn generate_bind_vars(bind_variables_names: str, bind_variables_values: str) -> Any\n\n\"\"\"If the connection in the client was successful the test will return OK\nif it wasn't an exception will be raised\"\"\"\nfn test_module(client: Client) -> Tuple[(str, Dict[(Any, Any)], List[Any])]\n\n\"\"\"Executes the sql query with the connection that was configured in the client\n:param client: the client object with the db connection\n:param args: demisto.args() including the sql query\n:return: Demisto outputs\"\"\"\nfn sql_query_execute(client: Client, args: dict) -> Tuple[(str, Dict[(str, Any)], List[Dict[(str, Any)]])]\n\nfn main()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from GenericSQL import Client, sql_query_execute\nimport pytest\n\nargs1 = {\n    'query': \"select Name from city\",\n    'limit': 5,\n    'skip': 0\n}\n\nraw1 = [{'Name': 'Kabul'}, {'Name': 'Qandahar'}, {'Name': 'Herat'}, {'Name': 'Mazar-e-Sharif'}]\n\nexpected_output = {\n    'GenericSQL(val.Query && val.Query === obj.Query)':\n        {'GenericSQL': {'Result': [{'Name': 'Kabul'},\n                                   {'Name': 'Qandahar'},\n                                   {'Name': 'Herat'},\n                                   {'Name': 'Mazar-e-Sharif'}],\n                        'Query': 'select Name from city',\n                        'InstanceName': 'sql_dialect_database'}}\n}\n\n\n@pytest.mark.parametrize('command, args, response, expected_result', [\n    (sql_query_execute, args1, raw1, expected_output),\n])\ndef test_sql_queries(command, args, response, expected_result, mocker):\n    \"\"\"Unit test\n    Given\n    - select query\n    - raw response of the database\n    When\n    - mock the database result\n    Then\n    - convert the result to human readable table\n    - create the context\n    validate the expected_result and the created context\n    \"\"\"\n    mocker.patch.object(Client, '_create_engine_and_connect')  # needed in order not to make a connection in tests\n    mocker.patch.object(Client, 'sql_query_execute_request', return_value=(raw1, ['Name']))\n    client = Client('sql_dialect', 'server_url', 'username', 'password', 'port', 'database', \"\")\n    result = command(client, args)\n    assert expected_result == result[1]  # entry context is found in the 2nd place in the result of the command\n<|endoftext|>",
    "repo": "david-burkett/content"
  },
  {
    "prompt": "class ApplicationError(RuntimeError):\n\tcode = 1\n\n\nclass CLIArgumentError(ApplicationError):\n\tcode = 2\n\n\nfn printerror()\n\n\"\"\"Parse arguments for `callmain` API.\n\n>>> parse_callmain(\"PkgName=u-u-i-d:main.function.name\") == {\n...     \"pkgname\": \"PkgName\",\n...     \"pkguuid\": \"u-u-i-d\",\n...     \"main\": \"main.function.name\",\n... }\nTrue\"\"\"\nfn parse_callmain(callmain: str)\n\nfn default_stdio_path(name)\n\nfn default_stdin_path()\n\nfn default_stdout_path()\n\nfn default_stderr_path()\n\nfn compile_request(eval, callmain, callany, adhoccli, julia, project, stdin, stdout, stderr, script, args, usemain, ignorereturn)\n\nfn send_request(request, connection)\n\nfn handle_response(response, print_result)\n\nfn jlcli()\n\nfn default_connection()\n\nclass CustomFormatter(argparse.RawDescriptionHelpFormatter, argparse.ArgumentDefaultsHelpFormatter):\n\n\nfn parse_args(args)\n\nfn main(args)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import os\nimport shlex\n\nimport pytest\n\nfrom jlcli import CLIArgumentError, compile_request, handle_response, parse_args\n\n\ndef compile_request_from_cliargs(cliargs):\n    request, kwargs = compile_request(**vars(parse_args(shlex.split(cliargs))))\n    assert request[\"jsonrpc\"] == \"2.0\"\n    return request\n\n\ndef test_require_one_command():\n    with pytest.raises(CLIArgumentError):\n        compile_request_from_cliargs(\"\")\n\n\ndef test_require_only_one_command():\n    with pytest.raises(SystemExit):\n        compile_request_from_cliargs(\n            \"\"\"--callany='Statistics=10745b16-79ce-11e8-11f9-7d13ad32a3b2:std'\"\"\"\n            \"\"\" --eval=CODE --\"\"\"\n        )\n\n\ndef test_request_eval():\n    request = compile_request_from_cliargs(\"--eval=CODE a b c\")\n    assert request[\"params\"][\"code\"] == \"CODE\"\n    assert request[\"params\"][\"args\"] == [\"a\", \"b\", \"c\"]\n\n\ndef test_request_callmain():\n    request = compile_request_from_cliargs(\n        \"--callmain=PkgName=u-u-i-d:main.function.name a b c\"\n    )\n    assert request[\"params\"][\"pkgname\"] == \"PkgName\"\n    assert request[\"params\"][\"pkguuid\"] == \"u-u-i-d\"\n    assert request[\"params\"][\"main\"] == \"main.function.name\"\n    assert request[\"params\"][\"args\"] == [\"a\", \"b\", \"c\"]\n\n\ndef test_request_callany():\n    request = compile_request_from_cliargs(\n        \"\"\"--callany='Statistics=10745b16-79ce-11e8-11f9-7d13ad32a3b2:std'\"\"\"\n        \"\"\" '{\"args\":[[1,2,3]]}'\"\"\"\n    )\n    assert request[\"params\"][\"pkgname\"] == \"Statistics\"\n    assert request[\"params\"][\"pkguuid\"] == \"10745b16-79ce-11e8-11f9-7d13ad32a3b2\"\n    assert request[\"params\"][\"main\"] == \"std\"\n    assert request[\"params\"][\"args\"] == [[1, 2, 3]]\n\n\ndef test_request_adhoccli():\n    request = compile_request_from_cliargs(\n        \"\"\"--print-result --adhoccli='Unicode=4ec0a83e-493e-50e2-b9ac-8f72acf5a8f5:normalize'\"\"\"\n        \"\"\" -- JuLiA --casefold\"\"\"\n    )\n    assert request[\"params\"][\"pkgname\"] == \"Unicode\"\n    assert request[\"params\"][\"pkguuid\"] == \"4ec0a83e-493e-50e2-b9ac-8f72acf5a8f5\"\n    assert request[\"params\"][\"main\"] == \"normalize\"\n    assert request[\"params\"][\"args\"] == [\"JuLiA\", \"--casefold\"]\n\n\ndef test_request_script():\n    scriptfullpath = os.devnull\n    request = compile_request_from_cliargs(scriptfullpath + \" -h\")\n    assert request[\"params\"][\"script\"] == scriptfullpath\n    assert request[\"params\"][\"args\"] == [\"-h\"]\n\n\ndef handle(**response):\n    response[\"id\"] = 0\n    return handle_response(response, print_result=True)\n\n\ndef test_response_stdout(capsys):\n    err = handle(result={\"stdout\": \"hello\"})\n    captured = capsys.readouterr()\n    assert captured.out == \"hello\"\n    assert not err\n\n\ndef test_response_result(capsys):\n    err = handle(result={\"result\": {\"some\": \"result\"}})\n    captured = capsys.readouterr()\n    assert \"some\" in captured.out\n    assert \"result\" in captured.out\n    assert not err\n\n\ndef test_response_error(capsys):\n    err = handle(error={\"message\": \"hello\"})\n    captured = capsys.readouterr()\n    assert captured.err == \"hello\\n\"\n    assert err\n\n\ndef test_response_invalid(capsys):\n    err = handle()\n    captured = capsys.readouterr()\n    assert \"Invalid response:\" in captured.err\n    assert err\n<|endoftext|>",
    "repo": "tkf/JuliaCLI.jl"
  },
  {
    "prompt": "\"\"\"A variant of `functools.singledispatch` which dispatches on type of the second argument.\"\"\"\nfn seconddispatch(func)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"\nThese tests are borrowed from CPython\n\"\"\"\nimport sys\nimport unittest\nimport collections\n\nimport pytest\n\nfrom sunpy.util.functools import seconddispatch\n\n\nclass TestSingleDispatch(unittest.TestCase):\n    def test_simple_overloads(self):\n\n        @seconddispatch\n        def g(dummy, obj):\n            return \"base\"\n\n        def g_int(dummy, i):\n            return \"integer\"\n        g.register(int, g_int)\n        self.assertEqual(g(None, \"str\"), \"base\")\n        self.assertEqual(g(None, 1), \"integer\")\n        self.assertEqual(g(None, [1,2,3]), \"base\")\n\n    def test_mro(self):\n\n        @seconddispatch\n        def g(obj):\n            return \"base\"\n\n        class A:\n            pass\n\n        class C(A):\n            pass\n\n        class B(A):\n            pass\n\n        class D(C, B):\n            pass\n\n        def g_A(dummy, a):\n            return \"A\"\n\n        def g_B(dummy, b):\n            return \"B\"\n        g.register(A, g_A)\n        g.register(B, g_B)\n        self.assertEqual(g(None, A()), \"A\")\n        self.assertEqual(g(None, B()), \"B\")\n        self.assertEqual(g(None, C()), \"A\")\n        self.assertEqual(g(None, D()), \"B\")\n\n    def test_register_decorator(self):\n\n        @seconddispatch\n        def g(dummy, obj):\n            return \"base\"\n\n        @g.register(int)\n        def g_int(dummy, i):\n            return \"int %s\" % (i,)\n        self.assertEqual(g(None, \"\"), \"base\")\n        self.assertEqual(g(None, 12), \"int 12\")\n        self.assertIs(g.dispatch(int), g_int)\n        self.assertIs(g.dispatch(object), g.dispatch(str))\n        # Note: in the assert above this is not g.\n        # @singledispatch returns the wrapper.\n\n    def test_wrapping_attributes(self):\n        @seconddispatch\n        def g(dummy, obj):\n            \"Simple test\"\n            return \"Test\"\n        self.assertEqual(g.__name__, \"g\")\n        if sys.flags.optimize < 2:\n            self.assertEqual(g.__doc__, \"Simple test\")\n\n    @pytest.mark.skipif(sys.version_info < (3, 7), reason=\"requires python3.7 or higher\")\n    def test_annotations(self):\n        @seconddispatch\n        def i(dummy, arg):\n            return \"base\"\n\n        @i.register\n        def _(dummy, arg: collections.abc.Mapping):\n            return \"mapping\"\n\n        @i.register\n        def _(dummy, arg: \"collections.abc.Sequence\"):\n            return \"sequence\"\n        self.assertEqual(i(None, None), \"base\")\n        self.assertEqual(i(None, {\"a\": 1}), \"mapping\")\n        self.assertEqual(i(None, [1, 2, 3]), \"sequence\")\n        self.assertEqual(i(None, (1, 2, 3)), \"sequence\")\n        self.assertEqual(i(None, \"str\"), \"sequence\")\n\n        # Registering classes as callables doesn't work with annotations,\n        # you need to pass the type explicitly.\n        @i.register(str)\n        class _:\n            def __init__(self, dummy, arg):\n                self.arg = arg\n\n            def __eq__(self, other):\n                return self.arg == other\n        self.assertEqual(i(None, \"str\"), \"str\")\n<|endoftext|>",
    "repo": "Cubostar/sunpy"
  },
  {
    "prompt": "fn make_tempfile(mode)\n\nfn make_tempdir()\n\n\"\"\"Create Doc object from given vocab, words and annotations.\"\"\"\nfn get_doc(vocab, words, pos, heads, deps, tags, ents)\n\n\"\"\"Perform a series of pre-specified transitions, to put the parser in a\ndesired state.\"\"\"\nfn apply_transition_sequence(parser, doc, sequence)\n\n\"\"\"Add list of vector tuples to given vocab. All vectors need to have the\nsame length. Format: [(\"text\", [1, 2, 3])]\"\"\"\nfn add_vecs_to_vocab(vocab, vectors)\n\n\"\"\"Get cosine for two given vectors\"\"\"\nfn get_cosine(vec1, vec2)\n\n\"\"\"Compare two Doc objects and assert that they're equal. Tests for tokens,\ntags, dependencies and entities.\"\"\"\nfn assert_docs_equal(doc1, doc2)\n\n\"\"\"Assert that two packed msgpack messages are equal.\"\"\"\nfn assert_packed_msg_equal(b1, b2)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# coding: utf-8\nfrom __future__ import unicode_literals\n\nimport pytest\nfrom spacy import displacy\nfrom spacy.displacy.render import DependencyRenderer\nfrom spacy.tokens import Span\nfrom spacy.lang.fa import Persian\n\nfrom .util import get_doc\n\n\ndef test_displacy_parse_ents(en_vocab):\n    \"\"\"Test that named entities on a Doc are converted into displaCy's format.\"\"\"\n    doc = get_doc(en_vocab, words=[\"But\", \"Google\", \"is\", \"starting\", \"from\", \"behind\"])\n    doc.ents = [Span(doc, 1, 2, label=doc.vocab.strings[\"ORG\"])]\n    ents = displacy.parse_ents(doc)\n    assert isinstance(ents, dict)\n    assert ents[\"text\"] == \"But Google is starting from behind \"\n    assert ents[\"ents\"] == [{\"start\": 4, \"end\": 10, \"label\": \"ORG\"}]\n\n\ndef test_displacy_parse_deps(en_vocab):\n    \"\"\"Test that deps and tags on a Doc are converted into displaCy's format.\"\"\"\n    words = [\"This\", \"is\", \"a\", \"sentence\"]\n    heads = [1, 0, 1, -2]\n    pos = [\"DET\", \"VERB\", \"DET\", \"NOUN\"]\n    tags = [\"DT\", \"VBZ\", \"DT\", \"NN\"]\n    deps = [\"nsubj\", \"ROOT\", \"det\", \"attr\"]\n    doc = get_doc(en_vocab, words=words, heads=heads, pos=pos, tags=tags, deps=deps)\n    deps = displacy.parse_deps(doc)\n    assert isinstance(deps, dict)\n    assert deps[\"words\"] == [\n        {\"lemma\": None, \"text\": \"This\", \"tag\": \"DET\"},\n        {\"lemma\": None, \"text\": \"is\", \"tag\": \"AUX\"},\n        {\"lemma\": None, \"text\": \"a\", \"tag\": \"DET\"},\n        {\"lemma\": None, \"text\": \"sentence\", \"tag\": \"NOUN\"},\n    ]\n    assert deps[\"arcs\"] == [\n        {\"start\": 0, \"end\": 1, \"label\": \"nsubj\", \"dir\": \"left\"},\n        {\"start\": 2, \"end\": 3, \"label\": \"det\", \"dir\": \"left\"},\n        {\"start\": 1, \"end\": 3, \"label\": \"attr\", \"dir\": \"right\"},\n    ]\n\n\ndef test_displacy_invalid_arcs():\n    renderer = DependencyRenderer()\n    words = [{\"text\": \"This\", \"tag\": \"DET\"}, {\"text\": \"is\", \"tag\": \"VERB\"}]\n    arcs = [\n        {\"start\": 0, \"end\": 1, \"label\": \"nsubj\", \"dir\": \"left\"},\n        {\"start\": -1, \"end\": 2, \"label\": \"det\", \"dir\": \"left\"},\n    ]\n    with pytest.raises(ValueError):\n        renderer.render([{\"words\": words, \"arcs\": arcs}])\n\n\ndef test_displacy_spans(en_vocab):\n    \"\"\"Test that displaCy can render Spans.\"\"\"\n    doc = get_doc(en_vocab, words=[\"But\", \"Google\", \"is\", \"starting\", \"from\", \"behind\"])\n    doc.ents = [Span(doc, 1, 2, label=doc.vocab.strings[\"ORG\"])]\n    html = displacy.render(doc[1:4], style=\"ent\")\n    assert html.startswith(\"<div\")\n\n\ndef test_displacy_raises_for_wrong_type(en_vocab):\n    with pytest.raises(ValueError):\n        displacy.render(\"hello world\")\n\n\ndef test_displacy_rtl():\n    # Source: http://www.sobhe.ir/hazm/ \u2013 is this correct?\n    words = [\"\u0645\u0627\", \"\u0628\u0633\u06cc\u0627\u0631\", \"\u06a9\u062a\u0627\u0628\", \"\u0645\u06cc\\u200c\u062e\u0648\u0627\u0646\u06cc\u0645\"]\n    # These are (likely) wrong, but it's just for testing\n    pos = [\"PRO\", \"ADV\", \"N_PL\", \"V_SUB\"]  # needs to match lang.fa.tag_map\n    deps = [\"foo\", \"bar\", \"foo\", \"baz\"]\n    heads = [1, 0, 1, -2]\n    nlp = Persian()\n    doc = get_doc(nlp.vocab, words=words, pos=pos, tags=pos, heads=heads, deps=deps)\n    doc.ents = [Span(doc, 1, 3, label=\"TEST\")]\n    html = displacy.render(doc, page=True, style=\"dep\")\n    assert \"direction: rtl\" in html\n    assert 'direction=\"rtl\"' in html\n    assert 'lang=\"{}\"'.format(nlp.lang) in html\n    html = displacy.render(doc, page=True, style=\"ent\")\n    assert \"direction: rtl\" in html\n    assert 'lang=\"{}\"'.format(nlp.lang) in html\n\n\ndef test_displacy_render_wrapper(en_vocab):\n    \"\"\"Test that displaCy accepts custom rendering wrapper.\"\"\"\n\n    def wrapper(html):\n        return \"TEST\" + html + \"TEST\"\n\n    displacy.set_render_wrapper(wrapper)\n    doc = get_doc(en_vocab, words=[\"But\", \"Google\", \"is\", \"starting\", \"from\", \"behind\"])\n    doc.ents = [Span(doc, 1, 2, label=doc.vocab.strings[\"ORG\"])]\n    html = displacy.render(doc, style=\"ent\")\n    assert html.startswith(\"TEST<div\")\n    assert html.endswith(\"/div>TEST\")\n    # Restore\n    displacy.set_render_wrapper(lambda html: html)\n<|endoftext|>",
    "repo": "Tsadoq/spaCy"
  },
  {
    "prompt": "fn get_json(file_name: str)\n\nfn get_file(file_name: str)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nimport typing\nfrom httpx import Response\n\nif typing.TYPE_CHECKING:\n    import sys\n    sys.path.append('./src/plugins')\n    import nonebot_bison\n\nfrom .utils import get_json\n\n@pytest.fixture(scope='module')\ndef bing_dy_list():\n    return get_json('bilibili_bing_list.json')['data']['cards']\n\n@pytest.fixture\ndef bilibili(plugin_module: 'nonebot_bison'):\n    return plugin_module.platform.platform_manager['bilibili']\n\n@pytest.mark.asyncio\nasync def test_video_forward(bilibili, bing_dy_list):\n    post = await bilibili.parse(bing_dy_list[1]) \n    assert(post.text == '\u7b54\u6848\u63ed\u6653\uff1a\u5bbf\u820d\uff01\u6765\u770b\u770b\u6295\u7968\u7ed3\u679c\\nhttps://t.bilibili.com/568093580488553786\\n--------------\\n#\u53ef\u9732\u5e0c\u5c14\u7684\u79d8\u5bc6\u6863\u6848# \\n11\uff1a\u6765\u5bbf\u820d\u4f11\u606f\u4e00\u4e0b\u5427 \\n\u6863\u6848\u6765\u6e90\uff1alambda:\\\\\u7f57\u5fb7\u5c9b\u5185\u52a1\\\\\u79d8\u5bc6\u6863\u6848 \\n\u53d1\u5e03\u65f6\u95f4\uff1a9/12 1:00 P.M. \\n\u6863\u6848\u7c7b\u578b\uff1a\u53ef\u89c1 \\n\u6863\u6848\u63cf\u8ff0\uff1a\u4eca\u5929\u8bf7\u4e86\u75c5\u5047\u5728\u5bbf\u820d\u4f11\u606f\u3002\u5f88\u8212\u9002\u3002 \\n\u63d0\u4f9b\u8005\uff1a\u8d6b\u9ed8')\n\n@pytest.mark.asyncio\nasync def test_article_forward(bilibili, bing_dy_list):\n    post = await bilibili.parse(bing_dy_list[4]) \n    assert(post.text == '#\u660e\u65e5\u65b9\u821f##\u997c\u5b66\u5927\u53a6#\\n9.11\u4e13\u680f\u66f4\u65b0\u5b8c\u6bd5\uff0c\u8fd9\u8fd8\u584c\u4e86\u5b9e\u5c5e\u6ca1\u8ddf\u65b0\u8fd0\u8425\u5bf9\u4e0a\\n\u540e\u8fb9\u9664\u4e86\u5468\u65e5\u53d1\u997c\u548cPV\u6ca1\u63d0\u53ca\u7684\u4e2d\u6587\u8bed\u97f3\uff0c\u7a33\u4e86\\n\u522b\u5fd8\u4e86\u6765\u53c2\u52a0#\u53ef\u9732\u5e0c\u5c14\u7684\u79d8\u5bc6\u6863\u6848#\u7684\u4e3b\u9898\u6295\u7968\\nhttps://t.bilibili.com/568093580488553786?tab=2' +\n        '\\n--------------\\n' +\n        '\u3010\u660e\u65e5\u65b9\u821f\u3011\u997c\u5b66\u5927\u53a6#12~14\uff08\u98ce\u66b4\u77ad\u671b&\u739b\u8389\u5a05\u00b7\u4e34\u5149&\u7ea2\u677e\u6797&\u611f\u8c22\u5e86\u5178\uff099.11\u66f4\u65b0 \u66f4\u65b0\u8bb0\u5f5509.11\u66f4\u65b0\uff1a\u8986\u76d609.10\u66f4\u65b0\uff1b\u4ee5\u53ca\u6392\u671f\u66f4\u65b0\uff0c\u731c\u6d4b\u5468\u4e00\u5468\u4e94\u5f00\u6d3b\u52a809.10\u66f4\u65b0\uff1a\u4ee5\u5468\u4e94\u5f00\u6d3b\u52a8\u4e3a\u5e95\uff0cPV/\u516c\u544a\u8c03\u6574\u4f4d\u7f6e\uff0c\u6574\u4f53\u7ed3\u6784\u66f4\u65b009.08\u66f4\u65b0\uff1a\u997c\u5b66\u5927\u53a6#12\u66f4\u65b0\uff0c\u65b0\u589e\u4e00\u4ef6\u516d\u661f\u5546\u5e97\u670d\u9970\uff08\u5468\u65e5\u53d1\u997c\uff0909.06\u66f4\u65b0\uff1a\u997c\u5b66\u5927\u53a6\u6574\u680b\u6574\u680b\u7ffb\u65b0\uff0c\u6539\u4e3a9.16\u5f00\u4e3b\u7ebf\uff08\u56db\u65e5\u65e0\u997c\uff01\uff0909.05\u51cc\u6668\u66f4\u65b0\uff1a10.13\u540e\u7684\u6392\u671f\uff08\u4e24\u65e5\u65e0\u997c\uff0c\u9e70\u89d2\u80cc\u523a\uff0c\u5fc3\u72e0\u624b\u8fa3\uff09\u524d\u8a00\u611f\u8c22\u696a\u7b71\u7948\u307a\u7684\u52a8\u6001-\u54d4\u54e9\u54d4\u54e9 (bilibili.com) \u5bf9\u997c\u5b66\u7684\u8d21\u732e\uff01\u540e\u7eed\u6392\u671f\uff1a9.17\u3010\u98ce\u66b4\u77ad\u671b\u3011\u300110.01\u3010\u739b\u8389\u5a05\u00b7\u4e34\u5149\u3011\u590d\u523b\u300110.1')\n\n@pytest.mark.asyncio\nasync def test_dynamic_forward(bilibili, bing_dy_list):\n    post = await bilibili.parse(bing_dy_list[5])\n    assert(post.text == '\u997c\u7ec4\u4e3b\u7ebf\u997c\u5b66\u9884\u6d4b\u2014\u20149.11\u7248\\n\u2460\u4eca\u65e5\u7ed3\u679c\\n9.11 \u6bbf\u5802\u4e0a\u7684\u6e38\u79bd-\u661f\u6781(x\uff0c\u65b0\u8fd0\u8425\u5b9e\u9524\u4e86)\\n\u2461\u540e\u7eed\u9884\u6d4b\\n9.12\u00a0#\u7f57\u5fb7\u5c9b\u76f8\u7c3f#+#\u53ef\u9732\u5e0c\u5c14\u7684\u79d8\u5bc6\u6863\u6848#11\u8bdd\\n9.13 \u516d\u661f\u5148\u950b(\u6267\u65d7\u624b)\u5e72\u5458-\u7434\u67f3\\n9.14 \u5ba3\u4f20\u7b56\u7565-\u7a7a\u5f26+\u5bb6\u5177\\n9.15\u00a0\u8f6e\u6362\u6c60\uff08+\u4e2d\u6587\u8bed\u97f3\u524d\u77bb\uff09\\n9.16 \u505c\u673a\\n9.17\u00a0#\u7f57\u5fb7\u5c9b\u95f2\u901b\u90e8#+\u65b0\u516d\u661fEP+EP09\u00b7\u98ce\u66b4\u77ad\u671b\u5f00\u542f\\n9.19 #\u7f57\u5fb7\u5c9b\u76f8\u7c3f#' +\n            '\\n--------------\\n' +\n            '#\u660e\u65e5\u65b9\u821f#\\n\u3010\u65b0\u589e\u670d\u9970\u3011\\n//\u6bbf\u5802\u4e0a\u7684\u6e38\u79bd - \u661f\u6781\\n\u585e\u58ec\u5531\u7247\u5076\u50cf\u4f01\u5212\u300a\u95ea\u8000\u9636\u68af\u300b\u7279\u4f9b\u670d\u9970/\u6bbf\u5802\u4e0a\u7684\u6e38\u79bd\u3002\u661f\u6781\u81ea\u8d39\u53c2\u52a0\u4e86\u8fd9\u9879\u4f01\u5212\uff0c\u5c1d\u8bd5\u7740\u7528\u5927\u4f17\u80fd\u63a5\u53d7\u7684\u65b9\u5f0f\u6f14\u7ece\u5929\u7a7a\u4e4b\u4e0a\u7684\u6545\u4e8b\u3002\\n\\n_____________\\n\u8c26\u900a\u7559\u7ed9\u89c2\u4f17\uff0c\u9a84\u50b2\u53d1\u81ea\u6b4c\u5589\uff0c\u6b64\u591c\uff0c\u552f\u6211\u7480\u74a8\u3002 ')\n<|endoftext|>",
    "repo": "58563528/nonebot-hk-reporter"
  },
  {
    "prompt": "\"\"\"A role for a user.\"\"\"\nclass Role(SurrogatePK, Model):\n\t__tablename__ = 'roles'\n\tname = Column(db.String(80), unique=True, nullable=False)\n\tuser_id = reference_col('users', nullable=True)\n\tuser = relationship('User', backref='roles')\n\t\"\"\"Create instance.\"\"\"\n\tfn __init__(self, name)\n\t\"\"\"Represent instance as a unique string.\"\"\"\n\tfn __repr__(self)\n\n\n\"\"\"A user of the app.\"\"\"\nclass User(UserMixin, SurrogatePK, Model):\n\t__tablename__ = 'users'\n\tusername = Column(db.String(80), unique=True, nullable=False)\n\temail = Column(db.String(80), unique=True, nullable=False)\n\tpassword = Column(db.Binary(128), nullable=True)\n\tcreated_at = Column(db.DateTime, nullable=False, default=dt.datetime.utcnow)\n\tfirst_name = Column(db.String(30), nullable=True)\n\tlast_name = Column(db.String(30), nullable=True)\n\tactive = Column(db.Boolean(), default=False)\n\tis_admin = Column(db.Boolean(), default=False)\n\t\"\"\"Create instance.\"\"\"\n\tfn __init__(self, username, email, password)\n\t\"\"\"Set password.\"\"\"\n\tfn set_password(self, password)\n\t\"\"\"Check password.\"\"\"\n\tfn check_password(self, value)\n\t\"\"\"Full user name.\"\"\"\n\tfn full_name(self)\n\t\"\"\"Represent instance as a unique string.\"\"\"\n\tfn __repr__(self)\n\n\n\"\"\"Base factory.\"\"\"\nclass BaseFactory(SQLAlchemyModelFactory):\n\n\n\"\"\"User factory.\"\"\"\nclass UserFactory(BaseFactory):\n\tusername = Sequence(lambda n: 'user{0}'.format(n))\n\temail = Sequence(lambda n: 'user{0}@example.com'.format(n))\n\tpassword = PostGenerationMethodCall('set_password', 'example')\n\tactive = True\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# -*- coding: utf-8 -*-\n\"\"\"Model unit tests.\"\"\"\nimport datetime as dt\n\nimport pytest\n\nfrom arthistapp.user.models import Role, User\n\nfrom .factories import UserFactory\n\n\n@pytest.mark.usefixtures('db')\nclass TestUser:\n    \"\"\"User tests.\"\"\"\n\n    def test_get_by_id(self):\n        \"\"\"Get user by ID.\"\"\"\n        user = User('foo', 'foo@bar.com')\n        user.save()\n\n        retrieved = User.get_by_id(user.id)\n        assert retrieved == user\n\n    def test_created_at_defaults_to_datetime(self):\n        \"\"\"Test creation date.\"\"\"\n        user = User(username='foo', email='foo@bar.com')\n        user.save()\n        assert bool(user.created_at)\n        assert isinstance(user.created_at, dt.datetime)\n\n    def test_password_is_nullable(self):\n        \"\"\"Test null password.\"\"\"\n        user = User(username='foo', email='foo@bar.com')\n        user.save()\n        assert user.password is None\n\n    def test_factory(self, db):\n        \"\"\"Test user factory.\"\"\"\n        user = UserFactory(password='myprecious')\n        db.session.commit()\n        assert bool(user.username)\n        assert bool(user.email)\n        assert bool(user.created_at)\n        assert user.is_admin is False\n        assert user.active is True\n        assert user.check_password('myprecious')\n\n    def test_check_password(self):\n        \"\"\"Check password.\"\"\"\n        user = User.create(username='foo', email='foo@bar.com',\n                           password='foobarbaz123')\n        assert user.check_password('foobarbaz123') is True\n        assert user.check_password('barfoobaz') is False\n\n    def test_full_name(self):\n        \"\"\"User full name.\"\"\"\n        user = UserFactory(first_name='Foo', last_name='Bar')\n        assert user.full_name == 'Foo Bar'\n\n    def test_roles(self):\n        \"\"\"Add a role to a user.\"\"\"\n        role = Role(name='admin')\n        role.save()\n        user = UserFactory()\n        user.roles.append(role)\n        user.save()\n        assert role in user.roles\n<|endoftext|>",
    "repo": "annielcook/arthistapp"
  },
  {
    "prompt": "fn get_open_port()\n\nclass ParityWeb3ModuleTest(Web3ModuleTest):\n\tfn _check_web3_clientVersion(self, client_version)\n\n\nclass ParityEthModuleTest(EthModuleTest):\n\tfn test_eth_uninstallFilter(self, web3)\n\tfn test_eth_newBlockFilter(self, web3)\n\tfn test_eth_replaceTransaction(self, web3, unlocked_account)\n\tfn test_eth_replaceTransaction_already_mined(self, web3, unlocked_account)\n\tfn test_eth_replaceTransaction_incorrect_nonce(self, web3, unlocked_account)\n\tfn test_eth_replaceTransaction_gas_price_too_low(self, web3, unlocked_account)\n\tfn test_eth_replaceTransaction_gas_price_defaulting_minimum(self, web3, unlocked_account)\n\tfn test_eth_replaceTransaction_gas_price_defaulting_strategy_higher(self, web3, unlocked_account)\n\tfn test_eth_replaceTransaction_gas_price_defaulting_strategy_lower(self, web3, unlocked_account)\n\tfn test_eth_modifyTransaction(self, web3, unlocked_account)\n\n\nclass ParityPersonalModuleTest(PersonalModuleTest):\n\tfn test_personal_importRawKey(self, web3)\n\tfn test_personal_listAccounts(self, web3)\n\tfn test_personal_lockAccount(self, web3, unlocked_account)\n\tfn test_personal_unlockAccount_success(self, web3)\n\tfn test_personal_unlockAccount_failure(self, web3, unlockable_account)\n\tfn test_personal_newAccount(self, web3)\n\tfn test_personal_sendTransaction(self, web3, unlockable_account, unlockable_account_pw)\n\tfn test_personal_sign_and_ecrecover(self, web3, unlockable_account, unlockable_account_pw)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import os\nimport pytest\nimport tempfile\n\nfrom tests.integration.parity.utils import (\n    wait_for_socket,\n)\nfrom web3 import Web3\nfrom web3.utils.module_testing import (\n    NetModuleTest,\n    VersionModuleTest,\n)\n\nfrom .common import (\n    ParityEthModuleTest,\n    ParityPersonalModuleTest,\n    ParityWeb3ModuleTest,\n)\n\n\n@pytest.fixture(scope='module')\ndef ipc_path(datadir):\n    ipc_dir_path = tempfile.mkdtemp()\n    _ipc_path = os.path.join(ipc_dir_path, 'jsonrpc.ipc')\n    yield _ipc_path\n\n    if os.path.exists(_ipc_path):\n        os.remove(_ipc_path)\n\n\n@pytest.fixture(scope=\"module\")\ndef parity_command_arguments(\n    parity_import_blocks_process,\n    parity_binary,\n    ipc_path,\n    datadir,\n    passwordfile,\n    author\n):\n    return (\n        parity_binary,\n        '--chain', os.path.join(datadir, 'chain_config.json'),\n        '--ipc-path', ipc_path,\n        '--base-path', datadir,\n        '--unlock', author,\n        '--password', passwordfile,\n        '--no-jsonrpc',\n        '--no-ws',\n    )\n\n\n@pytest.fixture(scope=\"module\")\ndef parity_import_blocks_command(parity_binary, ipc_path, datadir, passwordfile):\n    return (\n        parity_binary,\n        'import', os.path.join(datadir, 'blocks_export.rlp'),\n        '--chain', os.path.join(datadir, 'chain_config.json'),\n        '--ipc-path', ipc_path,\n        '--base-path', datadir,\n        '--password', passwordfile,\n        '--no-jsonrpc',\n        '--no-ws',\n    )\n\n\n@pytest.fixture(scope=\"module\")  # noqa: F811\ndef web3(parity_process, ipc_path):\n    wait_for_socket(ipc_path)\n    _web3 = Web3(Web3.IPCProvider(ipc_path))\n    return _web3\n\n\nclass TestParityWeb3ModuleTest(ParityWeb3ModuleTest):\n    pass\n\n\nclass TestParityEthModuleTest(ParityEthModuleTest):\n    pass\n\n\nclass TestParityVersionModule(VersionModuleTest):\n    pass\n\n\nclass TestParityNetModule(NetModuleTest):\n    pass\n\n\nclass TestParityPersonalModuleTest(ParityPersonalModuleTest):\n    pass\n<|endoftext|>",
    "repo": "nemaniarjun/web3.py"
  },
  {
    "prompt": "fn generate_point(item_type)\n\nfn create_options(item_type)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nimport fixtures\nfrom source.user_defined_functions.queue_sender.alert_definition import AlertDefinition\n\n@pytest.mark.parametrize(\"item_type\", [\"tiltmeter\"])\ndef test_alert_definitions(item_type, alert_definition):\n    alert = alert_definition\n    alert[\"alert\"][\"description\"] = \"\"\n    assert alert == fixtures.EXPECTED_ALERT\n\n@pytest.fixture(scope=\"function\")\ndef alert_definition(item_type):\n    options = fixtures.create_options(item_type)\n    point = fixtures.generate_point(item_type)\n    definition = AlertDefinition(options)\n    return definition.get_alert(point)\n<|endoftext|>",
    "repo": "stop-it-h2020-project/PublicWarningNotificationSystem"
  },
  {
    "prompt": "\"\"\"GloVe is pre-trained word vector named `Global Vectors for Word Representation`.\n\nReferences:\n\n        [1] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014.\n        GloVe: Global Vectors for Word Representation.\n\nArguments:\n        file_id (str): a str indicates the source of GloVe word vectors. If it is local file,\n                it can be a directory contains 'glove.txt' or just a text file.\n                Default: ``resources://Glove300d``.     A 300d glove is downloaded and cached.\"\"\"\nclass Glove(WordVector):\n\tfn __init__(self, file_id)\n\tfn _load_raw_word2vec(self)\n\t\"\"\"Refer to :meth:`.WordVector.load`.\"\"\"\n\tfn load_matrix(self, n_dims, vocab_list, mean, std, default_embeddings)\n\t\"\"\"Refer to :meth:`.WordVector.load_pretrain_embed`.\"\"\"\n\tfn load_dict(self, vocab_list)\n\n\n\"\"\"Base of all word vector loader\n        \"\"\"\nclass WordVector(LoadClassInterface):\n\tfn __init__(self)\n\t\"\"\"Load pretrained word vector and return a numpy 2-d array. The ith row is the feature\n\tof the ith word in ``vocab_list``. If some feature is not included in pretrained\n\tword vector, it will be initialized by:\n\t\n\t* ``default_embeddings``, if it is not ``None``.\n\t* normal distribution with ``mean`` and ``std``, otherwise.\n\t\n\tParameters:\n\t        n_dims (int): specify the dimension size of word vector. If ``n_dims``\n\t                is bigger than size of pretrained word vector, the rest embedding will be\n\t                initialized by ``default_embeddings`` or a normal distribution.\n\t        vocab_list (list): specify the vocab list used in data loader. If there\n\t                is any word not appeared in pretrained word vector, the embedding will be\n\t                initialized by ``default_embeddings`` or a normal distribution.\n\t        mean: The mean of normal distribution. Default: 0.\n\t        std: The standard deviation of normal distribution. Default: 0.1.\n\t        default_embeddings: The default embeddings, it size should be\n\t                ``[len(vocab_list), ndims]``.\n\t\n\t\n\tReturns:\n\t\n\t        (:class:`numpy.ndarray`): A  2-d array. Size:``[len(vocab_list), n_dims]``.\"\"\"\n\tfn load_matrix(self, n_dims: int, vocab_list: list, mean: None, std: None, default_embeddings: None) -> (\n\t\"\"\"Load word vector and return a dict that maps words to vectors.\n\t\n\tParameters:\n\t        vocab_list (list): specify the vocab list used in data loader. If there\n\t                is any word not appeared in pretrained word vector, the feature will\n\t                not be returned.\n\t\n\tReturns:\n\t\n\t        (dict): maps a word (str) to its pretrained embedding (:class:`numpy.ndarray`)\n\t                where its shape is [ndims].\"\"\"\n\tfn load_dict(self, vocab_list: list) -> (dict)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import copy\n\nimport pytest\nimport numpy as np\nfrom cotk.dataloader import LanguageGeneration, MSCOCO\nfrom cotk.metric import MetricBase\nfrom cotk.wordvector.wordvector import WordVector\nfrom cotk.wordvector.gloves import Glove\nimport logging\n\ndef setup_module():\n\timport random\n\trandom.seed(0)\n\timport numpy as np\n\tnp.random.seed(0)\n\nclass TestWordVector():\n\tdef base_test_init(self, dl):\n\t\tassert isinstance(dl, WordVector)\n\t\twith pytest.raises(Exception):\n\t\t\tWordVector.load(None, None, None)\n\t\tWordVector.get_all_subclasses()\n\t\tassert WordVector.load_class('Glove') == Glove\n\t\tassert WordVector.load_class('not_subclass') == None\n\n\tdef base_test_load(self, dl):\n\t\t# test WordVector.load_matrix\n\t\tvocab_list = ['the', 'of']\n\t\tn_dims = 300\n\t\twordvec = dl.load_matrix(n_dims, vocab_list)\n\t\tassert isinstance(wordvec, np.ndarray)\n\t\tassert wordvec.shape == (len(vocab_list), n_dims)\n\t\tprint(wordvec[1])\n\t\tassert wordvec[1][0] == -0.076947\n\n\n\t\tvocab_list = ['the', 'word_not_exist']\n\t\tn_dims = 300\n\t\twordvec = dl.load_matrix(n_dims, vocab_list)\n\t\tassert isinstance(wordvec, np.ndarray)\n\t\tassert wordvec.shape == (len(vocab_list), n_dims)\n\t\tassert wordvec[0][0] == 0.04656\n\n\t\t# test WordVector.load_dict\n\t\toov_list = ['oov', 'unk', '']\n\t\tfor vocab_list in (['the', 'and'], ['the', 'of'], ['the', 'oov'], ['oov'], ['of', 'unk', ''], []):\n\t\t\twordvec = dl.load_dict(vocab_list)\n\t\t\tassert isinstance(wordvec, dict)\n\t\t\tassert set(wordvec) == set([word for word in vocab_list if word not in oov_list])\n\t\t\tif not wordvec:\n\t\t\t\tcontinue\n\t\t\tvec_shape = next(iter(wordvec.values())).shape\n\t\t\tassert len(vec_shape) == 1\n\t\t\tfor vec in wordvec.values():\n\t\t\t\tassert isinstance(vec, np.ndarray)\n\t\t\t\tassert vec.shape == vec_shape\n\t\t\tif 'the' in wordvec:\n\t\t\t\tassert (wordvec['the'][-2:] == [-0.20989, 0.053913]).all()\n\t\t\tif 'and' in wordvec:\n\t\t\t\tassert (wordvec['and'][-2:] == [0.011807, 0.059703]).all()\n\t\t\tif 'of' in wordvec:\n\t\t\t\tassert (wordvec['of'][-2:] == [-0.29183, -0.046533]).all()\t\t\n\n\n@pytest.fixture\ndef load_glove():\n\tdef _load_glove():\n\t\treturn Glove(\"./tests/wordvector/dummy_glove/300d\")\n\treturn _load_glove\n\nclass TestGlove(TestWordVector):\n\tdef test_init(self, load_glove):\n\t\tsuper().base_test_init(load_glove())\n\n\tdef test_load(self, load_glove):\n\t\tsuper().base_test_load(load_glove())\n<|endoftext|>",
    "repo": "cdjhz/cotk"
  },
  {
    "prompt": "\"\"\"Widget to add a CodeMirror or DjangoCodeMirror instance on a textarea\nTake the same arguments than ``forms.Textarea`` and accepts one\nsuplementary optionnal arguments :\n\nArguments:\n    config_name (string): A Codemirror config name available in\n        ``settings.CODEMIRROR_SETTINGS``. Default is ``empty``.\n    embed_config (bool): If ``True`` will add Codemirror Javascript config\n        just below the input. Default is ``False``.\n\nAttributes:\n    config_name (string): For given config name.\n    template_name (string): Template path for widget rendering.\"\"\"\nclass CodeMirrorWidget(forms.Textarea):\n\tcodemirror_field_js = settings.CODEMIRROR_FIELD_INIT_JS\n\ttemplate_name = 'djangocodemirror/widget.html'\n\tfn __init__(self)\n\t\"\"\"Initialize a manifest instance\n\t\n\tArguments:\n\t    name (string): Config name to register.\n\t\n\tReturns:\n\t    CodeMirrorManifest: A manifest instance where config (from\n\t    ``config_name`` attribute) is registred.\"\"\"\n\tfn init_manifest(self, name: string) -> CodeMirrorManifest\n\t\"\"\"Return CodeMirror HTML template from\n\t``CodeMirrorWidget.codemirror_field_js``.\n\t\n\tReturns:\n\t    string: HTML template string.\"\"\"\n\tfn get_codemirror_field_js(self) -> string\n\t\"\"\"Shortcut to get Codemirror parameters.\n\t\n\tReturns:\n\t    dict: CodeMirror parameters.\"\"\"\n\tfn codemirror_config(self) -> dict\n\t\"\"\"Build CodeMirror HTML script tag which contains CodeMirror init.\n\t\n\tArguments:\n\t    inputid (string): Input id.\n\t\n\tReturns:\n\t    string: HTML for field CodeMirror instance.\"\"\"\n\tfn codemirror_script(self, inputid: string) -> string\n\tfn get_context(self, name, value, attrs)\n\t\"\"\"Returns this Widget rendered as HTML, as a Unicode string.\"\"\"\n\tfn render(self, name, value, attrs, renderer)\n\t\"\"\"Adds necessary files (Js/CSS) to the widget's medias.\n\t\n\tReturns:\n\t    django.forms.Media: Media object with all assets from registered\n\t    config.\"\"\"\n\tfn media(self) -> django.forms.Media\n\n\n\"\"\"CodeMirror widget suited for usage in models admins.\n\nAct like CodeMirrorWidget but allways embed Codemirror Javascript config.\"\"\"\nclass CodeMirrorAdminWidget(CodeMirrorWidget):\n\tfn __init__(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"\nTests against field widgets\n\"\"\"\nimport pytest\n\nfrom djangocodemirror.widgets import CodeMirrorWidget, CodeMirrorAdminWidget\n\nfrom tests.utils import assert_and_parse_html\n\n\ndef test_widget_init_manifest():\n    \"\"\"Check registered config\"\"\"\n    widget = CodeMirrorWidget(config_name=\"empty\")\n\n    config = widget.init_manifest(\"empty\")\n\n    assert config.get_configs() == {\n        'empty': {\n            'modes': [],\n            'addons': [],\n            'themes': [],\n            'css_bundle_name': 'dcm-empty_css',\n            'js_bundle_name': 'dcm-empty_js',\n        }\n    }\n\n\ndef test_widget_basic():\n    \"\"\"Basic widget usage\"\"\"\n    widget = CodeMirrorWidget(config_name=\"basic\")\n\n    rendered = widget.render(\"sample\", \"Hello World!\")\n\n    expected = (\"\"\"<textarea id=\"id_sample\" name=\"sample\" rows=\"10\" cols=\"40\">\"\"\"\n                \"\"\"Hello World!</textarea>\"\"\")\n\n    assert assert_and_parse_html(rendered) == assert_and_parse_html(expected)\n\n\n@pytest.mark.parametrize('name,expected', [\n    (\n        'basic',\n        (\"\"\"<textarea id=\"id_sample\" name=\"sample\" rows=\"10\" cols=\"40\">\"\"\"\n         \"\"\"Hello World!</textarea>\\n\"\"\"\n         \"\"\"<script>var id_sample_codemirror = \"\"\"\n         \"\"\"CodeMirror.fromTextArea(\"\"\"\n         \"\"\"document.getElementById(\"id_sample\"),\"\"\"\n         \"\"\"{\"mode\": \"rst\"});</script>\"\"\"),\n    ),\n    (\n        'with-options',\n        (\"\"\"<textarea id=\"id_sample\" name=\"sample\" rows=\"10\" cols=\"40\">\"\"\"\n         \"\"\"Hello World!</textarea>\\n\"\"\"\n         \"\"\"<script>var id_sample_codemirror = \"\"\"\n         \"\"\"CodeMirror.fromTextArea(\"\"\"\n         \"\"\"document.getElementById(\"id_sample\"),\"\"\"\n         \"\"\"{\"lineNumbers\": true, \"lineWrapping\": true, \"mode\": \"rst\"});\"\"\"\n         \"\"\"</script>\"\"\"),\n    ),\n], ids=[\"basic\", \"with-options\"])\ndef test_admin_widget(name, expected):\n    \"\"\"Admin widget usage\"\"\"\n    widget = CodeMirrorAdminWidget(config_name=name)\n\n    rendered = widget.render(\"sample\", \"Hello World!\")\n\n    assert assert_and_parse_html(rendered) == assert_and_parse_html(expected)\n\n\n@pytest.mark.parametrize('name,expected', [\n    (\n        'basic',\n        (\"\"\"<link href=\"/static/CodeMirror/lib/codemirror.css\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">\\n\"\"\"\n         \"\"\"<script type=\"text/javascript\" src=\"/static/CodeMirror/lib/codemirror.js\"></script>\"\"\")\n    ),\n    (\n        'with-modes',\n        (\"\"\"<link href=\"/static/CodeMirror/lib/codemirror.css\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">\\n\"\"\"\n         \"\"\"<script type=\"text/javascript\" src=\"/static/CodeMirror/lib/codemirror.js\"></script>\\n\"\"\"\n         \"\"\"<script type=\"text/javascript\" src=\"/static/CodeMirror/mode/rst/rst.js\"></script>\\n\"\"\"\n         \"\"\"<script type=\"text/javascript\" src=\"/static/CodeMirror/mode/python/python.js\"></script>\"\"\")\n    ),\n], ids=[\"basic\", \"with-modes\"])\ndef test_widget_medias(name, expected):\n    \"\"\"Get widget medias\"\"\"\n    widget = CodeMirrorWidget(config_name=name)\n    rendered = str(widget.media)\n\n    assert assert_and_parse_html(rendered) == assert_and_parse_html(expected)\n<|endoftext|>",
    "repo": "sveetch/djangocodemirror"
  },
  {
    "prompt": "\"\"\"This is a wrapper over :func:`subprocess.check_output`.\nIt adds all directories containing runnable scripts (the CLI tool and the demos) to PATH to make them invokable.\n\n:param args: The args to run.\n\n:param timeout: Give up waiting if the command could not be completed in this much time and raise TimeoutExpired.\n    No limit by default.\n\n:return: stdout of the command.\n\n>>> run_process('ping', '127.0.0.1', timeout=0.1)\nTraceback (most recent call last):\n...\nsubprocess.TimeoutExpired: ...\"\"\"\nfn run_process() -> str\n\n\"\"\"A wrapper over :func:`run_process` that runs the CLI tool with the specified arguments.\"\"\"\nfn run_cli_tool() -> str\n\n\"\"\"A wrapper over :class:`subprocess.Popen`.\nThis wrapper allows collection of stdout upon completion. At first I tried using a background reader\nthread that was blocked on ``stdout.readlines()``, but that solution ended up being dysfunctional because\nit is fundamentally incompatible with internal stdio buffering in the monitored process which\nwe have absolutely no control over from our local process. Sure, there exist options to suppress buffering,\nsuch as the ``-u`` flag in Python or the PYTHONUNBUFFERED env var, but they would make the test environment\nunnecessarily fragile, so I opted to use a simpler approach where we just run the process until it kicks\nthe bucket and then loot the output from its dead body.\n\n>>> p = BackgroundChildProcess('ping', '127.0.0.1')\n>>> p.wait(0.5)\nTraceback (most recent call last):\n...\nsubprocess.TimeoutExpired: ...\n>>> p.kill()\"\"\"\nclass BackgroundChildProcess:\n\tfn __init__(self)\n\t\"\"\"A convenience factory for running the CLI tool.\"\"\"\n\tfn cli() -> BackgroundChildProcess\n\tfn wait(self, timeout: float, interrupt: typing.Optional[bool]) -> typing.Tuple[(int, str)]\n\tfn kill(self) -> None\n\tfn interrupt(self) -> None\n\tfn pid(self) -> int\n\tfn alive(self) -> bool\n\tfn __del__(self) -> None\n\n\nfn _get_env(environment_variables: typing.Optional[typing.Dict[(str, str)]]) -> typing.Dict[(str, str)]\n\nfn _make_process_args(executable: str) -> typing.Sequence[str]\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "#\n# Copyright (c) 2019 UAVCAN Development Team\n# This software is distributed under the terms of the MIT License.\n# Author: Pavel Kirienko <pavel.kirienko@zubax.com>\n#\n\nimport pytest\nimport subprocess\nfrom ._subprocess import run_cli_tool\n\n\ndef _unittest_trivial() -> None:\n    run_cli_tool('show-transport', timeout=2.0)\n\n    with pytest.raises(subprocess.CalledProcessError):\n        run_cli_tool(timeout=2.0)\n\n    with pytest.raises(subprocess.CalledProcessError):\n        run_cli_tool('invalid-command', timeout=2.0)\n\n    with pytest.raises(subprocess.CalledProcessError):\n        run_cli_tool('dsdl-gen-pkg', 'nonexistent/path', timeout=2.0)\n\n    with pytest.raises(subprocess.CalledProcessError):  # Look-up of a nonexistent package requires large timeout\n        run_cli_tool('pub', 'nonexistent.data.Type.1.0', '{}', '--tr=Loopback(None)', timeout=5.0)\n<|endoftext|>",
    "repo": "sritank/public_regulated_data_types"
  },
  {
    "prompt": "\"\"\"Short term objective intelligibility\nComputes the STOI (See [1][2]) of a denoised signal compared to a clean\nsignal, The output is expected to have a monotonic relation with the\nsubjective speech-intelligibility, where a higher score denotes better\nspeech intelligibility.\n\n# Arguments\n    x (np.ndarray): clean original speech\n    y (np.ndarray): denoised speech\n    fs_sig (int): sampling rate of x and y\n    extended (bool): Boolean, whether to use the extended STOI described in [3]\n\n# Returns\n    float: Short time objective intelligibility measure between clean and\n    denoised speech\n\n# Raises\n    AssertionError : if x and y have different lengths\n\n# Reference\n    [1] C.H.Taal, R.C.Hendriks, R.Heusdens, J.Jensen 'A Short-Time\n        Objective Intelligibility Measure for Time-Frequency Weighted Noisy\n        Speech', ICASSP 2010, Texas, Dallas.\n    [2] C.H.Taal, R.C.Hendriks, R.Heusdens, J.Jensen 'An Algorithm for\n        Intelligibility Prediction of Time-Frequency Weighted Noisy Speech',\n        IEEE Transactions on Audio, Speech, and Language Processing, 2011.\n    [3] Jesper Jensen and Cees H. Taal, 'An Algorithm for Predicting the\n        Intelligibility of Speech Masked by Modulated Noise Maskers',\n        IEEE Transactions on Audio, Speech and Language Processing, 2016.\"\"\"\nfn stoi(x, y, fs_sig, extended)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nimport matlab.engine\nimport numpy as np\nimport scipy\nfrom numpy.testing import assert_allclose\nfrom pystoi.stoi import N_FRAME, NFFT, FS\n\nATOL = 1e-5\n\neng = matlab.engine.start_matlab()\neng.cd('matlab/')\n\n\ndef test_hanning():\n    \"\"\" Compare scipy and Matlab hanning window.\n        Matlab returns a N+2 size window without first and last samples\"\"\"\n    hanning = scipy.hanning(N_FRAME+2)[1:-1]\n    hanning_m = eng.hanning(float(N_FRAME))\n    hanning_m = np.array(hanning_m._data)\n    assert_allclose(hanning, hanning_m, atol=ATOL)\n\n\ndef test_fft():\n    x = np.random.randn(N_FRAME, )\n    x_m = matlab.double(list(x))\n    fft_m = eng.fft(x_m, NFFT)\n    fft_m = np.array(fft_m).transpose()\n    fft_m = fft_m[0:NFFT//2+1, 0]\n    fft = np.fft.rfft(x, n=NFFT)\n    assert_allclose(fft, fft_m, atol=ATOL)\n\n\ndef test_resampy():\n    \"\"\" Compare matlab and librosa resample : FAILING \"\"\"\n    from resampy import resample\n    from pystoi.stoi import FS\n    import matlab_wrapper\n    matlab = matlab_wrapper.MatlabSession()\n    matlab.put('FS', float(FS))\n    RTOL = 1e-4\n\n    for fs in [8000, 11025, 16000, 22050, 32000, 44100, 48000]:\n        x = np.random.randn(2*fs,)\n        x_r = resample(x, fs, FS)\n        matlab.put('x', x)\n        matlab.put('fs', float(fs))\n        matlab.eval('x_r = resample(x, FS, fs)')\n        assert_allclose(x_r, matlab.get('x_r'), atol=ATOL, rtol=RTOL)\n\n\ndef test_nnresample():\n    \"\"\" Compare matlab and nnresample resample : FAILING \"\"\"\n    from nnresample import resample\n    from pystoi.stoi import FS\n    import matlab_wrapper\n    matlab = matlab_wrapper.MatlabSession()\n    matlab.put('FS', float(FS))\n    RTOL = 1e-4\n\n    for fs in [8000, 11025, 16000, 22050, 32000, 44100, 48000]:\n        x = np.random.randn(2*fs,)\n        x_r = resample(x, FS, fs)\n        matlab.put('x', x)\n        matlab.put('fs', float(fs))\n        matlab.eval('x_r = resample(x, FS, fs)')\n        assert_allclose(x_r, matlab.get('x_r'), atol=ATOL, rtol=RTOL)\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n<|endoftext|>",
    "repo": "giamic/pystoi"
  },
  {
    "prompt": "\"\"\"An exception denoting an error during a native call.\"\"\"\nclass NativeContractException(Exception):\n\n\n\"\"\":param data:\n:return:\"\"\"\nfn ecrecover(data: List[int]) -> List[int]\n\n\"\"\":param data:\n:return:\"\"\"\nfn sha256(data: List[int]) -> List[int]\n\n\"\"\":param data:\n:return:\"\"\"\nfn ripemd160(data: List[int]) -> List[int]\n\n\"\"\":param data:\n:return:\"\"\"\nfn identity(data: List[int]) -> List[int]\n\n\"\"\"TODO: Some symbolic parts can be handled here\nModular Exponentiation\n:param data: Data with <length_of_BASE> <length_of_EXPONENT> <length_of_MODULUS> <BASE> <EXPONENT> <MODULUS>\n:return: modular exponentiation\"\"\"\nfn mod_exp(data: List[int]) -> List[int]\n\nfn ec_add(data: List[int]) -> List[int]\n\nfn ec_mul(data: List[int]) -> List[int]\n\nfn ec_pair(data: List[int]) -> List[int]\n\n\"\"\"Takes integer address 1, 2, 3, 4.\n\n:param address:\n:param data:\n:return:\"\"\"\nfn native_contracts(address: int, data: BaseCalldata) -> List[int]\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom eth_utils import decode_hex\nfrom mythril.laser.ethereum.natives import mod_exp\nfrom ethereum.utils import big_endian_to_int\n\n\nEIP198_VECTOR_A = decode_hex(\n    \"0000000000000000000000000000000000000000000000000000000000000001\"\n    \"0000000000000000000000000000000000000000000000000000000000000020\"\n    \"0000000000000000000000000000000000000000000000000000000000000020\"\n    \"03\"\n    \"fffffffffffffffffffffffffffffffffffffffffffffffffffffffefffffc2e\"\n    \"fffffffffffffffffffffffffffffffffffffffffffffffffffffffefffffc2f\"\n)\n\nEIP198_VECTOR_B = decode_hex(\n    \"0000000000000000000000000000000000000000000000000000000000000000\"\n    \"0000000000000000000000000000000000000000000000000000000000000020\"\n    \"0000000000000000000000000000000000000000000000000000000000000020\"\n    \"fffffffffffffffffffffffffffffffffffffffffffffffffffffffefffffc2e\"\n    \"fffffffffffffffffffffffffffffffffffffffffffffffffffffffefffffc2f\"\n)\n\nEIP198_VECTOR_C = decode_hex(\n    \"0000000000000000000000000000000000000000000000000000000000000001\"\n    \"0000000000000000000000000000000000000000000000000000000000000002\"\n    \"0000000000000000000000000000000000000000000000000000000000000020\"\n    \"03\"\n    \"ffff\"\n    \"8000000000000000000000000000000000000000000000000000000000000000\"\n    \"07\"\n)\n\nEIP198_VECTOR_D = decode_hex(\n    \"0000000000000000000000000000000000000000000000000000000000000001\"\n    \"0000000000000000000000000000000000000000000000000000000000000002\"\n    \"0000000000000000000000000000000000000000000000000000000000000020\"\n    \"03\"\n    \"ffff\"\n    \"80\"\n)\n\n\n@pytest.mark.parametrize(\n    \"data,expected\",\n    (\n        (EIP198_VECTOR_A, 1),\n        (EIP198_VECTOR_B, 0),\n        (\n            EIP198_VECTOR_C,\n            26689440342447178617115869845918039756797228267049433585260346420242739014315,\n        ),\n        (\n            EIP198_VECTOR_D,\n            26689440342447178617115869845918039756797228267049433585260346420242739014315,\n        ),\n    ),\n)\ndef test_modexp_result(data, expected):\n    actual = mod_exp(data)\n    assert big_endian_to_int(actual) == expected\n<|endoftext|>",
    "repo": "ncodebucks/mythril"
  },
  {
    "prompt": "fn test_new_job(db)\n\n\"\"\"Test that passing a release's id will cause the relevant release to have\na job log attached to it\"\"\"\nfn test_related_models_by_pk(db)\n\n\"\"\"Test that passing an instance of the model directly will cause a job log\nto be attached to it\"\"\"\nfn test_related_models_by_instance(db)\n\n\"\"\"Test that models that do not have a job_log field do not have job logs\nattached to them.\"\"\"\nfn test_related_models_no_property(db, mocker)\n\n\"\"\"Test that no relations are formed if the specified argument cant' be found\"\"\"\nfn test_related_models_arg_not_found(db, mocker)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import os\nimport pytest\nfrom creator.decorators import task\nfrom creator.jobs.models import Job, JobLog\nfrom creator.studies.models import Study\nfrom creator.studies.factories import StudyFactory\nfrom creator.releases.models import Release\nfrom creator.releases.factories import ReleaseFactory, ReleaseTaskFactory\n\n\ndef test_new_job(db):\n    @task(\"myjob\")\n    def my_job():\n        pass\n\n    my_job()\n\n    assert Job.objects.count() == 1\n    assert JobLog.objects.count() == 1\n    assert Job.objects.first().scheduled is False\n    assert Job.objects.first().name == \"myjob\"\n    assert os.path.exists(JobLog.objects.first().log_file.path)\n\n\ndef test_related_models_by_pk(db):\n    \"\"\"\n    Test that passing a release's id will cause the relevant release to have\n    a job log attached to it\n    \"\"\"\n\n    @task(\"myjob\", related_models={Release: \"release\"})\n    def my_job(release):\n        pass\n\n    release = ReleaseFactory()\n\n    my_job(release=release.pk)\n\n    release.refresh_from_db()\n    assert Job.objects.count() == 1\n    assert JobLog.objects.count() == 1\n    assert Job.objects.first().scheduled is False\n    assert Job.objects.first().name == \"myjob\"\n    assert release.job_log == JobLog.objects.first()\n\n\ndef test_related_models_by_instance(db):\n    \"\"\"\n    Test that passing an instance of the model directly will cause a job log\n    to be attached to it\n    \"\"\"\n\n    @task(\"myjob\", related_models={Release: \"release\"})\n    def my_job(release):\n        pass\n\n    release = ReleaseFactory()\n\n    my_job(release=release)\n\n    release.refresh_from_db()\n    assert Job.objects.count() == 1\n    assert JobLog.objects.count() == 1\n    assert Job.objects.first().scheduled is False\n    assert Job.objects.first().name == \"myjob\"\n    assert release.job_log == JobLog.objects.first()\n\n\ndef test_related_models_no_property(db, mocker):\n    \"\"\"\n    Test that models that do not have a job_log field do not have job logs\n    attached to them.\n    \"\"\"\n\n    @task(\"myjob\", related_models={Study: \"study\"})\n    def my_job(study):\n        pass\n\n    study = StudyFactory()\n    mock = mocker.patch(\"creator.decorators.logger.warning\")\n\n    my_job(study=study)\n\n    assert mock.call_count == 1\n    assert \"must have a\" in mock.call_args_list[0].args[0]\n\n\ndef test_related_models_arg_not_found(db, mocker):\n    \"\"\"\n    Test that no relations are formed if the specified argument cant' be found\n    \"\"\"\n\n    @task(\"myjob\", related_models={Release: \"release_id\"})\n    def my_job(release):\n        pass\n\n    mock = mocker.patch(\"creator.decorators.logger.warning\")\n\n    release = ReleaseFactory()\n\n    my_job(release=release)\n\n    assert mock.call_count == 1\n    assert \"Could not find\" in mock.call_args_list[0].args[0]\n<|endoftext|>",
    "repo": "kids-first/kf-api-study-creator"
  },
  {
    "prompt": "\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "#from server import mycrt\nimport pytest\nimport unittest\nimport requests\nimport json\nfrom datetime import datetime\nimport boto3\nfrom .context import *\n\n#TOTAL NUMBER OF FUNCTIONS FROM REPLAY FILE: 15\n#TOTAL NUMBER OF FUNCTIONS TESTED: 1\n\n\n\"\"\"\nif __name__ == '__main__':\n\n    if __package__ is None:\n        import sys\n        from os import path\n        sys.path.append( path.dirname( path.dirname( path.abspath(__file__) ) ) )\n        from server.mycrt import *\n    else:\n        from ..server.mycrt import *\n\"\"\"\n#mock class for the RDS Client\nclass FakeRDSClient():\n    db_id = \"blackfoot-rds\"\n    def describe_db_instances(DBInstanceIdentifier):\n        return {\n            'DBInstances':[{\n                'Endpoint': [\n                    {'Address': 'test'}\n                ]\n            }]\n        }\n\nclass TestFlaskApi(unittest.TestCase):\n    def setUp(self):\n        self.app = server.mycrt.application.test_client()\n#Do not work without credentials currently\n#TODO: Fix this test\n    # def test_get_active_db(self):\n    #     fakeRDSClient = FakeRDSClient()\n    #     hostname = server.utility.replay._get_hostname(fakeRDSClient, \"blackfoot-rds\")\n    #     print('heerree')\n        # print(hostname)\n\n    # def test_get_active_db(self):\n    #     test = server.utility.replay.get_active_replays()\n    #     print('HERE')\n    #     print(test)\n\n    def test_get_replays_from_table(self):\n        tester = server.utility.replay.get_replays_from_table()\n        exprectedReplays = {\n            \"replay\":[{\n                \"replay\" : 'test_replay', \n                \"capture\" : 'test_capture', \n                \"db\" : 'cdb', \n                \"mode\" : 'fast'\n            }]\n        }\n        print('HERE')\n        print(tester)\n\n\n    \n<|endoftext|>",
    "repo": "CPSECapstone/OliveChinos"
  },
  {
    "prompt": "class DisallowAuthInterceptor(grpc.ServerInterceptor):\n\tfn __init__(self)\n\tfn intercept_service(self, continuation, handler_call_details)\n\n\nclass AllowAuthInterceptor(grpc.ServerInterceptor):\n\tfn __init__(self)\n\tfn intercept_service(self, continuation, handler_call_details)\n\n\nclass CoreServicer(Core.CoreServiceServicer):\n\tfn __init__(self)\n\tfn GetFeastCoreVersion(self, request, context)\n\tfn GetFeatureTable(self, request: GetFeatureTableRequest, context)\n\tfn ListFeatureTables(self, request: ListFeatureTablesRequest, context)\n\tfn ApplyFeatureTable(self, request: ApplyFeatureTableRequest, context)\n\tfn DeleteFeatureTable(self, request: DeleteFeatureTableRequest, context)\n\tfn GetEntity(self, request: GetEntityRequest, context)\n\tfn ListEntities(self, request: ListEntitiesRequest, context)\n\tfn ListProjects(self, request, context)\n\tfn ApplyEntity(self, request: ApplyEntityRequest, context)\n\n\nfn serve()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright 2020 The Feast Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport socket\nfrom concurrent import futures\nfrom contextlib import closing\n\nimport grpc\nimport pytest\n\nfrom feast.client import Client\nfrom feast.core import CoreService_pb2_grpc as Core\nfrom feast.entity import Entity\nfrom feast.value_type import ValueType\nfrom feast_core_server import CoreServicer\n\n\ndef find_free_port():\n    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n        s.bind((\"\", 0))\n        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        return s.getsockname()[1]\n\n\nfree_port = find_free_port()\n\n\nclass TestEntity:\n    @pytest.fixture(scope=\"function\")\n    def server(self):\n        server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\n        Core.add_CoreServiceServicer_to_server(CoreServicer(), server)\n        server.add_insecure_port(f\"[::]:{free_port}\")\n        server.start()\n        yield server\n        server.stop(0)\n\n    @pytest.fixture\n    def client(self, server):\n        return Client(core_url=f\"localhost:{free_port}\")\n\n    def test_entity_import_export_yaml(self):\n\n        test_entity = Entity(\n            name=\"car_driver_entity\",\n            description=\"Driver entity for car rides\",\n            value_type=ValueType.STRING,\n            labels={\"team\": \"matchmaking\"},\n        )\n\n        # Create a string YAML representation of the entity\n        string_yaml = test_entity.to_yaml()\n\n        # Create a new entity object from the YAML string\n        actual_entity_from_string = Entity.from_yaml(string_yaml)\n\n        # Ensure equality is upheld to original entity\n        assert test_entity == actual_entity_from_string\n\n\ndef test_entity_class_contains_labels():\n    entity = Entity(\n        \"my-entity\",\n        description=\"My entity\",\n        value_type=ValueType.STRING,\n        labels={\"key1\": \"val1\", \"key2\": \"val2\"},\n    )\n    assert \"key1\" in entity.labels.keys() and entity.labels[\"key1\"] == \"val1\"\n    assert \"key2\" in entity.labels.keys() and entity.labels[\"key2\"] == \"val2\"\n\n\ndef test_entity_without_labels_empty_dict():\n    entity = Entity(\"my-entity\", description=\"My entity\", value_type=ValueType.STRING)\n    assert entity.labels == dict()\n    assert len(entity.labels) == 0\n<|endoftext|>",
    "repo": "szczeles/feast"
  },
  {
    "prompt": "\"\"\"Closes QMessageBox's that can appear when testing.\n\nYou can use this with QTimer to close a QMessageBox.\nBefore calling anything that may show a QMessageBox call:\nQTimer.singleShot(1000, lambda: close_message_box(qtbot))\"\"\"\nfn close_message_box(qtbot)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# -*- coding: utf-8 -*-\n#\n# Copyright \u00a9 Spyder Project Contributors\n# Licensed under the terms of the MIT License\n#\n\n\"\"\"\nTests for environ.py\n\"\"\"\n\n# Standard library imports\nimport os\n\n# Test library imports\nimport pytest\n\n# Third party imports\nfrom qtpy.QtCore import QTimer\n\n# Local imports\nfrom spyder.utils.test import close_message_box\n\n@pytest.fixture\ndef setup_environ(qtbot):\n    \"Setup the Environment variables Dialog taking into account the os.\"    \n    if os.name == 'nt':\n        from spyder.utils.environ import WinUserEnvDialog\n        dialog = WinUserEnvDialog()\n    else:        \n        from spyder.utils.environ import EnvDialog\n        dialog = EnvDialog()\n    qtbot.addWidget(dialog)\n    \n    return dialog\n\ndef test_environ(qtbot):\n    \"\"\"Test the environment variables dialog.\"\"\"\n    QTimer.singleShot(1000, lambda: close_message_box(qtbot))\n    dialog = setup_environ(qtbot)\n    dialog.show()\n    assert dialog\n\n\nif __name__ == \"__main__\":\n    pytest.main()\n<|endoftext|>",
    "repo": "jfunez/spyder"
  },
  {
    "prompt": "\"\"\"monkeypatch for get_or_create_project\"\"\"\nfn goc_project()\n\n\"\"\"Load test suites from file in YAML format:\n- test_1:\n    - /first/step/url:\n        method: GET\n        query_string: {\"a\": \"b\"}\n        data: {\"json\": \"payload\"}\n        status_code: 200\n        response: {\"json\": \"response\"}\n    - /second/step/url:\n- test_2:\"\"\"\nfn load_test_suite_from_file(test_suite_file)\n\nfn get_test_ids(test_val)\n\nfn test_suite_items(current_test_suite)\n\nfn make_request(client, url, params)\n\nfn compare_response_values(client_value, expected_value)\n\nfn compare_response(client_response, expected_response)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom .base import load_test_suite_from_file, test_suite_items, get_test_ids, make_request, compare_response\n\n\n@pytest.mark.parametrize('test_suite', load_test_suite_from_file('data_import.yml'), ids=get_test_ids)\ndef test_import(client, test_suite):\n    for url, params in test_suite_items(test_suite):\n        r = make_request(client, url, params)\n        assert r.status_code == params['status_code']\n        is_equal, error_message = compare_response(r.json, params['response'])\n        assert is_equal, error_message\n<|endoftext|>",
    "repo": "Asjidkalam/label-studio"
  },
  {
    "prompt": "fn elasticsearch(host)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from .fixtures import elasticsearch\nimport pytest\n\n\ndef test_setting_node_name_with_an_environment_variable(elasticsearch):\n    # The fixture for this test comes from tests/docker-compose.yml\n    assert elasticsearch.get_root_page()['name'].startswith('docker-test-node')\n\n\ndef test_setting_cluster_name_with_an_environment_variable(elasticsearch):\n    # The fixture for this test comes from tests/docker-compose.yml\n    assert elasticsearch.get_root_page()['cluster_name'] == ('docker-test-cluster')\n\n\ndef test_setting_heapsize_with_an_environment_variable(elasticsearch):\n    # The fixture for this test comes from tests/docker-compose.yml.\n    #\n    # The number of bytes that we assert is not exactly what we\n    # specify in the fixture. This is due to jvm honoring the\n    # generation and survivor ratios, rounding and alignments.  It is\n    # enough if Elasticsearch reports a max heap size within 64MB of\n    # the target value set in the fixture (=1152MB).\n\n    mem_delta_mb = 64\n    for jvm in elasticsearch.get_node_jvm_stats():\n        reported_heap_max_in_mb = int(jvm['mem']['heap_max_in_bytes'] / (1024**2))\n        assert abs(reported_heap_max_in_mb - 1152) < mem_delta_mb\n\n\ndef test_parameter_containing_underscore_with_an_environment_variable(elasticsearch):\n    # The fixture for this test comes from tests/docker-compose.yml\n    for thread_pool_queue_size in elasticsearch.get_node_thread_pool_bulk_queue_size():\n        assert '500' == thread_pool_queue_size\n\n\ndef test_setting_processors(elasticsearch):\n    # The fixture for this test comes from tests/docker-compose.yml\n    for processors in elasticsearch.get_processors_setting():\n        assert '1' == processors\n\n\ndef test_envar_not_including_a_dot_is_not_presented_to_elasticsearch(elasticsearch):\n    # The fixture for this test comes from tests/docker-compose.yml\n    assert 'irrelevantsetting' not in elasticsearch.es_cmdline()\n\n\ndef test_capitalized_envvar_is_not_presented_to_elasticsearch(elasticsearch):\n    # The fixture for this test comes from tests/docker-compose.yml\n    assert 'NonESRelatedVariable' not in elasticsearch.es_cmdline()\n\n\ndef test_setting_boostrap_memory_lock_with_an_environment_variable(elasticsearch):\n    # The fixture for this test comes from tests/docker-compose.yml\n    #\n    # When memory_lock=true ES bootstrap checks expect the memlock ulimit set to unlimited.\n    # https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration-memory.html#mlockall\n    for mlockall_node_value in elasticsearch.get_node_mlockall_state():\n        assert mlockall_node_value is True\n<|endoftext|>",
    "repo": "artturimatias/oscari-ES"
  },
  {
    "prompt": "class UserFactory(DjangoModelFactory):\n\tusername = Faker('user_name')\n\temail = Faker('email')\n\tname = Faker('name')\n\tfn password(self, create: bool, extracted: Sequence[Any])\n\n\n\"\"\"A pointless Celery task to demonstrate usage.\"\"\"\nfn get_users_count()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom celery.result import EagerResult\n\nfrom redirink.users.tasks import get_users_count\nfrom redirink.users.tests.factories import UserFactory\n\npytestmark = pytest.mark.django_db\n\n\ndef test_user_count(settings):\n    \"\"\"A basic test to execute the get_users_count Celery task.\"\"\"\n    UserFactory.create_batch(3)\n    settings.CELERY_TASK_ALWAYS_EAGER = True\n    task_result = get_users_count.delay()\n    assert isinstance(task_result, EagerResult)\n    assert task_result.result == 3\n<|endoftext|>",
    "repo": "Egor4ik325/redirink"
  },
  {
    "prompt": "class _FileLike:\n\tBLOCKSIZE = 1024 * 32\n\tfn __init__(self, o)\n\tfn set_descriptor(self, o)\n\tfn __getattr__(self, attr)\n\t\"\"\"Starts or resets the log.\n\t\n\tThis will store all bytes read or written.\"\"\"\n\tfn start_log(self)\n\t\"\"\"Stops the log.\"\"\"\n\tfn stop_log(self)\n\tfn is_logging(self)\n\t\"\"\"Returns the log as a string.\"\"\"\n\tfn get_log(self)\n\tfn add_log(self, v)\n\tfn reset_timestamps(self)\n\n\nclass Writer(_FileLike):\n\t\"\"\"May raise exceptions.TcpDisconnect\"\"\"\n\tfn flush(self)\n\t\"\"\"May raise exceptions.TcpDisconnect\"\"\"\n\tfn write(self, v)\n\n\nclass Reader(_FileLike):\n\t\"\"\"If length is -1, we read until connection closes.\"\"\"\n\tfn read(self, length)\n\tfn readline(self, size)\n\t\"\"\"Like .read, but is guaranteed to either return length bytes, or\n\traise an exception.\"\"\"\n\tfn safe_read(self, length)\n\t\"\"\"Tries to peek into the underlying file object.\n\t\n\tReturns:\n\t    Up to the next N bytes if peeking is successful.\n\t\n\tRaises:\n\t    exceptions.TcpException if there was an error with the socket\n\t    TlsException if there was an error with pyOpenSSL.\n\t    NotImplementedError if the underlying file object is not a [pyOpenSSL] socket\"\"\"\n\tfn peek(self, length)\n\n\n\"\"\"This is a wrapper around select.select() which also works for SSL.Connections\nby taking ssl_connection.pending() into account.\n\nCaveats:\n    If .pending() > 0 for any of the connections in rlist, we avoid the select syscall\n    and **will not include any other connections which may or may not be ready**.\n\nArgs:\n    rlist: wait until ready for reading\n\nReturns:\n    subset of rlist which is ready for reading.\"\"\"\nfn ssl_read_select(rlist: None, timeout)\n\n\"\"\"Does a hard close of a socket, without emitting a RST.\"\"\"\nfn close_socket(sock)\n\nclass _Connection:\n\trbufsize = -1\n\twbufsize = -1\n\t\"\"\"Set up .rfile and .wfile attributes from .connection\"\"\"\n\tfn _makefile(self)\n\tfn __init__(self, connection)\n\tfn get_current_cipher(self)\n\tfn finish(self)\n\n\nclass ConnectionCloser:\n\tfn __init__(self, conn)\n\t\"\"\"Cancel the current closer, and return a fresh one.\"\"\"\n\tfn pop(self)\n\tfn __enter__(self)\n\tfn __exit__(self)\n\n\nclass TCPClient(_Connection):\n\tfn __init__(self, address, source_address, spoof_source_address)\n\tfn ssl_verification_error(self) -> Optional[exceptions.InvalidCertificateException]\n\tfn close(self)\n\tfn convert_to_ssl(self, sni, alpn_protos)\n\tfn makesocket(self, family, type, proto)\n\tfn create_connection(self, timeout)\n\tfn connect(self)\n\tfn settimeout(self, n)\n\tfn gettimeout(self)\n\tfn get_alpn_proto_negotiated(self)\n\n\n\"\"\"The instantiator is expected to call the handle() and finish() methods.\"\"\"\nclass BaseHandler(_Connection):\n\tfn __init__(self, connection, address, server)\n\t\"\"\"Convert connection to SSL.\n\tFor a list of parameters, see tls.create_server_context(...)\"\"\"\n\tfn convert_to_ssl(self, cert, key)\n\tfn handle(self)\n\tfn settimeout(self, n)\n\tfn get_alpn_proto_negotiated(self)\n\n\nclass Counter:\n\tfn __init__(self)\n\tfn count(self)\n\tfn __enter__(self)\n\tfn __exit__(self)\n\n\nclass TCPServer:\n\trequest_queue_size = 20\n\tfn __init__(self, address)\n\tfn connection_thread(self, connection, client_address)\n\tfn serve_forever(self, poll_interval)\n\tfn shutdown(self)\n\t\"\"\"Called when handle_client_connection raises an exception.\"\"\"\n\tfn handle_error(self, connection_, client_address, fp)\n\t\"\"\"Called after client connection.\"\"\"\n\tfn handle_client_connection(self, conn, client_address)\n\t\"\"\"Called after server shutdown.\"\"\"\n\tfn handle_shutdown(self)\n\tfn wait_for_silence(self, timeout)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom mitmproxy import exceptions\nfrom mitmproxy.net import tls\nfrom mitmproxy.net.tcp import TCPClient\nfrom test.mitmproxy.net.test_tcp import EchoHandler\nfrom . import tservers\n\n\nclass TestMasterSecretLogger(tservers.ServerTestBase):\n    handler = EchoHandler\n    ssl = dict(\n        cipher_list=\"AES256-SHA\"\n    )\n\n    def test_log(self, tmpdir):\n        testval = b\"echo!\\n\"\n        _logfun = tls.log_master_secret\n\n        logfile = str(tmpdir.join(\"foo\", \"bar\", \"logfile\"))\n        tls.log_master_secret = tls.MasterSecretLogger(logfile)\n\n        c = TCPClient((\"127.0.0.1\", self.port))\n        with c.connect():\n            c.convert_to_ssl()\n            c.wfile.write(testval)\n            c.wfile.flush()\n            assert c.rfile.readline() == testval\n            c.finish()\n\n            tls.log_master_secret.close()\n            with open(logfile, \"rb\") as f:\n                assert f.read().count(b\"CLIENT_RANDOM\") == 2\n\n        tls.log_master_secret = _logfun\n\n    def test_create_logfun(self):\n        assert isinstance(\n            tls.MasterSecretLogger.create_logfun(\"test\"),\n            tls.MasterSecretLogger)\n        assert not tls.MasterSecretLogger.create_logfun(False)\n\n\nclass TestTLSInvalid:\n    def test_invalid_ssl_method_should_fail(self):\n        fake_ssl_method = 100500\n        with pytest.raises(exceptions.TlsException):\n            tls.create_client_context(method=fake_ssl_method)\n\n    def test_alpn_error(self):\n        with pytest.raises(exceptions.TlsException, match=\"must be a function\"):\n            tls.create_client_context(alpn_select_callback=\"foo\")\n\n        with pytest.raises(exceptions.TlsException, match=\"ALPN error\"):\n            tls.create_client_context(alpn_select=\"foo\", alpn_select_callback=\"bar\")\n<|endoftext|>",
    "repo": "takeratta/mitmproxy"
  },
  {
    "prompt": "class ConstantOfShape(nn.Module):\n\tfn __init__(self, constant)\n\tfn forward(self, shape: torch.Tensor)\n\tfn extra_repr(self) -> str\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import numpy as np\nimport torch\nimport pytest\n\nfrom onnx2pytorch.operations.constantofshape import ConstantOfShape\n\n\ndef test_constantofshape_float_ones():\n    op = ConstantOfShape()\n    x = torch.tensor([4, 3, 2], dtype=torch.int64)\n    y = torch.ones(*x, dtype=torch.float32)\n    assert torch.equal(op(x), y)\n\n\ndef test_constantofshape_int32_shape_zero():\n    constant = torch.tensor([0], dtype=torch.int32)\n    op = ConstantOfShape(constant=constant)\n    x = torch.tensor([0], dtype=torch.int64)\n    y = torch.zeros(*x, dtype=torch.int32)\n    assert torch.equal(op(x), y)\n\n\ndef test_constantofshape_int32_zeros():\n    constant = torch.tensor([0], dtype=torch.int32)\n    op = ConstantOfShape(constant=constant)\n    x = torch.tensor([10, 6], dtype=torch.int64)\n    y = torch.zeros(*x, dtype=torch.int32)\n    assert torch.equal(op(x), y)\n<|endoftext|>",
    "repo": "Piteryo/onnx2pytorch"
  },
  {
    "prompt": "\"\"\"Simple entity document model.\"\"\"\nclass MyEntityDocument(AbstractEntityDocumentModel):\n\tmy_field = models.CharField(max_length=MAX_LENGTH)\n\t\"\"\"Returns URL to download endpoint.\"\"\"\n\tfn url(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom django.utils.timezone import now\n\nfrom api.documents.tasks import delete_document\nfrom .my_entity_document.models import MyEntityDocument\nfrom api.documents.utils import get_bucket_name\n\nfrom api.documents.utils import get_s3_client_for_bucket\nfrom botocore.stub import Stubber\n\n\n# mark the whole module for db use\npytestmark = pytest.mark.django_db\n\n\n@pytest.fixture()\ndef s3_stubber():\n    \"\"\"S3 stubber using the botocore Stubber class.\"\"\"\n    s3_client = get_s3_client_for_bucket(\"default\")\n    with Stubber(s3_client) as s3_stubber:\n        yield s3_stubber\n\n\ndef test_delete_document(s3_stubber):\n    \"\"\"Tests if delete_document task deletes s3 document.\"\"\"\n    entity_document = MyEntityDocument.objects.create(\n        original_filename=\"test.txt\", my_field=\"lions\"\n    )\n    document = entity_document.document\n    document.uploaded_on = now()\n    document.mark_deletion_pending()\n\n    bucket_name = get_bucket_name(document.bucket_id)\n\n    s3_stubber.add_response(\n        \"delete_object\",\n        {\"ResponseMetadata\": {\"HTTPStatusCode\": 204}},\n        expected_params={\"Bucket\": bucket_name, \"Key\": document.path},\n    )\n\n    result = delete_document.apply(args=(document.pk,)).get()\n    assert result is None\n\n    with pytest.raises(MyEntityDocument.DoesNotExist):\n        MyEntityDocument.objects.include_objects_deletion_pending().get(\n            pk=entity_document.pk\n        )\n\n\ndef test_delete_document_s3_failure(s3_stubber):\n    \"\"\"\n    Tests if delete_document task won't delete document from the\n    database if deletion from S3 fails.\n    \"\"\"\n    entity_document = MyEntityDocument.objects.create(\n        original_filename=\"test.txt\", my_field=\"lions\"\n    )\n    document = entity_document.document\n    document.uploaded_on = now()\n    document.mark_deletion_pending()\n\n    bucket_name = get_bucket_name(document.bucket_id)\n\n    s3_stubber.add_client_error(\n        \"delete_object\",\n        service_error_code=500,\n        expected_params={\"Bucket\": bucket_name, \"Key\": document.path},\n    )\n\n    with pytest.raises(Exception):\n        delete_document.apply(args=(document.pk,)).get()\n\n    qs = MyEntityDocument.objects.include_objects_deletion_pending()\n    assert qs.filter(pk=entity_document.pk).exists() is True\n<|endoftext|>",
    "repo": "cad106uk/market-access-api"
  },
  {
    "prompt": "\"\"\"Create temporary directory with copy of requirements.\"\"\"\nfn temp_dir()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"End to end tests for CLI v2\"\"\"\n\nfrom functools import partial\n\ntry:\n    from unittest import mock\nexcept ImportError:\n    import mock\nfrom click.testing import CliRunner\n\nimport pytest\n\nfrom pipcompilemulti.cli_v2 import cli, read_config\nfrom .utils import temp_dir\n\n\n@pytest.fixture(autouse=True)\ndef requirements_dir():\n    \"\"\"Create temporary requirements directory for test time.\"\"\"\n    with temp_dir() as tmp_dir:\n        patch = partial(patched_config, tmp_dir)\n        with mock.patch('pipcompilemulti.cli_v2.read_config', patch):\n            yield\n\n\n@pytest.mark.parametrize('command', ['lock', 'upgrade', 'verify'])\ndef test_command_exits_with_zero(command):\n    \"\"\"Run requirements command on self\"\"\"\n    # pylint: disable=redefined-outer-name\n    runner = CliRunner()\n    result = runner.invoke(cli, [command])\n    assert result.exit_code == 0\n\n\ndef patched_config(base_dir):\n    \"\"\"Override base_dir in each section of config.\"\"\"\n    config_sections = read_config()\n    for _, section in config_sections:\n        section['base_dir'] = base_dir\n    return config_sections\n<|endoftext|>",
    "repo": "HALtheWise/pip-compile-multi"
  },
  {
    "prompt": "fn random_string(n)\n\nfn get_source_function(func)\n\n\"\"\"Get a unique name based on the prefix\nwhich does not coincide with a module which\nalready exists and is not being created by\nanother thread\n\nParameters\n----------\nprefix : str\n         The starting string of the random name\npath   : str\n         The folder where the lock file should be saved\n\nReturns\n-------\nmodule_name : str\n              A unique name for the new module\nmodule_lock : FileLock\n              A file lock preventing other threads\n              from creating a module with the same name\"\"\"\nfn get_unique_name(prefix: str, path: str) -> FileLock\n\nfn epyccel_seq(function_or_module)\n\n\"\"\"Accelerate Python function or module using Pyccel in \"embedded\" mode.\n\nParameters\n----------\npython_function_or_module : function | module\n    Python function or module to be accelerated.\n\nverbose : bool\n    Print additional information (default: False).\n\nlanguage : {'fortran', 'c', 'python'}\n    Language of generated code (default: 'fortran').\n\naccelerators : iterable of str, optional\n    Parallel multi-threading acceleration strategy\n    (currently supported: 'mpi', 'openmp', 'openacc').\n\nOptions for parallel mode\n-------------------------\ncomm : mpi4py.MPI.Comm, optional\n    MPI communicator for calling Pyccel in parallel mode (default: None).\n\nroot : int, optional\n    MPI rank of process in charge of accelerating code (default: 0).\n\nbcast : {True, False}\n    If False, only root process loads accelerated function/module (default: True).\n\nOther options\n-------------\ncompiler : str, optional\n    User-defined command for compiling generated source code.\n\nReturns\n-------\nres : object\n    Accelerated function or module.\n\nExamples\n--------\n>>> def one(): return 1\n>>> from pyccel.epyccel import epyccel\n>>> one_f = epyccel(one, language='fortran')\n>>> one_c = epyccel(one, language='c')\"\"\"\nfn epyccel(python_function_or_module: function | module) -> object\n\nfn lambdify(f)\n\nfn python(f)\n\nfn sympy(f)\n\nfn bypass(f)\n\nfn types()\n\n\"\"\"template decorator.\"\"\"\nfn template(name, types)\n\nfn pure(f)\n\nfn private(f)\n\nfn elemental(f)\n\n\"\"\"Decorator indicates that all arrays mentioned as args should be stored\non the stack.\n\nParameters\n----------\nf : Function\n    The function to which the decorator is applied\nargs : list of str\n    A list containing the names of all arrays which should be stored on the stack\"\"\"\nfn stack_array(f: Function)\n\n\"\"\"Decorator indicates that all arrays mentioned as args can be accessed with\nnegative indexes. As a result all non-constant indexing uses a modulo\nfunction. This can have negative results on the performance\n\nParameters\n----------\nf : Function\n    The function to which the decorator is applied\nargs : list of str\n    A list containing the names of all arrays which can be accessed\n    with non-constant negative indexes\"\"\"\nfn allow_negative_index(f: Function)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# pylint: disable=missing-function-docstring, missing-module-docstring/\n# coding: utf-8\nimport pytest\nimport numpy as np\n\nfrom pyccel.epyccel import epyccel\nfrom pyccel.decorators import types\n\n@pytest.fixture(params=[\n    pytest.param('fortran', marks = pytest.mark.fortran),\n    pytest.param('c'      , marks = pytest.mark.c),\n    pytest.param(\"python\", marks = [\n        pytest.mark.skip(reason=\"Confusion around ValuedVariable means it cannot be used in python\"),\n        pytest.mark.python]\n    )]\n)\ndef language(request):\n    return request.param\n\n#------------------------------------------------------------------------------\ndef test_f1(language):\n    @types('int')\n    def f1(x = 1):\n        y = x - 1\n        return y\n\n    f = epyccel(f1, language = language)\n\n    # ...\n    assert f(2) == f1(2)\n    assert f() == f1()\n    # ...\n#------------------------------------------------------------------------------\ndef test_f2(language):\n    @types('real [:]', 'int')\n    def f5(x, m1 = 2):\n        x[:] = 0.\n        for i in range(0, m1):\n            x[i] = i * 1.\n\n    f = epyccel(f5, language=language)\n\n    # ...\n    m1 = 3\n\n    x = np.zeros(m1)\n    f(x)\n\n    x_expected = np.zeros(m1)\n    f5(x_expected)\n\n    assert np.allclose( x, x_expected, rtol=1e-15, atol=1e-15 )\n    # ...\n\n    f(x, m1 = m1)\n\n    f5(x_expected, m1)\n\n    assert np.allclose( x, x_expected, rtol=1e-15, atol=1e-15 )\n\n\n#------------------------------------------------------------------------------\ndef test_f3(language):\n    @types('real','real')\n    def f3(x = 1.5, y = 2.5):\n        return x+y\n\n    f = epyccel(f3, language=language)\n\n    # ...\n    assert f(19.2,6.7) == f3(19.2,6.7)\n    assert f(4.5) == f3(4.5)\n    assert f(y = 8.2) == f3(y = 8.2)\n    assert f() == f3()\n    # ...\n\n#------------------------------------------------------------------------------\ndef test_f4(language):\n    @types('bool')\n    def f4(x = True):\n        if x:\n            return 1\n        else:\n            return 2\n\n    f = epyccel(f4, language = language)\n\n    # ...\n    assert f(True)  == f4(True)\n    assert f(False) == f4(False)\n    assert f()      == f4()\n    # ...\n\n#------------------------------------------------------------------------------\ndef test_f5(language):\n    @types('complex')\n    def f5(x = 1j):\n        y = x - 1\n        return y\n\n    f = epyccel(f5, language = language)\n\n    # ...\n    assert f(2.9+3j) == f5(2.9+3j)\n    assert f()       == f5()\n    # ...\n<|endoftext|>",
    "repo": "jalalium/pyccel"
  },
  {
    "prompt": "fn pytest_addoption(parser)\n\nfn num_cpus(pytestconfig)\n\nfn io_type(pytestconfig)\n\n\"\"\"Used as base class for pytests of qubit classes\"\"\"\nclass BaseTest:\n\tqbt = None\n\t\"\"\"Pytest fixture that provides a temporary directory for writing test files\"\"\"\n\tfn set_tmpdir(self, request)\n\tfn teardown_class(cls)\n\tfn eigenvals(self, io_type, evals_reference)\n\tfn eigenvecs(self, io_type, evecs_reference)\n\tfn plot_evals_vs_paramvals(self, num_cpus, param_name, param_list)\n\tfn get_spectrum_vs_paramvals(self, num_cpus, io_type, param_name, param_list, evals_reference, evecs_reference)\n\tfn matrixelement_table(self, io_type, op, matelem_reference)\n\tfn plot_matrixelements(self, op, evals_count)\n\tfn print_matrixelements(self, op)\n\tfn plot_matelem_vs_paramvals(self, num_cpus, op, param_name, param_list, select_elems)\n\tfn test_file_io(self)\n\n\nclass StandardTests(BaseTest):\n\tfn setup_class(cls)\n\tfn test_hamiltonian_is_hermitean(self, io_type)\n\tfn test_eigenvals(self, io_type)\n\tfn test_eigenvecs(self, io_type)\n\tfn test_plot_wavefunction(self, io_type)\n\tfn test_plot_evals_vs_paramvals(self, num_cpus, io_type)\n\tfn test_get_spectrum_vs_paramvals(self, num_cpus, io_type)\n\tfn test_matrixelement_table(self, io_type)\n\tfn test_plot_matrixelements(self, io_type)\n\tfn test_print_matrixelements(self, io_type)\n\tfn test_plot_matelem_vs_paramvals(self, num_cpus, io_type)\n\tfn test_plot_potential(self, io_type)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# testing.py\n#\n# This file is part of scqubits.\n#\n#    Copyright (c) 2019 and later, Jens Koch and Peter Groszkowski\n#    All rights reserved.\n#\n#    This source code is licensed under the BSD-style license found in the\n#    LICENSE file in the root directory of this source tree.\n############################################################################\n\nimport pytest\n\nfrom scqubits.tests.conftest import TESTDIR\n\n\ndef run():\n    \"\"\"\n    Run the pytest scripts for scqubits.\n    \"\"\"\n    # runs tests in scqubits.tests directory\n    pytest.main([\"-v\", TESTDIR])\n<|endoftext|>",
    "repo": "dmtvanzanten/scqubits"
  },
  {
    "prompt": "\"\"\"Utility Class to check if a payee is Sanction Hit/No with percentage.\"\"\"\nclass SanctionsScreening(object):\n\tfn __init__(self, sanction_list_file, matching_percentage)\n\t\"\"\"Check if payee is a Hit with matched percentage.\"\"\"\n\tfn payee_sanction_screening(self, payee_name)\n\tfn __read_sanction_list(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom SanctionsScreening import SanctionsScreening\n\n\nclass TestSanctionScreening(object):\n\n    @pytest.fixture\n    def screening_instance(self):\n        return SanctionsScreening(\"sanctions_list.csv\", 0.75)\n\n    def testHitWithPercentage1(self, screening_instance):\n        the_payee_name = \"Kristopher Toe\"\n        result = screening_instance.payee_sanction_screening(the_payee_name)\n        expected = ('Hit', 0.9285714285714286)\n        assert result == expected\n\n    def testHitWithPercentage2(self, screening_instance):\n        the_payee_name = \"Kristophre Doe\"\n        result = screening_instance.payee_sanction_screening(the_payee_name)\n        expected = ('Hit', 0.8571428571428571)\n        assert result == expected\n\n    def testHitWithPercentage3(self, screening_instance):\n        the_payee_name = \"Kristopher Doe\"\n        result = screening_instance.payee_sanction_screening(the_payee_name)\n        expected = ('Hit', 1.0)\n        assert result == expected\n\n    def testNoHitWithPercentage(self, screening_instance):\n        the_payee_name = \"Tianze Zhang\"\n        result = screening_instance.payee_sanction_screening(the_payee_name)\n        expected = ('No', 0.0)\n        assert result == expected\n\n    if __name__ == '__main__':\n        pytest.main(['-sv', 'SanctionScreening.py'])\n<|endoftext|>",
    "repo": "tomztz/JPMorgan-Spring-insight-challenge"
  },
  {
    "prompt": "class ExeParams:\n\tparallel: Any = '-'\n\tproc: Any = '-'\n\tmarshal: Any = '-'\n\n\n\"\"\"Collapse any whitespace in stringified `s` into a single space. \"\"\"\nfn oneliner(s) -> str\n\nfn flat_dict(d)\n\n\"\"\"Asserts captured nodes have `attr` satisfying `regex` one by one (in a cycle).\n\n:param count:\n    The number-of-nodes expected.\n    If ``True``, this number becomes the number-of-`regex`;\n    if none, no count check happens.\"\"\"\nfn attr_check(attr)\n\nfn check_xpath(etree, fname, path, check, be_found)\n\nfn dilled(i)\n\nfn pickled(i)\n\n\"\"\"Same as a + b + ....\"\"\"\nfn addall()\n\nfn abspow(a, p)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import logging\nimport os\nfrom collections import namedtuple\nfrom multiprocessing import Pool\nfrom multiprocessing import dummy as mp_dummy\nfrom multiprocessing import get_context\nfrom operator import add\n\nimport pytest\n\nfrom graphtik import compose, operation\nfrom graphtik.config import debug_enabled, execution_pool_plugged, tasks_marshalled\n\nfrom .helpers import (\n    _marshal,\n    _parallel,\n    _proc,\n    _slow,\n    _thread,\n    dilled,\n    exe_params,\n    pickled,\n)\n\n# Enable pytest-sphinx fixtures\n# See https://www.sphinx-doc.org/en/master/devguide.html#unit-testing\npytest_plugins = \"sphinx.testing.fixtures\"\n\n# TODO: is this needed along with norecursedirs?\n# See https://stackoverflow.com/questions/33508060/create-and-import-helper-functions-in-tests-without-creating-packages-in-test-di\ncollect_ignore = [\"helpers.py\"]\n\n########\n## From https://stackoverflow.com/a/57002853/548792\n##\ndef pytest_addoption(parser):\n    \"\"\"Add a command line option to disable logger.\"\"\"\n    parser.addoption(\n        \"--logger-disabled\",\n        action=\"append\",\n        default=[],\n        help=\"disable specific loggers\",\n    )\n\n\ndef pytest_configure(config):\n    \"\"\"Disable the loggers from CLI and silence sphinx markers warns.\"\"\"\n    for name in config.getoption(\"--logger-disabled\", default=[]):\n        logger = logging.getLogger(name)\n        logger.propagate = False\n\n    config.addinivalue_line(\"markers\", \"sphinx: parametrized sphinx test-launches\")\n    config.addinivalue_line(\n        \"markers\", \"test_params: for parametrized sphinx test-launches\"\n    )\n\n\n##\n########\n\n\n@pytest.fixture\ndef debug_mode():\n    from graphtik.config import debug_enabled\n\n    with debug_enabled(True):\n        yield\n\n\n@pytest.fixture(params=[dilled, pickled])\ndef ser_method(request):\n    return request.param\n\n\n@pytest.fixture(\n    params=[\n        # PARALLEL?, Thread/Proc?, Marshalled?\n        (None, None, None),\n        pytest.param((1, 0, 0), marks=(_parallel, _thread)),\n        pytest.param((1, 0, 1), marks=(_parallel, _thread, _marshal)),\n        pytest.param((1, 1, 1), marks=(_parallel, _proc, _marshal, _slow)),\n        pytest.param(\n            (1, 1, 0),\n            marks=(\n                _parallel,\n                _proc,\n                _slow,\n                pytest.mark.xfail(reason=\"ProcessPool non-marshaled may fail.\"),\n            ),\n        ),\n    ]\n)\ndef exemethod(request):\n    \"\"\"Returns exe-method combinations, and store them globally, for xfail checks.\"\"\"\n    parallel, proc_pool, marshal = request.param\n    exe_params.parallel, exe_params.proc, exe_params.marshal = request.param\n\n    nsharks = None  # number of pool swimmers....\n\n    with tasks_marshalled(marshal):\n        if parallel:\n            if proc_pool:\n                if os.name == \"posix\":  # Allthough it is the default ...\n                    # NOTE: \"spawn\" DEADLOCKS!.\n                    pool = get_context(\"fork\").Pool(nsharks)\n                else:\n                    pool = Pool(nsharks)\n            else:\n                pool = mp_dummy.Pool(nsharks)\n\n            with execution_pool_plugged(pool), pool:\n                yield parallel\n        else:\n            yield parallel\n\n\n@pytest.fixture\ndef samplenet():\n    \"\"\"sum1 = (a + b), sum2 = (c + d), sum3 = c + (c + d)\"\"\"\n    sum_op1 = operation(name=\"sum_op1\", needs=[\"a\", \"b\"], provides=\"sum1\")(add)\n    sum_op2 = operation(name=\"sum_op2\", needs=[\"c\", \"d\"], provides=\"sum2\")(add)\n    sum_op3 = operation(name=\"sum_op3\", needs=[\"c\", \"sum2\"], provides=\"sum3\")(add)\n    return compose(\"test_net\", sum_op1, sum_op2, sum_op3)\n\n\n@pytest.fixture(params=[10, 20])\ndef log_levels(request, caplog):\n    caplog.set_level(request.param)\n    return\n<|endoftext|>",
    "repo": "ankostis/graphkit"
  },
  {
    "prompt": "\"\"\"Generates a full mapping by using a simulated annealing algorithm from:\nOrsila, H., Kangas, T., Salminen, E., H\u00e4m\u00e4l\u00e4inen, T. D., & H\u00e4nnik\u00e4inen, M.\n(2007). Automated memory-aware application distribution for multi-processor\nsystem-on-chips. Journal of Systems Architecture, 53(11), 795-815.e.\n\nArgs:\n    platform (Platform): A platform\n    random_seed (int, optional): A random seed for the RNG. Defautls to 42.\n    record_statistics (bool, optional): Record statistics on mappings\n        evaluated? Defautls to False.\n    initial_temperature (float, optional): Initial temperature for\n        simmulated annealing. Defaults to 1.0.\n    final_temperature (float, optional): Final temperature for simmulated\n        annealing. Defaults to 0.1.\n    temperature_proportionality_constant (float, optional): Temperature\n        prop. constant for simmulated annealing. Defaults to 0.5.\n    radius (float, optional): The radius for searching when moving.\n        Defaults to 3.0.\n    dump_cache (bool, optional): Dump the mapping cache? Defaults to False.\n    chunk_size (int, optional): Size of chunks for parallel simulation.\n        Defaults to 10.\n    progress (bool, optional): Display simulation progress visually?\n        Defaults to False.\n    parallel (bool, optional): Execute simulations in parallel?\n        Defaults to False.\n    jobs (int, optional): Number of jobs for parallel simulation.\n        Defaults to 1.\"\"\"\nclass SimulatedAnnealingMapper(BaseMapper):\n\tfn __init__(self, platform, random_seed, record_statistics, initial_temperature, final_temperature, temperature_proportionality_constant, radius, dump_cache, chunk_size, progress, parallel, jobs)\n\tfn temperature_cooling(self, temperature, iteration, max_rejections)\n\tfn query_accept(self, time, temperature)\n\tfn move(self, representation, mapping, temperature)\n\t\"\"\"Generate a full mapping using simulated annealing.\n\t\n\tArgs:\n\t    graph (DataflowGraph): a dataflow graph\n\t    trace (TraceGenerator, optional): a trace generator\n\t    representation (MappingRepresentation, optional): a mapping\n\t        representation object\n\t    processors (:obj:`list` of :obj:`Processor`, optional): a list of\n\t        processors to map to.\n\t    partial_mapping (Mapping, optional): a partial mapping to complete\n\t\n\tReturns:\n\t    Mapping: the generated mapping.\"\"\"\n\tfn generate_mapping(self, graph: DataflowGraph, trace: TraceGenerator, representation: MappingRepresentation, processors, partial_mapping: Mapping) -> Mapping\n\n\nclass MockMappingCache:\n\tfn __init__(self, simres_evaluation_function, mocker)\n\tfn reset_statistics(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright (C) 2020 TU Dresden\n# Licensed under the ISC license (see LICENSE.txt)\n#\n# Authors: Felix Teweleit, Andres Goens\n\nfrom mocasin.mapper.test.mock_cache import MockMappingCache\nfrom mocasin.mapper.simulated_annealing import SimulatedAnnealingMapper\nfrom itertools import product\nimport pytest\nimport numpy as np\n\n\n@pytest.fixture\ndef conf():\n    return {\n        \"mapper\": {\n            \"random_seed\": 42,\n            \"record_statistics\": False,\n            \"initial_temperature\": 1.0,\n            \"final_temperature\": 0.01,\n            \"temperature_proportionality_constant\": 0.5,\n            \"radius\": 2,\n            \"dump_cache\": False,\n            \"chunk_size\": 10,\n            \"progress\": False,\n            \"parallel\": True,\n            \"jobs\": 4,\n        },\n        \"norm_p\": 2,\n        \"periodic_boundary_conditions\": False,\n        \"representation\": \"SimpleVector\",\n        \"channels\": False,\n    }\n\n\n@pytest.fixture\ndef mapper(\n    graph, platform, trace, representation, simres_evaluation_function, mocker\n):\n    m = SimulatedAnnealingMapper(\n        graph,\n        platform,\n        trace,\n        representation,\n        42,\n        False,\n        1.0,\n        0.01,\n        0.5,\n        2,\n        False,\n        10,\n        False,\n        True,\n        4,\n    )\n    m.simulation_manager = MockMappingCache(simres_evaluation_function, mocker)\n    return m\n\n\ndef test_sa(mapper, evaluation_function):\n    result_mapper = mapper.generate_mapping()\n    results = [\n        (evaluation_function([x, y]), x, y)\n        for x, y in product(range(7), range(7))\n    ]\n    expected = set([(x, y) for (_, x, y) in sorted(results)[:5]])\n\n    # result is top 5 best\n    assert tuple(result_mapper.to_list()) in expected\n\n\ndef test_temperature_cooling(conf, mapper):\n    timeout = 10000\n    temperature = conf[\"mapper\"][\"initial_temperature\"]\n\n    for i in range(timeout):\n        temperature = mapper.temperature_cooling(temperature, i)\n        if temperature <= conf[\"mapper\"][\"final_temperature\"]:\n            break\n\n    assert temperature <= conf[\"mapper\"][\"final_temperature\"]\n\n\ndef test_query_accept(mapper):\n    mapper.initial_cost = 1\n\n    assert np.isclose(mapper.query_accept(1000, 0.0001), 0)\n\n    positive = mapper.query_accept(1.000001, 1)\n    assert positive > 0\n    assert positive < 1\n<|endoftext|>",
    "repo": "tud-ccc/mocasin"
  },
  {
    "prompt": "fn generate_url(endpoint)\n\n\"\"\"Get Rate Limit Information from response headers (A Dictionary)\n:returns: Dictionary of 'remaining' (int), 'limit' (int), 'reset' (time)\"\"\"\nfn get_rate_limit_info(headers)\n\nfn get_result(response, return_args)\n\n\"\"\"Get access token for Twitter OAuth2 authentication.\n:param consumer_key: Twitter application consumer key\n:param consumer_secret: Twitter application consumer secret\"\"\"\nclass TwitterOAuth2(requests.auth.AuthBase):\n\tfn __init__(self, consumer_key, consumer_secret, proxies)\n\tfn _get_credentials(self)\n\tfn get_access_token(self)\n\tfn __call__(self, r)\n\n\n\"\"\"Raised when request fails\"\"\"\nclass TwitterError(Exception):\n\tfn __init__(self, status_code, error_messages)\n\tfn __str__(self)\n\n\n\"\"\"HTTP Status Code < HTTP_STATUS_INTERNAL_SERVER_ERROR (Should not retry)\"\"\"\nclass TwitterRequestError(TwitterError):\n\n\n\"\"\"HTTP Status Code >= HTTP_STATUS_INTERNAL_SERVER_ERROR (You may retry)\"\"\"\nclass TwitterServerError(TwitterError):\n\n\n\"\"\"HTTP Status Code 304 \"\"\"\nclass TwitterNotModifiedError(TwitterRequestError):\n\tfn __init__(self, error_messages)\n\n\n\"\"\"HTTP Status Code 400 \"\"\"\nclass TwitterBadRequestError(TwitterRequestError):\n\tfn __init__(self, error_messages)\n\n\n\"\"\"HTTP Status Code 401 \"\"\"\nclass TwitterUnauthorizedError(TwitterRequestError):\n\tfn __init__(self, error_messages)\n\n\n\"\"\"HTTP Status Code 403 \"\"\"\nclass TwitterForbiddenError(TwitterRequestError):\n\tfn __init__(self, error_messages)\n\n\n\"\"\"HTTP Status Code 404 \"\"\"\nclass TwitterNotFoundError(TwitterRequestError):\n\tfn __init__(self, error_messages)\n\n\n\"\"\"HTTP Status Code 410 \"\"\"\nclass TwitterGoneError(TwitterRequestError):\n\tfn __init__(self, error_messages)\n\n\n\"\"\"HTTP Status Code 420 \"\"\"\nclass TwitterEnhanceYourCalmError(TwitterRequestError):\n\tfn __init__(self, error_messages)\n\n\n\"\"\"HTTP Status Code 422 \"\"\"\nclass TwitterUnprocessableEntityError(TwitterRequestError):\n\tfn __init__(self, error_messages)\n\n\n\"\"\"HTTP Status Code 429 \"\"\"\nclass TwitterTooManyRequestsError(TwitterRequestError):\n\tfn __init__(self, error_messages)\n\n\n\"\"\"HTTP Status Code 500 \"\"\"\nclass TwitterInternalServerError(TwitterServerError):\n\tfn __init__(self, error_messages)\n\n\n\"\"\"HTTP Status Code 502 \"\"\"\nclass TwitterBadGatewayError(TwitterServerError):\n\tfn __init__(self, error_messages)\n\n\n\"\"\"HTTP Status Code 503 \"\"\"\nclass TwitterServiceUnavailableError(TwitterServerError):\n\tfn __init__(self, error_messages)\n\n\n\"\"\"HTTP Status Code 504 \"\"\"\nclass TwitterGatewayTimeoutError(TwitterServerError):\n\tfn __init__(self, error_messages)\n\n\nfn raise_twitter_error(status_code, error_messages)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# -*- coding: utf-8 -*-\n\nfrom .twitter_auth import TwitterOAuth2 \nfrom .twitter_error import *\nfrom .utils import generate_url\nfrom .variables import ENDPOINT_AUTH\n\nimport pytest\nimport requests\n\ndef test_get_access_token(requests_mock):\n    requests_mock.post(generate_url(ENDPOINT_AUTH), json={'token_type': 'bearer', 'access_token': 'ACCESSTOKENAAAAAAAAAALYX8wAAAAAAsgM6MD8%2F%2BqU75lSu8vhowzkfuZQ%3Db2MlUpIakaSUtLi0JsSSCsg53v5BTBPu9e3t4Bcgt7i5rzn6e9'})\n    consumer_key = 'key'\n    consumer_secret = 'secret'\n    client = TwitterOAuth2(consumer_key, consumer_secret)\n    token = client.get_access_token()\n    assert token  == 'ACCESSTOKENAAAAAAAAAALYX8wAAAAAAsgM6MD8%2F%2BqU75lSu8vhowzkfuZQ%3Db2MlUpIakaSUtLi0JsSSCsg53v5BTBPu9e3t4Bcgt7i5rzn6e9'\n\ndef test_get_access_token(requests_mock):\n    requests_mock.post(generate_url(ENDPOINT_AUTH), status_code=403, json={'errors': [{'code': 99, 'message': 'Unable to verify your credentials', 'label': 'authenticity_token_error'}]})\n    consumer_key = 'key'\n    consumer_secret = ''\n    client = TwitterOAuth2(consumer_key, consumer_secret)\n    expected = \"'code': 99, 'message': 'Unable to verify your credentials'\"\n    with pytest.raises(TwitterRequestError, match=expected) as exc_info:\n        token = client.get_access_token()\n    assert exc_info.value.errors == [{'code': 99, 'message': 'Unable to verify your credentials', 'label': 'authenticity_token_error'}]<|endoftext|>",
    "repo": "owensengoku/twitter-api"
  },
  {
    "prompt": "class PoolTestBase(TestScenarioBase):\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"\nCreated on Dec 8, 2017\n\n@author: nhan.nguyen\nVerify that user can open a pool ledger with valid data.\n\"\"\"\n\nfrom indy import pool\n\nfrom utilities import utils\nfrom utilities import common, constant\nfrom test_scripts.functional_tests.pool.pool_test_base import PoolTestBase\nimport pytest\n\n\nclass TestOpenPoolLedgerConfig(PoolTestBase):\n\n    @pytest.mark.asyncio\n    async def test(self):\n        # 1. Create pool ledger configuration.\n        self.steps.add_step(\"Create pool ledger config\")\n        await utils.perform(self.steps, common.create_pool_ledger_config,\n                            self.pool_name, constant.pool_genesis_txn_file)\n        # 2. Open pool ledger.\n        self.steps.add_step(\"Open pool ledger\")\n        self.pool_handle = await \\\n            utils.perform(self.steps, pool.open_pool_ledger,\n                          self.pool_name, None, ignore_exception=True)\n\n        # 3. Verify that returned pool_handle is a positive integer.\n        self.steps.add_step(\"Verify that returned pool_\"\n                            \"handle is a positive integer\")\n        utils.check(self.steps, error_message=\"Cannot open pool ledger\",\n                    condition=lambda: isinstance(self.pool_handle, int) and\n                    self.pool_handle >= 0)\n<|endoftext|>",
    "repo": "hyperledger/indy-post-install-automation"
  },
  {
    "prompt": "\"\"\"Load the UCI multiple features dataset [#1Data]_, taken from the UCI\nMachine Learning Repository at\nhttps://archive.ics.uci.edu/ml/datasets/Multiple+Features. This data set\nconsists of 6 views of handwritten digit images, with classes 0-9. The\n6 views are the following:\n\n1. 76 Fourier coefficients of the character shapes\n2. 216 profile correlations\n3. 64 Karhunen-Love coefficients\n4. 240 pixel averages of the images from 2x3 windows\n5. 47 Zernike moments\n6. 6 morphological features\n\nEach class contains 200 labeled examples.\n\nParameters\n----------\nselect_labeled : optional, array-like, shape (n_features,) default (all)\n    A list of the examples that the user wants by label. If not\n    specified, all examples in the dataset are returned. Repeated labels\n    are ignored.\n\nviews : optional, array-like, shape (n_views,) default (all)\n    A list of the data views that the user would like in the indicated\n    order. If not specified, all data views will be returned. Repeated\n    views are ignored.\n\nshuffle : bool, default=False\n    If ``True``, returns each array with its rows and corresponding\n    labels shuffled randomly according to ``random_state``.\n\nrandom_state : int, default=None\n    Determines the order data is shuffled if ``shuffle=True``. Used so\n    that data loaded is reproducible but shuffled.\n\nReturns\n-------\ndata : list of np.ndarray, each of size (200*num_classes, n_features)\n    List of length 6 with each element being the data for one of the\n    views.\n\nlabels : np.ndarray\n    Array of labels for the digit\n\nReferences\n----------\n.. [#1Data] M. van Breukelen, R.P.W. Duin, D.M.J. Tax, and J.E. den Hartog,\n        Handwritten digit recognition by combined classifiers, Kybernetika,\n        vol. 34, no. 4, 1998, 381-386\n\nExamples\n--------\n>>> from mvlearn.datasets import load_UCImultifeature\n>>> # Load 6-view dataset with all 10 classes\n>>> mv_data, labels = load_UCImultifeature()\n>>> print(len(mv_data))\n6\n>>> print([mv_data[i].shape for i in range(6)])\n[(2000, 76), (2000, 216), (2000, 64), (2000, 240), (2000, 47), (2000, 6)]\n>>> print(labels.shape)\n(2000,)\"\"\"\nfn load_UCImultifeature(select_labeled: optional, array-like, shape (n_features,) default (all), views: optional, array-like, shape (n_views,) default (all), shuffle: bool, default=False, random_state: int, default=None) -> np.ndarray\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Gavin Mischler\n# 10/17/2019\n\nimport pytest\nimport numpy as np\nfrom mvlearn.datasets.base import load_UCImultifeature\n\ndef test_UCImultifeature_dataloader():\n    # load data\n    data, labels = load_UCImultifeature()\n\n    assert len(data) == 6\n    assert labels.shape[0] == 2000\n\n    # check size of data\n    for i in range(6):\n        assert data[i].shape[0] == 2000\n\n    data1, labels1 = load_UCImultifeature()\n\n    # check data and labels are same\n    assert(np.allclose(data[0], data1[0]))\n    assert(np.allclose(labels, labels1))\n\ndef test_UCImultifeature_randomstate_sameordifferent():\n\n    # load data\n    data, labels = load_UCImultifeature(shuffle=True, random_state=2)\n    data1, labels1 = load_UCImultifeature(shuffle=True, random_state=5)\n    data2, labels2 = load_UCImultifeature(shuffle=True, random_state=2)\n    data3, labels3 = load_UCImultifeature(shuffle=False)\n\n    assert len(data) == 6\n    assert labels.shape[0] == 2000\n\n    # check size of data\n    for i in range(6):\n        assert data[i].shape[0] == 2000\n\n    # check data is same\n    for idx in range(6):\n        assert(np.allclose(data[idx], data2[idx]))\n        assert(not np.allclose(data[idx], data1[idx]))\n        assert(not np.allclose(data[idx], data3[idx]))\n        assert(not np.allclose(data1[idx], data3[idx]))\n\ndef test_UCImultifeature_dataloader_select_labels():\n    # load data\n    lab = [0,1,2]\n    data, labels = load_UCImultifeature(select_labeled=lab)\n\n    assert len(data) == 6\n\n    assert labels.shape[0] == 600\n    labels_set = list(set(labels))\n    assert len(labels_set) == len(lab)\n    for j, lab_in_set in enumerate(labels_set):\n        assert lab_in_set == lab[j]\n\n    # check size of data\n    for i in range(6):\n        assert data[i].shape[0] == 600\n\ndef test_UCImultifeature_dataloader_select_views():\n    # load data\n    views = [4, 5, 1]\n    o_data, o_labels = load_UCImultifeature()\n    data, labels = load_UCImultifeature(views=views)\n\n    assert len(data) == len(views)\n    assert labels.shape[0] == 2000\n\n    # Check the shape of the data\n    for i in range(len(views)):\n        assert data[i].shape == o_data[views[i]].shape\n\n        \ndef test_UCImultifeature_dataloader_badselect():\n    with pytest.raises(ValueError):\n        data, labels = load_UCImultifeature(select_labeled=[])\n\ndef test_UCImultifeature_dataloader_badselect2():\n    long_list = [0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n    with pytest.raises(ValueError):\n        data, labels = load_UCImultifeature(select_labeled=long_list)\n\ndef test_UCImultifeature_dataloader_badselect3():\n    bad_list = [0,2,4,-2]\n    with pytest.raises(ValueError):\n        data, labels = load_UCImultifeature(select_labeled=bad_list)\n\ndef test_dataloader_badviews1():\n    v_list = []\n    with pytest.raises(ValueError):\n        data, labels = load_UCImultifeature(views=v_list)\n\ndef test_dataloader_badviews2():\n    v_list = [4, 6]\n    with pytest.raises(ValueError):\n        data, labels = load_UCImultifeature(views=v_list)\n<|endoftext|>",
    "repo": "pierreablin/mvlearn"
  },
  {
    "prompt": "\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from __future__ import annotations\n\nimport threading\nimport time\nfrom typing import Any, List\nfrom unittest import TestCase\n\nimport pytest\n\nfrom joulehunter.low_level.stat_profile import setstatprofile\n\nfrom ..util import busy_wait, do_nothing\n\n\nclass CallCounter:\n    def __init__(self, thread) -> None:\n        self.thread = thread\n        self.count = 0\n\n    def __call__(self, *args: Any, **kwds: Any) -> Any:\n        assert self.thread is threading.current_thread()\n        self.count += 1\n\n\ndef test_threaded():\n    # assert that each thread gets its own callbacks, and check that it\n    # doesn't crash!\n\n    counters: list[CallCounter | None] = [None for _ in range(10)]\n    stop = False\n\n    def profile_a_busy_wait(i):\n        thread = threads[i]\n        counter = CallCounter(thread)\n        counters[i] = counter\n\n        setstatprofile(counter, 0.001)\n        while not stop:\n            do_nothing()\n        setstatprofile(None)\n\n    threads = [threading.Thread(target=profile_a_busy_wait, args=(i,)) for i in range(10)]\n    for thread in threads:\n        thread.start()\n\n    while not stop:\n        stop = all(c is not None and c.count > 10 for c in counters)\n\n    for thread in threads:\n        thread.join()\n<|endoftext|>",
    "repo": "akaminetzkyp/pyinstrument"
  },
  {
    "prompt": "class BaseSerializer(contracts.RpcEntitySerializer):\n\tfn __init__(self, version: str)\n\tfn handle_request(self, data: dict) -> protocol.RpcRequest\n\tfn handle_notification(self, data: dict) -> protocol.RpcNotification\n\tfn handle_result(self, data: dict) -> protocol.RpcResult\n\tfn handle_error(self, data: dict) -> protocol.RpcEntity\n\tfn handle_entity(self, data: dict) -> protocol.RpcEntity\n\n\nclass JsonSerializer(BaseSerializer):\n\tfn __init__(self, encoding: str, version: str)\n\tfn entity_to_bytes(self, entity: protocol.RpcEntity) -> bytes\n\tfn bytes_to_entity(self, data: bytes) -> protocol.RpcEntity\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from jsonrpc_stream.serializers import JsonSerializer\nfrom jsonrpc_stream import protocol as pro\n\nimport json\nimport pytest\n\n\n@pytest.fixture\ndef utf8() -> JsonSerializer: return JsonSerializer('utf-8')\n\n\ndef test_request_to_bytes(utf8: JsonSerializer):\n    r = pro.RpcRequest(0, 'kek', None)\n    data = {'jsonrpc': \"2.0\", 'method': 'kek', 'id': 0}\n    serialized = json.loads(\n        utf8.entity_to_bytes(r).decode('utf-8')\n    )\n    assert serialized == data\n\n\ndef test_bytes_to_request(utf8: JsonSerializer):\n    r = pro.RpcRequest(0, 'kek', None)\n    expectation = r.to_dict()\n    reality = utf8.bytes_to_entity(utf8.entity_to_bytes(r)).to_dict()\n\n    assert expectation == reality\n\n\ndef test_bytes_to_notification(utf8: JsonSerializer):\n    r = pro.RpcNotification('kek', None)\n    expectation = r.to_dict()\n    reality = utf8.bytes_to_entity(utf8.entity_to_bytes(r)).to_dict()\n\n    assert expectation == reality\n\n\ndef test_bytes_to_result(utf8: JsonSerializer):\n    r = pro.RpcResult(0, 'yeet')\n    expectation = r.to_dict()\n    reality = utf8.bytes_to_entity(utf8.entity_to_bytes(r)).to_dict()\n\n    assert expectation == reality\n\n\ndef test_bytes_to_error_nodata(utf8: JsonSerializer):\n    r = pro.RpcError(\n        0, pro.RpcErrorDetails(\n            123, 'yeet', None\n        )\n    )\n    expectation = r.to_dict()\n    reality = utf8.bytes_to_entity(utf8.entity_to_bytes(r)).to_dict()\n\n    assert expectation == reality\n\n\ndef test_bytes_to_error_data(utf8: JsonSerializer):\n    r = pro.RpcError(\n        0, pro.RpcErrorDetails(\n            123, 'yeet', 'additional errorz'\n        )\n    )\n    expectation = r.to_dict()\n    reality = utf8.bytes_to_entity(utf8.entity_to_bytes(r)).to_dict()\n\n    assert expectation == reality\n\n\ndef test_deserialize_doesnt_raise(utf8: JsonSerializer):\n    malformed = utf8.bytes_to_entity(b'{\"jsonrpc\": \"2.0\", \"kek\": \"top\"}')\n    assert isinstance(malformed, pro.RpcMalformed)\n    assert malformed.id is None\n\n\ndef test_deserialize_doesnt_raise_with_id(utf8: JsonSerializer):\n    malformed = utf8.bytes_to_entity(b'{\"jsonrpc\": \"2.0\", \"id\": \"yeet\"}')\n    assert isinstance(malformed, pro.RpcMalformed)\n    assert malformed.id == 'yeet'\n\n\ndef test_parser_error_doesnt_raise(utf8: JsonSerializer):\n    malformed = utf8.bytes_to_entity(b'{yeet}')\n    assert isinstance(malformed, pro.RpcMalformed)\n    assert malformed.id is None\n\n\ndef test_batch_with_malformed_to_entity(utf8: JsonSerializer):\n    r = [\n        {\"jsonrpc\": \"2.0\", \"method\": \"sum\", \"params\": [1, 2, 4], \"id\": \"1\"},\n        {\"jsonrpc\": \"2.0\", \"method\": \"notify_hello\", \"params\": [7]},\n        {\"jsonrpc\": \"2.0\", \"method\": \"a\", \"params\": [42, 23], \"id\": \"2\"},\n        {\"foo\": \"boo\"},\n        {\"jsonrpc\": \"2.0\", \"method\": \"b\", \"params\": {\"x\": \"y\"}, \"id\": \"5\"},\n        {\"jsonrpc\": \"2.0\", \"method\": \"get_data\", \"id\": \"9\"}\n    ]\n    reality = utf8.bytes_to_entity(json.dumps(r).encode())\n    expectation = pro.RpcBatch(entities=[\n        pro.RpcRequest(id='1', method='sum', params=[1, 2, 4], jsonrpc='2.0'),\n        pro.RpcNotification(method='notify_hello', params=[7], jsonrpc='2.0'),\n        pro.RpcRequest(id='2', method='a', params=[42, 23], jsonrpc='2.0'),\n        pro.RpcRequest(id='5', method='b', params={'x': 'y'}, jsonrpc='2.0'),\n        pro.RpcRequest(id='9', method='get_data', params=None, jsonrpc='2.0')]\n    )\n\n    assert isinstance(reality, pro.RpcBatch)\n    #  test if all entities are present except the malformed one\n    #  because exceptions are sadly not equatable\n    matches = len([x for x in expectation.entities if x in reality.entities])\n    assert matches == len(expectation.entities)\n    x = [x for x in reality.entities if isinstance(x, pro.RpcMalformed)]\n    assert len(x) == 1\n    assert not x[0].id\n<|endoftext|>",
    "repo": "sum-catnip/jsonrpc_stream"
  },
  {
    "prompt": "\"\"\"trims trailing data from the central directory\ncode taken from http://stackoverflow.com/a/7457686/570216, courtesy of Uri Cohen\"\"\"\nfn repair_central_directory(zipFile, is_file_instance)\n\n\"\"\"Check the file is a valid zipfile\"\"\"\nfn _validate_archive(filename)\n\nfn _find_workbook_part(package)\n\n\"\"\"Open the given filename and return the workbook\n\n:param filename: the path to open or a file-like object\n:type filename: string or a file-like object open in binary mode c.f., :class:`zipfile.ZipFile`\n\n:param read_only: optimised for reading, content cannot be edited\n:type read_only: bool\n\n:param keep_vba: preseve vba content (this does NOT mean you can use it)\n:type keep_vba: bool\n\n:param guess_types: guess cell content type and do not read it from the file\n:type guess_types: bool\n\n:param data_only: controls whether cells with formulae have either the formula (default) or the value stored the last time Excel read the sheet\n:type data_only: bool\n\n:param keep_links: whether links to external workbooks should be preserved. The default is True\n:type keep_links: bool\n\n:rtype: :class:`openpyxl2.workbook.Workbook`\n\n.. note::\n\n    When using lazy load, all worksheets will be :class:`openpyxl.worksheet.iter_worksheet.IterableWorksheet`\n    and the returned workbook will be read-only.\"\"\"\nfn load_workbook(filename: string or a file-like object open in binary mode c.f., :class:`zipfile.ZipFile`, read_only: bool, keep_vba: bool, data_only: bool, guess_types: bool, keep_links: bool) -> :class:`openpyxl2.workbook.Workbook`\n.. note::\n\n    When using lazy load, all worksheets will be :class:`openpyxl.worksheet.iter_worksheet.IterableWorksheet`\n    and the returned workbook will be read-only.\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from __future__ import absolute_import\n# Copyright (c) 2010-2018 openpyxl\n\nimport pytest\n\n# compatibility imports\nfrom openpyxl2.compat import unicode\n\n# package imports\nfrom openpyxl2.styles import numbers\nfrom openpyxl2.reader.excel import load_workbook\n\n\n@pytest.mark.parametrize(\"cell, number_format\",\n                    [\n                        ('A1', numbers.FORMAT_GENERAL),\n                        ('A2', numbers.FORMAT_DATE_XLSX14),\n                        ('A3', numbers.FORMAT_NUMBER_00),\n                        ('A4', numbers.FORMAT_DATE_TIME3),\n                        ('A5', numbers.FORMAT_PERCENTAGE_00),\n                    ]\n                    )\ndef test_read_general_style(datadir, cell, number_format):\n    datadir.join(\"genuine\").chdir()\n    wb = load_workbook('empty-with-styles.xlsx')\n    ws = wb[\"Sheet1\"]\n    assert ws[cell].number_format == number_format\n\n\ndef test_read_no_theme(datadir):\n    datadir.join(\"genuine\").chdir()\n    wb = load_workbook('libreoffice_nrt.xlsx')\n    assert wb\n\n\n@pytest.mark.parametrize(\"guess_types, dtype\",\n                         (\n                             (True, float),\n                             (False, unicode),\n                         )\n                        )\ndef test_guess_types(datadir, guess_types, dtype):\n    datadir.join(\"genuine\").chdir()\n    wb = load_workbook('guess_types.xlsx', guess_types=guess_types)\n    ws = wb.active\n    assert isinstance(ws['D2'].value, dtype)\n<|endoftext|>",
    "repo": "j5int/openpyxl2"
  },
  {
    "prompt": "\"\"\"Serves as a base exception for all exceptions in this library.\n\n.. todo::\n    Ensure this is actually the case given we call into both ``aiodocker``\n    and ``docker``\"\"\"\nclass DockerException(Exception):\n\n\n\"\"\"Raised when the end of a stream is not valid.\"\"\"\nclass ChunkedStreamingError(RuntimeError):\n\n\n\"\"\"An attribute or function that to to be removed before reaching version 1.\"\"\"\nclass DeprecatedInVersion1(DeprecationWarning):\n\n\n\"\"\"An attribute or function that is kept for ``aiodocker`` compatibility.\n\nThese will eventually be removed to ensure a consistent API\"\"\"\nclass AioDockerDeprecationWarning(DeprecatedInVersion1):\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import asyncio\n\nimport pytest\n\nimport adocker.api as a_api\nimport adocker.errors as a_errors\n\n\n@pytest.fixture()\ndef adocker_api(request, event_loop):\n    asyncio.set_event_loop(event_loop)\n    with pytest.warns(a_errors.AioDockerDeprecationWarning):\n        api = a_api.APIClient()\n\n    def cleanup():\n        event_loop.run_until_complete(api.close())\n    request.addfinalizer(cleanup)\n    return api\n<|endoftext|>",
    "repo": "leesdolphin/adocker"
  },
  {
    "prompt": "\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright (c) 2022 OpenCyphal\n# This software is distributed under the terms of the MIT License.\n# Author: Pavel Kirienko <pavel@opencyphal.org>\n\nfrom __future__ import annotations\nimport asyncio\nfrom typing import Any\nimport pytest\nimport pycyphal\nfrom pycyphal.transport.loopback import LoopbackTransport\nfrom yakut.subject_specifier_processor import process_subject_specifier, SubjectResolver\nfrom yakut.subject_specifier_processor import BadSpecifierError, NoFixedPortIDError, NetworkDiscoveryError\nfrom yakut import dtype_loader\n\n\n@pytest.mark.asyncio\nasync def _unittest_without_subject_resolver(compiled_dsdl: Any) -> None:\n    _ = compiled_dsdl\n    asyncio.get_running_loop().slow_callback_duration = 5.0\n\n    import uavcan.primitive\n    import uavcan.node\n\n    def get_subject_resolver() -> SubjectResolver:\n        raise RuntimeError(\"Subject resolver shall not be used in this test\")\n\n    async def once(specifier: str) -> tuple[int, Any]:\n        return await process_subject_specifier(specifier, get_subject_resolver)\n\n    assert (123, uavcan.primitive.Empty_1) == await once(\"123:uavcan.primitive.Empty.1\")\n\n    fpid = pycyphal.dsdl.get_fixed_port_id(uavcan.node.Heartbeat_1)\n    assert fpid is not None\n    assert (fpid, uavcan.node.Heartbeat_1) == await once(\"uavcan.node.Heartbeat.1\")\n\n    with pytest.raises(BadSpecifierError):\n        await once(\"99999999:uavcan.primitive.Empty.1\")\n\n    with pytest.raises(BadSpecifierError):\n        await once(\"not_a_number:uavcan.primitive.Empty.1\")\n\n    with pytest.raises(dtype_loader.FormatError):\n        await once(\"bad:format:error\")\n\n    with pytest.raises(dtype_loader.FormatError):\n        await once(\"123:123.123.123\")\n\n    with pytest.raises(dtype_loader.NotFoundError):\n        await once(\"123:uavcan.primitive.Empty.1.250\")\n\n    with pytest.raises(NoFixedPortIDError):\n        await once(\"uavcan.primitive.Empty\")\n\n\n@pytest.mark.asyncio\nasync def _unittest_with_subject_resolver(compiled_dsdl: Any) -> None:\n    _ = compiled_dsdl\n    asyncio.get_running_loop().slow_callback_duration = 5.0\n\n    from pycyphal.application import make_node, NodeInfo, register\n    import uavcan.primitive.scalar\n\n    local_node = make_node(info=NodeInfo(), transport=LoopbackTransport(2222))\n    subject_resolver = SubjectResolver(local_node)\n\n    def advertise(kind: str, name: str, dtype_name: str, port_id: int) -> None:\n        local_node.registry[f\"uavcan.{kind}.{name}.id\"] = register.ValueProxy(register.Natural16([port_id]))\n        local_node.registry[f\"uavcan.{kind}.{name}.type\"] = register.ValueProxy(register.String(dtype_name))\n\n    advertise(\"pub\", \"foo\", \"uavcan.primitive.scalar.Bit.1.200\", 500)\n    advertise(\"sub\", \"bar\", \"uavcan.primitive.scalar.Integer8.1.200\", 600)  # minor version ignored\n    advertise(\"pub\", \"bar\", \"uavcan.node.GetInfo.1.0\", 600)  # service type ignored\n    advertise(\"pub\", \"bad\", \"nonexistent.DataType.1.0\", 600)  # nonexistent type ignored\n    advertise(\"sub\", \"baz\", \"uavcan.primitive.scalar.Real16.1.200\", 0xFFFF)\n\n    local_node.start()\n\n    async def once(specifier: str) -> tuple[int, Any]:\n        return await process_subject_specifier(specifier, lambda: subject_resolver)\n\n    assert (500, uavcan.primitive.scalar.Bit_1_0) == await once(\"500\")\n    assert (600, uavcan.primitive.scalar.Integer8_1) == await once(\"600\")  # minor version ignored\n\n    with pytest.raises(NetworkDiscoveryError):\n        await once(\"700\")  # nonexistent\n\n    subject_resolver.close()\n    local_node.close()\n    await asyncio.sleep(1.0)\n<|endoftext|>",
    "repo": "pavel-kirienko/un"
  },
  {
    "prompt": "fn z_norm(a, axis, threshold)\n\nfn distance(a, b, axis)\n\nfn apply_exclusion_zone(D, trivial_idx, excl_zone)\n\nfn distance_profile(Q, T, m)\n\nfn distance_matrix(T_A, T_B, m)\n\nfn mass(Q, T, m, trivial_idx, excl_zone, ignore_trivial)\n\nfn stamp(T_A, m, T_B, exclusion_zone)\n\n\"\"\"Traverse distance matrix along the diagonals and update the matrix profile and\nmatrix profile indices\"\"\"\nfn stump(T_A, m, T_B, exclusion_zone)\n\nfn replace_inf(x, value)\n\nfn multi_mass(Q, T, m, include, discords)\n\nfn PI(D, trivial_idx, excl_zone)\n\nfn mstump(T, m, excl_zone, include, discords)\n\nfn get_array_ranges(a, n_chunks, truncate)\n\nfn aamp(T_A, m, T_B, exclusion_zone)\n\nclass aampi_egress(object):\n\tfn __init__(self, T, m, excl_zone)\n\tfn update(self, t)\n\n\nclass stumpi_egress(object):\n\tfn __init__(self, T, m, excl_zone)\n\tfn update(self, t)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import numpy as np\nimport numpy.testing as npt\nimport pandas as pd\nfrom stumpy import stumped, core\nfrom dask.distributed import Client, LocalCluster\nimport pytest\nimport warnings\nimport naive\n\n\n@pytest.fixture(scope=\"module\")\ndef dask_cluster():\n    cluster = LocalCluster(n_workers=2, threads_per_worker=2)\n    yield cluster\n    cluster.close()\n\n\ntest_data = [\n    (\n        np.array([9, 8100, -60, 7], dtype=np.float64),\n        np.array([584, -11, 23, 79, 1001, 0, -19], dtype=np.float64),\n    ),\n    (\n        np.random.uniform(-1000, 1000, [8]).astype(np.float64),\n        np.random.uniform(-1000, 1000, [64]).astype(np.float64),\n    ),\n]\n\nsubstitution_locations = [0, -1, slice(1, 3), [0, 3]]\n\n\n@pytest.mark.filterwarnings(\"ignore:numpy.dtype size changed\")\n@pytest.mark.filterwarnings(\"ignore:numpy.ufunc size changed\")\n@pytest.mark.filterwarnings(\"ignore:numpy.ndarray size changed\")\n@pytest.mark.filterwarnings(\"ignore:\\\\s+Port 8787 is already in use:UserWarning\")\n@pytest.mark.parametrize(\"T_A, T_B\", test_data)\n@pytest.mark.parametrize(\"substitution_location_B\", substitution_locations)\ndef test_stumped_one_subsequence_inf_A_B_join(\n    T_A, T_B, substitution_location_B, dask_cluster\n):\n    with Client(dask_cluster) as dask_client:\n        m = 3\n\n        T_B_sub = T_B.copy()\n        T_B_sub[substitution_location_B] = np.inf\n\n        ref_mp = naive.stump(T_A, m, T_B=T_B_sub)\n        comp_mp = stumped(dask_client, T_A, m, T_B_sub, ignore_trivial=False)\n        naive.replace_inf(ref_mp)\n        naive.replace_inf(comp_mp)\n        npt.assert_almost_equal(ref_mp, comp_mp)\n<|endoftext|>",
    "repo": "jrbourbeau/stumpy"
  },
  {
    "prompt": "\"\"\"Extracts an hour of test data for one hour and ground truth prediction\n5,10,15,30,45 and 60 minutes into the future.\n\nParameters\n----------\n\ndata\n    tensor of shape (24+, 495, 436, 8) of  type uint8\noffset\nto_torch:bool\n    convert to torch float tensor.\n\nReturns\n-------\n    test_data\n        tensor of shape (12, 495, 436, 8) of  type uint8\n    ground_truth_prediction\n        tensor of shape (6, 495, 436, 8) of  type uint8\"\"\"\nfn prepare_test(data: np.ndarray, offset: None, to_torch: bool) -> Tuple[(Union[(np.ndarray, torch.Tensor)], Union[(np.ndarray, torch.Tensor)])]\n\n\"\"\"Parameters\n----------\noffset: int\n\nReturns\n-------\n    the 6 indices for the prediction horizon, i.e. offset+12, offset+13, ...., offset+23\"\"\"\nfn prepare_within_day_indices_for_ground_truth(offset: int) -> np.ndarray\n\nfn _generate_test_slots()\n\nfn _create_test_manifest(config, test_slot_directory)\n\nfn _check_test_manifest(config, test_slot_directory)\n\nfn _check_exists_dir(city_output_directory)\n\nfn _get_contents_from_tarballs_and_directories(dynamic_data_path, check_tars, check_dirs) -> Dict[(Tuple[(str, str)], Tuple[(str, str)])]\n\nfn _extract_h5_for_city_and_date(city: str, city_date_finder, date: str, output_directory: str)\n\nfn _prepare_test_slots_city(arg, city_date_finder, competition, expected_number_of_slots, output_directory, output_directory_ground_truth, tempdir)\n\nfn _extract_and_slice_h5_files_for_testing(city_date_finder, output_directory, output_directory_ground_truth, test_slots_dir, expected_number_of_slots_per_city)\n\nfn _check_expanded_test_slots(expanded_test_slots, city_date_finder, output_directory, output_directory_ground_truth)\n\n\"\"\"Creates the argument parser for this program.\"\"\"\nfn create_parser() -> argparse.ArgumentParser\n\nfn main(params)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "#  Copyright 2021 Institute of Advanced Research in Artificial Intelligence (IARAI) GmbH.\n#  IARAI licenses this file to You under the Apache License, Version 2.0\n#  (the \"License\"); you may not use this file except in compliance with\n#  the License. You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\nimport numpy as np\nimport pytest\n\nfrom competition.prepare_test_data.prepare_test_data import prepare_test\nfrom data.data_layout import channel_labels\nfrom data.data_layout import speed_channel_indices\nfrom data.data_layout import static_channel_labels\nfrom data.data_layout import volume_channel_indices\n\n\n@pytest.mark.parametrize(\"offset\", [0, 5, 6])\ndef test_prepare_test(offset):\n    data = np.random.rand(24 + offset, 495, 436, 8)\n    test_data, ground_truth_prediction = prepare_test(data=data, offset=offset)\n    assert test_data.shape == (12, 495, 436, 8)\n    assert ground_truth_prediction.shape == (6, 495, 436, 8)\n    ub = offset + 12\n    assert (test_data == data[offset:ub]).all()\n    # 5,10,15,30,45 and 60 into the future\n    assert (ground_truth_prediction[0] == data[offset + 11 + 5 // 5]).all()\n    assert (ground_truth_prediction[1] == data[offset + 11 + 10 // 5]).all()\n    assert (ground_truth_prediction[2] == data[offset + 11 + 15 // 5]).all()\n    assert (ground_truth_prediction[3] == data[offset + 11 + 30 // 5]).all()\n    assert (ground_truth_prediction[4] == data[offset + 11 + 45 // 5]).all()\n    assert (ground_truth_prediction[5] == data[offset + 11 + 60 // 5]).all()\n\n\ndef test_channel_labels():\n    print(channel_labels)\n    print(static_channel_labels)\n    print(volume_channel_indices)\n    print(speed_channel_indices)\n<|endoftext|>",
    "repo": "shehel/traffic_forecasting"
  },
  {
    "prompt": "fn random_lower_string() -> str\n\nfn random_email() -> str\n\nfn get_superuser_token_headers(client: TestClient) -> Dict[(str, str)]\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from typing import Dict, Generator\n\nimport pytest\nfrom fastapi.testclient import TestClient\n\nfrom db_api.main import app\n\nfrom .utils import get_superuser_token_headers\n\n\n@pytest.fixture(scope=\"module\")\ndef client() -> Generator:\n    with TestClient(app) as c:\n        yield c\n\n\n@pytest.fixture(scope=\"module\")\ndef superuser_token_headers(client: TestClient) -> Dict[str, str]:\n    return get_superuser_token_headers(client)\n<|endoftext|>",
    "repo": "JungeAlexander/kbase_db_api"
  },
  {
    "prompt": "fn wait_for_socket(ipc_path, timeout)\n\nfn wait_for_http(endpoint_uri, timeout)\n\nfn wait_for_popen(proc, timeout)\n\nfn kill_proc_gracefully(proc)\n\nclass GoPlatonTest(Web3ModuleTest):\n\tfn _check_web3_clientVersion(self, client_version)\n\n\nclass GoPlatonPlatonModuleTest(PlatonModuleTest):\n\tfn test_platon_sign_typed_data(self, web3, unlocked_account_dual_type)\n\tfn test_invalid_platon_sign_typed_data(self, web3, unlocked_account_dual_type)\n\tfn test_platon_protocol_version(self, web3)\n\tfn test_platon_protocolVersion(self, web3)\n\n\nclass GoPlatonVersionModuleTest(VersionModuleTest):\n\tfn test_platon_protocol_version(self, web3)\n\tfn test_platon_protocolVersion(self, web3)\n\n\nclass GoPlatonNetModuleTest(NetModuleTest):\n\n\nclass GoPlatonAdminModuleTest(GoPlatonAdminModuleTest):\n\n\nclass GoPlatonPersonalModuleTest(GoPlatonPersonalModuleTest):\n\n\nclass GoPlatonAsyncPlatonModuleTest(AsyncPlatonModuleTest):\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom tests.utils import (\n    get_open_port,\n)\nfrom platon import Web3\nfrom platon.platon import (\n    AsyncPlaton,\n)\nfrom platon.middleware import (\n    async_buffered_gas_estimate_middleware,\n    async_gas_price_strategy_middleware,\n)\nfrom platon.providers.async_rpc import (\n    AsyncHTTPProvider,\n)\n\nfrom .common import (\n    GoPlatonAdminModuleTest,\n    GoPlatonAsyncPlatonModuleTest,\n    GoPlatonPlatonModuleTest,\n    GoPlatonNetModuleTest,\n    GoPlatonPersonalModuleTest,\n    GoPlatonTest,\n    GoPlatonVersionModuleTest,\n)\nfrom .utils import (\n    wait_for_aiohttp,\n    wait_for_http,\n)\n\n\n@pytest.fixture(scope=\"module\")\ndef rpc_port():\n    return get_open_port()\n\n\n@pytest.fixture(scope=\"module\")\ndef endpoint_uri(rpc_port):\n    return 'http://localhost:{0}'.format(rpc_port)\n\n\ndef _node_command_arguments(rpc_port,\n                            base_node_command_arguments,\n                            node_version):\n    yield from base_node_command_arguments\n    if node_version.major == 1:\n        yield from (\n            '--http',\n            '--http.port', rpc_port,\n            '--http.api', 'admin,platon,net,platon,personal,miner',\n            '--ipcdisable',\n            '--allow-insecure-unlock'\n        )\n    else:\n        raise AssertionError(\"Unsupported Node version\")\n\n\n@pytest.fixture(scope='module')\ndef node_command_arguments(rpc_port,\n                           base_node_command_arguments,\n                           get_node_version):\n\n    return _node_command_arguments(\n        rpc_port,\n        base_node_command_arguments,\n        get_node_version\n    )\n\n\n@pytest.fixture(scope=\"module\")\ndef web3(node_process, endpoint_uri):\n    wait_for_http(endpoint_uri)\n    _web3 = Web3(Web3.HTTPProvider(endpoint_uri))\n    return _web3\n\n\n@pytest.fixture(scope=\"module\")\nasync def async_w3(node_process, endpoint_uri):\n    await wait_for_aiohttp(endpoint_uri)\n    _web3 = Web3(\n        AsyncHTTPProvider(endpoint_uri),\n        middlewares=[\n            async_gas_price_strategy_middleware,\n            async_buffered_gas_estimate_middleware\n        ],\n        modules={'platon': (AsyncPlaton,)})\n    return _web3\n\n\nclass TestGoPlatonTest(GoPlatonTest):\n    pass\n\n\nclass TestGoPlatonAdminModuleTest(GoPlatonAdminModuleTest):\n    @pytest.mark.xfail(reason=\"running node with the --nodiscover flag doesn't allow peer addition\")\n    def test_admin_peers(self, web3: \"Web3\") -> None:\n        super().test_admin_peers(web3)\n\n    def test_admin_start_stop_rpc(self, web3: \"Web3\") -> None:\n        # This test causes all tests after it to fail on CI if it's allowed to run\n        pytest.xfail(reason='Only one RPC endpoint is allowed to be active at any time')\n        super().test_admin_start_stop_rpc(web3)\n\n    def test_admin_start_stop_ws(self, web3: \"Web3\") -> None:\n        # This test causes all tests after it to fail on CI if it's allowed to run\n        pytest.xfail(reason='Only one WS endpoint is allowed to be active at any time')\n        super().test_admin_start_stop_ws(web3)\n\n\nclass TestGoPlatonPlatonModuleTest(GoPlatonPlatonModuleTest):\n    pass\n\n\nclass TestGoPlatonVersionModuleTest(GoPlatonVersionModuleTest):\n    pass\n\n\nclass TestGoPlatonNetModuleTest(GoPlatonNetModuleTest):\n    pass\n\n\nclass TestGoPlatonPersonalModuleTest(GoPlatonPersonalModuleTest):\n    pass\n\n\nclass TestGoPlatonAsyncPlatonModuleTest(GoPlatonAsyncPlatonModuleTest):\n    pass\n<|endoftext|>",
    "repo": "shinnng/platon.py"
  },
  {
    "prompt": "fn rpc(method: str, param_dict: Optional[dict], _id: str) -> dict\n\n\"\"\"Represents a signal-cli/auxin-cli session.\nLifecycle: Downloads the datastore, runs and restarts signal client,\ntries to gracefully kill signal and upload before exiting.\nI/O: reads signal client's output into inbox,\nhas methods for sending commands to the signal client, and\nactually writes those json blobs to signal client's stdin.\"\"\"\nclass Signal:\n\tfn __init__(self, bot_number: Optional[str]) -> None\n\tsigints = 0\n\t\"\"\"Try to start async_shutdown and/or just sys.exit\"\"\"\n\tfn sync_signal_handler(self) -> None\n\t\"\"\"Done callback which logs task done result and/or restarts a task on error\n\targs:\n\t    task (asyncio.task): Finished task\n\t    _func (AsyncFunc): Async function to restart\n\t    f_args (dict): Args to pass to function on restart\"\"\"\n\tfn handle_task(self, task: asyncio.Task) -> None\n\tpending_requests: dict[str, asyncio.Future[Message]] = {}\n\tpending_messages_sent: dict[str, dict] = {}\n\tbackoff = False\n\tmessages_until_rate_limit = 50.0\n\tlast_update = time.time()\n\t\"\"\"Returns whether we think signal server will rate limit us for sending a\n\tmessage right now\"\"\"\n\tfn update_and_check_rate_limit(self) -> bool\n\n\nfn is_admin(msg: Message) -> bool\n\nfn requires_admin(command: Callable) -> Callable\n\nfn hide(command: Callable) -> Callable\n\n\"\"\"Handles messages and command dispatch, as well as basic commands.\nMust be instantiated within a running async loop.\nSubclass this with your own commands.\"\"\"\nclass Bot(Signal):\n\t\"\"\"Creates AND STARTS a bot that routes commands to do_x handlers\"\"\"\n\tfn __init__(self, bot_number: Optional[str]) -> None\n\tfn is_command(self, msg: Message) -> bool\n\tfn match_command(self, msg: Message) -> str\n\tfn documented_commands(self) -> str\n\t\"\"\"Returns a list of all known recipients by parsing underlying datastore.\"\"\"\n\tfn get_recipients(self) -> list[dict[(str, str)]]\n\t\"\"\"Queries the recipients-store file for a UUID, provided a phone number.\"\"\"\n\tfn get_uuid_by_phone(self, phonenumber: str) -> Optional[str]\n\t\"\"\"Queries the recipients-store file for a phone number, provided a uuid.\"\"\"\n\tfn get_number_by_uuid(self, uuid_: str) -> Optional[str]\n\n\nclass PayBot(Bot):\n\tPAYMENTS_HELPTEXT = 'Enable Signal Pay:\\n\\n    1. In Signal, tap \u201c\ud83e\udc14\u201c & tap on your profile icon in the top left & tap *Settings*\\n\\n    2. Tap *Payments* & tap *Activate Payments*\\n\\n    For more information on Signal Payments visit:\\n\\n    https://support.signal.org/hc/en-us/articles/360057625692-In-app-Payments'\n\n\n\"\"\"A common pattern is to store intermediate state for individual users as a dictionary.\nUsers can be referred to by some combination of source (a phone number) or uuid (underlying user ID)\nThis abstracts over the possibility space, returning a boolean indicator of whether the sender of a Message\nis referenced in a dict, and the value pointed at (if any).\"\"\"\nfn get_source_or_uuid_from_dict(msg: Message, dict_: dict[(str, Any)]) -> Tuple[(bool, Any)]\n\nfn is_first_device(msg: Message) -> bool\n\nclass QuestionBot(PayBot):\n\tfn __init__(self, bot_number: Optional[str]) -> None\n\n\nfn fmt_ms(ts: int) -> str\n\nfn run_bot(bot: Type[Bot], local_app: web.Application) -> None\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import asyncio\nimport os\nimport pathlib\nfrom importlib import reload\nimport pytest\nfrom forest import utils\nfrom forest.core import Message, QuestionBot\n\n\ndef test_secrets(tmp_path: pathlib.Path) -> None:\n    open(tmp_path / \"dev_secrets\", \"w\").write(\"A=B\\nC=D\")\n    os.chdir(tmp_path)\n    reload(utils)\n\n    assert utils.get_secret(\"A\") == \"B\"\n    assert utils.get_secret(\"C\") == \"D\"\n    assert utils.get_secret(\"E\") == \"\"\n\n\ndef test_root(tmp_path: pathlib.Path) -> None:\n    assert reload(utils).ROOT_DIR == \".\"\n    os.chdir(tmp_path)\n    open(tmp_path / \"dev_secrets\", \"w\").write(\"DOWNLOAD=1\")\n    assert reload(utils).ROOT_DIR == \"/tmp/local-signal\"\n    os.environ[\"FLY_APP_NAME\"] = \"A\"\n    assert reload(utils).ROOT_DIR == \"/app\"\n\n\nclass MockMessage(Message):\n    def __init__(self, text: str) -> None:\n        self.text = text\n        self.source = \"+\" + \"2\" * 11\n        self.uuid = \"cf3d7d34-2dcd-4fcd-b193-cbc6a666758b\"\n        super().__init__({})\n\n\nclass MockBot(QuestionBot):\n    async def start_process(self) -> None:\n        pass\n\n    async def get_output(self, text: str) -> str:\n        await self.inbox.put(MockMessage(text))\n        try:\n            msg = await asyncio.wait_for(self.outbox.get(), timeout=1)\n            return msg[\"params\"][\"message\"]\n        except asyncio.TimeoutError:\n            return \"\"\n\n\nalice = \"+\" + \"1\" * 11\n\n\n@pytest.mark.asyncio\nasync def test_commands() -> None:\n    bot = MockBot(alice)\n    os.environ[\"ENABLE_MAGIC\"] = \"1\"\n    assert await bot.get_output(\"/pingg foo\") == \"/pong foo\"\n    # slightly slow\n    # assert \"printer\" in (await bot.get_output(\"/printerfactt\")).lower()\n    assert (await bot.get_output(\"uptime\")).startswith(\"Uptime: \")\n    assert (\n        await bot.get_output(\"/eval 1+1\")\n    ) == \"you must be an admin to use this command\"\n<|endoftext|>",
    "repo": "isabella232/forest"
  },
  {
    "prompt": "fn medias()\n\nfn creatable_media()\n\nfn no_desc_media(medias)\n\n\"\"\"For use when building GuestMedia records \"\"\"\nfn no_prefs_media(no_desc_media)\n\nfn open_door_media(medias)\n\nfn not_authorized_media(medias)\n\nfn authorized_media_no_guest(medias)\n\n\"\"\"This media should be used when creating a media_perm record.\"\"\"\nfn media_for_creatable_media_perm(medias)\n\n\"\"\"This media should be used when creating a guest_media record.\"\"\"\nfn media_for_creatable_guest_media(medias)\n\n\"\"\"Each media returned should be associated with a guest \"\"\"\nfn media_for_guests(medias)\n\n\"\"\"Each media returned should NOT be associated with a guest \"\"\"\nfn media_without_guests(medias)\n\n\"\"\"Each media returned should be associated with a permission \"\"\"\nfn media_for_permissions(medias)\n\nfn add_media_helpers(monkeypatch_session)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom rfidsecuritysvc.model.media import Media\n\n\n@pytest.fixture(scope='session')\ndef medias():\n    # The DB will return these ordered by id, please build the list accordingly\n    return [\n        Media('NOT AUTHORIZED', 'Not Authorized', 'This media should never be used in a media_perm record.'),\n        Media('TEST FOR AUTHORIZED NO GUEST', 'test for authorized media without a guest media record', None),\n        Media('TEST FOR GUESTMEDIA CREATION', 'test for GuestMedia creation', None),\n        Media('TEST FOR MEDIAPERM CREATION', 'test for MediaPerm creation', None),\n        Media('TEST MEDIA 1', 'test media 1', 'Media for testing (1)'),\n        Media('TEST MEDIA 2', 'test media 2', 'Media for testing (2)'),\n        Media('TEST MEDIA 3', 'test media 3', 'Media for testing (3)'),\n        Media('TEST MEDIA 4', 'test media 4', 'Media for testing (4)'),\n        Media('TEST MEDIA 5', 'test media 5', 'Media for testing (5)'),\n        Media('TEST OPEN DOOR', 'test open door', 'This media will be assigned the permission Open Door'),\n        Media('TEST WITHOUT DESC', 'test without desc', None),\n    ]\n\n\n@pytest.fixture(scope='session')\ndef creatable_media():\n    return Media('CREATABLE ID', 'creatable name', 'creatable desc')\n\n\n@pytest.fixture(scope='session')\ndef no_desc_media(medias):\n    for m in medias:\n        if m.id == 'TEST WITHOUT DESC':\n            return m\n\n\n@pytest.fixture(scope='session')\ndef no_prefs_media(no_desc_media):\n    \"\"\" For use when building GuestMedia records \"\"\"\n    return no_desc_media\n\n\n@pytest.fixture(scope='session')\ndef open_door_media(medias):\n    for m in medias:\n        if m.id == 'TEST OPEN DOOR':\n            return m\n\n\n@pytest.fixture(scope='session')\ndef not_authorized_media(medias):\n    for m in medias:\n        if m.id == 'NOT AUTHORIZED':\n            return m\n\n\n@pytest.fixture(scope='session')\ndef authorized_media_no_guest(medias):\n    for m in medias:\n        if m.id == 'TEST FOR AUTHORIZED NO GUEST':\n            return m\n\n\n@pytest.fixture(scope='session')\ndef media_for_creatable_media_perm(medias):\n    \"\"\" This media should be used when creating a media_perm record.\"\"\"\n    for m in medias:\n        if m.id == 'TEST FOR MEDIAPERM CREATION':\n            return m\n\n\n@pytest.fixture(scope='session')\ndef media_for_creatable_guest_media(medias):\n    \"\"\" This media should be used when creating a guest_media record.\"\"\"\n    for m in medias:\n        if m.id == 'TEST FOR GUESTMEDIA CREATION':\n            return m\n\n\n@pytest.fixture(scope='session')\ndef media_for_guests(medias):\n    \"\"\" Each media returned should be associated with a guest \"\"\"\n    return medias[4:9]\n\n\n@pytest.fixture(scope='session')\ndef media_without_guests(medias):\n    \"\"\" Each media returned should NOT be associated with a guest \"\"\"\n    return medias[0:4]\n\n\n@pytest.fixture(scope='session')\ndef media_for_permissions(medias):\n    \"\"\" Each media returned should be associated with a permission \"\"\"\n    return medias[4:]\n\n\n@pytest.fixture(autouse=True, scope='session')\ndef add_media_helpers(monkeypatch_session):\n    def create(self):\n        return self.to_json()\n\n    def update(self):\n        copy = self.to_json()\n        del copy['id']\n        return copy\n\n    monkeypatch_session.setattr(Media, 'test_create', create, raising=False)\n    monkeypatch_session.setattr(Media, 'test_update', update, raising=False)\n<|endoftext|>",
    "repo": "bcurnow/rfid-security-svc"
  },
  {
    "prompt": "\"\"\"Manage an `Ingest Pipeline <https://www.elastic.co/guide/en/elasticsearch/reference/master/put-pipeline-api.html>`__\n\n:param id: ID of an ingest pipeline\n:type id: str\n\n:param processors: List of `Processors <https://www.elastic.co/guide/en/elasticsearch/reference/master/processors.html>`__ for an ingest pipeline.\n:type processors: list\n\n:param description: Description of the Ingest Pipeline.\n:type description: str, optional\n\n:param on_failure: List of `Processors <https://www.elastic.co/guide/en/elasticsearch/reference/master/processors.html>`__ in case of failure.\n:type on_failure: list\n\n:param version: Version number of the ingest pipeline\n:type version: str, optional\n\n:param _meta: Optional metadata about the ingest pipeline\n:type _meta: dict, optional\"\"\"\nclass IngestPipeline(BaseResource):\n\tid: str\n\tprocessors: list\n\tdescription: Optional[str] = None\n\ton_failure: Optional[list] = None\n\tversion: Optional[int] = None\n\t_meta: Optional[dict] = None\n\t\"\"\"Get an ingest pipeline from Elasticsearch\n\t\n\t:param client: Elasticsearch Client\n\t:type client: :py:mod:`Elasticsearch`\n\t\n\t:param pipeline_id: Ingest pipeline id\n\t:type pipeline_id: str\"\"\"\n\tfn get(cls, client: :py:mod:`Elasticsearch`, pipeline_id: str)\n\t\"\"\"Create an ingest pipeline in Elasticsearch\n\t\n\t:param client: Elasticsearch Client\n\t:type client: :py:mod:`Elasticsearch`\"\"\"\n\tfn create(self, client: :py:mod:`Elasticsearch`)\n\t\"\"\"Delete an ingestpipeline from Elasticsearch\n\t\n\t:param client: Elasticsearch Client\n\t:type client: :py:mod:`Elasticsearch`\"\"\"\n\tfn delete(self, client: :py:mod:`Elasticsearch`)\n\t\"\"\"Update an ingest pipeline in Elasticsearch\n\t\n\t:param client: Elasticsearch Client\n\t:type client: :py:mod:`Elasticsearch`\"\"\"\n\tfn update(self, client: :py:mod:`Elasticsearch`)\n\t\"\"\"Simulate an ingest pipeline in Elasticsearch\n\t\n\t:param client: Elasticsearch Client\n\t:type client: :py:mod:`Elasticsearch`\n\t\n\t:param docs: List of Documents to simulate in the ingest pipeline\n\t:type docs: list\"\"\"\n\tfn simulate(self, client: :py:mod:`Elasticsearch`, docs: list)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport pytest\nfrom dataclasses import FrozenInstanceError\nfrom nemesis.resources.elasticsearch.ingest_pipeline import IngestPipeline\nfrom elasticsearch import Elasticsearch, RequestError, NotFoundError\n\nfrom nemesis.tests import es_client, nemesis_client\n\n\n@pytest.fixture\ndef pipeline(es_client):\n    pipeline = IngestPipeline(\n        id=\"test-pipeline\",\n        processors=[\n            {\"pipeline\": {\"name\": \"pipelineA\"}},\n            {\"set\": {\"field\": \"outer_pipeline_set\", \"value\": \"outer\"}},\n        ],\n    )\n    yield pipeline\n\n    # clear remote resources if they exist\n    try:\n        result = pipeline.delete(es_client)\n        assert result == {\"acknowledged\": True}\n    except NotFoundError:\n        pass\n\n\ndef test_asdict(pipeline):\n    \"\"\" \"\"\"\n    assert pipeline.asdict() == {\n        \"id\": \"test-pipeline\",\n        \"processors\": [\n            {\"pipeline\": {\"name\": \"pipelineA\"}},\n            {\"set\": {\"field\": \"outer_pipeline_set\", \"value\": \"outer\"}},\n        ],\n    }\n\n\ndef test_frozen_object(pipeline):\n    with pytest.raises(FrozenInstanceError):\n        pipeline.processors = [{}]\n\n\ndef test_id(pipeline):\n    assert pipeline.id == \"test-pipeline\"\n\n\ndef test_get_not_found(es_client):\n    assert IngestPipeline.get(es_client, \"notfound\") is None\n\n\ndef test_create_multiple(es_client, pipeline):\n    result = pipeline.create(es_client)\n    assert result == {\"acknowledged\": True}\n\n    result = pipeline.create(es_client)\n    assert result == {\"acknowledged\": True}\n\n\ndef test_create_update_delete(pipeline, es_client):\n    result = pipeline.create(es_client)\n    assert result == {\"acknowledged\": True}\n\n    result = IngestPipeline.get(es_client, pipeline.id)\n    assert result == pipeline\n\n    # Test that create called multiple times is ok.\n    result = pipeline.create(es_client)\n    assert result == {\"acknowledged\": True}\n\n    pipeline.processors.append(\n        {\"set\": {\"field\": \"inner_pipeline_set\", \"value\": \"inner\"}}\n    )\n    result = pipeline.update(es_client)\n    assert result == {\"acknowledged\": True}\n\n    result = pipeline.delete(es_client)\n    assert result == {\"acknowledged\": True}\n<|endoftext|>",
    "repo": "fxdgear/nemesis"
  },
  {
    "prompt": "fn test_data()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nimport os\nimport numpy as np\nfrom math import cos, sin, pi\n\nfrom pclpy import pcl\nimport pclpy\nfrom .utils import test_data\n\n\ndef test_fit_line():\n    line = np.array([(1, 2, 3), (2, 4, 6), (3, 7, 9), (5, 10, 15)])\n    pc = pcl.PointCloud.PointXYZ(line)\n    inliers, coefficients = pclpy.fit(pc, \"line\", distance=0.1)\n    assert len(inliers.indices) == 3\n    assert np.allclose(coefficients.values, pcl.vectors.Float([2.66667, 5.33333, 8, 0.267261, 0.534522, 0.801784]))\n\n\ndef test_fit_cylinder():\n    points = np.array([(cos(a), sin(a), z) for z in np.linspace(0, 5, num=10) for a in np.linspace(0, 2 * pi, num=20)])\n    points = np.vstack((np.array([10, 10, 10]), points))\n    pc = pcl.PointCloud.PointXYZ(points)\n    inliers, coefficients = pclpy.fit(pc, \"circle2d\", distance=0.01, indices=np.arange(100))\n    assert 0 not in inliers.indices\n    assert len(inliers.indices) == 99\n    assert np.allclose(coefficients.values, [0., 0., 1.], atol=0.00001)\n<|endoftext|>",
    "repo": "sweptlaser/pclpy"
  },
  {
    "prompt": "fn pytest_sim_params(metafunc)\n\nfn outlines(capsys)\n\nfn define_simple_circuit(T, circ_name, has_clk)\n\nclass TestPeekCircuit(m.Circuit):\n\t__test__ = False\n\tIO = ['I', m.In(T), 'O0', m.Out(T), 'O1', m.Out(T)]\n\tfn definition(io)\n\n\nclass ConfigReg(m.Circuit):\n\tIO = ['D', m.In(m.Bits[2]), 'Q', m.Out(m.Bits[2])] + m.ClockInterface(has_ce=True)\n\tfn definition(io)\n\n\nclass SimpleALU(m.Circuit):\n\tIO = ['a', m.In(m.UInt[16]), 'b', m.In(m.UInt[16]), 'c', m.Out(m.UInt[16]), 'config_data', m.In(m.Bits[2]), 'config_en', m.In(m.Enable)] + m.ClockInterface()\n\tfn definition(io)\n\n\nclass AndCircuit(m.Circuit):\n\tIO = ['I0', m.In(m.Bit), 'I1', m.In(m.Bit), 'O', m.Out(m.Bit)]\n\tfn definition(io)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import fault\nimport magma as m\nfrom pathlib import Path\nfrom .common import pytest_sim_params\n\n\ndef pytest_generate_tests(metafunc):\n    pytest_sim_params(metafunc, 'system-verilog')\n\n\ndef test_def_vlog(target, simulator, n_bits=8, b_val=42):\n    # declare circuit\n    defadd = m.DeclareCircuit(\n        'defadd',\n        'a_val', m.In(m.Bits[n_bits]),\n        'c_val', m.Out(m.Bits[n_bits])\n    )\n\n    # instantiate tester\n    tester = fault.Tester(defadd)\n\n    # define test\n    tester.poke(defadd.a_val, 12)\n    tester.eval()\n    tester.expect(defadd.c_val, 54)\n    tester.poke(defadd.a_val, 34)\n    tester.eval()\n    tester.expect(defadd.c_val, 76)\n\n    # run simulation\n    tester.compile_and_run(\n        target=target,\n        simulator=simulator,\n        ext_libs=[Path('tests/verilog/defadd.sv').resolve()],\n        defines={'N_BITS': n_bits, 'B_VAL': b_val},\n        ext_model_file=True,\n        tmp_dir=True\n    )\n<|endoftext|>",
    "repo": "splhack/fault"
  },
  {
    "prompt": "\"\"\"A tool for visualizing the structure and performance of Random Forests\"\"\"\nfn main()\n\n\"\"\"Web-based graphical user interface.\n\nFOREST_DATA: Path to the JSON file that contains the forest's data. Can also take a directory or tar file which\n    contains the JSON.\"\"\"\nfn gui(forest_data, port)\n\n\"\"\"Command line interface to generate SVGs.\n\nAs Python is unable to render React components, we make a subprocess call to a small Node.js application which\nwill do the rendering and also store the created SVG files. This command requires that Node.js is installed on your\nsystem!\n\nFOREST_DATA: Path to the JSON file that contains the forest's data. Can also take a directory or tar file which\n    contains the JSON.\"\"\"\nfn cli(forest_data, out, width, height, trunk_length, display_depth, branch_color, leaf_color)\n\n\"\"\"Reads forest data from the filesystem\n\nArgs:\n    forest_data (str): Either the path a folder or .tar.gz that contains a file called \"forest.json\" or the path\n        to the forest JSON file itself.\nReturns:\n    (dict) Forest data\"\"\"\nfn _read_data(forest_data: str)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import os\nimport tarfile\n\nimport pytest\n\nfrom rfvis.__main__ import _read_data\n\n\n@pytest.fixture(params=[\"dir\", \"tar\", \"json\"])\ndef forest_data(request, tmp_path):\n    forest_dir = os.path.join(os.path.dirname(__file__), \"resources\")\n    if request.param == \"dir\":\n        return forest_dir\n    forest_json = os.path.join(forest_dir, \"forest.json\")\n    if request.param == \"json\":\n        return forest_json\n    forest_tar = os.path.join(tmp_path, \"forest.tar.gz\")\n    with tarfile.open(forest_tar, mode=\"w:gz\") as tar:\n        for p in os.listdir(forest_dir):\n            tar.add(os.path.join(forest_dir, p), arcname=p)\n    if request.param == \"tar\":\n        return forest_tar\n    raise ValueError(\"Unknown parameter {}\".format(request.param))\n\n\ndef test_read_data(forest_data):\n    forest = _read_data(forest_data)\n    assert len(forest[\"trees\"]) == 2\n    assert forest[\"trees\"][0][\"error\"] == 0.278413\n<|endoftext|>",
    "repo": "birnbaum/rfvis"
  },
  {
    "prompt": "fn left_join(h1, h2: HashTable) -> list\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom left_join import left_join\nfrom hashtable import HashTable  # noqa E402\n\n\ndef test_func_exists():\n    left_join(HashTable(), None)\n\n\ndef test_func_secondhashisnone():\n    expected = []\n    actual = left_join(HashTable(), None)\n    assert expected == actual\n\n\ndef test_func_onematch_onedoesnt():\n\n    h1 = HashTable()\n    h1.add('fond', 'enamored')\n    h1.add('wrath', 'anger')\n\n    h2 = HashTable()\n    h2.add('fond', 'averse')\n\n    expected = [\n        {'word': 'fond', 'synonym': 'enamored', 'antonym': 'averse' },  # noqa: E202\n        {'word': 'wrath', 'synonym': 'anger', 'antonym': None }         # noqa: E202\n    ]\n    actual = left_join(h1, h2)\n    assert expected == actual\n\n\ndef test_func_fullsample():\n    h1 = HashTable()\n    h1.add('fond', 'enamored')\n    h1.add('wrath', 'anger')\n    h1.add('diligent', 'employed')\n    h1.add('outfit', 'garb')\n    h1.add('guide', 'usher')\n\n    h2 = HashTable()\n    h2.add('fond', 'averse')\n    h2.add('wrath', 'delight')\n    h2.add('diligent', 'idle')\n    h2.add('guide', 'follow')\n    h2.add('flow', 'jam')\n\n    expected = [\n        {'word': 'diligent', 'synonym': 'employed', 'antonym': 'idle',    },  # noqa E202\n        {'word': 'outfit',   'synonym': 'garb',     'antonym': None,      },  # noqa E202\n        {'word': 'fond',     'synonym': 'enamored', 'antonym': 'averse',  },  # noqa E202\n        {'word': 'guide',    'synonym': 'usher',    'antonym': 'follow',  },  # noqa E202\n        {'word': 'wrath',    'synonym': 'anger',    'antonym': 'delight', }   # noqa E202\n    ]\n\n    actual = left_join(h1, h2)\n    assert expected == actual\n<|endoftext|>",
    "repo": "marvincolgin/data-structures-and-algorythms"
  },
  {
    "prompt": "\"\"\"A subclass of scipy.sparse.csr_matrix that skips the data format\nchecks that are run everytime a new csr_matrix is created.\"\"\"\nclass fast_csr_matrix(csr_matrix):\n\tfn __init__(self, args, shape, dtype, copy)\n\t\"\"\"Do the binary operation fn to two sparse matrices using\n\tfast_csr_matrix only when other is also a fast_csr_matrix.\"\"\"\n\tfn _binopt(self, other, op)\n\t\"\"\"Point-wise multiplication by another matrix, vector, or\n\tscalar.\"\"\"\n\tfn multiply(self, other)\n\t\"\"\"Do the sparse matrix mult returning fast_csr_matrix only\n\twhen other is also fast_csr_matrix.\"\"\"\n\tfn _mul_sparse_matrix(self, other)\n\t\"\"\"Scalar version of self._binopt, for cases in which no new nonzeros\n\tare added. Produces a new spmatrix in canonical form.\"\"\"\n\tfn _scalar_binopt(self, other, op)\n\tfn __eq__(self, other)\n\tfn __ne__(self, other)\n\tfn _inequality(self, other, op, op_name, bad_scalar_msg)\n\t\"\"\"Returns a matrix with the same sparsity structure as self,\n\tbut with different data.  By default the structure arrays\n\t(i.e. .indptr and .indices) are copied.\"\"\"\n\tfn _with_data(self, data, copy)\n\t\"\"\"Returns the transpose of the matrix, keeping\n\tit in fast_csr format.\"\"\"\n\tfn transpose(self)\n\t\"\"\"Same as transpose\"\"\"\n\tfn trans(self)\n\t\"\"\"Returns the conjugate-transpose of the matrix, keeping\n\tit in fast_csr format.\"\"\"\n\tfn getH(self)\n\t\"\"\"Same as getH\"\"\"\n\tfn adjoint(self)\n\n\nfn csr2fast(A, copy)\n\n\"\"\"Generates a sparse identity matrix in\nfast_csr format.\"\"\"\nfn fast_identity(N)\n\nfn _all_true(shape)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nimport scipy.sparse\nimport qutip\nfrom qutip.fastsparse import fast_csr_matrix\n\n\nclass TestOperationEffectsOnType:\n    @pytest.mark.parametrize(\"operation\", [\n        pytest.param(lambda x: x, id=\"identity\"),\n        pytest.param(lambda x: x + x, id=\"addition\"),\n        pytest.param(lambda x: x - x, id=\"subtraction\"),\n        pytest.param(lambda x: x * x, id=\"multiplication by op\"),\n        pytest.param(lambda x: 2*x, id=\"multiplication by scalar\"),\n        pytest.param(lambda x: x/3, id=\"division by scalar\"),\n        pytest.param(lambda x: -x, id=\"negation\"),\n        pytest.param(lambda x: x.copy(), id=\"copy\"),\n        pytest.param(lambda x: x.T, id=\"transpose [.T]\"),\n        pytest.param(lambda x: x.trans(), id=\"transpose [.trans()]\"),\n        pytest.param(lambda x: x.transpose(), id=\"transpose [.transpose()]\"),\n        pytest.param(lambda x: x.H, id=\"adjoint [.H]\"),\n        pytest.param(lambda x: x.getH(), id=\"adjoint [.getH()]\"),\n        pytest.param(lambda x: x.adjoint(), id=\"adjoint [.adjoint()]\"),\n    ])\n    def test_operations_preserve_type(self, operation):\n        op = qutip.rand_herm(5).data\n        assert isinstance(operation(op), fast_csr_matrix)\n\n    @pytest.mark.parametrize(\"operation\", [\n        pytest.param(lambda x, y: y, id=\"identity of other\"),\n        pytest.param(lambda x, y: x + y, id=\"addition\"),\n        pytest.param(lambda x, y: y + x, id=\"r-addition\"),\n        pytest.param(lambda x, y: x - y, id=\"subtraction\"),\n        pytest.param(lambda x, y: y - x, id=\"r-subtraction\"),\n        pytest.param(lambda x, y: x * y, id=\"multiplication\"),\n        pytest.param(lambda x, y: y * x, id=\"r-multiplication\"),\n    ])\n    def test_mixed_operations_yield_type(self, operation):\n        op = qutip.rand_herm(5).data\n        other = scipy.sparse.csr_matrix((op.data, op.indices, op.indptr),\n                                        copy=True, shape=op.shape)\n        assert not isinstance(operation(op, other), fast_csr_matrix)\n<|endoftext|>",
    "repo": "Sampreet/qutip"
  },
  {
    "prompt": "fn setup_function(f)\n\nfn teardown_function(f)\n\n\"\"\"Resolves a filename relative to the \"tests\" directory.\"\"\"\nfn get_filename(fn)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# -*- coding: utf-8 -*-\n\n# The MIT License (MIT) - Copyright (c) 2016-2021 Dave Vandenbout.\n\nimport pytest\n\nfrom skidl import TEMPLATE, Net, Part, generate_pcb\n\nfrom .setup_teardown import setup_function, teardown_function\n\n\ndef test_pcb_1():\n    \"\"\"Test PCB generation.\"\"\"\n\n    l1 = Part(\n        \"Device.lib\",\n        \"L\",\n        footprint=\"Inductor_SMD:L_0805_2012Metric_Pad1.15x1.40mm_HandSolder\",\n    )\n    r1, r2 = (\n        Part(\n            \"Device.lib\",\n            \"R\",\n            dest=TEMPLATE,\n            value=\"200.0\",\n            footprint=\"Resistor_SMD:R_0805_2012Metric\",\n        )\n        * 2\n    )\n    q1 = Part(\n        \"Device.lib\", \"Q_NPN_CBE\", footprint=\"Package_TO_SOT_SMD:SOT-223-3_TabPin2\"\n    )\n    c1 = Part(\n        \"Device.lib\",\n        \"C\",\n        value=\"10pF\",\n        footprint=\"Capacitor_SMD:C_0805_2012Metric_Pad1.18x1.45mm_HandSolder\",\n    )\n    r3 = r2(value=\"1K\", footprint=\"Resistor_SMD:R_0805_2012Metric\")\n    vcc, vin, vout, gnd = Net(\"VCC\"), Net(\"VIN\"), Net(\"VOUT\"), Net(\"GND\")\n    vcc & r1 & vin & r2 & gnd\n    vcc & r3 & vout & q1[\"C,E\"] & gnd\n    q1[\"B\"] += vin\n    vout & (l1 | c1) & gnd\n    rly = Part(\"Relay\", \"TE_PCH-1xxx2M\", footprint=\"Relay_THT:Relay_SPST_TE_PCH-1xxx2M\")\n    rly[1, 2, 3, 5] += gnd\n    led = Part(\"Device.lib\", \"LED_ARGB\", symtx=\"RH\", footprint=\"LED_SMD:LED_RGB_1210\")\n    r, g, b = Net(\"R\"), Net(\"G\"), Net(\"B\")\n    led[\"A,RK,GK,BK\"] += vcc, r, g, b\n    Part(\n        lib=\"MCU_Microchip_PIC10.lib\",\n        name=\"PIC10F200-IMC\",\n        footprint=\"Package_DFN_QFN:DFN-8-1EP_2x3mm_P0.5mm_EP0.61x2.2mm\",\n    )\n\n    generate_pcb()\n<|endoftext|>",
    "repo": "eduxstad/skidl"
  },
  {
    "prompt": "fn all_valid_kwargs(valid_kwargs)\n\nfn all_invalid_kwargs(valid_kwargs, invalid_kwargs)\n\nfn build_valid_kwargs_list(base, optional_kwargs)\n\nclass SometimesIncompleteReaderIO(io.BytesIO):\n\tfn __init__(self)\n\t\"\"\"Every other read request, return fewer than the requested number of bytes if more than one byte requested.\"\"\"\n\tfn read(self, size)\n\n\nclass NothingButRead(object):\n\tfn __init__(self, data)\n\tfn read(self, size)\n\n\nclass ExactlyTwoReads(SometimesIncompleteReaderIO):\n\tfn read(self, size)\n\n\nclass FailingTeller(object):\n\tfn tell(self)\n\n\nfn assert_prepped_stream_identity(prepped_stream, wrapped_type)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Unit test suite to validate aws_encryption_sdk.key_providers.raw.RawMasterKeyConfig\"\"\"\nimport pytest\nimport six\n\nfrom aws_encryption_sdk.identifiers import EncryptionKeyType, WrappingAlgorithm\nfrom aws_encryption_sdk.internal.crypto.wrapping_keys import WrappingKey\nfrom aws_encryption_sdk.key_providers.base import MasterKeyConfig\nfrom aws_encryption_sdk.key_providers.raw import RawMasterKeyConfig\n\nfrom .unit_test_utils import all_invalid_kwargs, all_valid_kwargs\n\npytestmark = [pytest.mark.unit, pytest.mark.local]\n\nSTATIC_WRAPPING_KEY = WrappingKey(\n    wrapping_algorithm=WrappingAlgorithm.AES_256_GCM_IV12_TAG16_NO_PADDING,\n    wrapping_key=b\"_________a symmetric key________\",\n    wrapping_key_type=EncryptionKeyType.SYMMETRIC,\n)\nVALID_KWARGS = {\n    RawMasterKeyConfig: [\n        dict(key_id=b\"a raw key\", provider_id=\"a provider\", wrapping_key=STATIC_WRAPPING_KEY),\n        dict(key_id=b\"a raw key\", provider_id=b\"a provider\", wrapping_key=STATIC_WRAPPING_KEY),\n    ]\n}\n\n\n@pytest.mark.parametrize(\"cls, kwargs\", all_valid_kwargs(VALID_KWARGS))\ndef test_attributes_valid_kwargs(cls, kwargs):\n    cls(**kwargs)\n\n\n@pytest.mark.parametrize(\"cls, kwargs\", all_invalid_kwargs(VALID_KWARGS))\ndef test_attributes_invalid_kwargs(cls, kwargs):\n    with pytest.raises(TypeError):\n        cls(**kwargs)\n\n\ndef test_parent():\n    assert issubclass(RawMasterKeyConfig, MasterKeyConfig)\n\n\n@pytest.mark.parametrize(\"cls, kwargs\", all_valid_kwargs(VALID_KWARGS))\ndef test_attributes_converts(cls, kwargs):\n    test = cls(**kwargs)\n    assert isinstance(test.provider_id, six.string_types)\n<|endoftext|>",
    "repo": "caitlin-tibbetts/aws-encryption-sdk-python"
  },
  {
    "prompt": "\"\"\"Document iterator for in-memory documents.\"\"\"\nclass InMemoryDocumentIterator(DocumentIterator):\n\t\"\"\"Initialize the list of document rows.\n\t\n\tParameters\n\t----------\n\trows: list\n\t    List of document rows. Each row in the list is a tuple of row\n\t    position, row index, and cell values.\"\"\"\n\tfn __init__(self, rows: List[Tuple[(int, RowIndex, DataRow)]])\n\t\"\"\"Set list of rows to None.\"\"\"\n\tfn close(self)\n\t\"\"\"Read the next row in the document.\n\t\n\tReturns the row position, row index and the list of cell values for each\n\tof the document columns. Raises a StopIteration error if an attempt is\n\tmade to read past the end of the document.\n\t\n\tReturns\n\t-------\n\ttuple of int, histore.document.base.RowIndex, histore.document.base.DataRow\"\"\"\n\tfn next(self) -> Tuple[(int, RowIndex, DataRow)]\n\n\n\"\"\"The in-memory document maintains a array of document rows (each as a\ntuple of row position, row index, and cell values). The row position is the\noriginal position of the row in a sorted document.\"\"\"\nclass InMemoryDocument(Document):\n\t\"\"\"Initialize document schema and rows.\n\t\n\tParameters\n\t----------\n\tcolumns: list\n\t    List of column names. The number of values in each row is expected\n\t    to be the same as the number of columns and the order of values in\n\t    each row is expected to correspond to their respective column in\n\t    this list.\n\trows: list\n\t    List of document rows. Each row in the list is a tuple of row\n\t    position, row index, and cell values.\"\"\"\n\tfn __init__(self, columns: DocumentSchema, rows: List[Tuple[(int, RowIndex, DataRow)]])\n\t\"\"\"Set list of rows to None when closing the document.\"\"\"\n\tfn close(self)\n\t\"\"\"Open the document to get a iterator for the rows in the document.\n\t\n\tReturns\n\t-------\n\thistore.document.base.DocumentIterator\"\"\"\n\tfn open(self) -> InMemoryDocumentIterator\n\t\"\"\"Create data frame from the document rows.\n\t\n\tReturns\n\t-------\n\tpd.DataFrame\"\"\"\n\tfn to_df(self) -> pd.DataFrame\n\t\"\"\"Sort the document rows based on the values in the key columns.\n\t\n\tKey columns are specified by their index position. Returns a new\n\tdocument.\n\t\n\tParameters\n\t----------\n\tkeys: list of int\n\t    Index position of sort columns.\n\tbuffersize: float, default=None\n\t    Maximum size (in bytes) of file blocks that are kept in main-memory.\n\t    Ignored. Included for API completeness.\n\t\n\tReturns\n\t-------\n\thistore.document.mem.InMemoryDocument\"\"\"\n\tfn sorted(self, keys: List[int], buffersize: Optional[float]) -> InMemoryDocument\n\n\n\"\"\"Empty document that only defines the document schema.\"\"\"\nclass Schema(InMemoryDocument):\n\t\"\"\"Initialize document schema.\n\t\n\tParameters\n\t----------\n\tcolumns: list\n\t    List of column names.\"\"\"\n\tfn __init__(self, columns: DocumentSchema)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# This file is part of the History Store (histore).\n#\n# Copyright (C) 2018-2021 New York University.\n#\n# The History Store (histore) is released under the Revised BSD License. See\n# file LICENSE for full license details.\n\n\"\"\"Unit tests for the in-memory document and document reader.\"\"\"\n\nimport pytest\n\nfrom histore.document.mem import InMemoryDocument, Schema\n\n\n@pytest.fixture\ndef document():\n    \"\"\"Get a document for test purposes.\"\"\"\n    return InMemoryDocument(\n        columns=['Name', 'Age', 'Dept'],\n        rows=[\n            (0, 0, ['Alice', 23, 'R&D']),\n            (1, 1, ['Bob', 35, 'Finance']),\n            (2, 3, ['Dave', 28, 'Sales']),\n            (3, 2, ['Claire', 33, 'Finance'])\n        ]\n    )\n\n\ndef test_document_iterrows(document):\n    \"\"\"Test the reader for an in-memory document.\"\"\"\n    rows = list()\n    for rowidx, row in document.iterrows():\n        rows.append((rowidx, row))\n    assert rows == [\n        (0, ['Alice', 23, 'R&D']),\n        (1, ['Bob', 35, 'Finance']),\n        (3, ['Dave', 28, 'Sales']),\n        (2, ['Claire', 33, 'Finance'])\n    ]\n\n\ndef test_empty_document():\n    \"\"\"Test the reader for an empty document.\"\"\"\n    doc = Schema(columns=['Name', 'Age'])\n    assert doc.columns == ['Name', 'Age']\n    rows = list()\n    for rowidx, row in doc.iterrows():\n        rows.append((rowidx, row))\n    assert rows == []\n\n\ndef test_error_read_after_close(document):\n    \"\"\"Error when reading closed document.\"\"\"\n    for rowidx, row in document.iterrows():\n        pass\n    document.close()\n    with document.open() as reader:\n        with pytest.raises(StopIteration):\n            next(reader)\n    document.close()\n\n\ndef test_to_df(document):\n    \"\"\"Test reading the document as a data frame.\"\"\"\n    df = document.to_df()\n    assert df.shape == (4, 3)\n    assert list(df.index) == [0, 1, 3, 2]\n\n\ndef test_sort_document(document):\n    \"\"\"Test sorting the document.\"\"\"\n    doc = document.sorted(keys=[0])\n    with document.open() as reader:\n        names = [r[0] for _, _, r in reader]\n        assert names == ['Alice', 'Bob', 'Dave', 'Claire']\n    with doc.open() as reader:\n        names = [r[0] for _, _, r in reader]\n        assert names == ['Alice', 'Bob', 'Claire', 'Dave']\n    doc = document.sorted(keys=[2, 0])\n    with doc.open() as reader:\n        names = [r[0] for _, _, r in reader]\n        assert names == ['Bob', 'Claire', 'Alice', 'Dave']\n<|endoftext|>",
    "repo": "heikomuller/histore"
  },
  {
    "prompt": "\"\"\"Mock for logger class \"\"\"\nclass LoggerMock(object):\n\tfn log(self, module, message)\n\n\n\"\"\"Common class for integration tests \"\"\"\nclass CommonIntegration(object):\n\tdb = None\n\t\"\"\"Prepare class for run tests \"\"\"\n\tfn setup_class(self)\n\t\"\"\"Clean tables for tests \"\"\"\n\tfn _clean_db(self)\n\t\"\"\"Add hashlist record \"\"\"\n\tfn _add_hashlist(self, id, name, alg_id, have_salts, status, common_by_alg, parsed, tmp_path, last_finder_checked, uncracked)\n\t\"\"\"Add hash record \"\"\"\n\tfn _add_hash(self, hashlist_id, hash, salt, summ, password, cracked, id)\n\t\"\"\"Add work task record \"\"\"\n\tfn _add_work_task(self, id, hashlist_id, task_id, status, priority, out_file)\n\t\"\"\"Add task record \"\"\"\n\tfn _add_task(self, id, name, group_id, type, source)\n\t\"\"\"Add dict record \"\"\"\n\tfn _add_dict(self, id, group_id, name, hash)\n\t\"\"\"Add dict group record \"\"\"\n\tfn _add_dict_group(self, id, name)\n\t\"\"\"Add rule record \"\"\"\n\tfn _add_rule(self, id, name, hash, count)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# -*- coding: utf-8 -*-\n\"\"\"\nThis is part of HashBruteStation software\nDocs EN: http://hack4sec.pro/wiki/index.php/Hash_Brute_Station_en\nDocs RU: http://hack4sec.pro/wiki/index.php/Hash_Brute_Station\nLicense: MIT\nCopyright (c) Anton Kuzmin <http://anton-kuzmin.ru> (ru) <http://anton-kuzmin.pro> (en)\n\nIntegration tests for FinderInsideProThread\n\"\"\"\n\nimport sys\nimport time\nimport pytest\n\nsys.path.append('../../')\n\nfrom libs.common import md5\nfrom classes.FinderInsideProThread import FinderInsideProThread\nfrom CommonIntegration import CommonIntegration\n\nclass Test_HashlistsLoaderThread(CommonIntegration):\n    \"\"\" Integration tests for FinderInsideProThread \"\"\"\n    thrd = None\n\n    def setup(self):\n        \"\"\" Tests setup \"\"\"\n        self._clean_db()\n        self.thrd = FinderInsideProThread()\n        self.thrd.catch_exceptions = False\n        self.db.update(\"algs\", {'finder_insidepro_allowed': 1}, \"id\")\n\n\n    def teardown(self):\n        \"\"\" Tests teardown \"\"\"\n        if isinstance(self.thrd, FinderInsideProThread):\n            self.thrd.available = False\n            time.sleep(1)\n            del self.thrd\n        self._clean_db()\n\n    test_data = [\n        (\n            74,\n            1,\n            [\n                {'id': 1, 'hash': '0065ffe5f9e4e5996c2c3f52f81c6e31', 'salt': 'cB6Ar', 'cracked': 1,\n                 'summ': md5(\"0065ffe5f9e4e5996c2c3f52f81c6e31:cB6Ar\"), 'password': 'y0007171'},\n                {'id': 2, 'hash': '20e153b046072c949562f3c939611db8', 'salt': '0RTV', 'cracked': 0,\n                 'summ': md5(\"20e153b046072c949562f3c939611db8:0RTV\"), 'password': ''},\n            ]\n        ),\n        (\n            4,\n            0,\n            [\n                {'id': 1, 'hash': md5('aaa'), 'salt': '', 'cracked': 1,\n                 'summ': md5(md5('aaa')), 'password': 'aaa'},\n                {'id': 2, 'hash': '10e153b046072c949562f3c939611db7', 'salt': '', 'cracked': 0,\n                 'summ': md5(\"10e153b046072c949562f3c939611db7\"), 'password': ''},\n            ]\n        )\n    ]\n    @pytest.mark.parametrize(\"alg_id,have_salt,hashes\", test_data)\n    def test_run(self, alg_id, have_salt, hashes):\n        \"\"\"\n        Test simple run\n        :param alg_id:\n        :param have_salt: does alg has salts?\n        :param hashes: Hashes rows\n        :return:\n        \"\"\"\n        self._add_hashlist(common_by_alg=alg_id, alg_id=alg_id, have_salts=have_salt,\n                           last_finder_checked=int(time.time()))\n        for _hash in hashes:\n            self._add_hash(id=_hash['id'], hash=_hash['hash'], salt=_hash['salt'], summ=_hash['summ'])\n\n        self.thrd.start()\n\n        time.sleep(10)\n\n        assert self.db.fetch_one(\"SELECT 1 FROM hashes WHERE id = 1 AND cracked = 0 AND password=''\") == 1\n        assert self.db.fetch_one(\"SELECT 1 FROM hashes WHERE id = 2 AND cracked = 0 AND password=''\") == 1\n        assert self.db.fetch_one(\"SELECT last_finder_checked FROM hashlists WHERE id = 1\") < time.time()\n\n        self.db.update(\"hashlists\", {\"last_finder_checked\": 0}, \"id = 1\")\n\n        time.sleep(10)\n\n        for _hash in hashes:\n            test_data = self.db.fetch_row(\"SELECT * FROM hashes WHERE id = {0}\".format(_hash['id']))\n            assert test_data['summ'] == _hash['summ']\n            assert test_data['cracked'] == _hash['cracked']\n            assert test_data['password'] == _hash['password']\n\n        assert self.db.fetch_one(\"SELECT last_finder_checked FROM hashlists WHERE id = 1\") > time.time() - 20\n<|endoftext|>",
    "repo": "AntonKuzminRussia/hbs-cli"
  },
  {
    "prompt": "class LinearNNModel(tf.keras.Model):\n\tfn __init__(self)\n\tfn call(self, x)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements. See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nfrom datetime import datetime\nfrom os import environ\n\nimport pytest\nimport tensorflow\n\nimport submarine\nfrom submarine.artifacts.repository import Repository\nfrom submarine.store.database import models\nfrom submarine.store.database.models import SqlExperiment, SqlMetric, SqlParam\nfrom submarine.tracking.client import SubmarineClient\n\nfrom .tf_model import LinearNNModel\n\nJOB_ID = \"application_123456789\"\nREGISTERED_MODEL_NAME = \"registerd_model_name\"\nMLFLOW_S3_ENDPOINT_URL = \"http://localhost:9000\"\n\n\n@pytest.mark.e2e\nclass TestTracking(unittest.TestCase):\n    def setUp(self):\n        environ[\"JOB_ID\"] = JOB_ID\n        submarine.set_db_uri(\n            \"mysql+pymysql://submarine_test:password_test@localhost:3306/submarine_test\"\n        )\n        self.db_uri = submarine.get_db_uri()\n        self.client = SubmarineClient(\n            db_uri=self.db_uri,\n            s3_registry_uri=MLFLOW_S3_ENDPOINT_URL,\n        )\n        from submarine.store.tracking.sqlalchemy_store import SqlAlchemyStore\n\n        self.store = SqlAlchemyStore(self.db_uri)\n        from submarine.store.model_registry.sqlalchemy_store import SqlAlchemyStore\n\n        self.model_registry = SqlAlchemyStore(self.db_uri)\n        # TODO: use submarine.tracking.fluent to support experiment create\n        with self.store.ManagedSessionMaker() as session:\n            instance = SqlExperiment(\n                id=JOB_ID,\n                experiment_spec='{\"value\": 1}',\n                create_by=\"test\",\n                create_time=datetime.now(),\n                update_by=None,\n                update_time=None,\n            )\n            session.add(instance)\n            session.commit()\n\n    def tearDown(self):\n        submarine.set_db_uri(None)\n        models.Base.metadata.drop_all(self.store.engine)\n        environ[\"MLFLOW_S3_ENDPOINT_URL\"] = MLFLOW_S3_ENDPOINT_URL\n        environ[\"AWS_ACCESS_KEY_ID\"] = \"submarine_minio\"\n        environ[\"AWS_SECRET_ACCESS_KEY\"] = \"submarine_minio\"\n        Repository().delete_folder(f\"experiment/{JOB_ID}\")\n        Repository().delete_folder(f\"registry/{REGISTERED_MODEL_NAME}\")\n\n    def test_log_param(self):\n        submarine.log_param(\"name_1\", \"a\")\n        # Validate params\n        with self.store.ManagedSessionMaker() as session:\n            params = session.query(SqlParam).options().filter(SqlParam.id == JOB_ID).all()\n            assert params[0].key == \"name_1\"\n            assert params[0].value == \"a\"\n            assert params[0].id == JOB_ID\n\n    def test_log_metric(self):\n        submarine.log_metric(\"name_1\", 5)\n        submarine.log_metric(\"name_1\", 6)\n        # Validate params\n        with self.store.ManagedSessionMaker() as session:\n            metrics = session.query(SqlMetric).options().filter(SqlMetric.id == JOB_ID).all()\n            assert len(metrics) == 2\n            assert metrics[0].key == \"name_1\"\n            assert metrics[0].value == 5\n            assert metrics[0].id == JOB_ID\n            assert metrics[1].value == 6\n\n    @pytest.mark.skipif(tensorflow.version.VERSION < \"2.0\", reason=\"using tensorflow 2\")\n    def test_save_model(self):\n        input_arr = tensorflow.random.uniform((1, 5))\n        model = LinearNNModel()\n        model(input_arr)\n        self.client.save_model(model, \"tensorflow\", \"name_1\", REGISTERED_MODEL_NAME)\n        self.client.save_model(model, \"tensorflow\", \"name_2\", REGISTERED_MODEL_NAME)\n        # Validate model_versions\n        model_versions = self.model_registry.list_model_versions(REGISTERED_MODEL_NAME)\n        assert len(model_versions) == 2\n        assert model_versions[0].name == REGISTERED_MODEL_NAME\n        assert model_versions[0].version == 1\n        assert model_versions[1].name == REGISTERED_MODEL_NAME\n        assert model_versions[1].version == 2\n<|endoftext|>",
    "repo": "JackLiu00521/submarine"
  },
  {
    "prompt": "\"\"\"Take a dictionary with microphone array\ncapsules and 3D polar coordinates to\nconvert them from degrees to radians\n\ncolatitude, azimuth, and radius (radius\nis left intact)\"\"\"\nfn _deg2rad(coords_dict)\n\n\"\"\"Take a dictionary with microphone array\ncapsules and polar coordinates and convert\nto cartesian\n\nParameters:\n    units: (str) indicating 'degrees' or 'radians'\"\"\"\nfn _polar2cart(coords_dict, units: None)\n\n\"\"\"Take a dictionary with microphone array\ncapsules and cartesian coordinates, find the center\nof such coordinates and subtract it to center all\nthe coordinates around zero.\"\"\"\nfn _centercoords(coords_dict)\n\n\"\"\"Take a dictionary with microphone array\ncapsules and cartesian coordinates, and convert\nto polar: colatitude (radians), azimuth (radians), radius\"\"\"\nfn _cart2polar(coords_dict)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from micarraylib.datasets import tau2021sse_nigens_loader\nfrom micarraylib.arraycoords.array_shapes_utils import _polar2cart\nimport numpy as np\nimport soundata\nimport pytest\nimport librosa\n\n\ndef test_tau2021sse_nigens_init():\n\n    a = tau2021sse_nigens_loader.tau2021sse_nigens(download=False, data_home=\"~/\")\n    assert a.name == \"tau2021sse_nigens\"\n    assert a.fs == 24000\n    assert len(a.array_format) == 1\n    assert a.array_format[\"Eigenmike\"] == \"A\"\n    assert len(a.capsule_coords[\"Eigenmike\"]) == 4\n    assert list(a.capsule_coords[\"Eigenmike\"].keys()) == [\"6\", \"10\", \"22\", \"26\"]\n    b = _polar2cart(a.capsule_coords[\"Eigenmike\"], \"radians\")\n    # TODO: analize why the atol is needed\n    assert np.allclose(\n        np.mean(np.array([c for c in b.values()]), axis=0), [0, 0, 0], atol=1e-4\n    )\n    assert (\n        len(a.micarray_clip_ids[\"Eigenmike\"])\n        == len(soundata.initialize(\"tau2021sse_nigens\", data_home=\"~/\").clip_ids) / 2\n    )  # because soundata has clip_ids for each format A and B\n    assert (\n        len(a.clips_list)\n        == len(soundata.initialize(\"tau2021sse_nigens\", data_home=\"~/\").clip_ids) / 2\n    )  # because soundata has clip_ids for each format A and B\n\n\ndef test_tau2021sse_nigens_get_audio_numpy():\n\n    a = tau2021sse_nigens_loader.tau2021sse_nigens(\n        download=False, data_home=\"tests/resources/datasets/tau2021sse_nigens\"\n    )\n    A = a.get_audio_numpy(\"dev/dev-train/fold1_room1_mix001\")\n    B = a.get_audio_numpy(\"dev/dev-train/fold1_room1_mix001\", fmt=\"B\")\n\n    Al = librosa.load(\n        \"tests/resources/datasets/tau2021sse_nigens/mic_dev/dev-train/fold1_room1_mix001.wav\",\n        sr=24000,\n        mono=False,\n    )[0]\n    Bl = librosa.load(\n        \"tests/resources/datasets/tau2021sse_nigens/foa_dev/dev-train/fold1_room1_mix001.wav\",\n        sr=24000,\n        mono=False,\n    )[0]\n\n    assert (B == Bl).all()\n    assert (A == Al).all()\n\n    with pytest.raises(ValueError):\n        a.get_audio_numpy(\"a\", \"b\")\n    with pytest.raises(ValueError):\n        a.get_audio_numpy(\"a\", \"Eigenmike\")\n\n\ndef test_tau2021sse_nigens_get_audio_events():\n\n    a = tau2021sse_nigens_loader.tau2021sse_nigens(\n        download=False, data_home=\"tests/resources/datasets/tau2021sse_nigens\"\n    )\n    A = a.get_audio_events(\"dev/dev-train/fold1_room1_mix001\")\n\n    with pytest.raises(ValueError):\n        a.get_audio_events(\"a\")\n<|endoftext|>",
    "repo": "Nianze/micarraylib"
  },
  {
    "prompt": "fn uuid()\n\nfn oauth_redirect_request(app, make_request)\n\nfn oauth_callback_request(app, make_request)\n\nfn config(config)\n\nfn get_email(profile_data)\n\nfn set_email(profile_data, email)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom .conftest import PROVIDERS_TO_TEST, PROVIDER_PARAMS\n\n\n@pytest.mark.django_db\nasync def test_oauth_redirect_unknown_provider(oauth_redirect_request):\n    await oauth_redirect_request(provider_name='xxx', expected_status=400)\n\n\n@pytest.mark.django_db\nasync def test_oauth_redirect_provider_not_configured(oauth_redirect_request):\n    await oauth_redirect_request(provider_name='google', expected_status=404)\n\n\n@pytest.mark.django_db\n@pytest.mark.parametrize('provider', PROVIDERS_TO_TEST)\nasync def test_oauth_redirect_successful(provider, oauth_redirect_request, config):\n    response = await oauth_redirect_request(provider_name=provider, expected_status=302)\n    location = response.headers['Location']\n    assert location.startswith(PROVIDER_PARAMS[provider]['auth_url'])\n    assert config.social[provider]['client_id'] in location\n    assert 'response_type=code' in location\n<|endoftext|>",
    "repo": "aborilov/dvhb-hybrid"
  },
  {
    "prompt": "fn number0() -> NumberExpressionSyntax\n\nfn number1() -> NumberExpressionSyntax\n\nfn number2() -> NumberExpressionSyntax\n\nfn open_paranthesis() -> SyntaxToken\n\nfn close_paranthesis() -> SyntaxToken\n\nfn addition_operator() -> SyntaxToken\n\nfn subtraction_operator() -> SyntaxToken\n\nfn multiplication_operator() -> SyntaxToken\n\nfn division_operator() -> SyntaxToken\n\nfn binary_expression_syntax_simple(number1, number2, addition_operator) -> BinaryExpressionSyntax\n\nfn paranthesized_expression_simple(number1, number2, addition_operator, open_paranthesis, close_paranthesis) -> ParanthesizedExpressionSyntax\n\nfn parser_simple(number1, number2, addition_operator, open_paranthesis, close_paranthesis) -> Parser\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from codeanalysis.syntaxkind import SyntaxKind\r\nfrom codeanalysis.tests.conftest import paranthesized_expression_simple\r\nimport pytest\r\n\r\n\r\nclass TestParanthesizedExpressionSyntax:\r\n    def test_kind(self, paranthesized_expression_simple) -> None:\r\n        assert (\r\n            paranthesized_expression_simple.kind() == SyntaxKind.paranthesizedexpression\r\n        )\r\n\r\n    def test_getChildren(self, paranthesized_expression_simple) -> None:\r\n        token_gernator = paranthesized_expression_simple.get_children()\r\n        assert next(token_gernator).kind() == SyntaxKind.openparanthesis\r\n        assert next(token_gernator).kind() == SyntaxKind.binaryexpression\r\n        assert next(token_gernator).kind() == SyntaxKind.closeparanthesis\r\n        with pytest.raises(Exception) as excinfo:\r\n            next(token_gernator)\r\n        assert str(excinfo) == \"<ExceptionInfo StopIteration() tblen=1>\"\r\n\r\n    def test_getLastChild(self, paranthesized_expression_simple) -> None:\r\n        assert (\r\n            paranthesized_expression_simple.get_last_child().kind()\r\n            == SyntaxKind.closeparanthesis\r\n        )\r\n\r\n<|endoftext|>",
    "repo": "judej/pyler"
  },
  {
    "prompt": "class UserDetailView(LoginRequiredMixin, DetailView):\n\tmodel = User\n\tslug_field = 'username'\n\tslug_url_kwarg = 'username'\n\n\nclass UserUpdateView(LoginRequiredMixin, UpdateView):\n\tmodel = User\n\tfields = ['name']\n\tfn get_success_url(self)\n\tfn get_object(self)\n\tfn form_valid(self, form)\n\n\nclass UserRedirectView(LoginRequiredMixin, RedirectView):\n\tpermanent = False\n\tfn get_redirect_url(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom django.conf import settings\nfrom django.test import RequestFactory\n\nfrom opensteer.users.views import UserRedirectView, UserUpdateView\n\npytestmark = pytest.mark.django_db\n\n\nclass TestUserUpdateView:\n    \"\"\"\n    TODO:\n        extracting view initialization code as class-scoped fixture\n        would be great if only pytest-django supported non-function-scoped\n        fixture db access -- this is a work-in-progress for now:\n        https://github.com/pytest-dev/pytest-django/pull/258\n    \"\"\"\n\n    def test_get_success_url(\n        self, user: settings.AUTH_USER_MODEL, request_factory: RequestFactory\n    ):\n        view = UserUpdateView()\n        request = request_factory.get(\"/fake-url/\")\n        request.user = user\n\n        view.request = request\n\n        assert view.get_success_url() == f\"/users/{user.username}/\"\n\n    def test_get_object(\n        self, user: settings.AUTH_USER_MODEL, request_factory: RequestFactory\n    ):\n        view = UserUpdateView()\n        request = request_factory.get(\"/fake-url/\")\n        request.user = user\n\n        view.request = request\n\n        assert view.get_object() == user\n\n\nclass TestUserRedirectView:\n    def test_get_redirect_url(\n        self, user: settings.AUTH_USER_MODEL, request_factory: RequestFactory\n    ):\n        view = UserRedirectView()\n        request = request_factory.get(\"/fake-url\")\n        request.user = user\n\n        view.request = request\n\n        assert view.get_redirect_url() == f\"/users/{user.username}/\"\n<|endoftext|>",
    "repo": "reckonsys/opensteer"
  },
  {
    "prompt": "fn assert_equal(left, right)\n\nfn skipif_unsupported(f)\n\nfn skipif_backend(skip_backend)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from posixpath import join as pjoin\n\nimport pytest\n\nimport ibis\nfrom ibis.tests.util import assert_equal\n\npytest.importorskip('hdfs')\npytest.importorskip('sqlalchemy')\npytest.importorskip('impala.dbapi')\n\nfrom ibis.impala.compat import HS2Error  # noqa: E402\n\npytestmark = pytest.mark.impala\n\n\ndef test_cleanup_tmp_table_on_gc(con, test_data_dir):\n    import gc\n    hdfs_path = pjoin(test_data_dir, 'parquet/tpch_region')\n    table = con.parquet_file(hdfs_path)\n    name = table.op().name\n    table = None\n    gc.collect()\n    assert not con.exists_table(name)\n\n\ndef test_persist_parquet_file_with_name(con, test_data_dir, temp_table_db):\n    import gc\n    hdfs_path = pjoin(test_data_dir, 'parquet/tpch_region')\n\n    tmp_db, name = temp_table_db\n    schema = ibis.schema([('r_regionkey', 'int16'),\n                          ('r_name', 'string'),\n                          ('r_comment', 'string')])\n    con.parquet_file(hdfs_path, schema=schema, name=name, database=tmp_db,\n                     persist=True)\n    gc.collect()\n\n    # table still exists\n    con.table(name, database=tmp_db)\n\n\ndef test_query_parquet_file_with_schema(con, test_data_dir):\n    hdfs_path = pjoin(test_data_dir, 'parquet/tpch_region')\n\n    schema = ibis.schema([('r_regionkey', 'int16'),\n                          ('r_name', 'string'),\n                          ('r_comment', 'string')])\n\n    table = con.parquet_file(hdfs_path, schema=schema)\n\n    name = table.op().name\n\n    # table exists\n    con.table(name)\n\n    expr = table.r_name.value_counts()\n    expr.execute()\n\n    assert table.count().execute() == 5\n\n\ndef test_query_parquet_file_like_table(con, test_data_dir):\n    hdfs_path = pjoin(test_data_dir, 'parquet/tpch_region')\n\n    ex_schema = ibis.schema([('r_regionkey', 'int16'),\n                             ('r_name', 'string'),\n                             ('r_comment', 'string')])\n\n    table = con.parquet_file(hdfs_path, like_table='tpch_region')\n\n    assert_equal(table.schema(), ex_schema)\n\n\ndef test_query_parquet_infer_schema(con, test_data_dir):\n    hdfs_path = pjoin(test_data_dir, 'parquet/tpch_region')\n    table = con.parquet_file(hdfs_path)\n\n    # NOTE: the actual schema should have an int16, but bc this is being\n    # inferred from a parquet file, which has no notion of int16, the\n    # inferred schema will have an int32 instead.\n    ex_schema = ibis.schema([('r_regionkey', 'int32'),\n                             ('r_name', 'string'),\n                             ('r_comment', 'string')])\n\n    assert_equal(table.schema(), ex_schema)\n\n\ndef test_create_table_persist_fails_if_called_twice(\n    con, temp_table_db, test_data_dir\n):\n    tmp_db, tname = temp_table_db\n\n    hdfs_path = pjoin(test_data_dir, 'parquet/tpch_region')\n    con.parquet_file(hdfs_path, name=tname, persist=True, database=tmp_db)\n\n    with pytest.raises(HS2Error):\n        con.parquet_file(hdfs_path, name=tname, persist=True, database=tmp_db)\n<|endoftext|>",
    "repo": "andrewseidl/ibis"
  },
  {
    "prompt": "\"\"\"Represents the binary state of events publication: ``published`` or\n``unpublished``\"\"\"\nclass EventStatus:\n\tUNPUBLISHED = 'unpublished'\n\tPUBLISHED = 'published'\n\n\n\"\"\"An event that can be published.\n\nArgs:\n    eventtype (:obj:`baroque.entities.eventtype.EventType` instance or `type` object): the type of the event\n    payload (dict, optional): the content of this event\n    description (str, optional): the description of this event\n    owner (str, optional): the owner of this event\"\"\"\nclass Event:\n\tfn __init__(self, eventtype, payload, description, owner)\n\t\"\"\"Sets the status of this event to published.\"\"\"\n\tfn set_published(self)\n\t\"\"\"Sets the status of this event to unpublished.\"\"\"\n\tfn set_unpublished(self)\n\t\"\"\"Sets the current time as timestamp of this event\"\"\"\n\tfn touch(self)\n\t\"\"\"Dumps this object to a JSON string.\n\t\n\tReturns:\n\t    str\"\"\"\n\tfn json(self)\n\t\"\"\"Returns the MD5 hash of this object.\n\t\n\tReturns:\n\t    str\"\"\"\n\tfn md5(self)\n\tfn __repr__(self)\n\n\n\"\"\"Describes generic events with a free-form content.\n\nArgs:\n    owner (str, optional): ID of the owner of this event type.\"\"\"\nclass GenericEventType(EventType):\n\tfn __init__(self, owner)\n\n\n\"\"\"Describes events cast when something changes its state.\nSuitable i.e. to track state machines changes.\nOld and new states are conveyed in the event payload, as well as\nthe cause of the state transition.\n\nArgs:\n    owner (str, optional): ID of the owner of this event type.\"\"\"\nclass StateTransitionEventType(EventType):\n\tfn __init__(self, owner)\n\n\n\"\"\"Describes events cast when some kind of operation is done on a\npiece of data.\nSuitable i.e. to track CRUD operations on DB tables or whole datastores.\nDetails about the impacted data entity and the operation are\nconveyed in the event payload.\n\nArgs:\n    owner (str, optional): ID of the owner of this event type.\"\"\"\nclass DataOperationEventType(EventType):\n\tfn __init__(self, owner)\n\n\n\"\"\"Describes events carrying metric data.\nSuitable i.e. to track values about measured physical quantities.\nThe metric name and value are conveyed in the event payload.\n\nArgs:\n    owner (str, optional): ID of the owner of this event type.\"\"\"\nclass MetricEventType(EventType):\n\tfn __init__(self, owner)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom baroque.entities.event import Event, EventStatus\nfrom baroque.defaults.eventtypes import GenericEventType\n\n\ndef test_constructor_failures():\n    with pytest.raises(TypeError):\n        Event()\n        pytest.fail()\n    with pytest.raises(AssertionError):\n        Event(None)\n        pytest.fail()\n    with pytest.raises(AssertionError):\n        Event(123)\n        pytest.fail()\n    with pytest.raises(AssertionError):\n        Event(GenericEventType(), payload=123)\n        pytest.fail()\n\n\ndef test_constructor():\n    e = Event(GenericEventType(), payload=dict(a=1, b=2), description='hello',\n              owner=1234)\n    assert isinstance(e.type, GenericEventType)\n    assert e.id is not None\n    assert not e.tags\n    assert e.status == EventStatus.UNPUBLISHED\n    assert e.timestamp is not None\n\n\ndef test_constructor_with_type_objects():\n    e1 = Event(GenericEventType(), payload=dict(a=1, b=2), description='hello',\n               owner=1234)\n    e2 = Event(GenericEventType, payload=dict(a=1, b=2), description='hello',\n               owner=1234)\n    assert isinstance(e1.type, GenericEventType)\n    assert isinstance(e2.type, GenericEventType)\n\n\ndef test_touch():\n    e = Event(GenericEventType(), payload=dict(a=1, b=2), description='hello',\n              owner=1234)\n    ts1 = e.timestamp\n    e = Event(GenericEventType(), payload=dict(a=1, b=2), description='hello',\n              owner=1234)\n    ts2 = e.timestamp\n    assert ts2 > ts1\n\n\ndef test_md5():\n    e = Event(GenericEventType(), payload=dict(a=1, b=2), description='hello',\n              owner=1234)\n    assert e.md5() is not None\n\n\ndef test_print():\n    print(Event(GenericEventType()))\n<|endoftext|>",
    "repo": "baroquehq/baroque"
  },
  {
    "prompt": "fn test_boolean()\n\nfn test_str_list()\n\nfn test_int_list()\n\nfn test_objectid_list()\n\nfn test_datetime()\n\nfn test_date()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from datetime import datetime as dt\n\nimport pytest\nfrom bson import ObjectId\n\nfrom flask_stupe.inputs import (boolean, date, datetime, int_list,\n                                objectid_list, str_list)\n\n\ndef test_boolean():\n    assert boolean(True) is True\n    assert boolean(False) is False\n\n    with pytest.raises(ValueError):\n        boolean(None)\n\n    assert boolean(\"0\") is False\n    assert boolean(\"1\") is True\n\n    assert boolean(\"true\") is True\n    assert boolean(\"false\") is False\n\n    assert boolean(\"tRuE\") is True\n    assert boolean(\"fAlSE\") is False\n\n    with pytest.raises(ValueError):\n        boolean(\"invalid_string\")\n\n\ndef test_str_list():\n    assert str_list(\"\") == []\n    assert str_list(\"foo\") == [\"foo\"]\n    assert str_list(\"foo,bar\") == [\"foo\", \"bar\"]\n    assert str_list(\"foo,baz,bar\") == [\"foo\", \"baz\", \"bar\"]\n\n\ndef test_int_list():\n    assert int_list(\"\") == []\n    assert int_list(\"42\") == [42]\n    assert int_list(\"13,37\") == [13, 37]\n    assert int_list(\"4,2,0\") == [4, 2, 0]\n\n\ndef test_objectid_list():\n    objectids = [ObjectId() for i in range(5)]\n    assert objectid_list(\"\") == []\n    assert objectid_list(str(objectids[0])) == [objectids[0]]\n\n    objectids_string = ','.join(str(o) for o in objectids)\n    assert objectid_list(objectids_string) == objectids\n\n\ndef test_datetime():\n    now = dt.now()\n    assert datetime(str(now)) == now\n\n\ndef test_date():\n    now = dt.now().date()\n    assert date(str(now)) == now\n<|endoftext|>",
    "repo": "pacud/flask-stupe"
  },
  {
    "prompt": "fn pytest_sim_params(metafunc)\n\nfn outlines(capsys)\n\nfn define_simple_circuit(T, circ_name, has_clk)\n\nclass TestPeekCircuit(m.Circuit):\n\t__test__ = False\n\tio = m.IO(I=m.In(T), O0=m.Out(T), O1=m.Out(T))\n\n\nclass ConfigReg(m.Circuit):\n\tio = m.IO(D=m.In(m.Bits[2]), Q=m.Out(m.Bits[2])) + m.ClockIO(has_ce=True)\n\treg = mantle.Register(2, has_ce=True, name='conf_reg')\n\n\nclass SimpleALU(m.Circuit):\n\tio = m.IO(a=m.In(m.UInt[16]), b=m.In(m.UInt[16]), c=m.Out(m.UInt[16]), config_data=m.In(m.Bits[2]), config_en=m.In(m.Enable)) + m.ClockIO()\n\topcode = ConfigReg(name='config_reg')(io.config_data, CE=io.config_en)\n\n\nclass AndCircuit(m.Circuit):\n\tio = m.IO(I0=m.In(m.Bit), I1=m.In(m.Bit), O=m.Out(m.Bit))\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from pathlib import Path\nimport fault\nimport magma as m\nfrom .common import pytest_sim_params\n\n\ndef pytest_generate_tests(metafunc):\n    pytest_sim_params(metafunc, 'system-verilog')\n\n\ndef test_ext_vlog(target, simulator):\n    # declare circuit\n    class myinv(m.Circuit):\n        io = m.IO(\n            in_=m.In(m.Bit),\n            out=m.Out(m.Bit)\n        )\n\n    # define test\n    tester = fault.InvTester(myinv)\n\n    # run the test\n    tester.compile_and_run(\n        target=target,\n        simulator=simulator,\n        ext_srcs=[Path('tests/verilog/myinv_extra_module.v').resolve()],\n        ext_model_file=True,\n        tmp_dir=True\n    )\n<|endoftext|>",
    "repo": "makaimann/fault"
  },
  {
    "prompt": "fn gpu_list(region)\n\nfn cpu_list(region)\n\n\"\"\"Decorator for running an Integ test with an instance_list and\nbreak on first success\n\nArgs:\n    instance_list (list): List of Compute instances for integ test.\nUsage:\n    @retry_with_instance_list(instance_list=[\"ml.g3.2\", \"ml.g2\"])\n    def sample_function():\n        print(\"xxxx....\")\"\"\"\nfn retry_with_instance_list(instance_list: list)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport time\n\nimport pytest\n\nfrom sagemaker import FactorizationMachines, FactorizationMachinesModel\nfrom sagemaker.utils import unique_name_from_base\nfrom tests.integ import datasets, TRAINING_DEFAULT_TIMEOUT_MINUTES\nfrom tests.integ.timeout import timeout, timeout_and_delete_endpoint_by_name\n\n\n@pytest.fixture\ndef training_set():\n    return datasets.one_p_mnist()\n\n\ndef test_factorization_machines(sagemaker_session, cpu_instance_type, training_set):\n    job_name = unique_name_from_base(\"fm\")\n\n    with timeout(minutes=TRAINING_DEFAULT_TIMEOUT_MINUTES):\n        fm = FactorizationMachines(\n            role=\"SageMakerRole\",\n            instance_count=1,\n            instance_type=cpu_instance_type,\n            num_factors=10,\n            predictor_type=\"regressor\",\n            epochs=2,\n            clip_gradient=1e2,\n            eps=0.001,\n            rescale_grad=1.0 / 100,\n            sagemaker_session=sagemaker_session,\n        )\n\n        # training labels must be 'float32'\n        fm.fit(\n            fm.record_set(training_set[0][:200], training_set[1][:200].astype(\"float32\")),\n            job_name=job_name,\n        )\n\n    with timeout_and_delete_endpoint_by_name(job_name, sagemaker_session):\n        model = FactorizationMachinesModel(\n            fm.model_data, role=\"SageMakerRole\", sagemaker_session=sagemaker_session\n        )\n        predictor = model.deploy(1, cpu_instance_type, endpoint_name=job_name)\n        result = predictor.predict(training_set[0][:10])\n\n        assert len(result) == 10\n        for record in result:\n            assert record.label[\"score\"] is not None\n\n\ndef test_async_factorization_machines(sagemaker_session, cpu_instance_type, training_set):\n    job_name = unique_name_from_base(\"fm\")\n\n    with timeout(minutes=5):\n        fm = FactorizationMachines(\n            role=\"SageMakerRole\",\n            instance_count=1,\n            instance_type=cpu_instance_type,\n            num_factors=10,\n            predictor_type=\"regressor\",\n            epochs=2,\n            clip_gradient=1e2,\n            eps=0.001,\n            rescale_grad=1.0 / 100,\n            sagemaker_session=sagemaker_session,\n        )\n\n        # training labels must be 'float32'\n        fm.fit(\n            fm.record_set(training_set[0][:200], training_set[1][:200].astype(\"float32\")),\n            job_name=job_name,\n            wait=False,\n        )\n\n        print(\"Detached from training job. Will re-attach in 20 seconds\")\n        time.sleep(20)\n        print(\"attaching now...\")\n\n    with timeout_and_delete_endpoint_by_name(job_name, sagemaker_session):\n        estimator = FactorizationMachines.attach(\n            training_job_name=job_name, sagemaker_session=sagemaker_session\n        )\n        model = FactorizationMachinesModel(\n            estimator.model_data, role=\"SageMakerRole\", sagemaker_session=sagemaker_session\n        )\n        predictor = model.deploy(1, cpu_instance_type, endpoint_name=job_name)\n        result = predictor.predict(training_set[0][:10])\n\n        assert len(result) == 10\n        for record in result:\n            assert record.label[\"score\"] is not None\n<|endoftext|>",
    "repo": "Snoyark/sagemaker-python-sdk"
  },
  {
    "prompt": "\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "#!/usr/bin/env python\n\nimport os\nimport boto3\nimport pytest\nfrom moto import mock_s3\nfrom moto import mock_ssm\n\n# our module(s)\nfrom nicolet_processor import NicoletProcessor\nfrom base_processor.tests import init_ssm\nfrom base_processor.timeseries.tests import timeseries_test, channels_test\n\nfrom params import params_channel\n\n@mock_s3\n@mock_ssm\n@pytest.mark.parametrize(\"ts_expected\", params_channel)\ndef test_channel(ts_expected):\n    init_ssm()\n\n    task = NicoletProcessor(inputs=ts_expected.inputs)\n    channels_test(task, ts_expected)\n<|endoftext|>",
    "repo": "Pennsieve/timeseries-processor"
  },
  {
    "prompt": "fn start_service(service_name, host, port)\n\nfn stop_process(process)\n\nfn dynamodb2_server()\n\nfn s3_server()\n\nfn kms_server()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import boto3\nfrom contextlib import contextmanager\nimport pytest\nfrom moto import mock_s3\nfrom mock_server import *  # noqa\nfrom pytest_lazyfixture import lazy_fixture\nimport tempfile\nimport os\n\nimport ray\nfrom ray.experimental import workflow\nfrom ray.experimental.workflow import storage\nfrom ray.tests.conftest import get_default_fixture_ray_kwargs\n\n\n@pytest.fixture(scope=\"session\")\ndef filesystem_storage():\n    with tempfile.TemporaryDirectory() as d:\n        yield d\n\n\n@pytest.fixture(scope=\"session\")\ndef aws_credentials():\n    old_env = os.environ\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"testing\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"testing\"\n    os.environ[\"AWS_SECURITY_TOKEN\"] = \"testing\"\n    os.environ[\"AWS_SESSION_TOKEN\"] = \"testing\"\n    yield (f\"aws_access_key_id={os.environ['AWS_ACCESS_KEY_ID']}&\"\n           f\"aws_secret_access_key={os.environ['AWS_SECRET_ACCESS_KEY']}&\"\n           f\"aws_session_token={os.environ['AWS_SESSION_TOKEN']}\")\n    os.environ = old_env\n\n\n@pytest.fixture(scope=\"session\")\ndef s3_storage(aws_credentials, s3_server):\n    with mock_s3():\n        client = boto3.client(\n            \"s3\", region_name=\"us-west-2\", endpoint_url=s3_server)\n        client.create_bucket(Bucket=\"test_bucket\")\n        url = (\"s3://test_bucket/workflow\"\n               f\"?region_name=us-west-2&endpoint_url={s3_server}\"\n               f\"&{aws_credentials}\")\n        yield url\n\n\n@contextmanager\ndef _workflow_start(storage_url, shared, **kwargs):\n    init_kwargs = get_default_fixture_ray_kwargs()\n    init_kwargs.update(kwargs)\n    if ray.is_initialized():\n        ray.shutdown()\n        storage.set_global_storage(None)\n    # Sometimes pytest does not cleanup all global variables.\n    # we have to manually reset the workflow storage. This\n    # should not be an issue for normal use cases, because global variables\n    # are freed after the driver exits.\n    address_info = ray.init(**init_kwargs)\n    workflow.init(storage_url)\n    yield address_info\n    # The code after the yield will run as teardown code.\n    ray.shutdown()\n    storage.set_global_storage(None)\n\n\n@pytest.fixture(scope=\"function\")\ndef workflow_start_regular(storage_url, request):\n    param = getattr(request, \"param\", {})\n    with _workflow_start(storage_url, False, **param) as res:\n        yield res\n\n\n@pytest.fixture(scope=\"session\")\ndef workflow_start_regular_shared(storage_url, request):\n    param = getattr(request, \"param\", {})\n    with _workflow_start(storage_url, True, **param) as res:\n        yield res\n\n\ndef pytest_generate_tests(metafunc):\n    if \"storage_url\" in metafunc.fixturenames:\n        metafunc.parametrize(\n            \"storage_url\",\n            [lazy_fixture(\"s3_storage\"),\n             lazy_fixture(\"filesystem_storage\")],\n            scope=\"session\")\n<|endoftext|>",
    "repo": "ddworak94/ray"
  },
  {
    "prompt": "fn countdown(n)\n\nfn multiply(x, y)\n\nfn count_letters(tup)\n\nfn count_letters2(obj)\n\nfn word_multiply(x, word)\n\nfn count_forever()\n\nfn get_task_name(task)\n\nfn get_user_id(user)\n\nfn hello()\n\nfn result(obj)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom django_q.tasks import async\nfrom django_q.brokers import get_broker\nfrom django_q.cluster import Cluster\nfrom django_q.monitor import monitor, info\nfrom django_q.status import Stat\nfrom django_q.conf import Conf\n\n\n@pytest.mark.django_db\ndef test_monitor(monkeypatch):\n    assert Stat.get(0).sentinel == 0\n    c = Cluster()\n    c.start()\n    stats = monitor(run_once=True)\n    c.stop()\n    assert len(stats) > 0\n    found_c = False\n    for stat in stats:\n        if stat.cluster_id == c.pid:\n            found_c = True\n            assert stat.uptime() > 0\n            assert stat.empty_queues() is True\n            break\n    assert found_c is True\n    # test lock size\n    monkeypatch.setattr(Conf, 'ORM', 'default')\n    b = get_broker('monitor_test')\n    b.enqueue('test')\n    b.dequeue()\n    assert b.lock_size() == 1\n    monitor(run_once=True, broker=b)\n    b.delete_queue()\n\n\n@pytest.mark.django_db\ndef test_info():\n    info()\n    do_sync()\n    info()\n    for _ in range(24):\n        do_sync()\n    info()\n\n\ndef do_sync():\n    async('django_q.tests.tasks.countdown', 1, sync=True, save=True)\n<|endoftext|>",
    "repo": "fintanMcor/django-q"
  },
  {
    "prompt": "class TaxonomyData:\n\tkingdom: str = dataclasses.field(metadata={'key': True})\n\torder: str = dataclasses.field(metadata={'key': True})\n\tfamily: str = dataclasses.field(metadata={'key': True})\n\tgenus: str = dataclasses.field(metadata={'key': True})\n\n\nclass TreeData:\n\tserial_number: int = dataclasses.field(metadata={'key': True})\n\ttaxonomy: TaxonomyData = dataclasses.field(metadata={'key': True})\n\tspecie: str = dataclasses.field(metadata={'key': True})\n\tdiameter_m: float = None\n\tlong_description: bytes = None\n\thas_flower: bool = None\n\tplantation_datetime: datetime.datetime = None\n\tlast_pruning_date: datetime.date = None\n\n\nclass PlantMagnoliaTask(Task):\n\tfn __init__(self, serial_number, specie, model)\n\tfn name(self)\n\n\nclass FailedTask(Task):\n\tfn name(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"\"\"\"\n\n# Standard library modules.\n\n# Third party modules.\nimport pytest\n\n# Local modules.\nfrom pipeline_async.task import Task\nimport mock\n\n# Globals and constants variables.\n\n\n@pytest.mark.asyncio\nasync def test_task_run_nomodel():\n    task = mock.PlantMagnoliaTask(1, 'Magnolia grandiflora')\n    assert await task.run()\n    assert await task.run() # No check that result was previously calculated\n\n\n\n<|endoftext|>",
    "repo": "ppinard/async_pipeline"
  },
  {
    "prompt": "\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from async_asgi_testclient import TestClient\nfrom myapp import main\nimport pytest\n\n\n@pytest.mark.asyncio\nasync def test_willpyre_app():\n\n    async with TestClient(main) as client:\n        resp = await client.get(\"/\")\n        assert resp.status_code == 200\n        assert resp.text == \"index page\"\n\n\n@pytest.mark.asyncio\nasync def test_willpyre_post():\n\n    async with TestClient(main) as client:\n        resp = await client.post(\"/login/\", data=\"a=anything\")\n        assert resp.status_code == 200\n        assert resp.text == \"anything\"\n\n\n@pytest.mark.asyncio\nasync def test_willpyre_get():\n\n    async with TestClient(main) as client:\n        resp = await client.get(\"/login/?user=admin\")\n        assert resp.status_code == 200\n        assert resp.text == \"Welcome admin\"\n\n\n@pytest.mark.asyncio\nasync def test_trailing_slash():\n\n    async with TestClient(main) as client:\n        resp = await client.get(\"/login\")\n        assert resp.status_code == 200\n        assert resp.text == \"Welcome ordinary user\"\n\n\n@pytest.mark.asyncio\nasync def test_url_vars():\n\n    async with TestClient(main) as client:\n        resp = await client.get(\"/api/hello\")\n        assert resp.status_code == 200\n        assert resp.text == \"You requested the variable hello\"\n\n\n@pytest.mark.asyncio\nasync def test_url_many():\n\n    async with TestClient(main) as client:\n        resp = await client.get(\"/static/foo/bar/baz\")\n        assert resp.status_code == 200\n        assert resp.text == \"foobarbaz\"\n\n\n@pytest.mark.asyncio\nasync def test_utils():\n\n    async with TestClient(main) as client:\n        resp = await client.get(\"/json\")\n        assert resp.json() == {'a': 'b'}\n        assert resp.headers[\"Content-Type\"] == \"application/json\"\n\n\n@pytest.mark.asyncio\nasync def test_response404():\n    async with TestClient(main) as client:\n        resp = await client.get(\"/non-exhistent\")\n        assert resp.text == \"Not found\"\n        assert resp.status_code == 404\n\n\n@pytest.mark.asyncio\nasync def test_response405():\n    async with TestClient(main) as client:\n        resp = await client.open(\"/login\", method=\"NO_SUCH_METHOD\")\n        assert resp.text == \"Method not allowed\"\n        assert resp.status_code == 405\n\n\n@pytest.mark.asyncio\nasync def test_put():\n\n    async with TestClient(main) as client:\n        resp = await client.put(\"/others\")\n        assert resp.text == \"others\"\n\n\n@pytest.mark.asyncio\nasync def test_patch():\n\n    async with TestClient(main) as client:\n        resp = await client.patch(\"/others\")\n        assert resp.text == \"others\"\n<|endoftext|>",
    "repo": "Nafi-Amaan-Hossain/willpyre"
  },
  {
    "prompt": "fn _verify_attr(name)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom konst._common import _verify_attr\n\n\n@pytest.mark.parametrize('name', ['lower', 'UPPER', 'PascalCase', 'camelCase'])\ndef test_regular_name(name):\n    _verify_attr(name)\n\n\n@pytest.mark.parametrize('name', ['_single', '__double', '__dunder__'])\ndef test_bad_name(name):\n    with pytest.raises(AttributeError):\n        _verify_attr(name)\n<|endoftext|>",
    "repo": "yehonatanz/konst"
  },
  {
    "prompt": "fn nodeIds(nodeSet)\n\nfn validUpgrade(nodeIds, tconf)\n\nfn sendUpgrade(client, wallet, upgradeData)\n\nfn ensureUpgradeSent(looper, trustee, trusteeWallet, upgradeData)\n\nfn checkUpgradeScheduled(nodes, version)\n\nfn checkNoUpgradeScheduled(nodes)\n\nfn codeVersion()\n\nfn bumpVersion(v)\n\nfn bumpedVersion()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom copy import deepcopy\n\nfrom plenum.common.eventually import eventually\nfrom plenum.common.txn import VERSION, NAME\nfrom plenum.common.util import randomString\nfrom plenum.test.test_node import checkNodesConnected\nfrom sovrin.common.txn import SHA256, CANCEL, ACTION\nfrom sovrin.test.conftest import tconf, testNodeClass, testClientClass\nfrom plenum.test.conftest import allPluginsPath\nfrom sovrin.test.helper import TestNode\nfrom sovrin.test.upgrade.helper import bumpVersion, sendUpgrade, \\\n    ensureUpgradeSent, checkUpgradeScheduled\n\n\n@pytest.fixture(scope=\"module\")\ndef txnPoolNodeSet(tconf, nodeSet):\n    for node in nodeSet:\n        node.config = tconf\n        node.upgrader.config = tconf\n    return nodeSet\n\n\ndef testUpgradeLatestUncancelledVersion(looper,\n                                        txnPoolNodeSet, tconf, nodeThetaAdded,\n                                        validUpgrade, trustee, trusteeWallet,\n                                        tdirWithPoolTxns, allPluginsPath):\n    \"\"\"\n    A node starts and finds several upgrades but selects the latest one which\n    is not cancelled, eg node is on version 1.2 but finds 1.3, 1.4 and 1.5 but\n    since 1.5 is cancelled, it selects 1.4\n    \"\"\"\n    nodeSet = txnPoolNodeSet\n    newSteward, newStewardWallet, newNode = nodeThetaAdded\n    for node in nodeSet[:-1]:\n        node.nodestack.removeRemoteByName(newNode.nodestack.name)\n        newNode.nodestack.removeRemoteByName(node.nodestack.name)\n    newNode.stop()\n    nodeSet = nodeSet[:-1]\n    looper.removeProdable(newNode)\n\n    upgr1 = deepcopy(validUpgrade)\n\n    upgr2 = deepcopy(upgr1)\n    upgr2[VERSION] = bumpVersion(upgr1[VERSION])\n    upgr2[NAME] += randomString(3)\n    upgr2[SHA256] = randomString(32)\n\n    upgr3 = deepcopy(upgr2)\n    upgr3[VERSION] = bumpVersion(upgr2[VERSION])\n    upgr3[NAME] += randomString(3)\n    upgr3[SHA256] = randomString(32)\n\n    upgr4 = deepcopy(upgr3)\n    upgr4[ACTION] = CANCEL\n\n    ensureUpgradeSent(looper, trustee, trusteeWallet, upgr1)\n    looper.run(eventually(checkUpgradeScheduled, nodeSet[:-1], upgr1[VERSION],\n                          retryWait=1, timeout=5))\n\n    ensureUpgradeSent(looper, trustee, trusteeWallet, upgr2)\n    looper.run(eventually(checkUpgradeScheduled, nodeSet[:-1], upgr2[VERSION],\n                          retryWait=1, timeout=5))\n\n    ensureUpgradeSent(looper, trustee, trusteeWallet, upgr3)\n    looper.run(eventually(checkUpgradeScheduled, nodeSet[:-1], upgr3[VERSION],\n                          retryWait=1, timeout=5))\n\n    ensureUpgradeSent(looper, trustee, trusteeWallet, upgr4)\n    looper.run(eventually(checkUpgradeScheduled, nodeSet[:-1], upgr2[VERSION],\n                          retryWait=1, timeout=5))\n\n    trustee.stopRetrying()\n\n    newNode = TestNode(newNode.name, basedirpath=tdirWithPoolTxns,\n                       config=tconf, pluginPaths=allPluginsPath,\n                       ha=newNode.nodestack.ha, cliha=newNode.clientstack.ha)\n    looper.add(newNode)\n    nodeSet.append(newNode)\n    looper.run(checkNodesConnected(nodeSet, overrideTimeout=30))\n\n    looper.run(eventually(checkUpgradeScheduled, [newNode, ], upgr2[VERSION],\n                          retryWait=1, timeout=10))\n\n\n<|endoftext|>",
    "repo": "sovrin-foundation/old-sovrin"
  },
  {
    "prompt": "fn _skip_word(word: str, no_skip_spaces: bool) -> bool\n\nfn _skip_date(date_from_word: str, cut_off_date: str) -> bool\n\nfn _scrape_once(data, config: Config) -> Iterator[WordPronPair]\n\n\"\"\"Scrapes with a given configuration.\"\"\"\nfn scrape(config: Config) -> Iterator[WordPronPair]\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import collections\n\nimport pytest\n\nfrom wikipron.scrape import scrape, _skip_word, _skip_date\nfrom wikipron.extract import EXTRACTION_FUNCTIONS\n\nfrom . import can_connect_to_wiktionary, config_factory\n\n\nSmokeTestLanguage = collections.namedtuple(\n    \"SmokeTestLanguage\", (\"key\", \"wik_name\", \"config_params\")\n)\nSmokeTestLanguage.__doc__ = \"\"\"\nRepresents a language to run a smoke test on.\n\nParameters\n----------\nkey : str\n    An ISO 639 code or language name.\nwik_name : str\n    The language name used by Wiktionary.\nconfig_params : dict\n    Parameters for the Config class.\n\"\"\"\n\n_SMOKE_TEST_LANGUAGES = [\n    SmokeTestLanguage(\"eng\", \"English\", {}),\n    # Test that 'sup[a[@title = \"wikipedia:Slovak phonology\"]]' works.\n    SmokeTestLanguage(\"slk\", \"Slovak\", {}),\n    # Test that the extra \"span\" layer for Korean is handled.\n    # Korean data is mostly phonetic transcription only.\n    SmokeTestLanguage(\"kor\", \"Korean\", {\"phonetic\": True}),\n    SmokeTestLanguage(\"khb\", \"L\u00fc\", {}),\n    SmokeTestLanguage(\"khm\", \"Khmer\", {}),\n    SmokeTestLanguage(\"shn\", \"Shan\", {}),\n    SmokeTestLanguage(\"tha\", \"Thai\", {}),\n    SmokeTestLanguage(\"lat\", \"Latin\", {}),\n    # Japanese data is mostly phonetic transcription.\n    SmokeTestLanguage(\"jpn\", \"Japanese\", {\"phonetic\": True}),\n    SmokeTestLanguage(\"cmn\", \"Chinese\", {}),\n    # Vietnamese data is mostly phonetic transcription.\n    SmokeTestLanguage(\"vie\", \"Vietnamese\", {\"phonetic\": True}),\n]\n\n\n@pytest.mark.skipif(not can_connect_to_wiktionary(), reason=\"need Internet\")\n@pytest.mark.parametrize(\"smoke_test_language\", _SMOKE_TEST_LANGUAGES)\ndef test_smoke_test_scrape(smoke_test_language):\n    \"\"\"A smoke test for scrape().\"\"\"\n    n = 10  # number of word-pron pairs to scrape\n    config = config_factory(\n        key=smoke_test_language.key, **smoke_test_language.config_params\n    )\n    assert config.language == smoke_test_language.wik_name\n    pairs = []\n    for i, (word, pron) in enumerate(scrape(config)):\n        if i >= n:\n            break\n        pairs.append((word, pron))\n    assert len(pairs) == n\n    assert all(word and pron for (word, pron) in pairs)\n\n\ndef test_special_languages_covered_by_smoke_test():\n    \"\"\"All languages handled by wikipron.extract must have a smoke test.\"\"\"\n    special_languages = {lang for lang in EXTRACTION_FUNCTIONS.keys()}\n    smoke_test_languages = {lang.wik_name for lang in _SMOKE_TEST_LANGUAGES}\n    assert special_languages.issubset(smoke_test_languages), (\n        \"These languages must also be included in the smoke test: \"\n        f\"{special_languages - smoke_test_languages}\"\n    )\n\n\n@pytest.mark.parametrize(\n    \"word, no_skip_spaces, expected\",\n    [\n        (\"foobar\", False, False),\n        (\"a phrase\", False, True),\n        (\"hyphen-ated\", False, True),\n        (\"prefix-\", False, True),\n        (\"-suffix\", False, True),\n        (\"hasdigit2\", False, True),\n        (\"a phrase\", True, False),\n        (\"foobar\", True, False),\n    ],\n)\ndef test__skip_word(word, no_skip_spaces, expected):\n    assert _skip_word(word, no_skip_spaces) == expected\n\n\n@pytest.mark.parametrize(\n    \"date_from_word, cut_off_date, expected\",\n    [\n        (\"2019-10-15\", \"2019-10-20\", False),\n        (\"2019-10-20\", \"2019-10-20\", False),\n        (\"2019-10-25\", \"2019-10-20\", True),\n    ],\n)\ndef test__skip_date(date_from_word, cut_off_date, expected):\n    assert _skip_date(date_from_word, cut_off_date) == expected\n<|endoftext|>",
    "repo": "Alireza-Sampour/wikipron"
  },
  {
    "prompt": "fn test_root() -> Path\n\nfn get_demo_config()\n\nfn app_sqlite_target(app_name: str) -> str\n\nfn create_lookup_db(app_name: str)\n\nfn configure_logging()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nimport sqlite3\n\nfrom create_lookups import app_sqlite_target\n\nfrom haiku_node.blockchain_helpers.eos import eosio_account\nfrom haiku_node.lookup.eos_lookup import UnificationLookup, default_db\n\n\n@pytest.mark.parametrize(\"app_name\", ['app1', 'app2', 'app3'])\ndef test_validate_lookup_dbs(app_name):\n    db_name = app_sqlite_target(app_name)\n    conn = sqlite3.connect(db_name)\n    c = conn.cursor()\n\n    t = ('user2',)\n    c.execute('SELECT native_id FROM lookup WHERE eos_account=?', t)\n    res = c.fetchone()[0]\n    conn.close()\n    assert res == '2'\n\n\n@pytest.mark.parametrize(\"app_name\", ['app1'])\ndef test_user_lookup(app_name):\n    db_name = app_sqlite_target(app_name)\n    ul = UnificationLookup(db_name)\n    user_lookup(ul)\n\n\ndef user_lookup(ul: UnificationLookup):\n    u1name = 15426359793685626880\n    u1name_str = '15426359793685626880'\n    u1_str = 'user1'\n    u1_n_id = 1\n\n    print(f\"eosio_account.name_to_string({u1name}) =\",\n          eosio_account.name_to_string(u1name))\n    print(f\"eosio_account.name_to_string('{u1name_str}') =\",\n          eosio_account.name_to_string(u1name_str))\n    print(f\"eosio_account.string_to_name('{u1_str}') =\",\n          eosio_account.string_to_name(u1_str))\n\n    assert (eosio_account.name_to_string(u1name) == u1_str) is True\n    assert (eosio_account.name_to_string(u1name_str) == u1_str) is True\n    assert (eosio_account.string_to_name(u1_str) == u1name) is True\n\n    print(f\"Lookup account as readable string: '{u1_str}'\")\n    n_id = ul.get_native_user_id(u1_str)\n    print(f\"{u1_str} native ID:\", n_id)\n\n    assert (int(n_id) == u1_n_id) is True\n\n    print(f\"Lookup account as account_name int: {u1name}\")\n    n_id = ul.get_native_user_id(u1name)\n    print(f\"{u1_str} native ID:\", n_id)\n\n    assert (int(n_id) == u1_n_id) is True\n\n    print(f\"Lookup account as account_name string: '{u1name_str}'\")\n    n_id = ul.get_native_user_id(u1name_str)\n    print(f\"{u1_str} native ID:\", n_id)\n\n    assert (int(n_id) == u1_n_id) is True\n\n    print(f\"Lookup native UID {u1_n_id} as EOS Account\")\n    eos_acc = ul.get_eos_account(u1_n_id)\n    print(f\"UID {u1_n_id} EOS account:\", eos_acc)\n\n    assert (eos_acc == u1_str) is True\n\n    meta_data = ul.get_native_user_meta()\n    print(\"Meta data:\")\n    print(meta_data)\n\n    schema_map = ul.get_schema_map('0')\n    print(\"Schema map for app1 schema_id 0:\")\n\n    print(schema_map)\n\n    real_table = ul.get_real_table_info('0', 'data_1')\n    print(\"real table data for app1 schema_id 0, data_1:\")\n    print(real_table)\n\n\nif __name__ == \"__main__\":\n    user_lookup(UnificationLookup(default_db()))\n<|endoftext|>",
    "repo": "unification-com/haiku-node-prototype"
  },
  {
    "prompt": "\"\"\"Backchannel Abstract Base Class.\n\nPluggable Backchannels should implement each of the methods relevant to\ntheir agent and testing scenario.\"\"\"\nclass Backchannel(ABC):\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\" Test Suite fixture definitions.\n\n    These fixtures define the core functionality of the testing agent.\n\n    For more information on how pytest fixtures work, see\n    https://docs.pytest.org/en/latest/fixture.html#fixture\n\"\"\"\n\nimport asyncio\nimport json\nimport os\nfrom importlib import import_module\nfrom contextlib import suppress\n\nimport pytest\nfrom aiohttp import web\n\nfrom . import Suite\nfrom .backchannel import SuiteConnectionInfo\n\n# pylint: disable=redefined-outer-name\n\n\n@pytest.fixture(scope='session')\ndef event_loop():\n    \"\"\" Create a session scoped event loop.\n\n        pytest.asyncio plugin provides a default function scoped event loop\n        which cannot be used as a dependency to session scoped fixtures.\n    \"\"\"\n    return asyncio.get_event_loop()\n\n\n@pytest.fixture(scope='session')\ndef config(pytestconfig):\n    \"\"\" Get suite configuration.\n    \"\"\"\n    yield pytestconfig.suite_config\n\n\n@pytest.fixture(scope='session')\ndef suite():\n    \"\"\"Get channel manager for test suite.\"\"\"\n    yield Suite()\n\n\n@pytest.fixture(scope='session')\nasync def http_endpoint(config, suite):\n    \"\"\"Create http server task.\"\"\"\n\n    async def handle(request):\n        \"\"\"aiohttp handle POST.\"\"\"\n        response = []\n        with suite.reply(response.append):\n            await suite.handle(await request.read())\n\n        if response:\n            return web.Response(body=response.pop())\n\n        raise web.HTTPAccepted()\n\n    app = web.Application()\n    app.router.add_post('/', handle)\n    runner = web.AppRunner(app)\n    await runner.setup()\n    site = web.TCPSite(runner, config['host'], config['port'])\n    server_task = asyncio.ensure_future(site.start())\n    yield\n    server_task.cancel()\n    with suppress(asyncio.CancelledError):\n        await server_task\n    await runner.cleanup()\n\n\n@pytest.fixture(scope='session')\nasync def backchannel(config, http_endpoint, suite):\n    \"\"\"Get backchannel to test subject.\"\"\"\n    if 'backchannel' in config and config['backchannel']:\n        path_parts = config['backchannel'].split('.')\n        mod_path, class_name = '.'.join(path_parts[:-1]), path_parts[-1]\n        mod = import_module(mod_path)\n        backchannel_class = getattr(mod, class_name)\n    else:\n        from default import ManualBackchannel\n        backchannel_class = ManualBackchannel\n\n    suite.set_backchannel(backchannel_class())\n    await suite.backchannel.setup(config, suite)\n    yield suite.backchannel\n    await suite.backchannel.close()\n\n\n@pytest.fixture(scope='session')\nasync def provider(config, suite):\n    \"\"\"Get provider to test subject.\"\"\"\n    if not 'provider' in config and not config['provider']:\n        raise \"No 'provider' was specified in the config file\"\n    path_parts = config['provider'].split('.')\n    mod_path, class_name = '.'.join(path_parts[:-1]), path_parts[-1]\n    mod = import_module(mod_path)\n    provider_class = getattr(mod, class_name)\n    suite.set_provider(provider_class())\n    await suite.provider.setup(config)\n    yield suite.provider\n\n\n@pytest.fixture(scope='session')\ndef temporary_channel(http_endpoint, suite):\n    \"\"\"Get contextmanager for using a temporary channel.\"\"\"\n    yield suite.temporary_channel\n\n\n@pytest.fixture\nasync def connection(config, temporary_channel, backchannel):\n    \"\"\"Fixture for active connection\"\"\"\n    with temporary_channel() as conn:\n        info = SuiteConnectionInfo(\n            conn.did,\n            conn.verkey_b58,\n            'test-suite',\n            config['endpoint']\n        )\n        their_info = await backchannel.new_connection(info)\n        conn.update(**their_info._asdict())\n        yield conn\n<|endoftext|>",
    "repo": "dbluhm/aries-protocol-test-auite"
  },
  {
    "prompt": "fn atp_get_original_url(safe_url)\n\nfn proofpoint_get_original_url(safe_url)\n\nfn unescape_url(escaped_url)\n\nfn get_fqdn(the_input)\n\nfn extract_fqdn(the_input)\n\nfn main()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# -*- coding: utf-8 -*-\nimport demistomock as demisto\nfrom ExtractDomainAndFQDNFromUrlAndEmail import extract_fqdn, main\nimport pytest\n\n\n@pytest.mark.parametrize('input,fqdn', [  # noqa: E501 disable-secrets-detection\n    ('www.static.attackiqtes.com', 'www.static.attackiqtes.com'),\n    ('http:www.static.attackiqtes.com', 'www.static.attackiqtes.com'),\n    ('attackiqtes.co.il', 'attackiqtes.co.il'),\n    ('ftp://www.test.com/test2/dev', 'www.test.com'),\n    ('http://www.test.com/test2/dev', 'www.test.com'),\n    ('www.test.fake', ''),\n    ('www[.]demisto[.]com', 'www.demisto.com'),\n    ('www[.]demisto[.]test2.com', 'www.demisto.test2.com'),\n    ('test.zip', ''),\n    ('https%3A%2F%2Fdulunggakada40[.]com', 'dulunggakada40.com'),\n    ('https%3A%2F%2Fpath.test.com', 'path.test.com'),\n    ('https://urldefense.com/v3/__http://survey.lavulcamktg.cl/index.php/783758', 'survey.lavulcamktg.cl'),\n    ('this.is.test.com', 'this.is.test.com'),\n    ('caseapi.phishlabs.com', 'caseapi.phishlabs.com'),\n    ('www.b\u00fccher.de', 'www.b\u00fccher.de'),\n    ('https://urldefense.proofpoint.com/v2/url?u=http-3A__go.getpostman.com_y4wULsdG0h0DDMY0Dv00100&d=DwMFaQ&c'\n     '=ywDJJevdGcjv4rm9P3FcNg&r=s5kA2oIAQRXsacJiBKmTORIWyRN39ZKhobje2GyRgNs&m'\n     '=vN1dVSiZvEoM9oExtQqEptm9Dbvq9tnjACDZzrBLaWI&s=zroN7KQdBCPBOfhOmv5SP1DDzZKZ1y9I3x4STS5PbHA&e=',\n     'go.getpostman.com'),  # noqa: E501\n    ('www[.]demisto[.]com', 'www.demisto.com'),\n    ('hxxp://www[.]demisto[.]com', 'www.demisto.com'),\n    ('www[.]demisto.test[.]com', 'www.demisto.test.com'),\n    ('https://emea01.safelinks.protection.outlook.com/?url=https%3A%2F%2Ftwitter.com%2FPhilipsBeLux&data=02|01'\n     '||cb2462dc8640484baf7608d638d2a698|1a407a2d76754d178692b3ac285306e4|0|0|636758874714819880&sdata'\n     '=dnJiphWFhnAKsk5Ps0bj0p%2FvXVo8TpidtGZcW6t8lDQ%3D&reserved=0%3E%5bcid:image003.gif@01CF4D7F.1DF62650%5d'\n     '%3C', 'twitter.com'),  # noqa: E501 disable-secrets-detection\n])  # noqa: E124\ndef test_extract_fqdn_or_domain(input, fqdn):\n    extracted_fqdn = extract_fqdn(input)\n    # extracted_domain = extract_fqdn_or_domain(input, is_domain=True)\n\n    assert extracted_fqdn == fqdn\n    # assert extracted_domain == domain\n\n\ndef test_extract_fqdn_or_domain_empty_indicators(mocker):\n    mocker.patch.object(demisto, 'args', return_value={'input': '1Ab.Vt'})\n    mocker.patch.object(demisto, 'results')\n\n    main()\n    results = demisto.results.call_args[0]\n\n    assert results[0] == [{'Contents': [], 'ContentsFormat': 'json', 'Type': 1}]\n<|endoftext|>",
    "repo": "jon-athon/content"
  },
  {
    "prompt": "\"\"\"Sets the RPO to ASYNC_TURBO, enabling the turbo replication feature\"\"\"\nfn set_rpo_async_turbo(bucket_name)\n\n\"\"\"Creates dual-region bucket with turbo replication enabled.\"\"\"\nfn create_bucket_turbo_replication(bucket_name)\n\n\"\"\"Sets the RPO to DEFAULT, disabling the turbo replication feature\"\"\"\nfn set_rpo_default(bucket_name)\n\n\"\"\"Gets the RPO of the bucket\"\"\"\nfn get_rpo(bucket_name)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport uuid\n\nfrom google.cloud import storage\nimport pytest\n\nimport storage_create_bucket_turbo_replication\nimport storage_get_rpo\nimport storage_set_rpo_async_turbo\nimport storage_set_rpo_default\n\n\n@pytest.fixture\ndef dual_region_bucket():\n    \"\"\"Yields a dual region bucket that is deleted after the test completes.\"\"\"\n    bucket = None\n    while bucket is None or bucket.exists():\n        bucket_name = \"bucket-lock-{}\".format(uuid.uuid4())\n        bucket = storage.Client().bucket(bucket_name)\n        bucket.location = \"NAM4\"\n    bucket.create()\n    yield bucket\n    bucket.delete(force=True)\n\n\ndef test_get_rpo(dual_region_bucket, capsys):\n    storage_get_rpo.get_rpo(dual_region_bucket.name)\n    out, _ = capsys.readouterr()\n    assert f\"RPO for {dual_region_bucket.name} is DEFAULT.\" in out\n\n\ndef test_set_rpo_async_turbo(dual_region_bucket, capsys):\n    storage_set_rpo_async_turbo.set_rpo_async_turbo(dual_region_bucket.name)\n    out, _ = capsys.readouterr()\n    assert f\"RPO is ASYNC_TURBO for {dual_region_bucket.name}.\" in out\n\n\ndef test_set_rpo_default(dual_region_bucket, capsys):\n    storage_set_rpo_default.set_rpo_default(dual_region_bucket.name)\n    out, _ = capsys.readouterr()\n    assert f\"RPO is DEFAULT for {dual_region_bucket.name}.\" in out\n\n\ndef test_create_bucket_turbo_replication(capsys):\n    bucket_name = \"test-rpo-{}\".format(uuid.uuid4())\n    storage_create_bucket_turbo_replication.create_bucket_turbo_replication(bucket_name)\n    out, _ = capsys.readouterr()\n    assert f\"{bucket_name} created with RPO ASYNC_TURBO in NAM4.\" in out\n<|endoftext|>",
    "repo": "LaudateCorpus1/python-storage"
  },
  {
    "prompt": "\"\"\"Fixed response as replacement for app.db.crud.observations.get_actuals_left_outer_join_with_predictions\n    \"\"\"\nfn get_actuals_left_outer_join_with_predictions()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\" Test machine learning code - collecting data, learning from data, and predicting a bias adjusted\nresult.\n\"\"\"\nfrom datetime import datetime\nfrom typing import List\nimport json\nimport pytest\nfrom pytest_bdd import scenario, given, then, when\nfrom app.db.models import PredictionModel, PredictionModelGridSubset\nfrom app.weather_models import machine_learning\nfrom app.tests.weather_models.crud import get_actuals_left_outer_join_with_predictions\n\n\n@pytest.fixture()\ndef mock_get_actuals_left_outer_join_with_predictions(monkeypatch):\n    \"\"\" Mock out call to DB returning actuals macthed with predictions \"\"\"\n    monkeypatch.setattr(machine_learning, 'get_actuals_left_outer_join_with_predictions',\n                        get_actuals_left_outer_join_with_predictions)\n\n\n@pytest.mark.usefixtures('mock_get_actuals_left_outer_join_with_predictions')\n@scenario(\"test_machine_learning.feature\", \"Learn weather\",\n          example_converters=dict(coordinate=json.loads,\n                                  points=json.loads,\n                                  model_temp=float,\n                                  model_rh=float,\n                                  timestamp=datetime.fromisoformat,\n                                  bias_adjusted_temp=lambda value: None if value == 'None' else float(\n                                      value),\n                                  bias_adjusted_rh=lambda value: None if value == 'None' else float(value)))\ndef test_machine_learning():\n    \"\"\" BDD Scenario for predictions \"\"\"\n\n\n@given(\"An instance of StationMachineLearning for <coordinate> within <points>\", target_fixture='instance')\ndef given_an_instance(coordinate: List, points: List):\n    \"\"\" Bind the data variable \"\"\"\n    return machine_learning.StationMachineLearning(\n        session=None,\n        model=PredictionModel(id=1),\n        grid=PredictionModelGridSubset(id=1),\n        points=points,\n        target_coordinate=coordinate,\n        station_code=None,\n        max_learn_date=datetime.now())\n\n\n@when('The machine learns')\ndef learn(instance: machine_learning.StationMachineLearning):\n    \"\"\" Train the machine learning model \"\"\"\n    instance.learn()\n\n\n@then('The <model_temp> for <timestamp> results in <bias_adjusted_temp>')\ndef assert_temperature(\n        instance: machine_learning.StationMachineLearning,\n        model_temp: float, timestamp: datetime, bias_adjusted_temp: float):\n    \"\"\" Assert that the ML algorithm predicts the temperature correctly \"\"\"\n    result = instance.predict_temperature(model_temp, timestamp)\n    assert result == bias_adjusted_temp\n\n\n@then('The <model_rh> for <timestamp> results in <bias_adjusted_rh>')\ndef assert_rh(instance: machine_learning.StationMachineLearning,\n              model_rh: float, timestamp: datetime, bias_adjusted_rh: float):\n    \"\"\" Assert that the ML algorithm predicts the relative humidity correctly \"\"\"\n    result = instance.predict_rh(model_rh, timestamp)\n    assert result == bias_adjusted_rh\n<|endoftext|>",
    "repo": "bcgov/wps"
  },
  {
    "prompt": "\"\"\"Delete a dataset.\"\"\"\nfn delete_dataset(project_id, dataset_id)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport datetime\nimport os\n\nfrom google.cloud import automl\nimport pytest\n\nimport delete_dataset\n\nPROJECT_ID = os.environ[\"AUTOML_PROJECT_ID\"]\nBUCKET_ID = \"{}-lcm\".format(PROJECT_ID)\n\n\n@pytest.fixture(scope=\"function\")\ndef dataset_id():\n    client = automl.AutoMlClient()\n    project_location = f\"projects/{PROJECT_ID}/locations/us-central1\"\n    display_name = \"test_\" + datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n    metadata = automl.TextExtractionDatasetMetadata()\n    dataset = automl.Dataset(\n        display_name=display_name, text_extraction_dataset_metadata=metadata\n    )\n    response = client.create_dataset(parent=project_location, dataset=dataset)\n    dataset_id = response.result().name.split(\"/\")[-1]\n\n    yield dataset_id\n\n\ndef test_delete_dataset(capsys, dataset_id):\n    # delete dataset\n    delete_dataset.delete_dataset(PROJECT_ID, dataset_id)\n    out, _ = capsys.readouterr()\n    assert \"Dataset deleted.\" in out\n<|endoftext|>",
    "repo": "renovate-bot/python-automl"
  },
  {
    "prompt": "class FlickrDataset:\n\tfn __init__(self, images_dir_path: str, texts_path: str)\n\t\"\"\"Creates a dictionary that holds:\n\t\n\tKey: The full path to the image.\n\tValue: A list of lists where each token in the inner list is a word. The number\n\tof sublists is 5.\n\t\n\tArgs:\n\t    texts_path: Path where the text doc with the descriptions is.\n\t\n\tReturns:\n\t    A dictionary that represents what is explained above.\"\"\"\n\tfn parse_captions_filenames(texts_path: str) -> Dict[(str, List[str])]\n\t\"\"\"Returns the image paths, the captions and the lengths of the captions.\n\t\n\tArgs:\n\t    imgs_file_path: A path to a file where all the images belonging to the\n\t    validation part of the dataset are listed.\n\t    img_path_caption: Image name to list of captions dict.\n\t    images_dir_path: A path where all the images are located.\n\t\n\tReturns:\n\t    Image paths, captions and lengths.\"\"\"\n\tfn get_data_wrapper(imgs_file_path: str, img_path_caption: Dict[(str, List[str])], images_dir_path: str)\n\n\nclass FlickrDatasetTrain(FlickrDataset, TorchDataset):\n\tfn __init__(self, images_dir_path: str, texts_path: str, train_images_file_path: str)\n\tfn __len__(self)\n\tfn __getitem__(self, idx: int)\n\n\nclass FlickrDatasetValTest(FlickrDataset, TorchDataset):\n\tfn __init__(self, images_dir_path: str, texts_path: str, val_images_file_path: str)\n\tfn __len__(self)\n\tfn __getitem__(self, idx: int)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom utils.datasets import FlickrDataset\n\n\n@pytest.fixture\ndef flickr_texts_path():\n    return \"data/testing_assets/flickr_tokens.txt\"\n\n\n@pytest.fixture\ndef flickr_images_path():\n    return \"data/testing_assets/flickr_images/\"\n\n\n@pytest.fixture\ndef flickr_train_path():\n    return \"data/testing_assets/flickr_train.txt\"\n\n\n@pytest.fixture\ndef flickr_val_path():\n    return \"data/testing_assets/flickr_val.txt\"\n\n\ndef test_flickr_parse_captions_filenames(flickr_texts_path):\n    img_path_caption = FlickrDataset.parse_captions_filenames(flickr_texts_path)\n    unique_img_paths = set()\n    for img_path in img_path_caption.keys():\n        assert len(img_path_caption[img_path]) == 5\n        unique_img_paths.add(img_path)\n    assert len(unique_img_paths) == 5\n<|endoftext|>",
    "repo": "gorjanradevski/cross_modal_full_transfer"
  },
  {
    "prompt": "class Connection:\n\tfn __init__(self, dsn: str, host: str, port: int, database: str, user: str, password: str, cursor_cls, echo, stack_track)\n\tfn __repr__(self)\n\tfn connected(self)\n\tfn host(self)\n\tfn port(self)\n\tfn user(self)\n\tfn password(self)\n\tfn database(self)\n\tfn echo(self)\n\tfn cursor(self, cursor: Type[Cursor]) -> Cursor\n\t\"\"\"Return a client configured from the given URL.\n\t\n\tFor example::\n\t\n\t    clickhouse://[user:password]@localhost:9000/default\n\t    clickhouses://[user:password]@localhost:9440/default\n\t\n\tThree URL schemes are supported:\n\t    clickhouse:// creates a normal TCP socket connection\n\t    clickhouses:// creates a SSL wrapped TCP socket connection\n\t\n\tAny additional querystring arguments will be passed along to\n\tthe Connection class's initializer.\"\"\"\n\tfn _parse_dsn(self, url)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom asynch.connection import Connection\n\n\n@pytest.mark.asyncio\nasync def test_pool(pool):\n    assert pool.minsize == 1\n    assert pool.maxsize == 10\n    assert pool.size == 1\n    assert pool.freesize == 1\n\n\n@pytest.mark.asyncio\nasync def test_pool_cursor(pool):\n    async with pool.acquire() as conn:\n        async with conn.cursor() as cursor:\n            await cursor.execute(\"SELECT 1\")\n            ret = cursor.fetchone()\n            assert ret == (1,)\n\n\n@pytest.mark.asyncio\nasync def test_acquire(pool):\n    conn = await pool.acquire()\n    assert isinstance(conn, Connection)\n    assert pool.freesize == 0\n    assert pool.size == 1\n    assert conn.connected\n    await pool.release(conn)\n    assert pool.freesize == 1\n    assert pool.size == 1\n<|endoftext|>",
    "repo": "i8enn/asynch"
  },
  {
    "prompt": "class UserFactory(DjangoModelFactory):\n\tusername = Faker('user_name')\n\temail = Faker('email')\n\tname = Faker('name')\n\tfn password(self, create: bool, extracted: Sequence[Any])\n\n\nclass UserChangeForm(forms.UserChangeForm):\n\n\nclass UserCreationForm(forms.UserCreationForm):\n\terror_message = forms.UserCreationForm.error_messages.update({'duplicate_username': _('This username has already been taken.')})\n\tfn clean_username(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom itscsapp.users.forms import UserCreationForm\nfrom itscsapp.users.tests.factories import UserFactory\n\npytestmark = pytest.mark.django_db\n\n\nclass TestUserCreationForm:\n\n    def test_clean_username(self):\n        # A user with proto_user params does not exist yet.\n        proto_user = UserFactory.build()\n\n        form = UserCreationForm(\n            {\n                \"username\": proto_user.username,\n                \"password1\": proto_user._password,\n                \"password2\": proto_user._password,\n            }\n        )\n\n        assert form.is_valid()\n        assert form.clean_username() == proto_user.username\n\n        # Creating a user.\n        form.save()\n\n        # The user with proto_user params already exists,\n        # hence cannot be created.\n        form = UserCreationForm(\n            {\n                \"username\": proto_user.username,\n                \"password1\": proto_user._password,\n                \"password2\": proto_user._password,\n            }\n        )\n\n        assert not form.is_valid()\n        assert len(form.errors) == 1\n        assert \"username\" in form.errors\n<|endoftext|>",
    "repo": "danyRivC/itscsapp"
  },
  {
    "prompt": "\"\"\"Default user for restaurant.\"\"\"\nclass User(AbstractUser):\n\tname = CharField(_('Name of User'), blank=True, max_length=255)\n\tfirst_name = None\n\tlast_name = None\n\t\"\"\"Get url for user's detail view.\n\t\n\tReturns:\n\t    str: URL for user detail.\"\"\"\n\tfn get_absolute_url(self) -> str\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom restaurant.users.models import User\n\npytestmark = pytest.mark.django_db\n\n\ndef test_user_get_absolute_url(user: User):\n    assert user.get_absolute_url() == f\"/users/{user.username}/\"\n<|endoftext|>",
    "repo": "Dezilok/restaurant"
  },
  {
    "prompt": "class Res1D:\n\t\"\"\"Read the res1d file\"\"\"\n\tfn __read(file_path)\n\tfn _get_time(file)\n\tfn _get_values(dataItemTypes, file, indices, queries, reachNums)\n\tfn _get_data(self, file, queries, dataItemTypes, indices, reachNums)\n\tfn format_string(s)\n\tfn find_items(self, file, queries, chainage_tolerance)\n\tfn read(self, file_path, queries)\n\n\nclass ExtractionPoint:\n\t\"\"\"Variable Type (eg. WaterLevel or Discharge)\"\"\"\n\tfn VariableType(VariableType)\n\t\"\"\"Name of the Branch\"\"\"\n\tfn BranchName(BranchName)\n\t\"\"\"Chainage number along branch\"\"\"\n\tfn Chainage(Chainage)\n\tfn __str__(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom mikeio.res1d import Res1D, ExtractionPoint\n\n\ndef get_test_query():\n    query = ExtractionPoint()\n    query.BranchName = \"104l1\"\n    query.Chainage = 34.4131\n    query.VariableType = \"WaterLevel\"\n\n    return query\n\n\ndef test_file_does_not_exist():\n    file = \"tests/testdata/not_a_file.res1d\"\n\n    query = get_test_query()\n\n    with pytest.raises(FileExistsError):\n        assert Res1D().read(file, [query])\n\n\ndef test_read_single_item():\n    file = \"tests/testdata/Exam6Base.res1d\"\n    query = get_test_query()\n    ts = Res1D().read(file, [query])\n\n    assert len(ts) == 110\n<|endoftext|>",
    "repo": "maximlt/mikeio"
  },
  {
    "prompt": "fn slow_double(x, sleep_dur)\n\n\"\"\"1. Launch a few apps and write the checkpoint once a few have completed\n    \"\"\"\nfn test_initial_checkpoint_write(n)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import argparse\nimport os\nimport time\n\nimport pytest\n\nimport parsl\nfrom parsl.app.app import App\nfrom parsl.tests.test_checkpointing.test_python_checkpoint_1 import \\\n    test_initial_checkpoint_write\nfrom parsl.tests.configs.local_threads_checkpoint import config\n\nrundir = test_initial_checkpoint_write()\nconfig.checkpoint_files = [os.path.join(rundir, 'checkpoint')]\n\nparsl.clear()\nparsl.load(config)\n\n\n@App('python', cache=True)\ndef slow_double(x, sleep_dur=1):\n    import time\n    time.sleep(sleep_dur)\n    return x * 2\n\n\n@pytest.mark.local\ndef test_loading_checkpoint(n=2):\n    \"\"\"2. Load the memoization table from previous checkpoint\n    \"\"\"\n\n    d = {}\n\n    start = time.time()\n    print(\"Launching : \", n)\n    for i in range(0, n):\n        d[i] = slow_double(i)\n    print(\"Done launching\")\n\n    for i in range(0, n):\n        d[i].result()\n    print(\"Done sleeping\")\n\n    delta = time.time() - start\n    assert delta < 1, \"Took longer than a second, restore from checkpoint failed\"\n\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-c\", \"--count\", default=\"10\",\n                        help=\"Count of apps to launch\")\n    parser.add_argument(\"-d\", \"--debug\", action='store_true',\n                        help=\"Count of apps to launch\")\n    args = parser.parse_args()\n\n    if args.debug:\n        parsl.set_stream_logger()\n\n    x = test_initial_checkpoint_write(n=4)\n<|endoftext|>",
    "repo": "samarthm/parsl-master"
  },
  {
    "prompt": "fn skip_upload(response, skip_existing, package)\n\nfn upload(upload_settings, dists)\n\nfn main(args)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright 2013 Donald Stufft\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport pretend\nimport pytest\n\nfrom twine import cli\nimport twine.commands.upload\n\n\ndef test_dispatch_to_subcommand(monkeypatch):\n    replaced_main = pretend.call_recorder(lambda args: None)\n    monkeypatch.setattr(twine.commands.upload, \"main\", replaced_main)\n\n    cli.dispatch([\"upload\", \"path/to/file\"])\n\n    assert replaced_main.calls == [pretend.call([\"path/to/file\"])]\n\n\ndef test_catches_enoent():\n    with pytest.raises(SystemExit):\n        cli.dispatch([\"non-existant-command\"])\n<|endoftext|>",
    "repo": "theacodes/twine"
  },
  {
    "prompt": "\"\"\"Samples elements randomly, with replacement,\nalways in blocks all elements of the dataset.\nOnly the remainder will be sampled with less elements.\n\nArguments:\n    data_source (Dataset): dataset to sample from\n    nsamples (int): number of total samples\"\"\"\nclass NSamplesRandomSampler(Sampler):\n\tfn __init__(self, data_source, nsamples)\n\tfn __iter__(self)\n\tfn __len__(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom memcnn.data.sampling import NSamplesRandomSampler\nimport torch.utils.data as data\nimport numpy as np\n\n\n@pytest.mark.parametrize('nsamples,data_samples', [(1, 1), (14, 10), (10, 14), (5, 1), (1, 5), (0, 10)])\ndef test_random_sampler(nsamples, data_samples):\n\n    class TestDataset(data.Dataset):\n        def __init__(self, elements):\n            self.elements = elements\n\n        def __getitem__(self, idx):\n            return idx, idx\n\n        def __len__(self):\n            return self.elements\n\n    datasrc = TestDataset(data_samples)\n    sampler = NSamplesRandomSampler(datasrc, nsamples=nsamples)\n    count = 0\n    elements = []\n    for e in sampler:\n        elements.append(e)\n        count += 1\n        if count % data_samples == 0:\n            assert len(np.unique(elements)) == len(elements)\n            elements = []\n    assert count == nsamples\n    assert len(sampler) == nsamples\n    assert sampler.__len__() == nsamples\n<|endoftext|>",
    "repo": "hzy5660251/memcnn"
  },
  {
    "prompt": "\"\"\"This is the decorator which performs recursive AST substitution of\nfunctions, and optional JIT-compilation using `numba`_.\n\nThis must only be used on functions with positional parameters\ndefined; this must not be used on functions with keyword parameters.\n\nThis decorator modifies the original function (and any nested\nfunction calls) by replacing any functions passed in using keyword\narguments. It replaces them in the AST and returns a new function\nobject with a new code object that calls the replacement functions\ninstead.\n\nFor example, a function hierarchy such as:\n\n>>> def calculate(x):\n...     return x * x\n>>> def my_func(x):\n...     a = calculate(x)\n...     return a / 2\n\nwill take the input variable `x`, square it, and then halve the\nresult:\n\n>>> my_func(6)\n18.0\n\nSix squared is 36, divided by two is 18.\n\nIf you wanted to replace the `calculate` function to\nreturn a different calculation, you could use this `@fs`\ndecorator:\n\n>>> @fs\n... def my_func(x):\n...     a = calculate(x)\n...     return a / 2\n\nNow the `my_func` callable is able to accept keyword arguments,\nwhich it will replace recursively throughout its hierarchy.\n\nIf you wanted to change the `calculate` in this function to:\n\n>>> def cube(x):\n...     return x * x * x\n\nthen after applying the `@fs` decorator you can do this:\n\n>>> my_func(6, calculate=cube)\n108.0\n\nSix cubed is 216, divided by two is 108.0.\n\nThis parametrisation can be decided at runtime - every time a\nnew keyword argument is passed in, it generates a new function\nobject with a new code object.\n\nTo store the new function object instead of executing it, pass\n`return_callable=True` to the decorated function:\n\n>>> new_func = my_func(6, calculate=cube, return_callable=True)\n>>> # At this point the new function has **not** been called.\n>>> new_func(6)\n108.0\"\"\"\nfn fs(func)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\nimport numpy as np\n\nfrom fastats.core.decorator import fs\n\n\ndef value(x, val_in, val_out, state):  # pragma: no cover\n    return val_in, state\n\n\n@fs\ndef windowed_stateful_pass(x, win):\n    \"\"\"\n    Performs a *stateful* rolling (windowed)\n    iteration over the first dimension of `x`.\n\n    This allows to improve performance by avoiding\n    re-calculating values from scratch for each\n    window.\n\n    This has the exact same properties as\n    :func:`windowed_pass`, but expects the\n    ``value`` function be \"stateful\" and have this\n    return signature:\n    ``(window_arr, val_in, val_out, state_arr)``\n\n    ``val_in`` and ``val_out`` are the values\n    coming \"in\" to the window and already thrown\n    \"out\" of the window in each iteration.\n\n    The state starts off as an empty array, then\n    the first return from the ``value`` function\n    sets the state by returning it along with the\n    window's value.  The function is free to\n    modify the state array in each iteration.\n\n    In the first iteration, you should pre-allocate\n    the state array to fit all the state variables\n    you will need in subsequent iterations.  Due\n    to optimizations made here, only first state\n    return will be respected. All subsequent\n    iterations can modify the array, but their\n    state return value will be ignored.\n\n    Example\n    -------\n    >>> import numpy as np\n    >>> def rolling_sum(x, val_in, val_out, state):\n    ...     if state.size == 0:   # state is empty\n    ...         state = np.array([np.sum(x)], dtype=x.dtype)\n    ...     else:\n    ...         state[0] += val_in - val_out\n    ...     return state[0], state\n    >>> x = np.arange(7, dtype='float')\n    >>> result = windowed_stateful_pass(x, 4, value=rolling_sum)\n    >>> result\n    array([ nan,  nan,  nan,   6.,  10.,  14.,  18.])\n    \"\"\"\n    result = np.full_like(x, np.nan)\n    state = np.empty(0, dtype=result.dtype)\n\n    val_in = x[win-1]\n    val_out = np.nan\n    # must call the value func to set the state\n    result[win-1], state = value(x[0:win], val_in, val_out, state)\n\n    # subsequent iterations ignore state returns\n    # (without this, numba won't optimize out the assignment\n    #  and this will be slow)\n    for i in range(win+1, x.shape[0]+1):\n        start = i - win\n        in_idx = i - 1\n\n        val_in = x[in_idx]\n        val_out = x[start-1]\n        result[in_idx], _ = value(x[start:i], val_in, val_out, state)\n\n    return result\n\n\nif __name__ == '__main__':\n    import pytest\n    pytest.main([__file__])\n<|endoftext|>",
    "repo": "rjenc29/fastats"
  },
  {
    "prompt": "\"\"\"Helper function to construct decorated arguments\n\nThis works only with positional and likely positional arguments\nstrongly keyword arguments are in **kwargs. It constructs kwargs'\nfrom positional values\"\"\"\nfn _rebind(decorator, func)\n\n\"\"\"Helper function to construct decorator from a simple function\n\narg: after means that decorated check should be run after the\ntarget function\n\nThis is a 'decorate' meta function that wraps new decorator around\ntarget function and merges decorated arguments with target arguments\nconvention: each new decorator should have a 'response' argument,\nwhich is an output of a target function\"\"\"\nfn decorate(after)\n\nfn response_does_not_contain_write_only_properties(resource_client, response)\n\nfn response_contains_resource_model_equal_current_model(response, current_resource_model)\n\nfn response_contains_resource_model_equal_updated_model(response, current_resource_model, update_resource_model)\n\nfn response_contains_primary_identifier(resource_client, response)\n\nfn response_contains_unchanged_primary_identifier(resource_client, response, current_resource_model)\n\nfn skip_not_writable_identifier(resource_client)\n\nfn failed_event(error_code, msg)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# fixture and parameter have the same name\n# pylint: disable=redefined-outer-name\n\nimport pytest\n\n# WARNING: contract tests should use fully qualified imports to avoid issues\n# when being loaded by pytest\nfrom rpdk.core.contract.interface import Action, HandlerErrorCode, OperationStatus\nfrom rpdk.core.contract.suite.contract_asserts import failed_event\n\n\n@pytest.mark.update\n@failed_event(error_code=(HandlerErrorCode.NotUpdatable, HandlerErrorCode.NotFound))\ndef contract_update_create_only_property(resource_client):\n\n    if resource_client.create_only_paths:\n        create_request = resource_client.generate_create_example()\n        try:\n            _status, response, _error = resource_client.call_and_assert(\n                Action.CREATE, OperationStatus.SUCCESS, create_request\n            )\n            created_model = response[\"resourceModel\"]\n            update_request = resource_client.generate_invalid_update_example(\n                created_model\n            )\n            _status, response, _error_code = resource_client.call_and_assert(\n                Action.UPDATE, OperationStatus.FAILED, update_request, created_model\n            )\n            assert response[\"message\"]\n        finally:\n            resource_client.call_and_assert(\n                Action.DELETE, OperationStatus.SUCCESS, created_model\n            )\n    else:\n        pytest.skip(\"No createOnly Properties. Skipping test.\")\n\n\n@pytest.mark.update\n@failed_event(\n    error_code=HandlerErrorCode.NotFound,\n    msg=\"cannot update a resource which does not exist\",\n)\ndef contract_update_non_existent_resource(resource_client):\n    create_request = resource_client.generate_invalid_create_example()\n    update_request = resource_client.generate_update_example(create_request)\n    _status, response, _error = resource_client.call_and_assert(\n        Action.UPDATE, OperationStatus.FAILED, update_request, create_request\n    )\n    assert response[\"message\"]\n    return _error\n<|endoftext|>",
    "repo": "xiwhuang/cloudformation-cli"
  },
  {
    "prompt": "\"\"\"Apply f(x,theta) to x array and theta in samples.\n\nParameters\n----------\nf: function\n    list of functions :math:`f(x;\\theta)`  with dependent variable\n    :math:`x`, parameterised by :math:`\\theta`.\n\nx: 1D array-like\n    x values to evaluate :math:`f(x;\\theta)` at.\n\nsamples: 2D array-like\n    list of theta samples to evaluate :math:`f(x;\\theta)` at.\n    `shape = (nfunc, nsamples, npars)`\n\nparallel, tqdm_kwargs: optional\n    see docstring for :func:`fgivenx.parallel.parallel_apply`\n\ncache: str, optional\n    File root for saving previous calculations for re-use\n    default None\n\nReturns\n-------\n2D numpy.array:\n    samples at each x. `shape=(len(x),len(samples),)`\"\"\"\nfn compute_samples(f: function, x: 1D array-like, samples: 2D array-like) -> \n\n\"\"\"Extract samples and weights from getdist chains.\n\nParameters\n----------\nparams: list(str)\n    Names of parameters to be supplied to second argument of f(x|theta).\n\nfile_root: str, optional\n    Root name for getdist chains files. This variable automatically\n    defines:\n    - chains_file = file_root.txt\n    - paramnames_file = file_root.paramnames\n    but can be overidden by chains_file or paramnames_file.\n\nlatex: bool, optional\n    Also return an array of latex strings for those paramnames.\n\nReturns\n-------\nsamples: numpy.array\n    2D Array of samples. `shape=(len(samples), len(params))`\n\nweights: numpy.array\n    Array of weights. `shape = (len(params),)`\n\nlatex: list(str), optional\n    list of latex strings for each parameter\n    (if latex is provided as an argument)\"\"\"\nfn samples_from_getdist_chains(params: list(str), file_root: str, latex: bool) -> list(str), optional\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import numpy\nimport pytest\nimport os\nfrom shutil import rmtree\nfrom numpy.testing import assert_allclose, assert_array_equal\nfrom fgivenx.samples import compute_samples, samples_from_getdist_chains\n\ntry:\n    import getdist\n\n    def test_samples_from_getdist_chains():\n\n        # Set up getdist chains\n        file_root = './.chains/test'\n        labels = [r'\\alpha', r'\\beta', r'\\gamma']\n        names = ['a', 'b', 'g']\n        nsamples = 1000\n        params = ['a', 'g']\n        i = [names.index(p) for p in params]\n        numpy.random.seed(0)\n        samples_ = numpy.random.rand(nsamples, len(names))\n        weights_ = numpy.random.rand(nsamples)\n        samples = getdist.mcsamples.MCSamples(samples=samples_, labels=labels,\n                                              names=names, weights=weights_)\n        samples.saveAsText(file_root, make_dirs=True)\n\n        samples, weights = samples_from_getdist_chains(params, file_root)\n        assert_allclose(samples, samples_[:, i])\n        assert_allclose(weights, weights_)\n\n        samples, weights, latex = samples_from_getdist_chains(params,\n                                                              file_root,\n                                                              latex=True)\n        assert_allclose(weights, weights_)\n        assert_array_equal(latex, numpy.array(labels)[i])\n\n        rmtree('./.chains')\n\nexcept ImportError:\n    pass\n\n\ndef test_compute_samples():\n\n    with pytest.raises(TypeError):\n        compute_samples(None, None, None, wrong_argument=None)\n\n    cache = '.test_cache/test'\n    numpy.random.seed(0)\n    nsamp = 5000\n    a, b, e, f = 0, 1, 0, 1\n    m = numpy.random.normal(a, b, nsamp)\n    c = numpy.random.normal(e, f, nsamp)\n    samples = numpy.array([list(_) for _ in zip(m, c)])\n    nx = 100\n    x = numpy.linspace(-1, 1, nx)\n    fsamps_ = numpy.outer(x, m) + c\n\n    def f(x, theta):\n        m, c = theta\n        return m*x+c\n\n    assert(not os.path.isfile(cache + '_fsamples.pkl'))\n    fsamps = compute_samples([f], x, [samples], cache=cache)\n    assert(os.path.isfile(cache + '_fsamples.pkl'))\n\n    assert_allclose(fsamps_, fsamps,)\n\n    fsamps = compute_samples([f], x, [samples], cache=cache)\n    assert_allclose(fsamps_, fsamps,)\n\n    rmtree('.test_cache')\n<|endoftext|>",
    "repo": "ejhigson/fgivenx"
  },
  {
    "prompt": "class Properties:\n\tlogger = logging.getLogger(__name__)\n\tfn __init__(self, env)\n\tfn __buildPropertiesFromFile(self)\n\tfn __buildPropertiesFromGCPMetadata(self)\n\tfn __getMetadataAttribute(self, attributeName)\n\tfn __outputCsccKey(self, csccKey)\n\tfn __log_properties(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import json\nimport pytest\nimport os\nfrom automate.automate_service import AutomateService\nfrom cscc.cscc_service import CsccService\nfrom properties import Properties\nfrom datetime import datetime, timedelta\nfrom google.protobuf import empty_pb2, struct_pb2, timestamp_pb2\n\nclass TestCsccService:\n    # Tests that findings built from an Automate Report equals the number of Failed Controls\n    def testBuildFindings(self):\n        #SETUP\n        properties = Properties()\n        testSecuritySource = 'organizations/827482578277/sources/9233151395087538604'\n        cscc = CsccService(properties, testSecuritySource)\n        automate = AutomateService(properties)\n        # GIVEN a report with Failing Controls\n        reports = []\n        path = f\"{os.getcwd()}/test_resources\"\n        with open(os.path.join(path, 'mock_report.json')) as r:\n            report = json.load(r)\n        reports.append(report)\n\n        # WHEN failed controls are extracted\n        failedControls = automate.getFailedControls(reports)\n\n        # THEN cscc findings can be built\n        findings = cscc.buildFindings(failedControls)\n        assert len(findings) == 23\n\n    # Tests that a finding is successfully created in Google Cloud Security Centre\n    @pytest.mark.skip(reason=\"Creates a new finding on cscc. Don't run until deletion is enabled.\")\n    def testCreateFindings(self):\n        #SETUP\n        properties = Properties()\n        testSecuritySource = 'organizations/827482578277/sources/9233151395087538604'\n        cscc = CsccService(properties, testSecuritySource)\n\n        # GIVEN a mock finding\n        finding = {'name': 'TESTFINDING123', 'parent': 'organizations/827482578277/sources/9233151395087538604', \n        'resource_name': 'organizations/827482578277/projects/test-project', 'state': 'INACTIVE', 'category': 'TEST-CATEGORY', \n        'external_uri': 'https://35.197.241.246/compliance/reporting/nodes/d35f7363-4d0a-40a3-b52d-d92ff1050c33', \n        'source_properties': {'control_id': struct_pb2.Value(string_value=\"cis-gcp-benchmark-vms-4.6\"),\n        'control_title': struct_pb2.Value(string_value='Title'), 'code_description': struct_pb2.Value(string_value=\"Instance chef-automate should have disks encrypted with csek\"),\n        'code_message': struct_pb2.Value(string_value=\"expected #has_disks_encrypted_with_csek? to return true, got false\"),\n        'summary': struct_pb2.Value(string_value=\"CIS Google Cloud Platform Foundation Benchmark Level 2\"),\n        'status': struct_pb2.Value(string_value='Fail')}, 'security_marks': {}, 'event_time': cscc.timestamp(datetime.now()),\n        'create_time': cscc.timestamp(datetime.now())}\n\n        # WHEN create finding is called\n        cscc.createFinding(finding)\n\n        # THEN findings are sent to cscc\n        findingList = cscc.getAllFindings()\n        assert len(findingList) >=1 <|endoftext|>",
    "repo": "chef-partners/GCP-CSCC-Automate-Integration"
  },
  {
    "prompt": "class UserChangeForm(admin_forms.UserChangeForm):\n\n\nclass UserCreationForm(admin_forms.UserCreationForm):\n\n\n\"\"\"Default user for trycookie.\"\"\"\nclass User(AbstractUser):\n\tname = CharField(_('Name of User'), blank=True, max_length=255)\n\tfirst_name = None\n\tlast_name = None\n\t\"\"\"Get url for user's detail view.\n\t\n\tReturns:\n\t    str: URL for user detail.\"\"\"\n\tfn get_absolute_url(self) -> str\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"\nModule for all Form Tests.\n\"\"\"\nimport pytest\nfrom django.utils.translation import gettext_lazy as _\n\nfrom trycookie.users.forms import UserCreationForm\nfrom trycookie.users.models import User\n\npytestmark = pytest.mark.django_db\n\n\nclass TestUserCreationForm:\n    \"\"\"\n    Test class for all tests related to the UserCreationForm\n    \"\"\"\n\n    def test_username_validation_error_msg(self, user: User):\n        \"\"\"\n        Tests UserCreation Form's unique validator functions correctly by testing:\n            1) A new user with an existing username cannot be added.\n            2) Only 1 error is raised by the UserCreation Form\n            3) The desired error message is raised\n        \"\"\"\n\n        # The user already exists,\n        # hence cannot be created.\n        form = UserCreationForm(\n            {\n                \"username\": user.username,\n                \"password1\": user.password,\n                \"password2\": user.password,\n            }\n        )\n\n        assert not form.is_valid()\n        assert len(form.errors) == 1\n        assert \"username\" in form.errors\n        assert form.errors[\"username\"][0] == _(\"This username has already been taken.\")\n<|endoftext|>",
    "repo": "thomas545/trycookie"
  },
  {
    "prompt": "\"\"\"Test a database self.driver for DB API 2.0 compatibility.\nThis implementation tests Gadfly, but the TestCase\nis structured so that other self.drivers can subclass this \ntest case to ensure compiliance with the DB-API. It is \nexpected that this TestCase may be expanded in the future\nif ambiguities or edge conditions are discovered.\n\nThe 'Optional Extensions' are not yet being tested.\n\nself.drivers should subclass this test, overriding setUp, tearDown,\nself.driver, connect_args and connect_kw_args. Class specification\nshould be as follows:\n\nimport dbapi20 \nclass mytest(dbapi20.DatabaseAPI20Test):\n   [...] \n\nDon't 'import DatabaseAPI20Test from dbapi20', or you will\nconfuse the unit tester - just 'import dbapi20'.\"\"\"\nclass DatabaseAPI20Test(unittest.TestCase):\n\tdriver = None\n\tconnect_args = ()\n\tconnect_kw_args = {}\n\ttable_prefix = 'dbapi20test_'\n\tddl1 = 'create table %sbooze (name varchar(20))' % table_prefix\n\tddl2 = 'create table %sbarflys (name varchar(20))' % table_prefix\n\txddl1 = 'drop table %sbooze' % table_prefix\n\txddl2 = 'drop table %sbarflys' % table_prefix\n\tlowerfunc = 'lower'\n\tfn executeDDL1(self, cursor)\n\tfn executeDDL2(self, cursor)\n\t\"\"\"self.drivers should override this method to perform required setup\n\tif any is necessary, such as creating the database.\"\"\"\n\tfn setUp(self)\n\t\"\"\"self.drivers should override this method to perform required cleanup\n\tif any is necessary, such as deleting the test database.\n\tThe default drops the tables that may be created.\"\"\"\n\tfn tearDown(self)\n\tfn _connect(self)\n\tfn test_connect(self)\n\tfn test_apilevel(self)\n\tfn test_threadsafety(self)\n\tfn test_paramstyle(self)\n\tfn test_Exceptions(self)\n\tfn test_ExceptionsAsConnectionAttributes(self)\n\tfn test_commit(self)\n\tfn test_rollback(self)\n\tfn test_cursor(self)\n\tfn test_cursor_isolation(self)\n\tfn test_description(self)\n\tfn test_rowcount(self)\n\tlower_func = 'lower'\n\tfn test_callproc(self)\n\tfn test_close(self)\n\tfn test_execute(self)\n\tfn _paraminsert(self, cur)\n\tfn test_executemany(self)\n\tfn test_fetchone(self)\n\tsamples = ['Carlton Cold', 'Carlton Draft', 'Mountain Goat', 'Redback', 'Victoria Bitter', 'XXXX']\n\t\"\"\"Return a list of sql commands to setup the DB for the fetch\n\ttests.\"\"\"\n\tfn _populate(self)\n\tfn test_fetchmany(self)\n\tfn test_fetchall(self)\n\tfn test_mixedfetch(self)\n\t\"\"\"Should create a procedure called deleteme\n\tthat returns two result sets, first the \n\tnumber of rows in booze then \"name from booze\"\"\"\"\n\tfn help_nextset_setUp(self, cur)\n\t\"\"\"If cleaning up is needed after nextSetTest\"\"\"\n\tfn help_nextset_tearDown(self, cur)\n\tfn test_nextset(self)\n\tfn test_nextset(self)\n\tfn test_arraysize(self)\n\tfn test_setinputsizes(self)\n\tfn test_setoutputsize_basic(self)\n\tfn test_setoutputsize(self)\n\tfn test_None(self)\n\tfn test_Date(self)\n\tfn test_Time(self)\n\tfn test_Timestamp(self)\n\tfn test_Binary(self)\n\tfn test_STRING(self)\n\tfn test_BINARY(self)\n\tfn test_NUMBER(self)\n\tfn test_DATETIME(self)\n\tfn test_ROWID(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import cyanodbc\nimport dbapi20\nfrom distro import linux_distribution\nimport pytest\n\nclass CyanodbcDBApiTest(dbapi20.DatabaseAPI20Test):\n    driver = cyanodbc\n    connect_args = (\"Driver={SQLite3 ODBC Driver};Database=\"\n    \"example.db;Timeout=1000;\", )\n    \"\"\n    def test_setoutputsize(self):\n        pass\n\n    def test_nextset(self):\n        pass # for sqlite no nextset()\n\n    @pytest.mark.skipif(linux_distribution()[2]==\"xenial\",\n            reason = \"Strange behavior seen in Xenial\")\n    def test_rowcount(self):\n        super().test_rowcount()\n<|endoftext|>",
    "repo": "cyanodbc/cyanodbc"
  },
  {
    "prompt": "class UserDetailView(LoginRequiredMixin, DetailView):\n\tmodel = User\n\tslug_field = 'username'\n\tslug_url_kwarg = 'username'\n\n\nclass UserUpdateView(LoginRequiredMixin, UpdateView):\n\tmodel = User\n\tfields = ['name']\n\tfn get_success_url(self)\n\tfn get_object(self)\n\tfn form_valid(self, form)\n\n\nclass UserRedirectView(LoginRequiredMixin, RedirectView):\n\tpermanent = False\n\tfn get_redirect_url(self)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom django.conf import settings\nfrom django.test import RequestFactory\n\nfrom plouffe.users.views import UserRedirectView, UserUpdateView\n\npytestmark = pytest.mark.django_db\n\n\nclass TestUserUpdateView:\n    \"\"\"\n    TODO:\n        extracting view initialization code as class-scoped fixture\n        would be great if only pytest-django supported non-function-scoped\n        fixture db access -- this is a work-in-progress for now:\n        https://github.com/pytest-dev/pytest-django/pull/258\n    \"\"\"\n\n    def test_get_success_url(\n        self, user: settings.AUTH_USER_MODEL, request_factory: RequestFactory\n    ):\n        view = UserUpdateView()\n        request = request_factory.get(\"/fake-url/\")\n        request.user = user\n\n        view.request = request\n\n        assert view.get_success_url() == f\"/users/{user.username}/\"\n\n    def test_get_object(\n        self, user: settings.AUTH_USER_MODEL, request_factory: RequestFactory\n    ):\n        view = UserUpdateView()\n        request = request_factory.get(\"/fake-url/\")\n        request.user = user\n\n        view.request = request\n\n        assert view.get_object() == user\n\n\nclass TestUserRedirectView:\n    def test_get_redirect_url(\n        self, user: settings.AUTH_USER_MODEL, request_factory: RequestFactory\n    ):\n        view = UserRedirectView()\n        request = request_factory.get(\"/fake-url\")\n        request.user = user\n\n        view.request = request\n\n        assert view.get_redirect_url() == f\"/users/{user.username}/\"\n<|endoftext|>",
    "repo": "goulu/plouffe"
  },
  {
    "prompt": "\"\"\"base class for interactive templar tests\"\"\"\nclass BaseClass:\n\tUPDATE_FIXTURES = False\n\tTEST_FOR_MODE: Optional[str] = None\n\t\"\"\"Return a new tmux session.\n\t\n\tThe EDITOR is set here such that vim will not create swap files.\"\"\"\n\tfn fixture_tmux_session(request)\n\t\"\"\"test interactive and ``stdout`` mode ``config``\"\"\"\n\tfn test(self, request, tmux_session, step)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"Tests for templar from CLI, interactive, without an EE.\n\"\"\"\nimport pytest\n\nfrom ..._interactions import Command\nfrom ..._interactions import UiTestStep\nfrom ..._interactions import add_indices\nfrom ..._interactions import step_id\nfrom .base import BaseClass\nfrom .base import base_steps\nfrom .base import inventory_path\nfrom .base import playbook_path\n\n\ncmdline = f\"{playbook_path} -i {inventory_path}\"\nCLI = Command(subcommand=\"run\", cmdline=cmdline, execution_environment=False).join()\n\ninitial_steps = (\n    UiTestStep(\n        user_input=CLI,\n        comment=\"ansible-navigator run playbook\",\n        search_within_response=[\"COMPLETE\", \"SUCCESSFUL\"],\n    ),\n)\n\nsteps = add_indices(initial_steps + base_steps)\n\n\n@pytest.mark.parametrize(\"step\", steps, ids=step_id)\nclass Test(BaseClass):\n    \"\"\"Run the tests for templar from CLI, interactive, without an EE.\"\"\"\n\n    UPDATE_FIXTURES = False\n<|endoftext|>",
    "repo": "goneri/ansible-navigator"
  },
  {
    "prompt": "\"\"\"A single physical button on the keyboard \"\"\"\nclass Button:\n\t__slots__ = ('width', 'isMarked', 'i', 'scancode')\n\t_idToName: Dict[int, Text] = {}\n\t_nameToId: Dict[Text, int] = {}\n\t_nextNameId = 0\n\tserializedName = 'standard'\n\tfn __init__(self, name: Text, width: float, isMarked: bool, scancode)\n\tfn __repr__(self)\n\tfn __eq__(self, other)\n\tfn __hash__(self)\n\tfn name(self)\n\tfn deserialize(self, data: Dict)\n\tfn serialize(self)\n\n\n\"\"\"A letter, number or symbol button, but not special keys like modifier, tab,\n\u2026\"\"\"\nclass LetterButton(Button):\n\tserializedName = 'letter'\n\tfn __init__(self, name, width, isMarked, scancode)\n\tfn __repr__(self)\n\n\n\"\"\"A button spanning multiple rows, like the return button on european\nkeyboards\"\"\"\nclass MultiRowButton(Button):\n\t__slots__ = ('span',)\n\tserializedName = 'multi'\n\tfn __init__(self, name, span, width, isMarked, scancode)\n\tfn __repr__(self)\n\tfn serialize(self)\n\n\nclass PhysicalKeyboard:\n\t__slots__ = ('name', 'description', 'rows', '_buttonToRow')\n\tfn __init__(self, name: Text, description: Text, rows)\n\tfn __iter__(self)\n\tfn __repr__(self)\n\tfn __len__(self)\n\t\"\"\"Find button by name \"\"\"\n\tfn __getitem__(self, name: Text) -> Button\n\t\"\"\"Iterate over all keys \"\"\"\n\tfn keys(self) -> Iterator[Button]\n\tfn find(self, name: Text) -> Button\n\tfn getRow(self, btn: Button)\n\tfn deserialize(cls, data: Dict)\n\tfn serialize(self)\n\n\n\"\"\"Limit the number of items drawn from iterable l to n. \"\"\"\nfn limit(l, n)\n\n\"\"\"Simple YAML loader that searches the current path and the package\u2019s\nresources (for defaults)\"\"\"\nclass YamlLoader:\n\t__slots__ = ('defaultDir', 'deserialize')\n\tfn __init__(self, defaultDir, deserialize)\n\tfn __getitem__(self, k, onlyRes)\n\tfn __iter__(self)\n\n\n\"\"\"Convert text into a string that is always renderable without combining,\ncontrol or invisible characters \"\"\"\nfn displayText(text)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright (c) 2020 lulua contributors\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n# THE SOFTWARE.\n\nimport pytest\n\nfrom .keyboard import defaultKeyboards, Button, dataDirectory\nfrom .util import YamlLoader\n\ndef test_defaults ():\n    k = defaultKeyboards['ibmpc105']\n    assert k.name == 'ibmpc105'\n\n    with pytest.raises (KeyError):\n        k = defaultKeyboards['nonexistent']\n\n    assert len (list (defaultKeyboards)) > 0\n\ndef test_keys_unique ():\n    for kbd in defaultKeyboards:\n        # both, ids and names must be unique\n        havei = set ()\n        havename = set ()\n        for btn in kbd.keys ():\n            assert btn.i not in havei\n            havei.add (btn.i)\n\n            assert btn.name not in havename\n            havename.add (btn.name)\n\ndef test_keyboard_getRow ():\n    k = defaultKeyboards['ibmpc105']\n    for btn, expect in [(k['Bl1'], 0), (k['Cr1'], 1), (k['Dr1'], 2)]:\n        assert k.getRow (btn) == expect\n    \ndef test_keyboard_getattr ():\n    k = defaultKeyboards['ibmpc105']\n    assert k['Dr1'] == k.find ('Dr1')\n    assert k['CD_ret'] == k.find ('CD_ret')\n    assert k['Cr1'] != k.find ('El1')\n\n    with pytest.raises (KeyError):\n        k['nonexistent_button']\n\ndef test_button_uniqname ():\n    a = Button ('a')\n    assert a.name == 'a'\n\n    b = Button ('b')\n    assert b.name == 'b'\n\n    assert a != b\n\n    c = Button ('a')\n    assert c.name == 'a'\n\n    assert a == c\n    assert b != c\n\n    d = dict ()\n    d[a] = 1\n    assert a in d\n    assert b not in d\n    assert c in d\n    d[b] = 2\n    assert b in d\n\n    # make sure we can only compare to Buttons\n    assert a != 'hello'\n    assert a != 1\n    assert a != dict ()\n\ndef test_serialize ():\n    \"\"\" Make sure serialize (deserialize (x)) of keyboards is identity \"\"\"\n\n    rawKeyboards = YamlLoader (dataDirectory, lambda x: x)\n    name = 'ibmpc105'\n    assert defaultKeyboards[name].serialize () == rawKeyboards[name]\n\n<|endoftext|>",
    "repo": "PromyLOPh/lulua"
  },
  {
    "prompt": "fn normal_log_prob_blobs(params)\n\nfn normal_log_prob(params)\n\nfn uniform_log_prob(params)\n\nfn _test_normal(proposal, ndim, nwalkers, nsteps, seed, check_acceptance, pool, blobs)\n\nfn _test_uniform(proposal, nwalkers, nsteps, seed)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# -*- coding: utf-8 -*-\n\nfrom __future__ import division, print_function\n\nimport pytest\nfrom emcee import moves\nfrom .test_proposal import _test_normal, _test_uniform\n\n__all__ = [\"test_normal_stretch\", \"test_uniform_stretch\",\n           \"test_nsplits_stretch\"]\n\n\n@pytest.mark.parametrize(\"blobs\", [True, False])\ndef test_normal_stretch(blobs, **kwargs):\n    kwargs[\"blobs\"] = blobs\n    _test_normal(moves.StretchMove(), **kwargs)\n\n\ndef test_uniform_stretch(**kwargs):\n    _test_uniform(moves.StretchMove(), **kwargs)\n\n\ndef test_nsplits_stretch(**kwargs):\n    _test_normal(moves.StretchMove(nsplits=5), **kwargs)\n\n\ndef test_randomize_stretch(**kwargs):\n    _test_normal(moves.StretchMove(randomize_split=True), **kwargs)\n<|endoftext|>",
    "repo": "mikekatz04/emcee"
  },
  {
    "prompt": "\"\"\"An event emitted by a service.\"\"\"\nclass ServiceEvent:\n\tfn __init__(self, service: Service)\n\n\n\"\"\"This event is emitted every time a new service has been initialized.\"\"\"\nclass ServiceInitializedEvent(ServiceEvent):\n\tfn __init__(self, name: str, service: Service)\n\n\n\"\"\"This event is emitted every time a service has been shut down.\"\"\"\nclass ServiceShutdownEvent(ServiceEvent):\n\tfn __init__(self, name: str, service: Service)\n\n\n\"\"\"This event is emitted from the application when it has been shut\ndown.\"\"\"\nclass ApplicationShutdown(ServiceEvent):\n\n\n\"\"\"The session is emitting this event when it's ready to shut down.\"\"\"\nclass SessionCreated(ServiceEvent):\n\tfn __init__(self, applicaton: Service, session: Service, filename: Optional[str], template: Optional[str])\n\n\n\"\"\"Event emitted when a session becomes the active session.\"\"\"\nclass ActiveSessionChanged(ServiceEvent):\n\n\n\"\"\"When the application is asked to terminate, it will inform all sessions.\n\nThe user can then save his/her work.\"\"\"\nclass SessionShutdownRequested(ServiceEvent):\n\n\n\"\"\"The session is emitting this event when it's ready to shut down.\"\"\"\nclass SessionShutdown(ServiceEvent):\n\n\nclass ModelLoaded:\n\tfn __init__(self, service, filename)\n\n\nclass ModelSaved:\n\tfn __init__(self, service, filename)\n\n\n\"\"\"This event denotes the beginning of a transaction.\n\nNested (sub-) transactions should not emit this signal.\"\"\"\nclass TransactionBegin:\n\n\n\"\"\"This event is emitted when a transaction (toplevel) is successfully\ncommitted.\"\"\"\nclass TransactionCommit:\n\n\n\"\"\"If a set of operations fail (e.i.\n\ndue to an exception) the transaction should be marked for rollback.\nThis event is emitted to tell the operation has failed.\"\"\"\nclass TransactionRollback:\n\n\n\"\"\"Signal if an action can be activated or not.\"\"\"\nclass ActionEnabled:\n\tfn __init__(self, action_name: str, enabled: bool) -> None\n\n\n\"\"\"Inform the user about important events.\"\"\"\nclass Notification:\n\tfn __init__(self, message)\n\n\n\"\"\"The distribution metadata for Gaphor.\"\"\"\nfn distribution()\n\nclass NotInitializedError(Exception):\n\n\n\"\"\"The Gaphor application is started from the gaphor.ui module.\n\nThis application instance is used to maintain application wide references\nto services and sessions (opened models). It behaves like a singleton in many ways.\n\nThe Application is responsible for loading services and plugins. Services\nare registered in the \"component_registry\" service.\"\"\"\nclass Application(Service, ActionProvider):\n\tfn __init__(self)\n\tfn get_service(self, name)\n\tfn sessions(self)\n\tfn active_session(self)\n\tfn new_session(self)\n\t\"\"\"Initialize an application session.\"\"\"\n\tfn _new_session(self, filename, template, services)\n\tfn has_sessions(self)\n\tfn has_session(self, filename)\n\tfn shutdown_session(self, session)\n\t\"\"\"Forcibly shut down all sessions. No questions asked.\n\t\n\tThis is mainly for testing purposes.\"\"\"\n\tfn shutdown(self)\n\t\"\"\"The user's application Quit command.\"\"\"\n\tfn quit(self)\n\tfn all(self, base: type[T]) -> Iterator[tuple[(str, T)]]\n\tfn _transaction_proxy(self, event)\n\n\n\"\"\"A user context is a set of services (including UI services) that define\na window with loaded model.\"\"\"\nclass Session(Service):\n\t\"\"\"Initialize the application.\"\"\"\n\tfn __init__(self, services)\n\tfn get_service(self, name)\n\tfn filename(self)\n\tfn foreground(self)\n\tfn shutdown(self)\n\tfn shutdown_service(self, name, srv)\n\tfn on_filename_changed(self, event)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom gaphor.application import Application\nfrom gaphor.event import ModelLoaded, ModelSaved\n\n\n@pytest.fixture\ndef application():\n    application = Application()\n    yield application\n    application.shutdown()\n\n\ndef test_service_load(application):\n    \"\"\"Test loading services and querying utilities.\"\"\"\n\n    session = application.new_session()\n\n    assert (\n        session.get_service(\"undo_manager\") is not None\n    ), \"Failed to load the undo manager service\"\n\n    assert (\n        session.get_service(\"file_manager\") is not None\n    ), \"Failed to load the file manager service\"\n\n\ndef test_model_loaded(application):\n    session = application.new_session()\n    session.event_manager.handle(ModelLoaded(None, \"some_file_name\"))\n\n    assert session.filename == \"some_file_name\"\n\n\ndef test_model_saved(application):\n    session = application.new_session()\n    session.event_manager.handle(ModelSaved(None, \"some_file_name\"))\n\n    assert session.filename == \"some_file_name\"\n\n\ndef test_new_session_from_template(application, test_models):\n    with (test_models / \"test-model.gaphor\").open() as model:\n        session = application.new_session(template=model)\n\n    assert any(session.get_service(\"element_factory\").select())\n<|endoftext|>",
    "repo": "Mu-L/gaphor"
  },
  {
    "prompt": "class JSONEncoder(object):\n\tfn __init__(self, obj)\n\tfn encode(self)\n\tfn _get_method(self, obj)\n\tfn encode_dict(self, val_dict)\n\tfn encode_list(self, val_ls)\n\tfn encode_bool(self, val_b, _bm)\n\tfn encode_int(self, val_num)\n\tencode_float = encode_int\n\tfn encode_NoneType(self, val_none)\n\tfn encode_str(self, val_s)\n\tencode_unicode = encode_str\n\tencode_bytes = encode_str\n\n\nfn dump(obj, file, encoder)\n\nfn dumps(obj, encoder)\n\nfn to_utf8(value)\n\nfn to_unicode(value)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# -*- coding: utf-8 -*-\n\nimport pytest\nfrom djson.encoder import JSONEncoder\nfrom djson.utils import to_utf8\n\n\n@pytest.fixture(scope='module')\ndef encoder():\n    return JSONEncoder(None)\n\n\ndef test_encode_none(encoder):\n    assert encoder.encode_NoneType(None) == 'null'\n\n\ndef test_encode_number(encoder):\n    ni = 123325\n    nf = 123.456\n    assert encoder.encode_int(ni) == '{0}'.format(ni)\n    assert encoder.encode_float(nf) == '{0}'.format(nf)\n\n\ndef test_encode_bool(encoder):\n    assert encoder.encode_bool(True) == 'true'\n    assert encoder.encode_bool(False) == 'false'\n\n\ndef test_encode_str(encoder):\n    s = \"12334324324\"\n    assert encoder.encode_str(s) == '\"{0}\"'.format(s)\n\n\ndef test_encode_list(encoder):\n    lobj = [1, 2.4, \"qwer\", '34', -456, True, None, False]\n    assert (sorted(encoder.encode_list(lobj)) ==\n            sorted('[1, 2.4, \"qwer\", \"34\", -456, true, null, false]'))\n\n\ndef test_encode_dict(encoder):\n    dobj = {\"1\": 2, 3: \"4\", True: \"hhe\", \"ww\": None, False: [1, -2]}\n    assert (sorted(encoder.encode_dict(dobj)) ==\n            sorted('{\"1\": 2, 3: \"4\", true: \"hhe\", \"ww\": null, false: [1, -2]}'))\n\n\nclass Custom(object):\n    def hahaha(self):\n        return 'hahahaha'\n\n\nclass MyEncoder(JSONEncoder):\n    def __init__(self, *args, **kwargs):\n        super(self.__class__, self).__init__(*args, **kwargs)\n\n    def encode_Custom(self, obj):\n        return '\"{0}\"'.format(obj.hahaha())\n\n\ndef test_encode_custom_obj():\n    dobj = {\"1\": Custom(), 3: \"4\", True: \"hhe\", \"ww\": None, False: [1, -2]}\n    encoder = MyEncoder(dobj)\n    assert (sorted(encoder.encode()) ==\n                  sorted(to_utf8('{\"1\": \"hahahaha\", true: \"hhe\", 3: \"4\",'\n                                 ' \"ww\": null, false: [1, -2]}')))\n<|endoftext|>",
    "repo": "damnever/djson"
  },
  {
    "prompt": "fn agent_disabled_checks() -> Generator[(List[str], None, None)]\n\nfn log_span_fmt() -> Generator[(str, None, None)]\n\nfn snapshot_dir(tmp_path: Path) -> Generator[(Path, None, None)]\n\nfn snapshot_ci_mode() -> Generator[(bool, None, None)]\n\nfn snapshot_ignored_attrs() -> Generator[(Set[str], None, None)]\n\nfn agent_url() -> Generator[(str, None, None)]\n\nfn v04_reference_http_trace_payload_data_raw()\n\nfn v04_reference_http_trace_payload_data(v04_reference_http_trace_payload_data_raw)\n\nfn v04_reference_http_trace_payload_headers() -> Generator[(Dict[(str, str)], None, None)]\n\nfn v04_trace(agent, traces: List[Trace], encoding: Literal[(msgpack, json)], token: Optional[str])\n\nfn do_reference_v04_http_trace(agent, v04_reference_http_trace_payload_headers, v04_reference_http_trace_payload_data)\n\nfn v06_reference_http_stats_payload_headers()\n\nfn v06_reference_http_stats_payload_data_raw()\n\nfn v06_reference_http_stats_payload_data(v06_reference_http_stats_payload_data_raw)\n\nfn do_reference_v06_http_stats(agent, v06_reference_http_stats_payload_headers, v06_reference_http_stats_payload_data)\n\nfn span(rnd: Random) -> Span\n\n\"\"\"Return a randomly generated trace tree with `n` spans.\n\nhttps://en.wikipedia.org/wiki/Pr%C3%BCfer_sequence\"\"\"\nfn _prufers_trace(n: int, rnd: Random) -> Trace\n\nfn random_trace(nspans: int, rng: Random) -> Trace\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom .conftest import v04_trace\nfrom .trace_utils import span\n\n\nasync def test_reference(\n    agent,\n    v04_reference_http_trace_payload_headers,\n    v04_reference_http_trace_payload_data,\n):\n    resp = await agent.put(\n        \"/v0.4/traces\",\n        headers=v04_reference_http_trace_payload_headers,\n        data=v04_reference_http_trace_payload_data,\n    )\n    assert resp.status == 200, await resp.text()\n\n\n@pytest.mark.parametrize(\"agent_disabled_checks\", [[], [\"trace_count_header\"]])\nasync def test_trace_count_header(\n    agent,\n    v04_reference_http_trace_payload_headers,\n    v04_reference_http_trace_payload_data,\n    agent_disabled_checks,\n):\n    del v04_reference_http_trace_payload_headers[\"X-Datadog-Trace-Count\"]\n    resp = await agent.put(\n        \"/v0.4/traces\",\n        headers=v04_reference_http_trace_payload_headers,\n        data=v04_reference_http_trace_payload_data,\n    )\n    if \"trace_count_header\" in agent_disabled_checks:\n        assert resp.status == 200, await resp.text()\n    else:\n        assert resp.status == 400, await resp.text()\n        assert \"Check 'trace_count_header' failed\" in await resp.text()\n\n\nasync def test_trace_count_header_mismatch(\n    agent,\n    v04_reference_http_trace_payload_headers,\n    v04_reference_http_trace_payload_data,\n):\n    v04_reference_http_trace_payload_headers[\"X-Datadog-Trace-Count\"] += \"1\"\n    resp = await agent.put(\n        \"/v0.4/traces\",\n        headers=v04_reference_http_trace_payload_headers,\n        data=v04_reference_http_trace_payload_data,\n    )\n    assert resp.status == 400, await resp.text()\n    assert \"Check 'trace_count_header' failed\" in await resp.text()\n\n\n@pytest.mark.parametrize(\"agent_disabled_checks\", [[], [\"meta_tracer_version_header\"]])\nasync def test_meta_tracer_version_header(\n    agent,\n    v04_reference_http_trace_payload_headers,\n    v04_reference_http_trace_payload_data,\n    agent_disabled_checks,\n):\n    del v04_reference_http_trace_payload_headers[\"Datadog-Meta-Tracer-Version\"]\n    resp = await agent.put(\n        \"/v0.4/traces\",\n        headers=v04_reference_http_trace_payload_headers,\n        data=v04_reference_http_trace_payload_data,\n    )\n    if \"meta_tracer_version_header\" in agent_disabled_checks:\n        assert resp.status == 200, await resp.text()\n    else:\n        assert resp.status == 400, await resp.text()\n        assert \"Check 'meta_tracer_version_header' failed\" in await resp.text()\n\n\nasync def test_trace_content_length(agent):\n    # Assume a trace will be at least 100 bytes each\n    s = span()\n    trace = [s for _ in range(int(5e7 / 100))]\n    resp = await v04_trace(agent, [trace], \"msgpack\")\n    assert resp.status == 400, await resp.text()\n    assert \"Check 'trace_content_length' failed: content length\" in await resp.text()\n<|endoftext|>",
    "repo": "Kyle-Verhoog/dd-apm-test-agent"
  },
  {
    "prompt": "fn run_validator_for_test_file(filename: str, max_expression_complexity: int, ignore_django_orm_queries: bool)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import sys\n\nimport pytest\nfrom conftest import run_validator_for_test_file\n\n\ndef test_fails():\n    errors = run_validator_for_test_file('long_expressions.py', max_expression_complexity=3)\n    assert len(errors) == 5\n\n\n@pytest.mark.skipif(sys.version_info < (3, 8), reason='runs only for python 3.8+')\ndef test_walrus():\n    errors = run_validator_for_test_file('walrus.py', max_expression_complexity=1)\n    assert len(errors) == 1\n<|endoftext|>",
    "repo": "derrix060/flake8-expression-complexity"
  },
  {
    "prompt": "fn corp_list()\n\nfn test_find_by_corp_name(corp_list)\n\nfn test_find_by_corp_code(corp_list)\n\nfn test_find_by_stock_code(corp_list)\n\nfn test_corp_to_dict(corp_list)\n\nfn test_corp_load(corp_list)\n\nfn test_corp_get_major_shareholder(corp_list)\n\nfn test_corp_get_executive_shareholder(corp_list)\n\nfn test_corp_search_filings(corp_list)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom dart_fss.fs.extract import find_all_columns\n\nfrom .test_case.crp_case import test_crp_list\nfrom .test_corp import corp_list\n\n@pytest.mark.parametrize(\"corp\", test_crp_list)\ndef test_crp_financial_statement(corp):\n    corp.run_test()\n\n\n@pytest.fixture(scope='session')\ndef fs_report(corp_list):\n    # 00164779: SK\ud558\uc774\ub2c9\uc2a4\n    skhynix = corp_list.find_by_corp_code('00164779')\n    return skhynix.extract_fs(bgn_de='20180101')\n\n\ndef test_fs_class_false(fs_report):\n    df = fs_report.show('bs', show_class=False)\n    columns = find_all_columns(df, 'class')\n    actual = len(columns)\n    expected = 0\n    assert actual == expected\n\n\ndef test_fs_concept_false(fs_report):\n    df = fs_report.show('bs', show_concept=False)\n    columns = find_all_columns(df, 'concept')\n    actual = len(columns)\n    expected = 0\n    assert actual == expected\n\n\ndef test_fs_show_depth(fs_report):\n    df = fs_report.show('bs', show_depth=1)\n    columns = find_all_columns(df, 'class')\n    actual = len(columns)\n    expected = 2\n    assert actual == expected\n\n\ndef test_fs_to_dict(fs_report):\n    info = fs_report.to_dict()\n    actual = info['corp_code']\n    expected = '00164779'\n    assert actual == expected\n\n\ndef test_fs_to_save(fs_report):\n    import os\n    import tempfile\n    with tempfile.TemporaryDirectory() as path:\n        file_path = fs_report.save(path=path)\n        actual = os.path.isfile(file_path)\n    expected = True\n    assert actual == expected\n<|endoftext|>",
    "repo": "J-hoplin1/dart-fss"
  },
  {
    "prompt": "\"\"\"Generates a nice random name from the dictionaries in words\n\n:param max_length: max length for the name.\n:type max_length: int, optional\n:param entropy: how unique th name will be, currently entropy - 1 adjectives are joined with one noun.\n:type entropy: int, optional\n:param sep: seperator between name parts.\n:type sep: str, optional\n\n:return: the generated name\n:rtype: str\n\n:raises ValueError: if ``param2`` is equal to ``param1``.\"\"\"\nfn random_nice_name(max_length: int, optional, entropy: int, optional, sep: str, optional) -> str\n\nfn random_string(length, charset)\n\nfn random_filename(length)\n\nfn random_buf(size)\n\nfn perchance(probabilty)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nimport random\n\nfrom easypy.random import random_nice_name, random_filename\n\n\ndef test_random_nice_name():\n    for _ in range(20):\n        length = random.randint(64, 85)\n        entropy = random.randint(1, 3)\n        sep = random.choice(['_', '..'])\n        name = random_nice_name(max_length=length, entropy=entropy, sep=sep)\n        assert len(name) <= length\n\n\ndef test_random_nice_name_raises():\n    with pytest.raises(ValueError):\n        random_nice_name(max_length=10, entropy=3)\n\n\ndef test_random_filename():\n    fn = random_filename(10)\n    assert len(fn) == 10\n\n    fn = random_filename((10, 11))\n    assert 10 <= len(fn) <= 11\n\n    fn = random_filename((11, 11))\n    assert len(fn) == 11\n<|endoftext|>",
    "repo": "nadavgolden/easypy"
  },
  {
    "prompt": "\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"Test irrigation_unlimited timing operations.\"\"\"\nimport pytest\nimport glob\nimport homeassistant.core as ha\nfrom homeassistant.config import (\n    load_yaml_config_file,\n    async_process_ha_core_config,\n)\nfrom tests.iu_test_support import (\n    begin_test,\n    finish_test,\n    quiet_mode,\n    test_config_dir,\n    check_summary,\n)\nfrom custom_components.irrigation_unlimited.irrigation_unlimited import (\n    IUCoordinator,\n)\nfrom custom_components.irrigation_unlimited.const import DOMAIN\nfrom custom_components.irrigation_unlimited.__init__ import CONFIG_SCHEMA\n\nquiet_mode()\n\n\nasync def test_timings(hass: ha.HomeAssistant, skip_setup):\n    \"\"\"Test timings. Process all the configuration files in the\n    test_config directory matching timing_*.yaml and check the results.\"\"\"\n\n    for fname in glob.glob(test_config_dir + \"timing_*.yaml\"):\n        print(f\"Processing: {fname}\")\n        config = CONFIG_SCHEMA(load_yaml_config_file(fname))\n        if ha.DOMAIN in config:\n            await async_process_ha_core_config(hass, config[ha.DOMAIN])\n        coordinator = IUCoordinator(hass).load(config[DOMAIN])\n\n        for t in range(coordinator.tester.total_tests):\n            next_time = await begin_test(t + 1, coordinator)\n            await finish_test(hass, coordinator, next_time, True)\n\n        check_summary(fname, coordinator)\n<|endoftext|>",
    "repo": "bsmith76s/irrigation_unlimited"
  },
  {
    "prompt": "class CallbackCounter(object):\n\tfn __init__(self, return_value)\n\tfn __call__(self)\n\n\nfn small_buffer(ds, size)\n\nfn webserver()\n\nfn df_server(ds_trimmed)\n\nfn df_server_huge()\n\n\"\"\"Don't close event loop at the end of every function decorated by\n@pytest.mark.asyncio\"\"\"\nfn event_loop()\n\nfn tornado_client(webserver, df_server, df_server_huge, event_loop)\n\nfn dummy_client(df_server, df_server_huge)\n\nfn client(request, dummy_client, tornado_client)\n\nfn ds_remote(client)\n\nfn df_remote(ds_remote)\n\nfn ds_filtered(df_filtered)\n\nfn df_filtered_cached()\n\nfn df_filtered(df_filtered_cached)\n\nfn ds_half_cache()\n\nfn ds_half(ds_half_cache)\n\nfn ds_trimmed_cache()\n\nfn ds_trimmed(ds_trimmed_cache)\n\nfn df_trimmed(ds_trimmed)\n\nfn df_concat_cache(ds_trimmed_cache)\n\nfn df_concat(df_concat_cache)\n\nfn df_executor(request, df_trimmed, df_remote)\n\nfn ds(request, ds_filtered, ds_half, ds_trimmed, ds_remote, df_concat, df_arrow)\n\nfn df(ds)\n\nfn ds_local(request, ds_filtered, ds_half, ds_trimmed, df_concat, df_arrow)\n\nfn df_local_non_arrow(request, ds_filtered, ds_half, ds_trimmed, df_concat)\n\nfn df_local(ds_local)\n\nfn df_arrow(df_arrow_cache)\n\nfn df_arrow_cache(df_arrow_raw)\n\nfn df_arrow_raw(df_filtered)\n\nfn ds_no_filter(request)\n\nfn create_filtered()\n\nfn create_base_ds()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from common import *\nimport collections\nimport numpy as np\nimport vaex\nimport pytest\n\ndef test_cat_string():\n    ds0 = vaex.from_arrays(colors=['red', 'green', 'blue', 'green'])\n    ds = ds0.ordinal_encode('colors')#, ['red', 'green'], inplace=True)\n    assert ds.is_category('colors')\n    assert ds.limits('colors', shape=128) == ([-0.5, 2.5], 3)\n\n    ds = ds0.ordinal_encode('colors', values=['red', 'green'])\n    assert ds.is_category('colors')\n    assert ds.limits('colors', shape=128) == ([-0.5, 1.5], 2)\n    assert ds.data.colors.tolist() == [0, 1, None, 1]\n\n    assert ds.copy().is_category(ds.colors)\n\n    # with pytest.raises(ValueError):\n    # \tassert ds.is_category('colors', values=['red', 'orange'])\n\ndef test_count_cat():\n    ds0 = vaex.from_arrays(colors=['red', 'green', 'blue', 'green'], counts=[1, 2, 3, 4])\n    ds0 = vaex.from_arrays(colors=['red', 'green', 'blue', 'green'], names=['apple', 'apple', 'berry', 'apple'])\n    ds = ds0.ordinal_encode(ds0.colors)\n    ds = ds0.ordinal_encode(ds0.names)\n\n    ds = ds0.ordinal_encode('colors', ['red', 'green', 'blue'])\n    assert ds.count(binby=ds.colors).tolist() == [1, 2, 1]\n    ds = ds0.ordinal_encode('colors', ['red', 'blue', 'green', ], inplace=True)\n    assert ds.count(binby=ds.colors).tolist() == [1, 1, 2]\n\n\ndef test_categorize():\n    ds0 = vaex.from_arrays(c=[0, 1, 1, 3])\n    ds0.categorize('c', labels=['a', 'b', 'c', 'd'], inplace=True)\n    assert ds0.is_category(ds0.c)\n    assert ds0.category_labels(ds0.c) == ['a', 'b', 'c', 'd']\n    assert ds0.category_count(ds0.c) == 4\n\ndef test_cat_missing_values():\n    colors = ['red', 'green', 'blue', 'green', 'MISSING']\n    mask   = [False, False,   False,   False,  True]\n    colors = np.ma.array(colors, mask=mask)\n    ds0 = vaex.from_arrays(colors=colors)\n    ds = ds0.ordinal_encode('colors', ['red', 'green', 'blue'])\n    assert ds.count(binby=ds.colors, edges=True).tolist() == [1, 0, 1, 2, 1, 0]\n\n    # if we want missing values and non-categorized values to be reported seperately\n    # the following is expected\n    # ds = ds0.ordinal_encode('colors', ['red', 'green'])\n    # assert ds.count(binby=ds.colors, edges=True).tolist() == [1, 0, 1, 2, 0, 1]\n\n\ndef test_categorize_integers():\n    df = vaex.from_arrays(x=range(5, 15))\n    df.categorize('x', min_value=5, labels=range(5, 15), inplace=True)\n    assert df.count(binby='x').tolist() == [1] * 10\n    assert df.binby('x', 'count').data.tolist() == [1] * 10\n\n    df = vaex.from_arrays(x=range(5, 15))\n    df.categorize('x', inplace=True)  # same, but calculated from data\n    assert df.count(binby='x').tolist() == [1] * 10\n    assert df.binby('x', 'count').data.tolist() == [1] * 10\n<|endoftext|>",
    "repo": "cyrusradfar/vaex"
  },
  {
    "prompt": "fn dd_environment(instance_e2e, mock_local_tls_dns)\n\nfn mock_local_tls_dns()\n\nfn certs(dd_environment)\n\nfn instance_local_no_server_hostname()\n\nfn instance_local_not_found()\n\nfn instance_local_ok(certs)\n\nfn instance_local_ok_der(certs)\n\nfn instance_local_hostname(certs)\n\nfn instance_local_hostname_mismatch(certs)\n\nfn instance_local_cert_bad()\n\nfn instance_local_cert_expired(certs)\n\nfn instance_local_cert_critical_days(certs)\n\nfn instance_local_cert_critical_seconds(certs)\n\nfn instance_local_cert_warning_days(certs)\n\nfn instance_local_cert_warning_seconds(certs)\n\nfn instance_remote_no_server()\n\nfn instance_remote_ok()\n\nfn instance_e2e()\n\nfn instance_remote_ok_ip()\n\nfn instance_remote_ok_udp()\n\nfn instance_remote_no_resolve()\n\nfn instance_remote_no_connect()\n\nfn instance_remote_no_connect_port_in_host()\n\nfn instance_remote_version_default_1_1()\n\nfn instance_remote_version_default_1_2()\n\nfn instance_remote_version_default_1_3()\n\nfn instance_remote_hostname_mismatch()\n\nfn instance_remote_self_signed_ok()\n\nfn instance_remote_cert_expired()\n\nfn instance_remote_fetch_intermediate_certs()\n\nfn instance_remote_cert_critical_days()\n\nfn instance_remote_cert_critical_seconds()\n\nfn instance_remote_cert_warning_days()\n\nfn instance_remote_cert_warning_seconds()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# (C) Datadog, Inc. 2019-present\n# All rights reserved\n# Licensed under a 3-clause BSD style license (see LICENSE)\nimport pytest\n\nfrom datadog_checks.tls import TLSCheck\n\nfrom .conftest import CA_CERT, PRIVATE_KEY\n\n\ndef test_tags_local():\n    instance = {'name': 'foo', 'local_cert_path': 'cert.pem'}\n    c = TLSCheck('tls', {}, [instance])\n\n    assert c._tags == ['name:foo']\n\n\ndef test_tags_local_hostname():\n    instance = {'name': 'foo', 'local_cert_path': 'cert.pem', 'server_hostname': 'www.google.com'}\n    c = TLSCheck('tls', {}, [instance])\n\n    assert c._tags == ['name:foo', 'server_hostname:www.google.com']\n\n\ndef test_tags_local_hostname_no_validation():\n    instance = {\n        'name': 'foo',\n        'local_cert_path': 'cert.pem',\n        'server_hostname': 'www.google.com',\n        'tls_validate_hostname': False,\n    }\n    c = TLSCheck('tls', {}, [instance])\n\n    assert c._tags == ['name:foo']\n\n\n# This is a special edge case where both supported `validate_hostname` configs are used with differing values.\n# This case should realistically never happen, but theoretically can happen\n# If either `tls_validate_hostname` or `validate_hostname` is false, then `tls_validate_hostname` is False\ndef test_tags_local_hostname_no_validation_legacy_edge_case():\n    instance = {\n        'name': 'foo',\n        'local_cert_path': 'cert.pem',\n        'server_hostname': 'www.google.com',\n        'validate_hostname': False,\n        'tls_validate_hostname': True,\n    }\n    c = TLSCheck('tls', {}, [instance])\n\n    assert c._tags == ['name:foo']\n    assert c._tls_validate_hostname is False\n\n\ndef test_tags_remote():\n    instance = {'name': 'foo', 'server': 'https://www.google.com'}\n    c = TLSCheck('tls', {}, [instance])\n\n    assert c._tags == ['name:foo', 'server_hostname:www.google.com', 'server:www.google.com', 'port:443']\n\n\ndef test_validation_data():\n    c = TLSCheck('tls', {}, [{}])\n\n    assert c._validation_data is None\n    assert c.validation_data == c._validation_data\n    assert isinstance(c.validation_data, tuple)\n\n\n@pytest.mark.parametrize(\n    'extra_config, expected_http_kwargs',\n    [\n        pytest.param({'ca_cert': CA_CERT}, {'tls_ca_cert': CA_CERT}, id='legacy ca_cert param'),\n        pytest.param({'private_key': PRIVATE_KEY}, {'tls_private_key': PRIVATE_KEY}, id='legacy private_key param'),\n        pytest.param(\n            {'validate_hostname': False}, {'tls_validate_hostname': False}, id='legacy validate_hostname param'\n        ),\n        pytest.param({'validate_cert': False}, {'tls_verify': False}, id='legacy validate_cert param'),\n    ],\n)\ndef test_config(extra_config, expected_http_kwargs):\n    instance = {\n        'name': 'foo',\n    }\n    instance.update(extra_config)\n    c = TLSCheck('tls', {}, [instance])\n    c.get_tls_context()  # need to call this for config values to be saved by _tls_context_wrapper\n    actual_options = {k: v for k, v in c._tls_context_wrapper.config.items() if k in expected_http_kwargs}\n    assert expected_http_kwargs == actual_options\n<|endoftext|>",
    "repo": "RedisLabs-Field-Engineering/integrations-core"
  },
  {
    "prompt": "\"\"\"Raised when an unexpected error happens in the watch-stream API.\"\"\"\nclass WatchingError(Exception):\n\n\n\"\"\"Special marks sent in the stream among raw events. \"\"\"\nclass Bookmark(enum.Enum):\n\tLISTED = enum.auto()\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import asyncio\nimport json\nimport logging\n\nimport aiohttp.web\nimport pytest\n\nfrom kopf.clients.watching import Bookmark, continuous_watch\n\n\nasync def test_listed_is_inbetween(\n        settings, resource, namespace, hostname, aresponses):\n\n    # Resource version is used as a continutation for the watch-queries.\n    list_data = {'metadata': {'resourceVersion': '123'}, 'items': [\n        {'spec': 'a'},\n        {'spec': 'b'},\n    ]}\n    list_resp = aiohttp.web.json_response(list_data)\n    list_url = resource.get_url(namespace=namespace)\n\n    # The same as in the `stream` fixture. But here, we also mock lists.\n    stream_data = [\n        {'type': 'ADDED', 'object': {'spec': 'c'}},  # stream.feed()\n        {'type': 'ADDED', 'object': {'spec': 'd'}},  # stream.feed()\n        {'type': 'ERROR', 'object': {'code': 410}},  # stream.close()\n    ]\n    stream_text = '\\n'.join(json.dumps(event) for event in stream_data)\n    stream_resp = aresponses.Response(text=stream_text)\n    stream_query = {'watch': 'true', 'resourceVersion': '123'}\n    stream_url = resource.get_url(namespace=namespace, params=stream_query)\n\n    aresponses.add(hostname, list_url, 'get', list_resp, match_querystring=True)\n    aresponses.add(hostname, stream_url, 'get', stream_resp, match_querystring=True)\n\n    events = []\n    async for event in continuous_watch(settings=settings,\n                                        resource=resource,\n                                        namespace=namespace,\n                                        operator_pause_waiter=asyncio.Future()):\n        events.append(event)\n\n    assert len(events) == 5\n    assert events[0]['object']['spec'] == 'a'\n    assert events[1]['object']['spec'] == 'b'\n    assert events[2] == Bookmark.LISTED\n    assert events[3]['object']['spec'] == 'c'\n    assert events[4]['object']['spec'] == 'd'\n<|endoftext|>",
    "repo": "SergeyTsaplin/kopf"
  },
  {
    "prompt": "fn validate_leaner(data_generator, leaner, fit_kwargs, estimate_kwargs, check_fitted, check_effect, check_effect_nji)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom ylearn.estimator_model.causal_tree import CausalTree\nfrom . import _dgp\nfrom ._common import validate_leaner\n\n_test_settings = {\n    # data_generator: options\n    _dgp.generate_data_x1b_y1_w0v5: dict(min_samples_leaf=3, max_depth=5),\n    _dgp.generate_data_x2b_y1_w0v5: dict(min_samples_leaf=3,min_samples_split=3, max_depth=5),\n    _dgp.generate_data_x1m_y1_w0v5: dict(min_samples_leaf=3, max_depth=5),\n}\n\n\n@pytest.mark.parametrize('dg', _test_settings.keys())\ndef test_causal_tree(dg):\n    options = _test_settings[dg]\n    dr = CausalTree(**options)\n    validate_leaner(dg, dr, check_effect=True)\n<|endoftext|>",
    "repo": "DataCanvasIO/YLearn"
  },
  {
    "prompt": "class Termination(SystemExit):\n\n\nclass TimeoutExit(BaseException):\n\n\nfn _terminate(signum, frame)\n\nfn isUp(server_addr)\n\nfn _timeout(signum, frame)\n\nfn pytest_sessionstart(session)\n\nfn pytest_sessionfinish(session, exitstatus)\n\nfn pytest_addoption(parser)\n\n\"\"\"Get the address of the Neptune server\n\nArgs:\n    config (Config): Pytest config object containing the options\n\nReturns:\n    str: Address of the Neptune server\"\"\"\nfn get_server_addr(config: Config) -> str\n\n\"\"\"Get the address of the Neptune's websocket server\n\nArgs:\n    config (Config): Pytest config object containing the options\n\nReturns:\n    str: Address of Neptune's websocket server\"\"\"\nfn get_ws_addr(config: Config) -> str\n\nfn pytest_configure(config)\n\nfn pytest_runtest_protocol(item, nextitem)\n\nfn pytest_collection_modifyitems(config, items)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright 2019 Xilinx Inc.\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\nimport pytest\nimport requests\nimport threading\nimport time\n\nfrom neptune.neptune_api import NeptuneService, NeptuneServiceArgs\nfrom neptune.tests.conftest import get_server_addr\n\n@pytest.mark.timeout(15)\ndef test_ping(request):\n    \"\"\"\n    Gets n pings from Neptune using the ping service\n\n    Args:\n        request (fixture): get the cmdline options\n    \"\"\"\n    server_addr = get_server_addr(request.config)\n    service = NeptuneService(server_addr, 'ping')\n\n    service.start()\n\n    r = requests.get('%s/serve/ping' % server_addr)\n    assert r.status_code == 200\n\n    response = r.json()\n    # verify both the id and pong keys are present in the response\n    assert type(response) is dict\n    assert 'pong' in response\n    assert 'id' in response\n\n    float(response['pong']) # check if pong is a float\n\n    service.stop()\n\n@pytest.fixture(params=[\n    0, # don't spawn processes\n    1,\n    8\n])\ndef args_ping(request):\n    if request.param is None:\n        return None\n    else:\n        args = NeptuneServiceArgs()\n        args.insert_add_arg('ping', 'processes', request.param)\n        return args\n\ndef _run_test_ping_benchmark(server_addr, iterations):\n    for i in range(iterations):\n        r = requests.get('%s/serve/ping' % server_addr)\n        assert r.status_code == 200\n\n        response = r.json()\n\n        # verify both the id and pong keys are present in the response\n        assert type(response) is dict\n        assert 'pong' in response\n        assert 'id' in response\n\n        float(response['pong']) # check if pong is a float\n\n        # optional delay between pings\n        # time.sleep(0.1)\n\n@pytest.mark.timeout(30)\n@pytest.mark.benchmark\ndef test_ping_benchmark(request, args_ping):\n    \"\"\"\n    Benchmarks ping times using args_ping to configure the ping service\n\n    Args:\n        request (fixture): get the cmdline options\n        args_ping (NeptuneServiceArgs): a Neptune service args object\n    \"\"\"\n    server_addr = get_server_addr(request.config)\n    service = NeptuneService(server_addr, 'ping')\n    ITERATION_COUNT = 1\n    THREAD_COUNT = 8\n\n    service.start(args_ping)\n    processes_count = args_ping.get_add_arg('ping', 'processes')\n    threads = []\n\n    for i in range(THREAD_COUNT):\n        thread = threading.Thread(\n            target=_run_test_ping_benchmark,\n            args=(server_addr, ITERATION_COUNT)\n        )\n        threads.append(thread)\n\n    start_time = time.time()\n    for thread in threads:\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    end_time = time.time()\n\n    logging.info(\"ping_benchmark: %d pings took %fs with %d processes\" % \\\n        (ITERATION_COUNT*THREAD_COUNT, (end_time - start_time), processes_count))\n\n    service.stop()\n<|endoftext|>",
    "repo": "powderluv/Vitis-AI"
  },
  {
    "prompt": "class Path(object):\n\tGLOB_CHARACTER = '*'\n\t\"\"\"Args:\n\t    path (Union(str, None)): A path to perform matches against\"\"\"\n\tfn __init__(self, path)\n\t\"\"\"Args:\n\t    filename (Union(str, Path)): A path in the repository that we're matching against\n\t\n\tReturns:\n\t    Path: A path object that correlates to the matched file.\n\t      If there isn't a match returns a :class:`Path` with the\n\t      value ``None``.\"\"\"\n\tfn match(self, filename: Union(str, Path)) -> Path\n\tfn _match_glob(self, filename)\n\tfn __eq__(self, other)\n\tfn __hash__(self)\n\tfn __str__(self)\n\tfn __repr__(self)\n\tfn __bool__(self)\n\tfn __nonzero__(self)\n\tfn startswith(self, needle)\n\tfn __add__(self, other)\n\tfn __radd__(self, other)\n\tfn __lt__(self, other)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from __future__ import unicode_literals\n\nimport pytest\n\nfrom radish.path import Path\n\n\nclass TestPath(object):\n    def test_casts_to_boolean_when_a_path(self):\n        assert Path('/')\n        assert not Path(None)\n\n    def test_boolean_for_both_py2_and_py3(self):\n        p = Path('/')\n        assert p.__bool__() and p.__nonzero__()\n\n    def test_return_false_when_file_doesnt_match(self):\n        path = Path('extensions/cool-extension')\n\n        assert not path.match('src/a.py')\n\n    def test_matches_simple_file(self):\n        path = Path('extensions/cool-extension/')\n\n        assert path.match('extensions/cool-extension/src/a.py')\n\n    def test_matches_against_glob_expressions(self):\n        path = Path('extensions/*/')\n\n        assert path.match('extensions/cool-extension/src/a.py')\n\n    def test_match_against_path_object(self):\n        path = Path('extensions/*/')\n\n        assert path.match(Path('extensions/cool-extension/'))\n\n    def test_doesnt_match_against_glob_when_its_a_file(self):\n        path = Path('extensions/*/')\n\n        assert not path.match('extensions/cool-extension')\n\n    def test_the_same_path_is_equal(self):\n        assert Path('extension/') == Path('extension/')\n\n    def test_path_is_same_as_path_which_would_match_glob(self):\n        assert Path('extension/cool-extension/') == Path('extension/*/')\n\n    def test_path_can_compare_to_a_string(self):\n        assert Path('extension/cool-extension/') == 'extension/cool-extension/'\n\n    def test_expanded_path(self):\n        path = Path('extension/*/')\n\n        assert path.match('extension/cool-extension/') == 'extension/cool-extension/'\n\n    def test_assert_none_paths_are_equal(self):\n        assert Path(None) == Path(None)\n\n    def test__str__is_the_path_passed_in(self):\n        assert str(Path('hello/')) == 'hello/'\n\n    def test__repr__has_a_marker_for_none_when_its_none(self):\n        assert repr(Path(None)) == '<Path: <None>>'\n\n    def test_responds_to_startswith(self):\n        assert Path('/root').startswith('/')\n        assert not Path('root').startswith('/')\n\n    def test_path_can_be_concatenated_with_a_string(self):\n        assert (Path('/') + 'root') == '/root'\n        assert ('/' + Path('root')) == '/root'\n\n    def test_path_can_be_concatenated_with_another_path(self):\n        assert (Path('/') + Path('root')) == '/root'\n\n    class TestSorting(object):\n        def test_sorts_with_other_paths(self):\n            assert sorted([Path('/b'), Path('/a')]) == [Path('/a'), Path('/b')]\n\n        def test_sorts_against_strings(self):\n            assert sorted(['/b', Path('/a')]) == [Path('/a'), '/b']\n\n        def test_raises_not_implemented_when_sorting_for_others(self):\n            with pytest.raises(NotImplementedError):\n                sorted([1, Path('/a')])\n<|endoftext|>",
    "repo": "gaqzi/radish"
  },
  {
    "prompt": "class ContactFactory(factory.django.DjangoModelFactory):\n\thourly_rate = Decimal('20.00')\n\tuser = factory.SubFactory(UserFactory)\n\tfn address_1(n)\n\tfn company_name(n)\n\tfn slug(n)\n\n\nclass UserContactFactory(factory.django.DjangoModelFactory):\n\tcontact = factory.SubFactory(ContactFactory)\n\tuser = factory.SubFactory(UserFactory)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# -*- encoding: utf-8 -*-\nimport pytest\n\nfrom search.tests.helper import check_search_methods\nfrom .factories import ContactFactory\n\n\n@pytest.mark.django_db\ndef test_search_methods():\n    contact = ContactFactory()\n    check_search_methods(contact)\n\n\n@pytest.mark.django_db\ndef test_str():\n    contact = ContactFactory()\n    str(contact)\n<|endoftext|>",
    "repo": "pkimber/contact"
  },
  {
    "prompt": "fn helper(vm, opstr)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom .utils import helper\n\nfrom hera.data import DEFAULT_DATA_START\nfrom hera.vm import VirtualMachine\n\n\n@pytest.fixture\ndef vm():\n    return VirtualMachine()\n\n\ndef test_INTEGER_fills_in_memory(vm):\n    helper(vm, \"INTEGER(42)\")\n\n    assert vm.memory[DEFAULT_DATA_START] == 42\n\n\ndef test_INTEGER_increments_data_counter(vm):\n    helper(vm, \"INTEGER(42)\")\n\n    assert vm.dc == DEFAULT_DATA_START + 1\n\n\ndef test_INTEGER_does_not_increment_pc(vm):\n    helper(vm, \"INTEGER(42)\")\n\n    assert vm.pc == 0\n\n\ndef test_DSKIP_increments_data_counter(vm):\n    helper(vm, \"DSKIP(10)\")\n\n    assert vm.dc == DEFAULT_DATA_START + 10\n\n\ndef test_DSKIP_does_not_increment_pc(vm):\n    helper(vm, \"DSKIP(10)\")\n\n    assert vm.pc == 0\n\n\ndef test_LP_STRING_fills_in_memory(vm):\n    helper(vm, 'LP_STRING(\"hello\")')\n\n    assert vm.memory[DEFAULT_DATA_START] == 5\n    assert vm.memory[DEFAULT_DATA_START + 1] == ord(\"h\")\n    assert vm.memory[DEFAULT_DATA_START + 2] == ord(\"e\")\n    assert vm.memory[DEFAULT_DATA_START + 3] == ord(\"l\")\n    assert vm.memory[DEFAULT_DATA_START + 4] == ord(\"l\")\n    assert vm.memory[DEFAULT_DATA_START + 5] == ord(\"o\")\n\n\ndef test_LP_STRING_increments_data_counter(vm):\n    helper(vm, 'LP_STRING(\"hello\")')\n\n    assert vm.dc == DEFAULT_DATA_START + 6\n\n\ndef test_LP_STRING_does_not_increment_pc(vm):\n    helper(vm, 'LP_STRING(\"hello\")')\n\n    assert vm.pc == 0\n<|endoftext|>",
    "repo": "iafisher/hera-py"
  },
  {
    "prompt": "fn matches(actual, expected)\n\nfn assert_meta(envelope)\n\nfn assert_stacktrace(envelope, inside_exception, check_size)\n\nfn assert_breadcrumb(envelope)\n\nfn assert_attachment(envelope)\n\nfn assert_minidump(envelope)\n\nfn assert_timestamp(ts, now)\n\nfn assert_event(envelope)\n\nfn assert_crash(envelope)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nimport subprocess\nimport sys\nimport os\nfrom . import cmake, check_output, run, Envelope\nfrom .conditions import has_inproc, has_breakpad, is_android\nfrom .assertions import (\n    assert_attachment,\n    assert_meta,\n    assert_breadcrumb,\n    assert_stacktrace,\n    assert_event,\n    assert_crash,\n    assert_minidump,\n    assert_timestamp,\n)\n\n\ndef test_capture_stdout(tmp_path):\n    # backend does not matter, but we want to keep compile times down\n    cmake(\n        tmp_path,\n        [\"sentry_example\"],\n        {\n            \"SENTRY_BACKEND\": \"none\",\n            \"SENTRY_TRANSPORT\": \"none\",\n            \"BUILD_SHARED_LIBS\": \"OFF\",\n        },\n    )\n\n    # on linux we can use `ldd` to check that we don\u2019t link to `libsentry.so`\n    if sys.platform == \"linux\":\n        output = subprocess.check_output(\"ldd sentry_example\", cwd=tmp_path, shell=True)\n        assert b\"libsentry.so\" not in output\n\n    # on windows, we use `sigcheck` to check that the exe is compiled correctly\n    if sys.platform == \"win32\":\n        output = subprocess.run(\n            \"sigcheck sentry_example.exe\",\n            cwd=tmp_path,\n            shell=True,\n            stdout=subprocess.PIPE,\n        ).stdout\n        assert (b\"32-bit\" if os.environ.get(\"TEST_X86\") else b\"64-bit\") in output\n    # similarly, we use `file` on linux\n    if sys.platform == \"linux\":\n        output = subprocess.check_output(\n            \"file sentry_example\", cwd=tmp_path, shell=True\n        )\n        assert (\n            b\"ELF 32-bit\" if os.environ.get(\"TEST_X86\") else b\"ELF 64-bit\"\n        ) in output\n\n    output = check_output(\n        tmp_path,\n        \"sentry_example\",\n        [\"stdout\", \"attachment\", \"capture-event\", \"add-stacktrace\"],\n    )\n    envelope = Envelope.deserialize(output)\n\n    assert_meta(envelope)\n    assert_breadcrumb(envelope)\n    assert_attachment(envelope)\n    assert_stacktrace(envelope)\n\n    assert_event(envelope)\n\n\n@pytest.mark.skipif(not has_inproc, reason=\"test needs inproc backend\")\ndef test_inproc_crash_stdout(tmp_path):\n    cmake(\n        tmp_path,\n        [\"sentry_example\"],\n        {\"SENTRY_BACKEND\": \"inproc\", \"SENTRY_TRANSPORT\": \"none\"},\n    )\n\n    child = run(tmp_path, \"sentry_example\", [\"attachment\", \"crash\"])\n    assert child.returncode  # well, its a crash after all\n\n    output = check_output(tmp_path, \"sentry_example\", [\"stdout\", \"no-setup\"])\n    envelope = Envelope.deserialize(output)\n\n    # The crash file should survive a `sentry_init` and should still be there\n    # even after restarts. On Android, we can\u2019t look inside the emulator.\n    if not is_android:\n        with open(\"{}/.sentry-native/last_crash\".format(tmp_path)) as f:\n            crash_timestamp = f.read()\n        assert_timestamp(crash_timestamp)\n\n    assert_meta(envelope)\n    assert_breadcrumb(envelope)\n    assert_attachment(envelope)\n\n    assert_crash(envelope)\n\n\n@pytest.mark.skipif(not has_breakpad, reason=\"test needs breakpad backend\")\ndef test_breakpad_crash_stdout(tmp_path):\n    cmake(\n        tmp_path,\n        [\"sentry_example\"],\n        {\"SENTRY_BACKEND\": \"breakpad\", \"SENTRY_TRANSPORT\": \"none\"},\n    )\n\n    child = run(tmp_path, \"sentry_example\", [\"attachment\", \"crash\"])\n    assert child.returncode  # well, its a crash after all\n\n    with open(\"{}/.sentry-native/last_crash\".format(tmp_path)) as f:\n        crash_timestamp = f.read()\n    assert_timestamp(crash_timestamp)\n\n    output = check_output(tmp_path, \"sentry_example\", [\"stdout\", \"no-setup\"])\n    envelope = Envelope.deserialize(output)\n\n    assert_meta(envelope)\n    assert_breadcrumb(envelope)\n    assert_attachment(envelope)\n\n    assert_minidump(envelope)\n<|endoftext|>",
    "repo": "aykamko/sentry-native"
  },
  {
    "prompt": "\"\"\"Parse the provided Python source string and return an (typed AST, source) tuple.\"\"\"\nfn parse_source(src: str) -> t.Tuple[(ast.Module, t.List[str])]\n\n\"\"\"Helper for generating linting errors from the provided source code.\"\"\"\nfn check_source(src: str, suppress_none_returns: bool, suppress_dummy_args: bool, allow_untyped_defs: bool, allow_untyped_nested: bool, mypy_init_return: bool, allow_star_arg_any: bool, dispatch_decorators: t.AbstractSet[str], overload_decorators: t.AbstractSet[str]) -> t.Generator[(FORMATTED_ERROR, None, None)]\n\n\"\"\"Helper for obtaining a list of Function objects from the provided source code.\"\"\"\nfn functions_from_source(src: str) -> t.List[Function]\n\n\"\"\"Iterate over a list of Function objects & find the first matching named function.\n\nDue to the definition of the test cases, this should always return something, but there is no\nprotection if a match isn't found & will raise an `IndexError`.\"\"\"\nfn find_matching_function(func_list: t.Iterable[Function], match_name: str) -> Function\n\n\"\"\"Check whether the input sequence is empty.\"\"\"\nfn check_is_empty(in_sequence: t.Sequence, msg: str) -> None\n\n\"\"\"Check whether the input sequence is not empty.\"\"\"\nfn check_is_not_empty(in_sequence: t.Sequence, msg: str) -> None\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from itertools import zip_longest\nfrom typing import List, Tuple\n\nimport pytest\nimport pytest_check as check\n\nfrom flake8_annotations import Argument, FunctionVisitor\nfrom testing.helpers import parse_source\nfrom testing.test_cases.type_comment_test_cases import parser_test_cases\n\nARG_FIXTURE_TYPE = Tuple[List[Argument], List[Argument], str]\n\n\nclass TestArgumentParsing:\n    \"\"\"Test for proper argument parsing from source.\"\"\"\n\n    @pytest.fixture(params=parser_test_cases.items())\n    def argument_lists(self, request) -> ARG_FIXTURE_TYPE:  # noqa: ANN001\n        \"\"\"\n        Build a pair of lists of arguments to compare and return as a (truth, parsed) tuple.\n\n        `parser_test_cases` is a dictionary of ParserTestCase named tuples, which provide the\n        following:\n            * `src` - Source code for the test case to be parsed\n            * `args` - A list of Argument objects to be used as the truth values\n            * `should_yield_ANN301` - Boolean flag indicating whether the source should yield ANN301\n\n        A list of parsed Argument objects is taken from the class-level source parser\n\n        The function name is also returned in order to provide a more verbose message for a failed\n        assertion\n        \"\"\"\n        test_case_name, test_case = request.param\n\n        tree, lines = parse_source(test_case.src)\n        visitor = FunctionVisitor(lines)\n        visitor.visit(tree)\n        parsed_arguments = visitor.function_definitions[0].args\n\n        return test_case.args, parsed_arguments, test_case_name\n\n    def test_argument_parsing(self, argument_lists: ARG_FIXTURE_TYPE) -> None:\n        \"\"\"\n        Test argument parsing of the testing source code.\n\n        Argument objects are provided as a tuple of (truth, source) lists\n        \"\"\"\n        for truth_arg, parsed_arg in zip_longest(*argument_lists[:2]):\n            failure_msg = (\n                f\"Comparison check failed for arg '{parsed_arg.argname}' in '{argument_lists[2]}'\"\n            )\n            check.is_true(self._is_same_arg(truth_arg, parsed_arg), msg=failure_msg)\n\n    @staticmethod\n    def _is_same_arg(arg_a: Argument, arg_b: Argument) -> bool:\n        \"\"\"\n        Compare two Argument objects for \"equality\".\n\n        Because we are testing argument parsing in another test, we can make this comparison less\n        fragile by ignoring line & column indices and instead comparing only the following:\n          * argname\n          * has_type_annotation\n          * has_3107_annotation\n          * has_type_comment\n        \"\"\"\n        return all(\n            (\n                arg_a.argname == arg_b.argname,\n                arg_a.has_type_annotation == arg_b.has_type_annotation,\n                arg_a.has_3107_annotation == arg_b.has_3107_annotation,\n                arg_a.has_type_comment == arg_b.has_type_comment,\n            )\n        )\n<|endoftext|>",
    "repo": "sco1/flake8-annotations"
  },
  {
    "prompt": "\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom django.urls import reverse\n\nfrom cursodjango.django_assertions import assert_contains\n\n\n@pytest.fixture\ndef resp(client):\n    return client.get(reverse('aperitivos:video', args=('motivacao',)))\n\n\ndef test_status_code(resp):\n    assert resp.status_code == 200\n\n\ndef test_title_video(resp):\n    assert_contains(resp, 'Video Aperitivo: Motiva\u00e7\u00e3o</h1>')\n\n\ndef test_conteudo_video(resp):\n    assert_contains(resp, '<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/gtgI7PC_TJk\" '\n                          'frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; '\n                          'picture-in-picture\" allowfullscreen></iframe>')\n<|endoftext|>",
    "repo": "jona04/app-curso-django"
  },
  {
    "prompt": "fn create_dataset_sample(project: str, display_name: str, metadata_schema_uri: str, location: str, api_endpoint: str, timeout: int)\n\nfn get_name(out, key)\n\nfn get_state(out)\n\n\"\"\"Waits until the Job state of provided resource name is a particular state.\n\nArgs:\n    get_job_method: Callable[[str], \"proto.Message\"]\n        Required. The GAPIC getter method to poll. Takes 'name' parameter\n        and has a 'state' attribute in its response.\n    name (str):\n        Required. Complete uCAIP resource name to pass to get_job_method\n    expected_state (str):\n        The state at which this method will stop waiting.\n        Default is \"CANCELLED\".\n    timeout (int):\n        Maximum number of seconds to wait for expected_state. If the job\n        state is not expected_state within timeout, a TimeoutError will be raised.\n        Default is 90 seconds.\n    freq (float):\n        Number of seconds between calls to get_job_method.\n        Default is 1.5 seconds.\"\"\"\nfn wait_for_job_state(get_job_method: Callable[([str], proto.Message)], name: str, expected_state: str, timeout: int, freq: float) -> None\n\nfn delete_dataset_sample(project: str, dataset_id: str, location: str, api_endpoint: str, timeout: int)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nfrom uuid import uuid4\n\nimport pytest\n\nimport create_dataset_sample\nimport delete_dataset_sample\nimport helpers\n\nPROJECT_ID = os.getenv(\"BUILD_SPECIFIC_GCLOUD_PROJECT\")\nIMAGE_METADATA_SCHEMA_URI = (\n    \"gs://google-cloud-aiplatform/schema/dataset/metadata/image_1.0.0.yaml\"\n)\n\n\n@pytest.fixture\ndef shared_state():\n    shared_state = {}\n    yield shared_state\n\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef teardown(shared_state):\n    yield\n\n    assert \"/\" in shared_state[\"dataset_name\"]\n\n    dataset_id = shared_state[\"dataset_name\"].split(\"/\")[-1]\n\n    # Delete the created dataset\n    delete_dataset_sample.delete_dataset_sample(\n        project=PROJECT_ID, dataset_id=dataset_id\n    )\n\n\ndef test_ucaip_generated_create_dataset_sample_vision(capsys, shared_state):\n    create_dataset_sample.create_dataset_sample(\n        display_name=f\"temp_create_dataset_test_{uuid4()}\",\n        metadata_schema_uri=IMAGE_METADATA_SCHEMA_URI,\n        project=PROJECT_ID,\n    )\n    out, _ = capsys.readouterr()\n    assert \"create_dataset_response\" in out\n\n    shared_state[\"dataset_name\"] = helpers.get_name(out)\n<|endoftext|>",
    "repo": "ivanmkc/python-aiplatform"
  },
  {
    "prompt": "fn __null_line_strip(line)\n\nfn __line_strip(line)\n\nfn split_line_list(line_list, re_block_separator, is_include_match_line, is_strip)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"\n.. codeauthor:: Tsuyoshi Hombashi <tsuyoshi.hombashi@gmail.com>\n\"\"\"\n\n\nimport re\n\nimport pytest\n\nfrom tcconfig._split_line_list import split_line_list\n\n\nclass Test_split_line_list:\n    @pytest.mark.parametrize(\n        [\"value\", \"separator\", \"is_include_matched_line\", \"is_strip\", \"expected\"],\n        [\n            [\n                [\"abcdefg\", \"ABCDEFG\", \"1234\"],\n                re.compile(\"DEFG$\"),\n                False,\n                True,\n                [[\"abcdefg\"], [\"1234\"]],\n            ],\n            [\n                [\"abcdefg\", \"ABCDEFG\", \"ABCDEFG\", \"1234\"],\n                re.compile(\"DEFG$\"),\n                False,\n                True,\n                [[\"abcdefg\"], [\"1234\"]],\n            ],\n            [\n                [\"ABCDEFG\", \"abcdefg\", \"ABCDEFG\", \"1234\", \"ABCDEFG\"],\n                re.compile(\"DEFG$\"),\n                False,\n                True,\n                [[\"abcdefg\"], [\"1234\"]],\n            ],\n            [\n                [\"abcdefg\", \"ABCDEFG\", \"1234\"],\n                re.compile(\"DEFG$\"),\n                True,\n                True,\n                [[\"abcdefg\"], [\"ABCDEFG\", \"1234\"]],\n            ],\n            [[\"a\", \"  \", \"b\", \"c\"], re.compile(\"^$\"), False, True, [[\"a\"], [\"b\", \"c\"]]],\n            [[\"a\", \"  \", \"b\", \"c\"], re.compile(\"^$\"), False, False, [[\"a\", \"  \", \"b\", \"c\"]]],\n            [[\"a\", \"b\", \"c\"], None, False, True, [[\"a\", \"b\", \"c\"]]],\n        ],\n    )\n    def test_normal(self, value, separator, is_include_matched_line, is_strip, expected):\n        assert split_line_list(value, separator, is_include_matched_line, is_strip) == expected\n\n    @pytest.mark.parametrize(\n        [\"value\", \"separator\", \"is_include_matched_line\", \"is_strip\", \"expected\"],\n        [\n            [None, \"\", False, True, TypeError],\n            [[1, 2, 3], re.compile(\"\"), False, True, AttributeError],\n            [[1, 2, 3], re.compile(\"\"), False, False, TypeError],\n        ],\n    )\n    def test_exception(self, value, separator, is_include_matched_line, is_strip, expected):\n        with pytest.raises(expected):\n            split_line_list(value, separator, is_include_matched_line, is_strip)\n<|endoftext|>",
    "repo": "cnutshell/tcconfig"
  },
  {
    "prompt": "class TestBaseClass:\n\tTEST_UNDEFINED_PARAMETER = 'this is an undefined parameter to work around pytest limitations'\n\tfn setup_class(cls)\n\tfn __pytest_empty_object_fixture(self, _input, default)\n\tfn _create_tempfile(self, _file, _input)\n\tfn _create_args(self, input, extraopts)\n\tfn _create_args_fix(self, input, extraopts)\n\tfn fix_and_check(self, args, id)\n\tfn _create_args_parser(self)\n\tfn check_for_id(self, args, id, occurrences)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import os\nimport sys\n\nimport pytest\n\nsys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))\n\nfrom base import TestBaseClass\n\n\nclass TestClassOelintVarsDoubleModify(TestBaseClass):\n\n    @pytest.mark.parametrize('id', ['oelint.vars.doublemodify'])\n    @pytest.mark.parametrize('occurrence', [1])\n    @pytest.mark.parametrize('input', \n        [\n            {\n            'oelint_adv_test.bb':\n            '''\n            A_append_prepend_remove += \"1\"\n            '''\n            },\n            {\n            'oelint_adv_test.bb':\n            '''\n            A_append_prepend += \"1\"\n            '''\n            },\n            {\n            'oelint_adv_test.bb':\n            '''\n            A_append += \"1\"\n            '''\n            },\n            {\n            'oelint_adv_test.bb':\n            '''\n            A_prepend_remove += \"1\"\n            '''\n            },\n            {\n            'oelint_adv_test.bb':\n            '''\n            A_append_remove += \"1\"\n            '''\n            },\n            {\n            'oelint_adv_test.bb':\n            '''\n            A_remove += \"1\"\n            '''\n            },\n            {\n            'oelint_adv_test.bb':\n            '''\n            A_append_prepend_remove = \" 1 \"\n            '''\n            },\n            {\n            'oelint_adv_test.bb':\n            '''\n            A_append_remove = \" 1 \"\n            '''\n            },\n            {\n            'oelint_adv_test.bb':\n            '''\n            A_prepend_remove = \" 1 \"\n            '''\n            },\n        ],\n    )\n    def test_bad(self, input, id, occurrence):\n        self.check_for_id(self._create_args(input), id, occurrence)\n\n    @pytest.mark.parametrize('id', ['oelint.vars.doublemodify'])\n    @pytest.mark.parametrize('occurrence', [0])\n    @pytest.mark.parametrize('input', \n        [\n            {\n            'oelint_adv_test.bb':\n            '''\n            A_append = \" a\"\n            '''\n            },\n            {\n            'oelint_adv_test.bb':\n            '''\n            A_prepend = \"a \"\n            '''\n            },\n            {\n            'oelint_adv_test.bb':\n            '''\n            A_remove = \"a\"\n            '''\n            },\n            {\n            'oelint_adv_test.bb':\n            '''\n            A += \"a\"\n            '''\n            },\n            {\n            'oelint_adv_test.bb':\n            '''\n            A =+ \"a\"\n            '''\n            },\n        ],\n    )\n    def test_good(self, input, id, occurrence):\n        self.check_for_id(self._create_args(input), id, occurrence)\n<|endoftext|>",
    "repo": "vermaete/oelint-adv"
  },
  {
    "prompt": "fn skip_if_no_reference_system(func)\n\n\"\"\"Counts number of CallNode(s) in the IRModule\"\"\"\nfn count_num_calls(mod)\n\n\"\"\"if KCompiler attribute is missing, this function raises an assertion.\n\nParameters\n----------\norig_mod : IRModule\n    Pre-partitioning module\ncmsisnn_mod : IRModule\n    Post-partitioning module\nis_num_calls_same: bool\n    Are number of CallNode(s) before and after partitioning expected to be the same\"\"\"\nfn assert_partitioned_function(orig_mod: IRModule, cmsisnn_mod: IRModule, expected_ops_unchanged)\n\nfn assert_no_external_function(mod)\n\n\"\"\"Creates IRModule from Function\"\"\"\nfn make_module(func)\n\n\"\"\"Provides CMSIS-NN padding when output dim == input dim.\nThis is TFLu's \"SAME\" padding case.\"\"\"\nfn get_same_padding(in_shape, kernel, dilation, stride)\n\n\"\"\"Returns (kernel_dtype, bias_dtype) based on input's dtype.\"\"\"\nfn get_kernel_bias_dtype(input_dtype)\n\n\"\"\"Calculate the output quantization parameters for convolution based on the input and\nkernel quantization paramters and the data types.\n\nParameters\n----------\nkernel_shape : List[int]\n    shape of the kernel\ninput_scale : float\n    scale of the input tensor\ninput_zp : int\n    zero point of the input tensor\nkernel_scale : Union[float, List[float]]\n    scale(s) of the kernel tensor\nkernel_zp : int\n    zero point of the kernel tensor\nis_depthwise : bool\n    whether it is a depthwise convolution\ninput_dtype : str\n    data type of the input tensor\nkernel_dtype : str\n    data type of the kernel tensor\noutput_dtype : str\n    data type of the output tensor\n\nReturns\n-------\noutput_scale : float\n    scale of the output tensor\noutput_zp : int\n    zero point of the output tensor\"\"\"\nfn get_conv2d_qnn_params(kernel_shape: List[int], input_scale: float, input_zp: int, kernel_scale: Union[(float, List[float])], kernel_zp: int, input_dtype: str, kernel_dtype: str, output_dtype: str, is_depthwise: bool) -> Tuple[(float, int)]\n\n\"\"\"Mimics convert_qnn_fused_activation_function from TFLite frontend\"\"\"\nfn make_qnn_relu(expr, fused_activation_fn, scale, zero_point, dtype)\n\n\"\"\"Provides method to test number of pads present inside the function being visited.\"\"\"\nclass CheckForPadsWithinCompositeFunc(tvm.relay.ExprVisitor):\n\tfn __init__(self)\n\tfn visit_call(self, call)\n\tfn assert_no_pads_within_func(self)\n\tfn assert_pads_within_func(self)\n\n\n\"\"\"Creates AOT test runner for CMSIS-NN tests.\n\nParameters\n----------\ncompiler_cpu : str\n   Equivalent of gcc option mcpu\n   Options:  cortex-m55, cortex-m7\ncpu_flags: str\n    Disable Arm(R) Cortex(R)-M profile vector extension (mve)\n    Options:\n    Arm(R) Cortex(R)-M55: when null +mve is set by default.\n        +nomve disables vector extensions.\n    Arm(R) Cortex(R)-M7 does not support mve.\"\"\"\nfn create_test_runner(compiler_cpu: str, cpu_flags: str)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\"\"\"CMSIS-NN integration tests: Tests invalid graphs\"\"\"\nimport itertools\nimport numpy as np\nimport pytest\nimport tvm\nfrom tvm import relay\n\nfrom tvm.testing.aot import AOTTestModel, compile_and_run, generate_ref_data\nfrom tvm.micro.testing.aot_test_utils import (\n    AOT_USMP_CORSTONE300_RUNNER,\n)\nfrom utils import (\n    skip_if_no_reference_system,\n    get_range_for_dtype_str,\n)\n\n\n@skip_if_no_reference_system\n@tvm.testing.requires_cmsisnn\ndef test_empty_function():\n    ORIGINAL_MODEL = \"\"\"\n#[version = \"0.0.5\"]\ndef @main(%data : Tensor[(16, 29), int8]) -> Tensor[(16, 29), int8] {\n    add(%data, %data)\n}\n\"\"\"\n    CMSISNN_MODEL = \"\"\"\n#[version = \"0.0.5\"]\ndef @tvmgen_default_cmsis_nn_main_1(%i1: Tensor[(16, 29), int8], Inline=1, Compiler=\"cmsis-nn\", global_symbol=\"tvmgen_default_cmsis_nn_main_1\", Primitive=1) -> Tensor[(16, 29), int8] {\n  add(%i1, %i1)\n}\ndef @main(%data : Tensor[(16, 29), int8]) -> Tensor[(16, 29), int8] {\n  %1 = @tvmgen_default_cmsis_nn_main_1(%data) /* ty=Tensor[(16, 29), int8] */;\n  %1\n}\n\"\"\"\n    orig_mod = tvm.parser.fromtext(ORIGINAL_MODEL)\n    cmsisnn_mod = tvm.parser.fromtext(CMSISNN_MODEL)\n    params = {}\n\n    # validate the output\n    interface_api = \"c\"\n    use_unpacked_api = True\n    test_runner = AOT_USMP_CORSTONE300_RUNNER\n    dtype = \"int8\"\n    in_min, in_max = get_range_for_dtype_str(dtype)\n    rng = np.random.default_rng(12345)\n    inputs = {\"data\": rng.integers(in_min, high=in_max, size=(16, 29), dtype=dtype)}\n    outputs = generate_ref_data(orig_mod[\"main\"], inputs, params)\n    compile_and_run(\n        AOTTestModel(\n            module=cmsisnn_mod,\n            inputs=inputs,\n            outputs=outputs,\n            params=params,\n            output_tolerance=0,\n        ),\n        test_runner,\n        interface_api,\n        use_unpacked_api,\n    )\n<|endoftext|>",
    "repo": "billishyahao/tvm"
  },
  {
    "prompt": "\"\"\"Find state abbrevations from a long-lat bbox.\n\nExamples\n--------\ngssurgo.state_by_bbox(fpath = \"tifs\", ext = \"tif\",  xmax = -88.34945, \\\n                      xmin = -88.35470, ymin = 38.70095, ymax = 38.70498)\n\nNotes\n-----\ndownload:\nhttp://www2.census.gov/geo/tiger/GENZ2017/shp/cb_2017_us_state_500k.zip\n`ogr2ogr -f GPKG states.gpkg cb_2017_us_state_50k.shp`\"\"\"\nfn state_by_bbox(fpath, ext, xmax, xmin, ymin, ymax)\n\n\"\"\"Crop original raster to bounding box so we can read in memory.\n\nExamples\n--------\ngssurgo.aoi(in_raster_path = \"tifs\", out_raster = \"path/to/aoi.tif\", \\\n            xmax = -88.34945, xmin = -88.35470, ymin = 38.70095, \\\n            ymax = 38.70498)\n\nNotes\n-----\nhttps://gis.stackexchange.com/a/237412/32531\"\"\"\nfn aoi(in_raster_path, out_raster, xmin, ymax, xmax, ymin, src_tif)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom gssurgo.aoi import (\n    state_by_bbox\n)\n\n\ndef test_query_gpkg():\n    assert state_by_bbox(fpath = \"tests/tifs\", ext = \"tif\",\n                         xmax = -88.34945, xmin = -88.35470,\n                         ymin = 38.70095, ymax = 38.70498) == \\\n        ['tests/tifs/IL.tif']\n<|endoftext|>",
    "repo": "jsta/gssurgo"
  },
  {
    "prompt": "\"\"\"ISIC4 categories enum.\"\"\"\nclass Category(_Enum):\n\tSECTION = 1\n\tDIVISION = 2\n\tGROUP = 3\n\tCLASS = 4\n\tSUBCLASS = 5\n\n\n\"\"\"Industry classification standards enum.\"\"\"\nclass Standards(_Enum):\n\tISIC3 = 'ISIC3'\n\tISIC31 = 'ISIC31'\n\tISIC4 = 'ISIC4'\n\tNACE1 = 'NACE1'\n\tNACE2 = 'NACE2'\n\tNAICS2017 = 'NAICS2017'\n\tTSIC2552 = 'TSIC2552'\n\tJSIC13 = 'JSIC13'\n\tKSIC10 = 'KSIC10'\n\tSKD2002 = 'SKD2002'\n\tSKD2008 = 'SKD2008'\n\tSKIS2010 = 'SKIS2010'\n\tCNAE2 = 'CNAE2'\n\tNACEBEL2003 = 'NACEBEL2003'\n\tNACEBEL2008 = 'NACEBEL2008'\n\tNAF1 = 'NAF1'\n\tNAF2 = 'NAF2'\n\tGCED2011 = 'GCED2011'\n\tSCIAN2018 = 'SCIAN2018'\n\tCCNAE2021 = 'CCNAE2021'\n\tCAEM2005 = 'CAEM2005'\n\tCAEM2009 = 'CAEM2009'\n\tSBI2008 = 'SBI2008'\n\tSIC = 'SIC'\n\tSSIC2020 = 'SSIC2020'\n\n\n\"\"\"Classification base object for use across standards.\n\nExamples:\n    The NAICS2017 class \"Space Research and Technology\" can be represented as\n\n    >>> Classification(\n    ...     code=\"927110\",\n    ...     description=\"Space Research and Technology\"\n    ... )\n    Classification(code='927110', description='Space Research and Technology', category=None)\"\"\"\nclass Classification:\n\tcode: str\n\tdescription: str = None\n\tcategory: Category = None\n\t\"\"\"Return self as dict.\"\"\"\n\tfn to_dict(self)\n\n\nclass Standard(_OrderedDict):\n\t\"\"\"Industry classification standard.\n\t\n\tA Standard defines all classes (i.e., classifications) belonging to a\n\tstandard.\n\t\n\tArgs:\n\t    standard: enum to reference standard\n\t    classes: all included classes (i.e., codes) defined in the standard\n\t\n\tExamples:\n\t    A subset of the NAICS2017 standard can be represented as\n\t\n\t    >>> Standard(\n\t    ...     standard=Standards.NAICS2017,\n\t    ...     classes=[\n\t    ...         Classification(\"111110\", \"Soybean Farming\"),\n\t    ...         Classification(\"111120\", \"Oilseed (except Soybean) Farming\"),\n\t    ...         Classification(\"111130\", \"Dry Pea and Bean Farming\"),\n\t    ...     ]\n\t    ... )\"\"\"\n\tfn __init__(self, standard: Standards, classes: _List[Classification])\n\t\"\"\"Return repr(self).\"\"\"\n\tfn __repr__(self) -> str\n\n\nclass Concordance(_nx.DiGraph):\n\t\"\"\"Industry classification concordance.\n\t\n\tConcordances define direct relationships between classification\n\tsystems. The relationships are directed (from => to) from classes\n\tin a source standard to classes in a destination standard. Accordingly,\n\tConcordances are implemented as a directed graph.\n\t\n\tGiven source and destination classification standards, all defined\n\tclasses are added as nodes in the graph. Concordances are added as\n\trelationships between nodes in the graph.\n\t\n\tConcordant classes are discovered by returning all descendants of a\n\tgiven node.\n\t\n\tNote:\n\t    Concordances must be directed acyclic graphs.\n\t\n\tArgs:\n\t    src: source standard to map concordances from\n\t    dst: destination standard to map concordances to\n\t    concordances: directed relationships between classes in the source\n\t        and destination standards\"\"\"\n\tfn __init__(self, src: Standard, dst: Standard, concordances: _List[tuple])\n\t\"\"\"Return repr(self).\"\"\"\n\tfn __repr__(self) -> str\n\t\"\"\"Return set of concordant industry classifications (i.e., descendants).\n\t\n\tArgs:\n\t    code: source classification code (e.g., \"927110\" from NAICS2017)\n\t\n\tReturns:\n\t    set: concordant (i.e., descandant) nodes from the given code\n\t\n\tExamples:\n\t    >>> pyisic.NAICS2017_to_ISIC4.concordant(\"927110\")\n\t    {(<Standards.ISIC4: 'ISIC4'>, '5120'), (<Standards.ISIC4: 'ISIC4'>, '8413')}\"\"\"\n\tfn concordant(self, code: str) -> set\n\n\nclass ComposedGraph(_nx.DiGraph):\n\t\"\"\"Composed graph of industry classification concordance.\n\t\n\tArgs:\n\t    dst: destination standard\n\t    concordances: iterable of concordances\"\"\"\n\tfn __init__(self, dst: Standard, concordances: None)\n\t\"\"\"Return set of concordant industry classifications.\n\t\n\tArgs:\n\t    code: source classification code (e.g., \"927110\" from NAICS2017)\n\t    src: source standard (e.g., \"NAICS2017\")\n\t\n\tReturns:\n\t    set: concordant nodes from the given source code\n\t\n\tExamples:\n\t    >>> pyisic.ToISIC4(\"927110\", Standards.NAICS2017)\n\t    {(<Standards.ISIC4: 'ISIC4'>, '5120'), (<Standards.ISIC4: 'ISIC4'>, '8413')}\"\"\"\n\tfn __call__(self, code: str, src: str) -> set\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# -*- coding: utf-8 -*-\nimport pytest\n\nfrom pyisic import JSIC13_to_ISIC4\nfrom pyisic.types import Standards\n\n\n@pytest.mark.parametrize(\n    \"code,expected\",\n    [\n        (\"DOESNT EXIST\", set()),\n        (\"A\", set()),\n        (\"0100\", {(Standards.ISIC4, \"7010\")}),\n    ],\n)\ndef test_jsic13_to_isic4_concordance(code: str, expected: set):\n    \"\"\"Test JSIC13 to ISIC4 sample concordances.\"\"\"\n    assert JSIC13_to_ISIC4.concordant(code) == expected\n<|endoftext|>",
    "repo": "sayari-analytics/pyisic"
  },
  {
    "prompt": "\"\"\"update the cluster configuration file\"\"\"\nfn update_config(config_dict, config, amount, role)\n\n\"\"\"Create a new host(s) in OpenStack which will join the cluster as a node(s)\n\nThis hosts boots with all paramerters required for it to join the cluster\nwith a cloud-init.\n\nArgs:\n    cloud_config (``koris.cloud.openstack.OSClusterInfo``): the content\n       cloud.conf file\n    os_cluster_info (``koris.cloud.openstack.OSClusterInfo``)\n    role (str): the host role currently only \"node\" is supported here.\n    zone (str): the AZ in OpenStack in which the hosts are created\n    amount (int): the number of instance to create\n    flavour (str): the flavor in OpenStack to create\n    k8s (``koris.deploy.K8S``): an instance which creates a bootstrap token.\n    config_dict (dict): the koris configuration yaml as ``dict``\"\"\"\nfn add_node(cloud_config, os_cluster_info, role, zone, amount, flavor, k8s, config_dict)\n\n\"\"\"Add a new master to OpenStack and the Kubernetes cluster.\n\nWill add a new node to OpenStack, adjust the config, bootstrap the master\nand add the IP to the LoadBalancer pool.\n\nArgs:\n     builder (:class:`.cloud.openstack.ControlPlaneBuilder`): A\n        ControlPlanBuilder instance.\n    zone (str): The AZ to add the new node in.\n    flavor (str): The node flavor of the node.\n    config (str): The path of the koris config.\n    config_dict (dict): The parsed koris config.\n    os_cluster_info (:class:`.cloud.openstack.OSClusterInfo`): A\n        OSClusterInfo instance.\n    k8s (:class:`.deploy.K8S`): A K8S instance.\"\"\"\nfn add_master(builder, zone, flavor, config, config_dict, k8s)\n\n\"\"\"Delete a master or worker node from the cluster.\n\nWill perform basic validity checks on the name.\n\nArgs:\n    config_dict (dict): A dictionary representing the config.\n    name (str): The name of the node to delete.\n    conn: An OpenStack connection object.\n\nRaises:\n    ValueError if name is invalid or resources are not found.\"\"\"\nfn delete_node(config_dict, name)\n\n\"\"\"The main entry point for the program. This class does the CLI parsing\nand descides which action shoud be taken\"\"\"\nclass Koris:\n\tfn __init__(self)\n\tfn _get_version(self)\n\tfn _get_verbosity(self)\n\t\"\"\"Bootstrap a Kubernetes cluster\n\t\n\tconfig - configuration file\"\"\"\n\tfn apply(self, config)\n\t\"\"\"Delete the complete cluster stack\"\"\"\n\tfn destroy(self, config: str, force: bool)\n\t\"\"\"Delete a node from the cluster, or the complete cluster.\n\t\n\tconfig - koris configuration file.\n\tresource - the type of resource to delete. [node | cluster]\n\tname - the name of the resource to delete.\n\tforce - Force deletion of resource.\"\"\"\n\tfn delete(self, config: str, resource: str, name: str, force: bool)\n\t\"\"\"Add a worker node or master node to the cluster.\n\t\n\tconfig - configuration file\n\tflavor - the machine flavor\n\tzone - the availablity zone\n\trole - one of node or master\n\tamount - the number of worker nodes to add (masters are not supported)\n\t---\n\tAdd a node or a master to the current active context in your KUBECONFIG.\n\tYou can specify any other configuration file by overriding the\n\tKUBECONFIG environment variable.\n\tIf you specify a name and IP address the program will only try to join\n\tit to the cluster without trying to create the host in the cloud first.\"\"\"\n\tfn add(self, config: str, flavor: str, zone: str, role: str, amount: int)\n\n\n\"\"\"run and execute koris\"\"\"\nfn main()\n\nfn mock_listener()\n\nfn mock_pool()\n\nfn mock_pool_info()\n\nfn mock_member(nr)\n\n\"\"\"A default LoadBalancer Object as returned from the OpenStack SDK\"\"\"\nfn default_data()\n\nfn default_project()\n\nfn default_location()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import os\nimport subprocess\n\nimport pytest\n\nfrom .testdata import CONFIG\nfrom koris.koris import delete_node\n\n\ndef _get_clean_env():\n    \"\"\"\n    A helper method that ensures that it seems that we do not have an RC\n    file sourced.\n    \"\"\"\n    env = {}\n    for (key, val) in dict(os.environ).items():\n        if not key.startswith(\"OS_\"):\n            env[key] = val\n    return env\n\n\ndef test_help():\n    \"\"\"\n    It should be possible to call koris --help without sourcing an\n    RC file.\n    \"\"\"\n    cmd = ['koris', '--help']\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n                            stderr=subprocess.PIPE, env=_get_clean_env())\n    stdout, stderr = proc.communicate()\n    output = stdout.decode(\"utf-8\").strip()\n    assert proc.returncode == 0\n    assert \"usage: koris\" in output\n\n\ndef test_need_rc_file():\n    \"\"\"\n    For building a cluster, we need to source the RC file.\n    \"\"\"\n    cmd = ['koris', 'apply', 'tests/koris_test.yml']\n    try:\n        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE,\n                                stderr=subprocess.PIPE, env=_get_clean_env())\n        proc.communicate()\n\n        # We should not reach that, since an AssertionError should be thrown.\n        assert False\n    except AssertionError:\n        pass\n\n\ndef test_delete_node():\n    invalid_names = [\"\", None]\n\n    for name in invalid_names:\n        with pytest.raises(ValueError):\n            delete_node(CONFIG, name)\n<|endoftext|>",
    "repo": "noris-network/koris"
  },
  {
    "prompt": "fn load_spacy_nlp_model() -> spacy.language.Language\n\nfn _doc_text(doc: Doc) -> str\n\nfn get_doc(text: str) -> spacy.tokens.Doc\n\nfn get_tokens_df(text: str) -> pd.DataFrame\n\nfn _get_tokens_df(doc: Doc, attrs: List[str]) -> pd.DataFrame\n\nfn annotated_names_to_keys(annotate_settings: Dict[(str, dict)], annotated_names: List[str]) -> List[str]\n\nfn get_pos_annotation(text: str, annotated_keys: List[str], annotations_settings: Dict, colors: Dict[(str, str)], row: pd.Series, idx: int, next_idx: int) -> Union[(Tuple[(str, Any, str)], str)]\n\nfn get_cases_annotation(text: str, annotated_keys: List[str], _: Dict, colors: Dict[(str, str)], row: pd.Series, idx: int, next_idx: int) -> Union[(Tuple[(str, Any, str)], str)]\n\nfn merge_same_annotation_texts(annotations: list) -> list\n\nfn get_annotated_text(what_to_annotate: str, text: str, annotated_names: List[str], colors: Dict[(str, str)]) -> list\n\nfn split_annotated_text(annotated_text: list, delimiter: str) -> list\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pandas.api.types as ptypes\nimport pytest\n\nfrom satzify import constants\nfrom satzify.helpers import (\n    annotated_names_to_keys,\n    get_annotated_text,\n    get_tokens_df,\n    split_annotated_text,\n)\n\n\n@pytest.fixture\ndef texts():\n    return (\n        \"Alles hat ein Ende, nur die Wurst hat zwei.\",\n        \"Also morgen ist ja genau genommen schon heute.\",\n        \"Das Jahr ist wieder irgendwie so schnell vergangen.\",\n        \"Ich war nur kurz was zu trinken holen und als ich zur\u00fcckkam, wart ihr irgendwie weg.\",\n        \"\"\"\n        Was wir alles nicht wissen wollen! Und was wir nicht alles wissen wollen!\n        Das m\u00f6chte ich gar nicht wissen. Und wenn man es dann doch wei\u00df, weil der andere sein Wissen einfach nicht f\u00fcr sich behalten konnte, dann k\u00f6nnen wir ja nichts daf\u00fcr, wir wollten es ja gar nicht wissen. Und eine Klatschtante sind wir dann auch nicht.\n        \"\"\",\n    )\n\n\n@pytest.fixture\ndef cases():\n    return constants.CASES\n\n\n@pytest.fixture\ndef pos():\n    return constants.POS\n\n\n@pytest.fixture\ndef colors():\n    cases_and_pos_settings = {**constants.CASES, **constants.POS}\n    colors = {k: v[\"color\"] for k, v in cases_and_pos_settings.items()}\n    return colors\n\n\n@pytest.fixture\ndef example_text():\n    return \"Alles hat ein Ende, nur die Wurst hat zwei.\"\n\n\n@pytest.fixture\ndef example_pos_annotations(example_text):\n    return [\n        ([\"Alles \"], \"Pronoun\", \"#fea\"),\n        ([\"hat \"], \"Verb\", \"#8ef\"),\n        \"ein \",\n        ([\"Ende\"], \"Noun\", \"#afa\"),\n        \", \",\n        ([\"nur \"], \"Adverb\", \"#d94\"),\n        \"die \",\n        ([\"Wurst \"], \"Noun\", \"#afa\"),\n        ([\"hat \"], \"Verb\", \"#8ef\"),\n        \"zwei\",\n        \".\",\n    ]\n\n\n@pytest.fixture\ndef example_cases_annotations(example_text):\n    return [\n        ([\"Alles \"], \"Nom+N\", \"#afa\"),\n        \"hat \",\n        ([\"ein \", \"Ende\"], \"Acc+N\", \"#fea\"),\n        \", \",\n        \"nur \",\n        ([\"die \", \"Wurst \"], \"Nom+F\", \"#afa\"),\n        \"hat \",\n        \"zwei\",\n        \".\",\n    ]\n\n\ndef test_get_tokens_df(texts):\n    for text in texts:\n        tokens_df = get_tokens_df(text)\n        assert ptypes.is_numeric_dtype(tokens_df[\"idx\"].apply(int))\n        assert ptypes.is_string_dtype(tokens_df[\"text\"])\n        assert ptypes.is_string_dtype(tokens_df[\"morph\"])\n        assert ptypes.is_string_dtype(tokens_df[\"pos_\"])\n        morphs = tokens_df[tokens_df[\"morph\"] != \"\"][\"morph\"].to_list()\n        for morph in morphs:\n            parts = morph.split(\"|\")\n            # ensure that each and every part of a morph has a key=value structure, i.e. Case=Nom\n            assert all(len(part.split(\"=\")) == 2 for part in parts)\n\n\ndef test_annotated_names_to_keys(pos):\n    annotated_names = [\"Noun\", \"Verb\", \"Adposition\"]\n    keys = annotated_names_to_keys(pos, annotated_names)\n    assert keys == [\"NOUN\", \"VERB\", \"ADP\"]\n\n\ndef test_pos_annotations(pos, colors, example_text, example_pos_annotations):\n    pos_annotated_names = [v[\"name\"] for v in pos.values()]\n    annotations = get_annotated_text(\"POS\", example_text, pos_annotated_names, colors)\n    assert annotations == example_pos_annotations\n\n\ndef test_cases_annotations(cases, colors, example_text, example_cases_annotations):\n    pos_annotated_names = [v[\"name\"] for v in cases.values()]\n    annotations = get_annotated_text(\"CASES\", example_text, pos_annotated_names, colors)\n    assert annotations == example_cases_annotations\n\n\ndef test_split_annotated_text(example_pos_annotations, example_cases_annotations):\n    combined_annotations = example_pos_annotations + [\"\\n\"] + example_cases_annotations\n    list_annotated_text = split_annotated_text(combined_annotations)\n    assert len(list_annotated_text) == 2\n    assert list_annotated_text[0] == example_pos_annotations\n    assert list_annotated_text[1] == example_cases_annotations\n<|endoftext|>",
    "repo": "michdr/satzify"
  },
  {
    "prompt": "\"\"\"Compute the phase differential along a given axis\n\nParameters\n----------\nphase : np.ndarray\n    Input phase (in radians)\n\nconv: {None, 'tf', 'th', 'channels_last', 'channels_first'}\n    Convolution mode\n\nReturns\n-------\ndphase : np.ndarray like `phase`\n    The phase differential.\"\"\"\nfn phase_diff(phase: np.ndarray, conv: {None, 'tf', 'th', 'channels_last', 'channels_first'}) -> np.ndarray like `phase`\n\n\"\"\"Quantize array entries to a fixed dtype.\n\nParameters\n----------\nx : np.ndarray\n    The data to quantize\n\nref_min : None or float\n\nref_max : None or float\n    The reference minimum (maximum) value for quantization.\n    By default, `x.min()` (`x.max()`)\n\ndtype : np.dtype {'uint8', 'uint16'}\n    The target data type.  Any unsigned int type is supported,\n    but most cases will call for `uint8`.\n\nReturns\n-------\ny : np.ndarray, dtype=dtype\n    The values of `x` quantized to integer values\"\"\"\nfn quantize(x: np.ndarray, ref_min: None or float, ref_max: None or float, dtype: np.dtype {'uint8', 'uint16'}) -> np.ndarray, dtype=dtype\n\n\"\"\"Convert an array to a target dtype.  Quantize if integrable.\n\nParameters\n----------\nx : np.ndarray\n    The input data\n\ndtype : np.dtype or type specification\n    The target dtype\n\nReturns\n-------\nx_dtype : np.ndarray, dtype=dtype\n    The converted data.\n\n    If dtype is integrable, `x_dtype` will be quantized.\n\nSee Also\n--------\nquantize\"\"\"\nfn to_dtype(x: np.ndarray, dtype: np.dtype or type specification) -> np.ndarray, dtype=dtype\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n'''Tests for feature utility helpers'''\n\nimport pytest\nimport numpy as np\n\nimport pumpp\nimport pumpp.feature._utils\n\n\n@pytest.mark.parametrize('dtype', ['uint8', np.uint8])\ndef test_quantize(dtype):\n\n    # The range -5 to 5 is broken into 256 equal pieces\n    # -5/3 lands at 85 (1/3)\n    # 5/3 lands at 2*85 = 270\n    # 5 lands at the max\n    x = np.asarray([-5, -5/3, 5/3, 5])\n    y = pumpp.feature._utils.quantize(x, dtype=dtype)\n    assert np.allclose(y, [0, 85, 170, 255])\n\n\ndef test_quantize_min():\n    x = np.asarray([-5, -5/3, 5/3, 5])\n    y = pumpp.feature._utils.quantize(x, ref_min=0)\n    assert np.allclose(y, [0, 0, 85, 255])\n\n\ndef test_quantize_max():\n    x = np.asarray([-5, -5/3, 5/3, 5])\n    y = pumpp.feature._utils.quantize(x, ref_max=0)\n    assert np.allclose(y, [0, 170, 255, 255])\n\n\n@pytest.mark.xfail(raises=pumpp.ParameterError)\n@pytest.mark.parametrize('dtype', ['int8', 'float32'])\ndef test_quantize_bad_dtype(dtype):\n    x = np.asarray([-5, -5/3, 5/3, 5])\n    pumpp.feature._utils.quantize(x, dtype=dtype)\n<|endoftext|>",
    "repo": "tomxi/pumpp"
  },
  {
    "prompt": "fn soma(a, b)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\nfrom Python import soma\n\ndef test():\n    assert soma(5,7) == 12<|endoftext|>",
    "repo": "katelynmrs/Integra-ao-Continua"
  },
  {
    "prompt": "class ApiType(Enum):\n\tJENKINS = 0\n\tSCRIPT = 1\n\tMOCK = 2\n\n\nfn direct_url(api_type)\n\nfn public_url(api_type)\n\nfn direct_cli_url(api_type)\n\nfn public_cli_url(api_type)\n\nfn proxied_public_cli_url(api_type)\n\nfn script_dir()\n\n\"\"\"speedup is used by the mock api\"\"\"\nfn select_speedup(speedup)\n\nfn speedup()\n\nfn skip_job_delete()\n\nfn skip_job_load()\n\nfn skip_job_load_sh_export_str()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright (c) 2012 - 2015 Lars Hupfeldt Nielsen, Hupfeldt IT\n# All rights reserved. This work is under a BSD license, see LICENSE.TXT.\n\nimport pytest\n\nfrom .. import jenkins_api\nfrom .framework import api_select\nfrom .cfg import ApiType\n\n\n@pytest.mark.not_apis(ApiType.MOCK, ApiType.SCRIPT)\ndef test_api_class_repr_job(api_type):\n    api = api_select.api(__file__, api_type, login=True)\n    job = jenkins_api.ApiJob(api, {}, 'my-job')\n\n    jrd = eval(repr(job))\n    assert jrd == {'name': 'my-job', 'dct': {}}\n\n    invocation = jenkins_api.Invocation(job, \"http://dummy\", 'hello')\n    assert repr(invocation) == \"Invocation: 'http://dummy' None None\"\n<|endoftext|>",
    "repo": "velniukas/jenkinsflow"
  },
  {
    "prompt": "\"\"\"the build class is the object passed to the main method of the\nuranium script.\n\nit's designed to serve as the public API to controlling the build process.\n\nBuild is designed to be executed within the sandbox\nitself. Attempting to execute this outside of the sandbox could\nlead to corruption of the python environment.\"\"\"\nclass Build(object):\n\tURANIUM_CACHE_DIR = '.uranium'\n\tHISTORY_NAME = 'history.json'\n\tfn __init__(self, root, config, with_sandbox, cache_requests)\n\t\"\"\":return: a uranium.config.Config object\n\t\n\tthis is a generic dict to store / retrieve config data\n\tthat tasks may find valuable\"\"\"\n\tfn config(self)\n\t\"\"\":return: a uranium.environment_variables.EnvironmentVariables object\n\t\n\tthis is an interface to the environment variables of the\n\tsandbox.  variables modified here will be preserved when\n\texecuting entry points in the sandbox.\"\"\"\n\tfn envvars(self)\n\t\"\"\":return: uranium.executables.Executables\n\t\n\tan interface to execute scripts\"\"\"\n\tfn executables(self)\n\t\"\"\":return: uranium.hooks.Hooks\n\t\n\tprovides hooks to attach functions to be executed during\n\tvarious phases of Uranium (like initializiation and finalization)\"\"\"\n\tfn hooks(self)\n\t\"\"\":return: uranium.history.History\n\t\n\ta dictionary that can contain basic data structures, that is\n\tpreserved across executions.\n\t\n\tideal for storing state, such as if a file was already downloaded.\"\"\"\n\tfn history(self)\n\t\"\"\":return: uranium.options.Options\n\t\n\tan interface to arguments passed into the uranium command line.\"\"\"\n\tfn options(self)\n\t\"\"\":return: uranium.packages.Packages\n\t\n\tan interface to the python packages currently installed.\"\"\"\n\tfn packages(self)\n\t\"\"\":return: str\n\t\n\treturns the root of the uranium build.\"\"\"\n\tfn root(self)\n\t\"\"\":return: uranium.tasks.Tasks\n\t\n\tan interface to the tasks that uranium has registered,\n\tor has discovered in the ubuild.py\"\"\"\n\tfn tasks(self)\n\tfn as_current_build(self)\n\tfn run_task(self, task_name)\n\t\"\"\"a decorator that adds the given function as a task.\n\t\n\te.g.\n\t\n\t@build.task\n\tdef main(build):\n\t    build.packages.install(\"httpretty\")\n\t\n\tthis is useful in the case where tasks are being sourced from\n\ta different file, besides ubuild.py\"\"\"\n\tfn task(self, f)\n\t\"\"\"executes the script at the specified path. \"\"\"\n\tfn include(self, script_path, cache)\n\tfn run(self, options)\n\tfn _run(self, options)\n\t\"\"\"override_func: if this is not None, the _run_script will\n\texecute this function (passing in the script object) instead\n\tof executing the task_name.\"\"\"\n\tfn _run_script(self, path, task_name, override_func)\n\tfn _warmup(self)\n\tfn _finalize(self)\n\n\nfn _get_formatted_public_tasks(script)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import httpretty\nfrom uranium import get_remote_script\nfrom uranium.build import Build\nimport pytest\n\nSCRIPT_URL = \"http://www.myinternalwebsite.com/uranium_base.py\"\n\nREMOTE_SCRIPT = \"\"\"\ndef setup(build):\n    build.index_urls = [\"http://www.myinternalwebsite.com/python_index\"]\n\"\"\".strip()\n\n\ndef test_get_remote_script(tmpdir):\n    httpretty.enable()\n    httpretty.register_uri(httpretty.GET, SCRIPT_URL,\n                           body=REMOTE_SCRIPT)\n    script = get_remote_script(SCRIPT_URL)\n    build = Build(tmpdir.strpath)\n    script[\"setup\"](build)\n    httpretty.disable()\n    httpretty.reset()\n\n\ndef test_get_remote_script_with_cache(tmpdir):\n    httpretty.enable()\n    httpretty.register_uri(httpretty.GET, SCRIPT_URL,\n                           body=REMOTE_SCRIPT)\n    get_remote_script(SCRIPT_URL, cache_dir=tmpdir.strpath)\n    httpretty.disable()\n    httpretty.reset()\n    get_remote_script(SCRIPT_URL, cache_dir=tmpdir.strpath)\n\n\ndef test_build_include_cache(tmpdir):\n    httpretty.enable()\n    httpretty.register_uri(httpretty.GET, SCRIPT_URL,\n                           body=REMOTE_SCRIPT)\n    build = Build(tmpdir.strpath)\n    build.URANIUM_CACHE_DIR = tmpdir.strpath\n    build.include(SCRIPT_URL, cache=True)\n    httpretty.disable()\n    httpretty.reset()\n    build.include(SCRIPT_URL, cache=True)\n\n\ndef test_build_include_without_cache(tmpdir):\n    httpretty.enable()\n    httpretty.register_uri(httpretty.GET, SCRIPT_URL,\n                           body=REMOTE_SCRIPT)\n    build = Build(tmpdir.strpath)\n    build.URANIUM_CACHE_DIR = tmpdir.strpath\n    build.include(SCRIPT_URL, cache=False)\n    httpretty.disable()\n    httpretty.reset()\n    with pytest.raises(Exception):\n        build.include(SCRIPT_URL, cache=True)\n<|endoftext|>",
    "repo": "yunstanford/uranium"
  },
  {
    "prompt": "class MissingLibrariesError(Exception):\n\tfn __init__(self, libs)\n\n\nfn printerr()\n\nfn load_exclude_list(filename)\n\n\"\"\"Return a dict of the form soname => path for all the dependency of\nthe executable\"\"\"\nfn list_soname_paths(executable)\n\n\"\"\"Return a dict of the form soname => path\"\"\"\nfn parse_ldd_output(ldd_output)\n\n\"\"\"Return the list of sonames this binary *directly* depends on\"\"\"\nfn list_dependencies(binary)\n\nfn is_excluded(dependency, exclude_list)\n\nfn copy(dependency, destdir)\n\nclass App:\n\tfn __init__(self, exclude_list, destdir, dry_run, dot_fp)\n\tfn run(self, binary_path)\n\tfn _traverse_tree(self, binary)\n\tfn _graph_excluded_dependency(self, binary, soname)\n\tfn _graph_dependency(self, binary, soname)\n\n\nfn main()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom copydeps import parse_ldd_output, MissingLibrariesError\n\n\ndef test_parse_ldd_output():\n    LDD_OUTPUT = b\"\"\"linux-vdso.so.1 =>  (0x00007ffd6f3cd000)\n    libcrypto.so.1.0.0 => /home/ci/build/genymotion/./libcrypto.so.1.0.0 (0x00007f5ea40b6000)\n    ./libfoo.so.2 (0x0000562cf1094000)\n    \"\"\"\n    dct = parse_ldd_output(LDD_OUTPUT)\n    assert dct == {\n        'libcrypto.so.1.0.0': '/home/ci/build/genymotion/./libcrypto.so.1.0.0',\n        './libfoo.so.2': './libfoo.so.2',\n    }\n\n\ndef test_parse_ldd_output_library_not_found():\n    LDD_OUTPUT = b\"\"\"linux-vdso.so.1 =>  (0x00007ffd6f3cd000)\n    libstdc++.so.5 => not found\n    libbar.so.2 => not found\n    \"\"\"\n    with pytest.raises(MissingLibrariesError) as excinfo:\n        parse_ldd_output(LDD_OUTPUT)\n    assert excinfo.value.libs == ['libstdc++.so.5', 'libbar.so.2']\n<|endoftext|>",
    "repo": "Genymobile/copydeps"
  },
  {
    "prompt": "fn add_point_data(mesh, dim, num_tags, seed, dtype)\n\nfn add_cell_data(mesh, dim, num_tags, dtype)\n\nfn add_field_data(mesh, value, dtype)\n\nfn add_point_sets(mesh)\n\n\"\"\"Write and read a file, and make sure the data is the same as before.\n    \"\"\"\nfn write_read(writer, reader, input_mesh, atol, extension)\n\nfn generic_io(filename)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import os\n\nimport numpy\nimport pytest\n\nimport helpers\nimport meshio\n\n\n@pytest.mark.parametrize(\"mesh\", [helpers.tri_mesh])\ndef test_wkt(mesh):\n    def writer(*args, **kwargs):\n        return meshio.wkt.write(*args, **kwargs)\n\n    helpers.write_read(writer, meshio.wkt.read, mesh, 1.0e-12)\n\n\n@pytest.mark.parametrize(\n    \"filename, ref_sum, ref_num_cells\",\n    [(\"simple.wkt\", 4, 2), (\"whitespaced.wkt\", 3.2, 2)],\n)\ndef test_reference_file(filename, ref_sum, ref_num_cells):\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    filename = os.path.join(this_dir, \"meshes\", \"wkt\", filename)\n    mesh = meshio.read(filename)\n    tol = 1.0e-5\n    s = numpy.sum(mesh.points)\n    assert abs(s - ref_sum) < tol * abs(ref_sum)\n    assert mesh.cells[0].type == \"triangle\"\n    assert len(mesh.cells[0].data) == ref_num_cells\n<|endoftext|>",
    "repo": "nate-sime/meshio"
  },
  {
    "prompt": "fn get_os_type() -> str\n\n\"\"\"Accept an argument like {'py37': 1, 'py38': 2},\nor {\"windows\": 1, \"linux\": 2, \"mac\": 3}\n\nUsed for version-dependent and OS tests.\"\"\"\nfn get_value(value_dict: dict[(str, any)])\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from __future__ import annotations\n\nimport json\nimport msgpack\nimport os\nimport pytest\nimport responses\n\nfrom cyberbrain import _TracerFSM, trace\nfrom utils import python_version, get_os_type\n\n\ndef pytest_addoption(parser):\n    parser.addoption(\"--debug_mode\", action=\"store_true\", default=False)\n\n\n@pytest.fixture(scope=\"function\", name=\"tracer\")\ndef fixture_tracer(request):\n    trace.debug_mode = request.config.getoption(\"--debug_mode\")\n\n    yield trace\n\n    # Do cleanup because the trace decorator is reused across tests.\n    trace.raw_frame = None\n    trace.decorated_function_code_id = None\n    trace.frame_logger = None\n    trace.tracer_state = _TracerFSM.INITIAL\n\n\n@pytest.fixture(scope=\"function\", name=\"trace\")\ndef fixture_trace(request):\n    trace.debug_mode = request.config.getoption(\"--debug_mode\")\n\n    yield trace\n\n    # Do cleanup because the trace decorator is reused across tests.\n    trace.raw_frame = None\n    trace.decorated_function_code_id = None\n    trace.frame_logger = None\n    trace.tracer_state = _TracerFSM.INITIAL\n\n\n@pytest.fixture\ndef mocked_responses(request):\n    with responses.RequestsMock() as resp:\n        resp.add(\n            responses.POST,\n            f\"http://localhost:{trace.rpc_client.port}/frame\",\n            status=200,\n            content_type=\"application/octet-stream\",\n        )\n        yield resp\n\n        frame_data = msgpack.unpackb(resp.calls[0].request.body)\n        frame_name = frame_data[\"metadata\"][\"frame_name\"]\n\n        # Generates golden data.\n        golden_filepath = f\"test/data/{python_version}/{frame_name}.json\"\n        directory = os.path.dirname(golden_filepath)\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n\n        if not os.path.exists(golden_filepath):\n            with open(golden_filepath, \"wt\") as f:\n                json.dump(frame_data, f, ensure_ascii=False, indent=4)\n            return\n\n        # Assuming run in root directory.\n        with open(golden_filepath, \"rt\") as f:\n            golden_frame_data = json.loads(f.read())\n\n        # Don't check request body on Windows because it has a different format.\n        if get_os_type() == \"windows\" and frame_name in {\"test_numpy\", \"test_pandas\"}:\n            return\n\n        assert frame_data == golden_frame_data, json.dumps(frame_data, indent=4)\n<|endoftext|>",
    "repo": "victorjzsun/Cyberbrain"
  },
  {
    "prompt": "class IncorrectPart(Exception):\n\n\nclass MissingConfigParser(Exception):\n\n\nclass ErrorConfigParsing(Exception):\n\n\nclass MissingVcsModule(Exception):\n\n\nclass VersionNotFound(Exception):\n\n\nclass VCSException(Exception):\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom bamp.vcs import _get_vcs_module, make_tag_name, create_tag\nfrom bamp.exc import MissingVcsModule\n\n\ndef test_get_vcs_module_git():\n    vcs = _get_vcs_module('git')\n    assert vcs.__name__ == 'bamp.vcs.git'\n\n\ndef test_get_vcs_module_failed():\n    with pytest.raises(MissingVcsModule):\n        _get_vcs_module('Albatros')\n\n\ndef test_make_tag_name_no_substitution():\n    tag_message = 'it\\'s a tag'\n    assert make_tag_name(tag_message, '0.9.0') == tag_message\n\n\ndef test_make_tag_name_with_substitution():\n    tag_message = '{new_version}-tag'\n    assert make_tag_name(tag_message, '1.1.1') == '1.1.1-tag'\n<|endoftext|>",
    "repo": "flyte/bamp"
  },
  {
    "prompt": "fn cli(ctx)\n\nfn unity(ctx)\n\nfn unity_list()\n\nfn unity_acquire(model: Union[(str, WeatherModel)], timestamp: Union[(str, datetime)], target: Union[(str, Path)])\n\nfn dwd(ctx)\n\nfn smith(ctx)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"\nInvoke the tests in this file using::\n\n    pytest -vvv -m \"bbox and command\"\n\"\"\"\nimport json\n\nimport pytest\nfrom click.testing import CliRunner\n\nfrom gribmagic.commands import cli\n\n\n@pytest.mark.bbox\n@pytest.mark.command\ndef test_bbox_basic_success(gm_data_path, capsys):\n    runner = CliRunner()\n\n    # Acquire data.\n    result = runner.invoke(\n        cli,\n        [\n            \"dwd\",\n            \"acquire\",\n            \"--recipe=tests/dwd/recipe_icon_d2_vmax10m.py\",\n        ],\n    )\n    assert result.exit_code == 0, result.output\n\n    # Run bbox on data.\n    result = runner.invoke(\n        cli,\n        [\n            \"smith\",\n            \"bbox\",\n            \"--country=AT\",\n            f\"{gm_data_path}/icon-d2/**/*regular-lat-lon*.grib2\",\n        ],\n    )\n    assert result.exit_code == 0, result.output\n\n    # Check results.\n    report = json.loads(result.output)\n    assert len(report) == 3\n\n    first_item = report[0]\n\n    assert \"input\" in first_item\n    assert \"output\" in first_item\n    assert \"plot\" in first_item\n\n    assert first_item[\"input\"].endswith(\".grib2\")\n    assert first_item[\"output\"].endswith(\".grib2\")\n    assert \"bbox_\" in first_item[\"output\"]\n    assert first_item[\"plot\"] is None\n\n\n@pytest.mark.bbox\n@pytest.mark.command\ndef test_bbox_basic_no_area_failure(gm_data_path, capsys):\n    runner = CliRunner()\n\n    # Run bbox on data.\n    result = runner.invoke(\n        cli,\n        [\n            \"smith\",\n            \"bbox\",\n            f\"{gm_data_path}/icon-d2/**/*regular-lat-lon*.grib2\",\n        ],\n    )\n    assert result.exit_code == 2, result.output\n    assert (\n        \"Error: Missing one of the required mutually exclusive options from 'area' option group\"\n        in result.output\n    )\n\n\n@pytest.mark.bbox\n@pytest.mark.command\ndef test_bbox_basic_input_files_failure(gm_data_path, capsys):\n    runner = CliRunner()\n\n    # Run bbox on data.\n    result = runner.invoke(\n        cli,\n        [\n            \"smith\",\n            \"bbox\",\n            \"--country=AT\",\n        ],\n    )\n    assert result.exit_code == 2, result.output\n    assert \"Error: Missing argument 'INPUT...'\" in result.output\n\n\n@pytest.mark.bbox\n@pytest.mark.command\ndef test_bbox_plot_success(gm_data_path, capsys):\n    runner = CliRunner()\n\n    # Acquire data.\n    result = runner.invoke(\n        cli,\n        [\n            \"dwd\",\n            \"acquire\",\n            \"--recipe=tests/dwd/recipe_icon_d2_vmax10m.py\",\n        ],\n    )\n    assert result.exit_code == 0, result.output\n\n    # Run bbox on data.\n    result = runner.invoke(\n        cli,\n        [\n            \"smith\",\n            \"bbox\",\n            \"--country=AT\",\n            f\"{gm_data_path}/icon-d2/**/*regular-lat-lon*.grib2\",\n            \"--plot\",\n        ],\n    )\n    assert result.exit_code == 0, result.output\n\n    # Check results.\n    report = json.loads(result.output)\n    assert len(report) == 3\n\n    first_item = report[0]\n\n    assert \"input\" in first_item\n    assert \"output\" in first_item\n    assert \"plot\" in first_item\n\n    assert first_item[\"input\"].endswith(\".grib2\")\n    assert \"bbox_\" in first_item[\"output\"]\n    assert first_item[\"plot\"].endswith(\".png\")\n<|endoftext|>",
    "repo": "earthobservations/gribmagic"
  },
  {
    "prompt": "fn force_bytes(string, encoding, errors)\n\n\"\"\"\"\"\"\nfn download_and_store(url, path)\n\n\"\"\"Expand the ZIP file at the given path to the cache directory.\n    \"\"\"\nfn expand(self, local)\n\nfn base_settings()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# -*- coding: utf-8 -*-\n\"\"\"\nProfile: http://hl7.org/fhir/StructureDefinition/ClinicalImpression\nRelease: R4\nVersion: 4.0.1\nBuild ID: 9346c8cc45\nLast updated: 2019-11-01T09:29:23.356+11:00\n\"\"\"\n\nimport io\nimport json\nimport os\nimport unittest\n\nimport pytest\n\nfrom .. import clinicalimpression\nfrom ..fhirdate import FHIRDate\nfrom .fixtures import force_bytes\n\n\n@pytest.mark.usefixtures(\"base_settings\")\nclass ClinicalImpressionTests(unittest.TestCase):\n    def instantiate_from(self, filename):\n        datadir = os.environ.get(\"FHIR_UNITTEST_DATADIR\") or \"\"\n        with io.open(os.path.join(datadir, filename), \"r\", encoding=\"utf-8\") as handle:\n            js = json.load(handle)\n            self.assertEqual(\"ClinicalImpression\", js[\"resourceType\"])\n        return clinicalimpression.ClinicalImpression(js)\n\n    def testClinicalImpression1(self):\n        inst = self.instantiate_from(\"clinicalimpression-example.json\")\n        self.assertIsNotNone(\n            inst, \"Must have instantiated a ClinicalImpression instance\"\n        )\n        self.implClinicalImpression1(inst)\n\n        js = inst.as_json()\n        self.assertEqual(\"ClinicalImpression\", js[\"resourceType\"])\n        inst2 = clinicalimpression.ClinicalImpression(js)\n        self.implClinicalImpression1(inst2)\n\n    def implClinicalImpression1(self, inst):\n        self.assertEqual(inst.date.date, FHIRDate(\"2014-12-06T22:33:00+11:00\").date)\n        self.assertEqual(inst.date.as_json(), \"2014-12-06T22:33:00+11:00\")\n        self.assertEqual(\n            force_bytes(inst.description),\n            force_bytes(\n                \"This 26 yo male patient is brought into ER by ambulance after being involved in a motor vehicle accident\"\n            ),\n        )\n        self.assertEqual(\n            inst.effectivePeriod.end.date, FHIRDate(\"2014-12-06T22:33:00+11:00\").date\n        )\n        self.assertEqual(\n            inst.effectivePeriod.end.as_json(), \"2014-12-06T22:33:00+11:00\"\n        )\n        self.assertEqual(\n            inst.effectivePeriod.start.date, FHIRDate(\"2014-12-06T20:00:00+11:00\").date\n        )\n        self.assertEqual(\n            inst.effectivePeriod.start.as_json(), \"2014-12-06T20:00:00+11:00\"\n        )\n        self.assertEqual(\n            force_bytes(inst.finding[0].itemCodeableConcept.coding[0].code),\n            force_bytes(\"850.0\"),\n        )\n        self.assertEqual(\n            force_bytes(inst.finding[0].itemCodeableConcept.coding[0].system),\n            force_bytes(\"http://hl7.org/fhir/sid/icd-9\"),\n        )\n        self.assertEqual(force_bytes(inst.id), force_bytes(\"example\"))\n        self.assertEqual(force_bytes(inst.identifier[0].value), force_bytes(\"12345\"))\n        self.assertEqual(\n            force_bytes(inst.investigation[0].code.text),\n            force_bytes(\"Initial Examination\"),\n        )\n        self.assertEqual(force_bytes(inst.meta.tag[0].code), force_bytes(\"HTEST\"))\n        self.assertEqual(\n            force_bytes(inst.meta.tag[0].display), force_bytes(\"test health data\")\n        )\n        self.assertEqual(\n            force_bytes(inst.meta.tag[0].system),\n            force_bytes(\"http://terminology.hl7.org/CodeSystem/v3-ActReason\"),\n        )\n        self.assertEqual(force_bytes(inst.status), force_bytes(\"completed\"))\n        self.assertEqual(\n            force_bytes(inst.summary),\n            force_bytes(\n                \"provisional diagnoses of laceration of head and traumatic brain injury (TBI)\"\n            ),\n        )\n        self.assertEqual(force_bytes(inst.text.status), force_bytes(\"generated\"))\n<|endoftext|>",
    "repo": "mmabey/fhir.resources"
  },
  {
    "prompt": "\"\"\"Converts a month to an integer.\"\"\"\nfn month_to_int(m)\n\n\"\"\"Converts a month to an int form, str type, with a leading zero\"\"\"\nfn month_to_str_int(m)\n\n\"\"\"Converts a day to an int form, str type, with a leading zero\"\"\"\nfn day_to_str_int(d)\n\n\"\"\"Converts years / months / days to a float, eg 2015.0818 is August\n18th 2015. \"\"\"\nfn date_to_float(y, m, d)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom regolith.dates import (month_to_str_int, day_to_str_int)\n\n@pytest.mark.parametrize(\n    \"input,expected\",\n    [\n        (1, \"01\"),\n        (10, \"10\"),\n        (\"Oct\", \"10\"),\n        (\"Jan\", \"01\"),\n    ],\n)\ndef test_month_to_str(input, expected):\n    assert month_to_str_int(input) == expected\n\n@pytest.mark.parametrize(\n    \"input,expected\",\n    [\n        (1, \"01\"),\n        (10, \"10\"),\n    ],\n)\ndef test_day_to_str(input, expected):\n    assert day_to_str_int(input) == expected\n\n<|endoftext|>",
    "repo": "dragonyanglong/regolith"
  },
  {
    "prompt": "\"\"\"Define a smart locator property.\n\nThis behaves pretty much like a normal read-only property and the\ndecorated function should return a dict containing the necessary\ndata to build a URL for the object.\n\nThis decorator should usually be applied to a method named\n``locator`` as this name is required for `get_locator` to find it\nautomatically when just passing the object.\n\nIf you need more than one locator, you can define it like this::\n\n    @locator_property\n    def locator(self):\n        return {...}\n\n    @locator.other\n    def locator(self):\n        return {...}\n\nThe ``other`` locator can then be accessed by passing\n``obj.locator.other`` to the code expecting an object with\na locator.\"\"\"\nclass locator_property:\n\tfn __init__(self, fn)\n\tfn __get__(self, obj, objtype)\n\tfn __getattr__(self, name)\n\tfn __repr__(self)\n\n\n\"\"\"Retrieve the locator data from an object.\n\nThe object may be a dictionary (in case a locator is passed) or\nan object with a ``locator`` property.\"\"\"\nfn get_locator(obj)\n\nclass _LocatorDict(UserDict):\n\tfn __init__(self, locator, obj)\n\tfn data(self)\n\tfn __getattr__(self, name)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# This file is part of Indico.\n# Copyright (C) 2002 - 2021 CERN\n#\n# Indico is free software; you can redistribute it and/or\n# modify it under the terms of the MIT License; see the\n# LICENSE file for more details.\n\nimport pytest\n\nfrom indico.util.locators import get_locator, locator_property\n\n\ndef test_get_locator_none():\n    with pytest.raises(TypeError):\n        get_locator(object())\n    with pytest.raises(TypeError):\n        get_locator(None)\n\n\ndef test_get_locator():\n    class _Prop:\n        @property\n        def locator(self):\n            return {'foo': 'bar'}\n\n    class _Locator:\n        @locator_property\n        def locator(self):\n            return {'foo': 'bar'}\n\n    assert get_locator(_Prop()) == {'foo': 'bar'}\n    assert get_locator(_Locator()) == {'foo': 'bar'}\n    assert get_locator({'foo': 'bar'}) == {'foo': 'bar'}\n\n\ndef test_locator_property():\n    class _Test:\n        @locator_property\n        def locator(self):\n            return {'foo': 'bar'}\n\n        @locator.fancy\n        def locator(self):\n            return dict(self.locator, awesome='magic')\n\n    assert isinstance(_Test.locator, locator_property)\n    t = _Test()\n    assert get_locator(t) == {'foo': 'bar'}\n    assert get_locator(t.locator) == {'foo': 'bar'}\n    assert get_locator(t.locator.fancy) == {'foo': 'bar', 'awesome': 'magic'}\n    with pytest.raises(AttributeError):\n        t.locator.invalid\n\n\ndef test_locator_property_lazy():\n    class _Test:\n        @locator_property\n        def locator(self):\n            accessed.append('locator')\n            return {'foo': 'bar'}\n\n        @locator.fancy\n        def locator(self):\n            accessed.append('fancy')\n            return {'awesome': 'magic'}\n\n    accessed = []\n    # creating the object or accessing the locator itself doesn't\n    # call the underlying function\n    t = _Test()\n    t.locator\n    assert not accessed\n\n    # accessing the locator in any way that needs to access its data\n    # obviously triggers a call to the accessor\n    assert t.locator == {'foo': 'bar'}\n    assert accessed == ['locator']\n\n    # accessing a special locator only triggers that locator's accessor\n    # but not the one of the default locator\n    accessed = []\n    t.locator.fancy\n    assert accessed == ['fancy']\n\n\ndef test_locator_property_nested():\n    with pytest.raises(AttributeError):\n        class _Test:\n            @locator_property\n            def locator(self):\n                pass\n\n            @locator.x\n            def locator(self):\n                pass\n\n            @locator.x.y\n            def locator(self):\n                pass\n<|endoftext|>",
    "repo": "errikos/indico"
  },
  {
    "prompt": "\"\"\"setup any state specific to the execution of the given module.\"\"\"\nfn setup_module(module)\n\n\"\"\"teardown any state that was previously setup with a setup_module\nmethod.\"\"\"\nfn teardown_module(module)\n\n\"\"\"setup any state specific to the execution of the given function.\"\"\"\nfn setup_function(function)\n\n\"\"\"teardown any state that was previously setup with a setup_function\nmethod.\"\"\"\nfn teardown_function(function)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import os\nimport os.path\nimport time\nimport pytest\nimport fs\n\nfrom .setup import *\n\ndef test_touch_on_existing_file():\n\n    before_time = time.ctime(os.path.getmtime(TEST_FILE))\n\n    fs.touch(TEST_FILE)\n    after_time = time.ctime(os.path.getmtime(TEST_FILE))\n\n    assert after_time >= before_time\n\ndef test_touch_on_new_file():\n    \n    new_file = os.path.join(TEST_DIR, \"new_file.txt\")\n\n    if (os.path.exists(new_file)):\n        raise ValueError(\"File new_file.txt already exists!\")\n\n    fs.touch(new_file)\n\n    assert os.path.exists(new_file) is True\n\n@pytest.mark.skipif(os.name == \"nt\", reason=\"does not work on windows\")\ndef test_touch_on_directory():\n\n    before_time = time.ctime(os.path.getmtime(TEST_DIR))\n\n    fs.touch(TEST_DIR)\n    after_time = time.ctime(os.path.getmtime(TEST_DIR))\n\n    assert after_time >= before_time\n<|endoftext|>",
    "repo": "sturmianseq/python-fs"
  },
  {
    "prompt": "fn parse_args() -> argparse.Namespace\n\nfn get_alphaed_image(circle_image: numpy.ndarray, working_image: numpy.ndarray) -> numpy.ndarray\n\nfn _radius(max_circle_radius: float) -> int\n\nfn _center(height: int, width: int) -> Tuple[(int, int)]\n\nfn _color() -> Tuple[(int, int, int)]\n\nfn draw_circle_on_image(image: numpy.ndarray, max_circle_radius: float) -> numpy.ndarray\n\nfn annealing_draw(original_image: numpy.ndarray, current_image: numpy.ndarray, max_circle_radius: float, round_per_generation: int, cool_down_count: int) -> Tuple[(numpy.ndarray, float, int)]\n\nfn download_original_image(raw_image_url: str, filename: str) -> None\n\nfn resize_original_image(image: numpy.ndarray, max_longside_length: float) -> numpy.ndarray\n\nfn draw_current_image(generation: int, current_image: numpy.ndarray, sub_generation: Optional[int]) -> Path\n\nfn draw_pointillism(args: argparse.Namespace) -> None\n\nfn generate_gif_annimation() -> None\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from unittest.mock import call\n\nimport cv2\nimport numpy\nimport pytest\n\nfrom hill_climbing_study.generate import (\n    _center,\n    _color,\n    _radius,\n    draw_circle_on_image,\n    resize_original_image,\n)\n\n\n@pytest.mark.parametrize(\"max_circle_radius,expect\", [(0.999, 1), (1.000, 2)])\ndef test__radius(mocker, max_circle_radius, expect):\n    mock = mocker.patch(\"numpy.random.randint\", return_value=int(max_circle_radius + 1))\n    assert _radius(max_circle_radius) == expect\n    mock.has_calls([call(1, 1), call(1, 2)], any_order=False)\n\n\ndef test__center(mocker):\n    def _side_effect(low, high):\n        return high\n\n    mock = mocker.patch(\"numpy.random.randint\", side_effect=_side_effect)\n    assert _center(300, 400) == (400, 300)\n    mock.has_calls([call(400), call(300)], any_order=False)\n    mock.reset_mock()\n\n\ndef test__color(mocker):\n    mock = mocker.patch(\"numpy.random.randint\", return_value=128)\n    assert _color() == (128, 128, 128)\n    mock.has_calls([call(128), call(128), call(128)], any_order=False)\n\n\ndef test_draw_circle_on_image(mocker):\n    _radius_mock = mocker.patch(\"hill_climbing_study.generate._radius\", return_value=110)\n    _center_mock = mocker.patch(\"hill_climbing_study.generate._center\", return_value=(200, 150))\n    _color_mock = mocker.patch(\"hill_climbing_study.generate._color\", return_value=(63, 127, 255))\n    image = numpy.full((300, 400), 127, dtype=numpy.uint8)\n    expect_image = cv2.circle(image.copy(), (200, 150), 110, (63, 127, 255), -1)\n    assert numpy.array_equal(draw_circle_on_image(image, 110), expect_image)\n    _radius_mock.assert_called_once()\n    _center_mock.assert_called_once()\n    _color_mock.assert_called_once()\n\n\ndef test_resize_original_image():\n    original_image = cv2.imread(\"tests/data/test_image.jpeg\")\n    resized = resize_original_image(original_image)\n    assert cv2.absdiff(resized, cv2.imread(\"tests/data/test_resized.jpeg\")).sum() < 1250000\n<|endoftext|>",
    "repo": "hirokinko/hill_climbing_study"
  },
  {
    "prompt": "\"\"\"Derive a permuted hetnet from an input hetnet. This method applies the\nXSwap algorithm separately for each metaedge. Hence, node degree is\npreserved for each type of edge. However, edges are randomized / shuffled.\n\nUsers are recommended to interrogate the reported statistics to ensure that\nedges appear to be sufficiently randomized. Primarily, the number of edges\nof a given metaedge that remain unchanged from the original hetnet should\nhave reached an assymptote. If the number of unchanged edges has not yet\nstabalized, further randomization is possible with this approach.\n\nParameters\n----------\ngraph : hetnetpy.hetnet.Graph\n    Input hetnet to create a permuted derivative from\nmultiplier : int or float\n    This is multiplied by the number of edges for each metaedge to\n    determine the number of swaps to attempt.\nseed : int\n    Seed to initialize Python random number generator. When creating many\n    permuted hetnets, it's recommended to increment this number, such that\n    each round of permutation shuffles edges in a different order.\nmetaedge_to_excluded : dict (metaedge -> set)\n    Edges to exclude. This argument has not been extensively used in\n    practice.\nlog : bool\n    Whether to log diagnostic INFO via python's logging module.\n\nReturns\n-------\npermuted_graph : hetnetpy.hetnet.Graph\n    A permuted hetnet derived from the input graph.\nstats : list of dicts\n    A list where each item is a dictionary of permutation statistics at a\n    checkpoint for a specific metaedge. These statistics allow tracking the\n    progress of the permutation as the number of attempted swaps increases.\"\"\"\nfn permute_graph(graph: hetnetpy.hetnet.Graph, multiplier: int or float, seed: int, metaedge_to_excluded: dict (metaedge -> set), log: bool) -> list of dicts\n\n\"\"\"Permute edges (of a single type) in a graph according to the XSwap function\ndescribed in https://doi.org/f3mn58. This method selects two edges and\nattempts to swap their endpoints. If the swap would result in a valid edge,\nthe swap proceeds. Otherwise, the swap is skipped. The end result is that\nnode degree is preserved, but edges are shuffled, thereby losing their\noriginal meaning.\n\nParameters\n----------\npair_list : list of tuples\n    List of edges to permute. Each edge is represented as a (source,\n    target) tuple. source and target represent nodes and can be any Python\n    objects that define __eq__. In other words, this function does not\n    assume any specific format for nodes. If the edges are from a bipartite\n    or directed graph, then all tuples must have the same alignment. For\n    example, if the edges represent the bipartite Compound-binds-Gene\n    relationship, all tuples should be of the form (compound, gene) and not\n    intermixed with (gene, compound) tuples. The only instance where order\n    of the source and target is not important is for an undirected edge\n    type where the source and target nodes are of the same type, such as\n    Gene-interacts-Gene.\ndirected : bool\n    Whether the edge should be considered directed. If False, a swap that\n    creates an a-b edge will be invalid if a b-a edge already exists.\nmultiplier : int or float\n    This is multiplied by the number of edges in pair_list to determine the\n    number of swaps to attempt.\nexcluded_pair_set : set of tuples:\n    Set of possible edges to forbid. If a swap would create an edge in this\n    set, it would be considered invalid and hence skipped.\nseed : int\n    Seed to initialize Python random number generator.\nlog : bool\n    Whether to log diagnostic INFO via python's logging module.\ninplace : bool\n    Whether to modify the edge list in place.\n\nReturns\n-------\npair_list : list of tuples\n    The permuted edges, derived from the input pair_list.\nstats : list of dicts\n    A list where each item is a dictionary of permutation statistics at a\n    checkpoint. Statistics are collected at 10 checkpoints, spaced evenly\n    by the number of attempts.\"\"\"\nfn permute_pair_list(pair_list: list of tuples, directed: bool, multiplier: int or float, excluded_pair_set: set of tuples:, seed: int, log: bool, inplace: bool) -> list of dicts\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nimport hetnetpy.permute\n\n\n@pytest.mark.parametrize(\n    \"edges,inplace\",\n    [\n        ([(0, 0), (1, 1), (1, 2), (2, 3)], True),\n        ([(0, 0), (1, 1), (1, 2), (2, 3)], False),\n    ],\n)\ndef test_permute_inplace(edges, inplace):\n    old_edges = edges.copy()\n    new_edges, stats = hetnetpy.permute.permute_pair_list(edges, inplace=inplace)\n    assert old_edges != new_edges\n\n    if inplace:\n        assert edges == new_edges\n    else:\n        assert edges != new_edges\n<|endoftext|>",
    "repo": "dhimmel/hetnetpy"
  },
  {
    "prompt": "fn _fix_subparsers(subparsers)\n\nclass DeepBGCParser(argparse.ArgumentParser):\n\tfn parse_args(self, args, namespace)\n\tfn error(self, message)\n\n\nfn run(argv)\n\nfn main(argv)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from deepbgc.main import run\nimport pytest\n\n\ndef test_unit_main_help():\n    with pytest.raises(SystemExit) as excinfo:\n        run(['--help'])\n    assert excinfo.value.code == 0\n\n\ndef test_unit_pipeline_help():\n    with pytest.raises(SystemExit) as excinfo:\n        run(['pipeline', '--help'])\n    assert excinfo.value.code == 0\n\n\ndef test_unit_prepare_help():\n    with pytest.raises(SystemExit) as excinfo:\n        run(['prepare', '--help'])\n    assert excinfo.value.code == 0\n\n\ndef test_unit_train_help():\n    with pytest.raises(SystemExit) as excinfo:\n        run(['train', '--help'])\n    assert excinfo.value.code == 0\n\n\ndef test_unit_main_invalid_command():\n    with pytest.raises(SystemExit) as excinfo:\n        run(['invalid'])\n    assert excinfo.value.code == 2<|endoftext|>",
    "repo": "gkapatai/deepbgc"
  },
  {
    "prompt": "fn cleanup_third_party_modules_output()\n\nfn get_output_filenames()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright 1999-2021 Alibaba Group Holding Ltd.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nfrom ....tests.core import require_ray\nfrom ....utils import lazy_import\nfrom ..ray import new_cluster\nfrom ..tests import test_local\nfrom .modules.utils import (  # noqa: F401; pylint: disable=unused-variable\n    cleanup_third_party_modules_output,\n    get_output_filenames,\n)\n\nray = lazy_import(\"ray\")\n\n\n@pytest.fixture\nasync def speculative_cluster():\n    client = await new_cluster(\n        \"test_cluster\",\n        worker_num=5,\n        worker_cpu=2,\n        worker_mem=512 * 1024**2,\n        supervisor_mem=100 * 1024**2,\n        config={\n            # coloring based fusion will make subtask too heterogeneous such that the speculative scheduler can't\n            # get enough homogeneous subtasks to calculate statistics\n            \"task\": {\"default_config\": {\"fuse_enabled\": False}},\n            \"scheduling\": {\n                \"speculation\": {\n                    \"enabled\": True,\n                    \"interval\": 0.5,\n                    \"threshold\": 0.2,\n                    \"min_task_runtime\": 2,\n                    \"multiplier\": 1.5,\n                },\n                # used to kill hanged subtask to release slot.\n                \"subtask_cancel_timeout\": 0.1,\n            },\n        },\n    )\n    async with client:\n        yield client\n\n\n@pytest.mark.parametrize(\"ray_large_cluster\", [{\"num_nodes\": 2}], indirect=True)\n@pytest.mark.timeout(timeout=1000)\n@require_ray\n@pytest.mark.asyncio\nasync def test_task_speculation_execution(ray_large_cluster, speculative_cluster):\n    await test_local.test_task_speculation_execution(speculative_cluster)\n<|endoftext|>",
    "repo": "yuyiming/mars"
  },
  {
    "prompt": "\"\"\"Base class for profile event handler hparams.\"\"\"\nclass ProfilerEventHandlerHparams(hp.Hparams, abc.ABC):\n\t\"\"\"Constructs and returns an instance of the :class:`ProfilerEventHandler`.\n\t\n\tReturns:\n\t    ProfilerEventHandler: The event handler.\"\"\"\n\tfn initialize_object(self) -> ProfilerEventHandler\n\n\n\"\"\":class:`.JSONTraceHandler` hyperparameters.\n\nSee :class:`.JSONTraceHandler` for documentation.\n\nExample usage with :class:`.TrainerHparams`\\:\n\n.. code-block:: yaml\n\n    prof_event_handlers:\n        - json:\n            flush_every_n_batches: 100\n            buffering: -1\n            output_directory: profiler_traces\"\"\"\nclass JSONTraceHandlerHparams(ProfilerEventHandlerHparams):\n\tflush_every_n_batches: int = hp.optional('Interval at which to flush the logfile.', default=100)\n\tbuffering: int = hp.optional('Buffering parameter passed to :meth:`open` when opening the logfile.', default=-1)\n\toutput_directory: str = hp.optional('Directory, relative to the run directory, to store traces.', default='composer_profiler')\n\tfn initialize_object(self) -> JSONTraceHandler\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "# Copyright 2021 MosaicML. All Rights Reserved.\n\nimport json\nimport os\nimport pathlib\n\nimport pytest\n\nfrom composer.profiler.profiler_hparams import CyclicProfilerScheduleHparams, JSONTraceHparams\nfrom composer.trainer import TrainerHparams\n\n\n@pytest.mark.timeout(10)\ndef test_json_trace_profiler_handler(composer_trainer_hparams: TrainerHparams, tmpdir: pathlib.Path):\n    profiler_file = os.path.join(tmpdir, 'trace.json')\n    json_trace_handler_params = JSONTraceHparams(folder=str(tmpdir), merged_trace_filename='trace.json')\n\n    composer_trainer_hparams.prof_trace_handlers = [json_trace_handler_params]\n    composer_trainer_hparams.prof_schedule = CyclicProfilerScheduleHparams(\n        wait=0,\n        warmup=0,\n        active=1000,\n        repeat=0,\n    )\n    composer_trainer_hparams.max_duration = \"2ep\"\n    composer_trainer_hparams.sys_prof_cpu = False\n    composer_trainer_hparams.sys_prof_net = False\n    composer_trainer_hparams.sys_prof_disk = False\n    composer_trainer_hparams.sys_prof_memory = False\n\n    trainer = composer_trainer_hparams.initialize_object()\n    trainer.fit()\n\n    with open(profiler_file, \"r\") as f:\n        trace_json = json.load(f)\n        has_epoch_start_event = False\n        has_epoch_end_event = False\n        for event in trace_json:\n            if event[\"name\"] == \"event/epoch\" and event[\"ph\"] == \"B\":\n                has_epoch_start_event = True\n            if event[\"name\"] == \"event/epoch\" and event[\"ph\"] == \"E\":\n                has_epoch_end_event = True\n        assert has_epoch_start_event\n        assert has_epoch_end_event\n<|endoftext|>",
    "repo": "IanWorley/composer"
  },
  {
    "prompt": "fn max_atoms()\n\nfn num_data()\n\nfn property_spec()\n\nfn empty_asedata(tmpdir, max_atoms, property_spec)\n\nfn example_data(max_atoms, num_data)\n\nfn example_asedata(tmpdir, max_atoms, property_spec, example_data)\n\nfn test_add_and_read(empty_asedata, example_data)\n\nfn test_empty_subset_of_subset(empty_asedata, example_data)\n\nfn partition_names(request)\n\nfn test_merging(tmpdir, example_asedata, partition_names)\n\nfn batch_size(request)\n\nfn test_loader(example_asedata, batch_size)\n\nfn test_dataset(qm9_dbpath, qm9_avlailable_properties)\n\nfn test_extension_check()\n\nfn h2o()\n\nfn o2()\n\nfn test_get_center(h2o, o2)\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nimport schnetpack.data\nimport schnetpack.representation.hdnn as rep\nfrom .test_data import batch_size\n\n\n@pytest.mark.usefixtures(\"example_asedata\", \"batch_size\")\ndef test_triples_exception(example_asedata, batch_size):\n    loader = schnetpack.data.AtomsLoader(example_asedata, batch_size)\n\n    reps = rep.BehlerSFBlock(n_radial=22, n_angular=5, elements=frozenset(range(100)))\n\n    for batch in loader:\n        with pytest.raises(rep.HDNNException):\n            reps(batch)\n        break\n<|endoftext|>",
    "repo": "otayfuroglu/schnetpack"
  },
  {
    "prompt": "fn compare_arrays(arr, res, precision)\n\nfn check_encode_decode(arr, codec, precision)\n\nfn check_encode_decode_partial(arr, codec, precision)\n\nfn assert_array_items_equal(res, arr)\n\nfn check_encode_decode_array(arr, codec)\n\nfn check_config(codec)\n\nfn check_repr(stmt)\n\nfn check_backwards_compatibility(codec_id, arrays, codecs, precision, prefix)\n\nfn check_err_decode_object_buffer(compressor)\n\nfn check_err_encode_object_buffer(compressor)\n\nfn check_max_buffer_size(codec)\n\n\"\"\"Codec providing gzip compression using zlib via the Python standard library.\n\nParameters\n----------\nlevel : int\n    Compression level.\"\"\"\nclass GZip(Codec):\n\tcodec_id = 'gzip'\n\tfn __init__(self, level)\n\tfn encode(self, buf)\n\tfn decode(self, buf, out)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import itertools\n\n\nimport numpy as np\nimport pytest\n\n\nfrom numcodecs.gzip import GZip\nfrom numcodecs.tests.common import (check_encode_decode, check_config, check_repr,\n                                    check_backwards_compatibility,\n                                    check_err_decode_object_buffer,\n                                    check_err_encode_object_buffer)\n\n\ncodecs = [\n    GZip(),\n    GZip(level=-1),\n    GZip(level=0),\n    GZip(level=1),\n    GZip(level=5),\n    GZip(level=9),\n]\n\n\n# mix of dtypes: integer, float, bool, string\n# mix of shapes: 1D, 2D, 3D\n# mix of orders: C, F\narrays = [\n    np.arange(1000, dtype='i4'),\n    np.linspace(1000, 1001, 1000, dtype='f8'),\n    np.random.normal(loc=1000, scale=1, size=(100, 10)),\n    np.random.randint(0, 2, size=1000, dtype=bool).reshape(100, 10, order='F'),\n    np.random.choice([b'a', b'bb', b'ccc'], size=1000).reshape(10, 10, 10),\n    np.random.randint(0, 2**60, size=1000, dtype='u8').view('M8[ns]'),\n    np.random.randint(0, 2**60, size=1000, dtype='u8').view('m8[ns]'),\n    np.random.randint(0, 2**25, size=1000, dtype='u8').view('M8[m]'),\n    np.random.randint(0, 2**25, size=1000, dtype='u8').view('m8[m]'),\n    np.random.randint(-2**63, -2**63 + 20, size=1000, dtype='i8').view('M8[ns]'),\n    np.random.randint(-2**63, -2**63 + 20, size=1000, dtype='i8').view('m8[ns]'),\n    np.random.randint(-2**63, -2**63 + 20, size=1000, dtype='i8').view('M8[m]'),\n    np.random.randint(-2**63, -2**63 + 20, size=1000, dtype='i8').view('m8[m]'),\n]\n\n\ndef test_encode_decode():\n    for arr, codec in itertools.product(arrays, codecs):\n        check_encode_decode(arr, codec)\n\n\ndef test_config():\n    codec = GZip(level=3)\n    check_config(codec)\n\n\ndef test_repr():\n    check_repr(\"GZip(level=3)\")\n\n\ndef test_eq():\n    assert GZip() == GZip()\n    assert not GZip() != GZip()\n    assert GZip(1) == GZip(1)\n    assert GZip(1) != GZip(9)\n    assert GZip() != 'foo'\n    assert 'foo' != GZip()\n    assert not GZip() == 'foo'\n\n\ndef test_backwards_compatibility():\n    check_backwards_compatibility(GZip.codec_id, arrays, codecs)\n\n\ndef test_err_decode_object_buffer():\n    check_err_decode_object_buffer(GZip())\n\n\ndef test_err_encode_object_buffer():\n    check_err_encode_object_buffer(GZip())\n\n\ndef test_err_encode_list():\n    data = ['foo', 'bar', 'baz']\n    for codec in codecs:\n        with pytest.raises(TypeError):\n            codec.encode(data)\n\n\ndef test_err_encode_non_contiguous():\n    # non-contiguous memory\n    arr = np.arange(1000, dtype='i4')[::2]\n    for codec in codecs:\n        with pytest.raises(ValueError):\n            codec.encode(arr)\n\n\ndef test_err_out_too_small():\n    arr = np.arange(10, dtype='i4')\n    out = np.empty_like(arr)[:-1]\n    for codec in codecs:\n        with pytest.raises(ValueError):\n            codec.decode(codec.encode(arr), out)\n\n\ndef test_out_too_large():\n    out = np.empty((10,), dtype='i4')\n    arr = out[:-1]\n    arr[:] = 5\n    for codec in codecs:\n        codec.decode(codec.encode(arr), out)\n<|endoftext|>",
    "repo": "tushar-deepsource/numcodecs"
  },
  {
    "prompt": "fn test_get_seeds_different_under_mp_pool()\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from bw2calc.errors import MultipleValues\nfrom matrix_utils.errors import AllArraysEmpty\nfrom bw2calc.single_value_diagonal_matrix import SingleValueDiagonalMatrix as SVDM\nfrom bw2calc.utils import get_datapackage\nfrom pathlib import Path\nimport numpy as np\nimport pytest\n\nfixture_dir = Path(__file__).resolve().parent / \"fixtures\"\n\n\ndef test_svdm_missing_dimension_kwarg():\n    with pytest.raises(TypeError):\n        SVDM(packages=[\"something\"], matrix=\"something\")\n\n\ndef test_svdm_no_data():\n    with pytest.raises(AllArraysEmpty):\n        SVDM(\n            packages=[get_datapackage(fixture_dir / \"basic_fixture.zip\")],\n            matrix=\"weighting_matrix\",\n            dimension=5,\n        )\n\n\ndef test_svdm_multiple_dataresources():\n    with pytest.raises(MultipleValues):\n        SVDM(\n            packages=[\n                get_datapackage(fixture_dir / \"svdm.zip\"),\n                get_datapackage(fixture_dir / \"svdm2.zip\"),\n            ],\n            matrix=\"weighting_matrix\",\n            dimension=500,\n        )\n\n\ndef test_svdm_multiple_values():\n    with pytest.raises(MultipleValues):\n        SVDM(\n            packages=[get_datapackage(fixture_dir / \"basic_fixture.zip\")],\n            matrix=\"technosphere_matrix\",\n            dimension=500,\n        )\n\n\ndef test_svdm_basic():\n    obj = SVDM(\n        packages=[\n            get_datapackage(fixture_dir / \"svdm.zip\"),\n        ],\n        matrix=\"weighting_matrix\",\n        use_arrays=False,\n        dimension=500,\n    )\n    assert obj.matrix.shape == (500, 500)\n    assert obj.matrix.sum() == 500 * 42\n    assert np.allclose(obj.matrix.tocoo().row, np.arange(500))\n    assert np.allclose(obj.matrix.tocoo().col, np.arange(500))\n\n\ndef test_svdm_iteration():\n    obj = SVDM(\n        packages=[\n            get_datapackage(fixture_dir / \"svdm.zip\"),\n        ],\n        matrix=\"weighting_matrix\",\n        use_arrays=False,\n        dimension=500,\n    )\n    assert obj.matrix.shape == (500, 500)\n    assert obj.matrix.sum() == 500 * 42\n    next(obj)\n    assert obj.matrix.shape == (500, 500)\n    assert obj.matrix.sum() == 500 * 42\n\n\ndef test_svdm_distributions():\n    obj = SVDM(\n        packages=[\n            get_datapackage(fixture_dir / \"svdm.zip\"),\n        ],\n        matrix=\"weighting_matrix\",\n        use_distributions=True,\n        use_arrays=False,\n        dimension=500,\n    )\n    assert obj.matrix.shape == (500, 500)\n    total = obj.matrix.sum()\n    assert total\n    next(obj)\n    assert total != obj.matrix.sum()\n    next(obj)\n    assert total != obj.matrix.sum()\n\n\ndef test_svdm_arrays():\n    obj = SVDM(\n        packages=[\n            get_datapackage(fixture_dir / \"svdm.zip\"),\n        ],\n        matrix=\"weighting_matrix\",\n        use_vectors=False,\n        use_arrays=True,\n        dimension=500,\n    )\n    assert obj.matrix.shape == (500, 500)\n    assert obj.matrix.sum() == 500 * 1\n    next(obj)\n    assert obj.matrix.shape == (500, 500)\n    assert obj.matrix.sum() == 500 * 2\n    next(obj)\n    assert obj.matrix.shape == (500, 500)\n    assert obj.matrix.sum() == 500 * 3\n<|endoftext|>",
    "repo": "mixib/brightway2-calc"
  },
  {
    "prompt": "class OnlineStandardizingLayer(nn.Module):\n\t\"\"\"Accumulate mean and variance online using the \"parallel algorithm\" algorithm from [1].\n\t\n\tArgs:\n\t    shape: shape of mean, variance, and std array. do not include batch dimension!\n\t    stable: (optional) compute using the stable version of the algorithm [1]\n\t    epsilon: (optional) added to the computation of the standard deviation for numerical stability.\n\t    use_average_std: (optional) ``True`` to normalize using std averaged over the whole observation, ``False`` to normalize using std of each component of the observation.\n\t\n\tReferences:\n\t    [1] https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\"\"\"\n\tfn __init__(self, shape: Tuple[(int, ...)], stable: bool, epsilon: float, use_average_std: bool) -> None\n\tfn _parallel_algorithm(self, x: torch.Tensor) -> Tuple[(torch.Tensor, torch.Tensor, torch.Tensor)]\n\tfn forward(self, x: torch.Tensor) -> torch.Tensor\n\tfn mean(self) -> torch.Tensor\n\tfn var(self) -> torch.Tensor\n\tfn std(self) -> torch.Tensor\n\n\nclass OnlineDictStandardizingLayer(nn.Module):\n\tfn __init__(self, shapes: Dict[(Hashable, Tuple[(int, ...)])], stable: bool, epsilon: float, use_average_std: bool) -> None\n\tfn forward(self, x: Dict[(Hashable, torch.Tensor)]) -> torch.Tensor\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from itertools import product\n\nimport pytest\nimport torch\n\nfrom swyft.networks.standardization import OnlineStandardizingLayer\n\n\nclass TestStandardizationLayer:\n    bss = [1, 128]\n    shapes = [(1,), (5,), (10, 5, 2, 1), (1, 1, 1)]\n    means = [2, 139]\n    stds = [1, 80]\n    stables = [True, False]\n\n    @pytest.mark.parametrize(\n        \"bs, shape, mean, std, stable\", product(bss, shapes, means, stds, stables)\n    )\n    def test_online_standardization_layer_update(self, bs, shape, mean, std, stable):\n        osl = OnlineStandardizingLayer(shape, stable=stable)\n        osl.train()\n        old_stats = osl.n.clone(), osl.mean.clone(), osl.var.clone(), osl.std.clone()\n\n        data = torch.randn(bs, *shape) * std + mean\n        _ = osl(data)\n\n        new_stats = osl.n.clone(), osl._mean.clone(), osl.var.clone(), osl.std.clone()\n\n        assert old_stats != new_stats\n\n    @pytest.mark.parametrize(\n        \"bs, shape, mean, std, stable\", product(bss, shapes, means, stds, stables)\n    )\n    def test_online_standardization_layer_mean(self, bs, shape, mean, std, stable):\n        torch.manual_seed(0)\n\n        osl = OnlineStandardizingLayer(shape, stable=stable)\n        osl.train()\n\n        data = torch.randn(bs, *shape) * std + mean\n        _ = osl(data)\n\n        assert torch.allclose(osl.mean, data.mean(0))\n\n    @pytest.mark.parametrize(\n        \"bs, shape, mean, std, stable\", product(bss, shapes, means, stds, stables)\n    )\n    def test_online_standardization_layer_std(self, bs, shape, mean, std, stable):\n        torch.manual_seed(0)\n\n        osl = OnlineStandardizingLayer(shape, stable=stable)\n        osl.train()\n\n        data = torch.randn(bs, *shape) * std + mean\n        _ = osl(data)\n\n        if torch.isnan(data.std(0)).all():\n            replacement_std = torch.sqrt(torch.ones_like(osl.std) * osl.epsilon)\n            assert torch.allclose(osl.std, replacement_std)\n        else:\n            assert torch.allclose(osl.std, data.std(0))\n\n    @pytest.mark.parametrize(\n        \"bs, shape, mean, std, stable\", product(bss, shapes, means, stds, stables)\n    )\n    def test_online_standardization_layer_std_average(\n        self, bs, shape, mean, std, stable\n    ):\n        torch.manual_seed(0)\n\n        osl = OnlineStandardizingLayer(shape, stable=stable, use_average_std=True)\n        osl.train()\n\n        data = torch.randn(bs, *shape) * std + mean\n        _ = osl(data)\n\n        if torch.isnan(data.std(0)).all():\n            replacement_std = torch.sqrt(torch.ones_like(osl.std) * osl.epsilon)\n            assert torch.allclose(osl.std, replacement_std.mean())\n        else:\n            assert torch.allclose(osl.std, data.std(0).mean())\n\n\nif __name__ == \"__main__\":\n    pass\n<|endoftext|>",
    "repo": "undark-lab/swyft"
  },
  {
    "prompt": "class LockfileCorruptedError(DvcException):\n\tfn __init__(self, path)\n\n\nfn is_valid_filename(path)\n\nfn is_dvc_file(path)\n\nfn check_dvc_filename(path)\n\nclass FileMixin:\n\tSCHEMA = None\n\tfn __init__(self, repo, path)\n\tfn __repr__(self)\n\tfn __hash__(self)\n\tfn __eq__(self, other)\n\tfn __str__(self)\n\tfn relpath(self)\n\tfn exists(self)\n\tfn _load(self)\n\tfn validate(cls, d, fname)\n\tfn remove_with_prompt(self, force)\n\tfn remove(self, force)\n\tfn dump(self, stage)\n\n\nclass SingleStageFile(FileMixin):\n\tfn __init__(self, repo, path)\n\tfn stage(self)\n\tfn stages(self)\n\t\"\"\"Dumps given stage appropriately in the dvcfile.\"\"\"\n\tfn dump(self, stage)\n\tfn remove_with_prompt(self, force)\n\n\n\"\"\"Abstraction for pipelines file, .yaml + .lock combined.\"\"\"\nclass PipelineFile(FileMixin):\n\tfn _lockfile(self)\n\t\"\"\"Dumps given stage appropriately in the dvcfile.\"\"\"\n\tfn dump(self, stage, update_pipeline)\n\tfn _dump_lockfile(self, stage)\n\tfn _dump_pipeline_file(self, stage)\n\tfn stage(self)\n\tfn stages(self)\n\tfn remove(self, force)\n\n\nclass Lockfile(FileMixin):\n\tfn load(self)\n\tfn dump(self, stage)\n\n\nclass Dvcfile:\n\tfn __new__(cls, repo, path)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from dvc.stage import PipelineStage\nfrom dvc.dvcfile import Lockfile, LockfileCorruptedError\nimport yaml\nimport pytest\n\n\ndef test_stage_dump_no_outs_deps(tmp_dir, dvc):\n    stage = PipelineStage(name=\"s1\", repo=dvc, path=\"path\", cmd=\"command\")\n    lockfile = Lockfile(dvc, \"path.lock\")\n    lockfile.dump(stage)\n    assert lockfile.load() == {\"s1\": {\"cmd\": \"command\"}}\n\n\ndef test_stage_dump_when_already_exists(tmp_dir, dvc):\n    data = {\"s1\": {\"cmd\": \"command\", \"deps\": [], \"outs\": []}}\n    with open(\"path.lock\", \"w+\") as f:\n        yaml.dump(data, f)\n    stage = PipelineStage(name=\"s2\", repo=dvc, path=\"path\", cmd=\"command2\")\n    lockfile = Lockfile(dvc, \"path.lock\")\n    lockfile.dump(stage)\n    assert lockfile.load() == {\n        **data,\n        \"s2\": {\"cmd\": \"command2\"},\n    }\n\n\ndef test_stage_dump_with_deps_and_outs(tmp_dir, dvc):\n    data = {\n        \"s1\": {\n            \"cmd\": \"command\",\n            \"deps\": [{\"md5\": \"1.txt\", \"path\": \"checksum\"}],\n            \"outs\": [{\"md5\": \"2.txt\", \"path\": \"checksum\"}],\n        }\n    }\n    with open(\"path.lock\", \"w+\") as f:\n        yaml.dump(data, f)\n\n    lockfile = Lockfile(dvc, \"path.lock\")\n    stage = PipelineStage(name=\"s2\", repo=dvc, path=\"path\", cmd=\"command2\")\n    lockfile.dump(stage)\n    assert lockfile.load() == {\n        **data,\n        \"s2\": {\"cmd\": \"command2\"},\n    }\n\n\ndef test_stage_overwrites_if_already_exists(tmp_dir, dvc):\n    lockfile = Lockfile(dvc, \"path.lock\",)\n    stage = PipelineStage(name=\"s2\", repo=dvc, path=\"path\", cmd=\"command2\")\n    lockfile.dump(stage)\n    stage = PipelineStage(name=\"s2\", repo=dvc, path=\"path\", cmd=\"command3\")\n    lockfile.dump(stage)\n    assert lockfile.load() == {\n        \"s2\": {\"cmd\": \"command3\"},\n    }\n\n\ndef test_load_when_lockfile_does_not_exist(tmp_dir, dvc):\n    assert {} == Lockfile(dvc, \"pipelines.lock\").load()\n\n\n@pytest.mark.parametrize(\n    \"corrupt_data\",\n    [\n        {\"s1\": {\"outs\": []}},\n        {\"s1\": {}},\n        {\n            \"s1\": {\n                \"cmd\": \"command\",\n                \"outs\": [\n                    {\"md5\": \"checksum\", \"path\": \"path\", \"random\": \"value\"}\n                ],\n            }\n        },\n        {\"s1\": {\"cmd\": \"command\", \"deps\": [{\"md5\": \"checksum\"}]}},\n    ],\n)\ndef test_load_when_lockfile_is_corrupted(tmp_dir, dvc, corrupt_data):\n    with open(\"Dvcfile.lock\", \"w+\") as f:\n        yaml.dump(corrupt_data, f)\n    lockfile = Lockfile(dvc, \"Dvcfile.lock\")\n    with pytest.raises(LockfileCorruptedError) as exc_info:\n        lockfile.load()\n    assert \"Dvcfile.lock\" in str(exc_info.value)\n<|endoftext|>",
    "repo": "dchichkov/dvc"
  },
  {
    "prompt": "class TestUtils:\n\tfn save_and_load(sol, ocp, test_solve_of_loaded)\n\tfn deep_assert(first_elem, second_elem)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"\nTest for file IO\n\"\"\"\nimport importlib.util\nfrom pathlib import Path\n\nimport pytest\nimport numpy as np\n\nfrom biorbd_optim import Data, OdeSolver\nfrom .utils import TestUtils\n\n# Load align_segment_on_rt\nPROJECT_FOLDER = Path(__file__).parent / \"..\"\nspec = importlib.util.spec_from_file_location(\n    \"align_segment_on_rt\", str(PROJECT_FOLDER) + \"/examples/align/align_segment_on_rt.py\"\n)\nalign_segment_on_rt = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(align_segment_on_rt)\n\n\n@pytest.mark.parametrize(\"ode_solver\", [OdeSolver.RK])\ndef test_align_segment_on_rt(ode_solver):\n    ocp = align_segment_on_rt.prepare_ocp(\n        biorbd_model_path=str(PROJECT_FOLDER) + \"/examples/align/cube_and_line.bioMod\",\n        final_time=0.5,\n        number_shooting_points=8,\n        ode_solver=ode_solver,\n    )\n    sol = ocp.solve()\n\n    # Check objective function value\n    f = np.array(sol[\"f\"])\n    np.testing.assert_equal(f.shape, (1, 1))\n    np.testing.assert_almost_equal(f[0, 0], 12320.059717265229)\n\n    # Check constraints\n    g = np.array(sol[\"g\"])\n    np.testing.assert_equal(g.shape, (91, 1))\n    np.testing.assert_almost_equal(g, np.zeros((91, 1)))\n\n    # Check some of the results\n    states, controls = Data.get_data(ocp, sol[\"x\"])\n    q, qdot, tau = states[\"q\"], states[\"q_dot\"], controls[\"tau\"]\n\n    # initial and final position\n    np.testing.assert_almost_equal(q[:, 0], np.array([0.305837645, 6.07684988e-18, -1.57, -1.57]))\n    np.testing.assert_almost_equal(q[:, -1], np.array([0.305837645, 2.34331392e-17, 1.57, 1.57]))\n    # initial and final velocities\n    np.testing.assert_almost_equal(qdot[:, 0], np.array((0, 0, 0, 0)))\n    np.testing.assert_almost_equal(qdot[:, -1], np.array((0, 0, 0, 0)))\n    # initial and final controls\n    np.testing.assert_almost_equal(tau[:, 0], np.array([1.09038782e-23, 9.81, 66.9866667, 66.9866667]))\n    np.testing.assert_almost_equal(tau[:, -1], np.array([-1.61910771e-23, 9.81, -66.9866667, -66.9866667]))\n\n    # save and load\n    TestUtils.save_and_load(sol, ocp, False)\n\n\n# Load align_marker_on_segment\nPROJECT_FOLDER = Path(__file__).parent / \"..\"\nspec = importlib.util.spec_from_file_location(\n    \"align_marker_on_segment\", str(PROJECT_FOLDER) + \"/examples/align/align_marker_on_segment.py\"\n)\nalign_marker_on_segment = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(align_marker_on_segment)\n\n\n@pytest.mark.parametrize(\"ode_solver\", [OdeSolver.RK])\ndef test_align_marker_on_segment(ode_solver):\n    ocp = align_marker_on_segment.prepare_ocp(\n        biorbd_model_path=str(PROJECT_FOLDER) + \"/examples/align/cube_and_line.bioMod\",\n        final_time=0.5,\n        number_shooting_points=8,\n        ode_solver=ode_solver,\n        initialize_near_solution=True,\n    )\n    sol = ocp.solve()\n\n    # Check objective function value\n    f = np.array(sol[\"f\"])\n    np.testing.assert_equal(f.shape, (1, 1))\n    np.testing.assert_almost_equal(f[0, 0], 2632.9408063562532)\n\n    # Check constraints\n    g = np.array(sol[\"g\"])\n    np.testing.assert_equal(g.shape, (88, 1))\n    np.testing.assert_almost_equal(g, np.zeros((88, 1)))\n\n    # Check some of the results\n    states, controls = Data.get_data(ocp, sol[\"x\"])\n    q, qdot, tau = states[\"q\"], states[\"q_dot\"], controls[\"tau\"]\n\n    # initial and final position\n    np.testing.assert_almost_equal(q[:, 0], np.array([1, 0, 0, 0.46364761]))\n    np.testing.assert_almost_equal(q[:, -1], np.array([2, 0, 1.57, 0.78539785]))\n    # initial and final velocities\n    np.testing.assert_almost_equal(qdot[:, 0], np.array((0, 0, 0, 0)))\n    np.testing.assert_almost_equal(qdot[:, -1], np.array((0, 0, 0, 0)))\n    # initial and final controls\n    np.testing.assert_almost_equal(tau[:, 0], np.array([23.6216484, 12.25908094, 31.52069129, 12.94722946]))\n    np.testing.assert_almost_equal(tau[:, -1], np.array([-16.65951028, 14.58725991, -36.10090439, 4.41782565]))\n\n    # save and load\n    TestUtils.save_and_load(sol, ocp, False)\n<|endoftext|>",
    "repo": "mickaelbegon/BiorbdOptim"
  },
  {
    "prompt": "\"\"\"Set up a component and all its dependencies.\"\"\"\nfn setup_component(opp: core.OpenPeerPower, domain: str, config: Dict) -> bool\n\n\"\"\"Call a method when a component is setup.\"\"\"\nfn async_when_setup(opp: core.OpenPeerPower, component: str, when_setup_cb: Callable[([core.OpenPeerPower, str], Awaitable[None])]) -> None\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"Test the default_config init.\"\"\"\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom openpeerpower.setup import async_setup_component\n\nfrom tests.common import MockDependency, mock_coro\n\n\n@pytest.fixture(autouse=True)\ndef zeroconf_mock():\n    \"\"\"Mock zeroconf.\"\"\"\n    with MockDependency(\"zeroconf\") as mocked_zeroconf:\n        mocked_zeroconf.Zeroconf.return_value.register_service.return_value = mock_coro(\n            True\n        )\n        yield\n\n\n@pytest.fixture(autouse=True)\ndef netdisco_mock():\n    \"\"\"Mock netdisco.\"\"\"\n    with MockDependency(\"netdisco\", \"discovery\"):\n        yield\n\n\n@pytest.fixture(autouse=True)\ndef recorder_url_mock():\n    \"\"\"Mock recorder url.\"\"\"\n    with patch(\"openpeerpower.components.recorder.DEFAULT_URL\", \"sqlite://\"):\n        yield\n\n\nasync def test_setup(opp):\n    \"\"\"Test setup.\"\"\"\n    assert await async_setup_component(opp, \"default_config\", {})\n<|endoftext|>",
    "repo": "OpenPeerPower/openpeerpower"
  },
  {
    "prompt": "\"\"\"Handle encrypted files for tarfile library.\"\"\"\nclass SecureTarFile:\n\t\"\"\"Initialize encryption handler.\"\"\"\n\tfn __init__(self, name: Path, mode: str, key: Optional[bytes], gzip: bool) -> None\n\t\"\"\"Start context manager tarfile.\"\"\"\n\tfn __enter__(self) -> tarfile.TarFile\n\t\"\"\"Close file.\"\"\"\n\tfn __exit__(self, exc_type, exc_value, traceback) -> None\n\t\"\"\"Write data.\"\"\"\n\tfn write(self, data: bytes) -> None\n\t\"\"\"Read data.\"\"\"\n\tfn read(self, size: int) -> bytes\n\t\"\"\"Return path object of tarfile.\"\"\"\n\tfn path(self) -> Path\n\t\"\"\"Return backup size.\"\"\"\n\tfn size(self) -> float\n\n\n\"\"\"Generate an iv from data.\"\"\"\nfn _generate_iv(key: bytes, salt: bytes) -> bytes\n\n\"\"\"Security safe check of path.\n\nPrevent ../ or absolut paths\"\"\"\nfn secure_path(tar: tarfile.TarFile) -> Generator[(tarfile.TarInfo, None, None)]\n\n\"\"\"Filter to filter excludes.\"\"\"\nfn _is_excluded_by_filter(path: PurePath, exclude_list: list[str]) -> bool\n\n\"\"\"Append directories and/or files to the TarFile if excludes wont filter.\"\"\"\nfn atomic_contents_add(tar_file: tarfile.TarFile, origin_path: Path, excludes: list[str], arcname: str) -> None\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"Test Tarfile functions.\"\"\"\n\nimport attr\nimport pytest\n\nfrom supervisor.utils.tar import secure_path, exclude_filter\n\n\n@attr.s\nclass TarInfo:\n    \"\"\"Fake TarInfo\"\"\"\n\n    name: str = attr.ib()\n\n\ndef test_secure_path():\n    \"\"\"Test Secure Path.\"\"\"\n    test_list = [\n        TarInfo(\"test.txt\"),\n        TarInfo(\"data/xy.blob\"),\n        TarInfo(\"bla/blu/ble\"),\n        TarInfo(\"data/../xy.blob\"),\n    ]\n    assert test_list == list(secure_path(test_list))\n\n\ndef test_not_secure_path():\n    \"\"\"Test Not secure path.\"\"\"\n    test_list = [\n        TarInfo(\"/test.txt\"),\n        TarInfo(\"data/../../xy.blob\"),\n        TarInfo(\"/bla/blu/ble\"),\n    ]\n    assert [] == list(secure_path(test_list))\n\n\ndef test_exclude_filter_good():\n    \"\"\"Test exclude filter.\"\"\"\n    filter_funct = exclude_filter([\"not/match\", \"/dev/xy\"])\n    test_list = [\n        TarInfo(\"test.txt\"),\n        TarInfo(\"data/xy.blob\"),\n        TarInfo(\"bla/blu/ble\"),\n        TarInfo(\"data/../xy.blob\"),\n    ]\n\n    assert test_list == [filter_funct(result) for result in test_list]\n\n\ndef test_exclude_filter_bad():\n    \"\"\"Test exclude filter.\"\"\"\n    filter_funct = exclude_filter([\"*.txt\", \"data/*\", \"bla/blu/ble\"])\n    test_list = [\n        TarInfo(\"test.txt\"),\n        TarInfo(\"data/xy.blob\"),\n        TarInfo(\"bla/blu/ble\"),\n        TarInfo(\"data/test_files/kk.txt\"),\n    ]\n\n    for info in [filter_funct(result) for result in test_list]:\n        assert info is None\n<|endoftext|>",
    "repo": "pssc/supervisor"
  },
  {
    "prompt": "\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from urllib import urlencode\n\nimport pytest\nfrom mock import patch\n\nfrom grouper.constants import USER_METADATA_SHELL_KEY\nfrom grouper.models.user import User\nfrom grouper.user_metadata import get_user_metadata_by_key\nfrom tests.fixtures import (  # noqa: F401\n    fe_app as app,\n    graph,\n    groups,\n    permissions,\n    service_accounts,\n    session,\n    standard_graph,\n    users,\n)\nfrom tests.url_util import url\n\n\n@pytest.mark.gen_test\ndef test_shell(session, users, http_client, base_url):  # noqa: F811\n    with patch(\"grouper.fe.handlers.user_shell.settings\") as mock_settings:\n        mock_settings.shell = [[\"/bin/bash\", \"bash\"], [\"/bin/zsh\", \"zsh\"]]\n\n        user = users[\"zorkian@a.co\"]\n        assert not get_user_metadata_by_key(session, user.id, USER_METADATA_SHELL_KEY)\n\n        user = User.get(session, name=user.username)\n        fe_url = url(base_url, \"/users/{}/shell\".format(user.username))\n        resp = yield http_client.fetch(\n            fe_url,\n            method=\"POST\",\n            body=urlencode({\"shell\": \"/bin/bash\"}),\n            headers={\"X-Grouper-User\": user.username},\n        )\n        assert resp.code == 200\n\n        user = User.get(session, name=user.username)\n\n        assert (\n            get_user_metadata_by_key(session, user.id, USER_METADATA_SHELL_KEY) is not None\n        ), \"The user should have shell metadata\"\n        assert (\n            get_user_metadata_by_key(session, user.id, USER_METADATA_SHELL_KEY).data_value\n            == \"/bin/bash\"\n        )\n\n        fe_url = url(base_url, \"/users/{}/shell\".format(user.username))\n        resp = yield http_client.fetch(\n            fe_url,\n            method=\"POST\",\n            body=urlencode({\"shell\": \"/bin/fish\"}),\n            headers={\"X-Grouper-User\": user.username},\n        )\n        assert resp.code == 200\n\n        user = User.get(session, name=user.username)\n\n        assert (\n            get_user_metadata_by_key(session, user.id, USER_METADATA_SHELL_KEY) is not None\n        ), \"The user should have shell metadata\"\n        assert (\n            get_user_metadata_by_key(session, user.id, USER_METADATA_SHELL_KEY).data_value\n            == \"/bin/bash\"\n        )\n\n        fe_url = url(base_url, \"/users/{}/shell\".format(user.username))\n        resp = yield http_client.fetch(\n            fe_url,\n            method=\"POST\",\n            body=urlencode({\"shell\": \"/bin/zsh\"}),\n            headers={\"X-Grouper-User\": user.username},\n        )\n        assert resp.code == 200\n\n        user = User.get(session, name=user.username)\n\n        assert (\n            get_user_metadata_by_key(session, user.id, USER_METADATA_SHELL_KEY) is not None\n        ), \"The user should have shell metadata\"\n        assert (\n            get_user_metadata_by_key(session, user.id, USER_METADATA_SHELL_KEY).data_value\n            == \"/bin/zsh\"\n        )\n<|endoftext|>",
    "repo": "bonniech3n/merou"
  },
  {
    "prompt": "fn read_file(filename: str) -> str\n\nfn separate_comma_imports(xmldoc: lxml.etree.Element) -> str\n\nfn remove_duplicated_imports(xmldoc: lxml.etree.Element) -> str\n\nfn apply_import_ordering(xmldoc: lxml.etree.Element) -> str\n\nfn fix_whitespace_after_imports(xmldoc: lxml.etree.Element) -> str\n\n\"\"\"Performs a step of the transformation.\n\n:param text file_contents: Contends of the cheetah template\n:param function step: Function taking xmldoc and returning new contents\n:returns: new contents of the file.\"\"\"\nfn perform_step(file_contents: str, step: Callable[([lxml.etree.Element], str)]) -> str\n\nfn main(argv: Sequence[str] | None) -> int\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import os.path\n\nimport pytest\n\nfrom cheetah_lint.reorder_imports import main\nfrom cheetah_lint.reorder_imports import perform_step\nfrom cheetah_lint.reorder_imports import STEPS\nfrom cheetah_lint.util import read_file\n\n\nINPUT_DIRECTORY = 'tests/inputs'\nOUTPUT_DIRECTORY = 'tests/outputs'\n\n\nTESTS = tuple(\n    template for template in os.listdir(INPUT_DIRECTORY)\n    if not template.startswith('_')\n)\n\n\ndef get_input_output(template):\n    return tuple(\n        read_file(os.path.join(base, template))\n        for base in (INPUT_DIRECTORY, OUTPUT_DIRECTORY)\n    )\n\n\n@pytest.mark.parametrize('template', TESTS)\ndef test_integration_template(template):\n    contents, expected = get_input_output(template)\n    for step in STEPS:\n        contents = perform_step(contents, step)\n    assert expected == contents\n\n\n@pytest.mark.parametrize('template', TESTS)\ndef test_integration_calls_main(tmpdir, template):\n    contents, expected = get_input_output(template)\n    template_path = os.path.join(tmpdir.strpath, 'tmp.tmpl')\n\n    with open(template_path, 'w') as template_file:\n        template_file.write(contents)\n\n    main([template_path])\n\n    end_contents = read_file(template_path)\n    assert expected == end_contents\n<|endoftext|>",
    "repo": "asottile/cheetah_lint"
  },
  {
    "prompt": "\"\"\"URLValidator that also validates about: and chrome:// urls\"\"\"\nclass EnhancedURLValidator(URLValidator):\n\tfn __call__(self, value)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "from django.core.exceptions import ValidationError\n\nimport pytest\n\nfrom fjord.base.validators import EnhancedURLValidator\n\n\nclass TestEnhancedURLValidator:\n    def test_valid_urls(self):\n        test_data = [\n            'example.com',\n            'example.com:80',\n            'example.com:80/foo',\n            'http://example.com',\n            'http://example.com/foo',\n            'http://example.com:80',\n            'http://example.com:80/foo',\n            'https://example.com',\n            'https://example.com/foo',\n            'https://example.com:80',\n            'https://example.com:80/foo',\n            'ftp://example.com',\n            'about:mozilla',\n            'chrome://foo',\n\n            # Taken from Django's URLValidator test case data\n            'http://www.djangoproject.com/',\n            'http://localhost/',\n            'http://example.com/',\n            'http://www.example.com/',\n            'http://www.example.com:8000/test'\n            'http://valid-with-hyphens.com/',\n            'http://subdomain.example.com/',\n            'http://200.8.9.10/',\n            'http://200.8.9.10:8000/test',\n            'http://valid-----hyphens.com/',\n            'http://example.com?something=value',\n            'http://example.com/index.php?something=value&another=value2',\n        ]\n\n        validator = EnhancedURLValidator()\n\n        for url in test_data:\n            validator(url)\n\n    def test_invalid_urls(self):\n        test_data = [\n            'foo',\n            'http://',\n            'http://example',\n            'http://example.'\n            'http://.com',\n            'http://invalid-.com',\n            'http://-invalid.com',\n            'http://inv-.alid-.com',\n            'http://inv-.-alid.com'\n        ]\n\n        validator = EnhancedURLValidator()\n\n        for url in test_data:\n            with pytest.raises(ValidationError):\n                validator(url)\n<|endoftext|>",
    "repo": "lgp171188/fjord"
  },
  {
    "prompt": "class Role(SurrogatePK, Model):\n\t__tablename__ = 'roles'\n\tname = Column(db.String(80), unique=True, nullable=False)\n\tuser_id = ReferenceCol('users', nullable=True)\n\tuser = relationship('User', backref='roles')\n\tfn __init__(self, name)\n\tfn __repr__(self)\n\n\nclass User(UserMixin, SurrogatePK, Model):\n\t__tablename__ = 'users'\n\tusername = Column(db.String(80), unique=True, nullable=False)\n\temail = Column(db.String(80), unique=True, nullable=False)\n\tpassword = Column(db.String(128), nullable=True)\n\tcreated_at = Column(db.DateTime, nullable=False, default=dt.datetime.utcnow)\n\tfirst_name = Column(db.String(30), nullable=True)\n\tlast_name = Column(db.String(30), nullable=True)\n\tactive = Column(db.Boolean(), default=False)\n\tis_admin = Column(db.Boolean(), default=False)\n\tfn __init__(self, username, email, password)\n\tfn set_password(self, password)\n\tfn check_password(self, value)\n\tfn full_name(self)\n\tfn __repr__(self)\n\tfn get_auth_token()\n\n\n\"\"\"An application factory, as explained here:\n    http://flask.pocoo.org/docs/patterns/appfactories/\n\n:param config_object: The configuration object to use.\"\"\"\nfn create_app(config_object: None)\n\nfn register_extensions(app)\n\nfn register_blueprints(app)\n\nfn register_errorhandlers(app)\n\n\"\"\"Mixin that adds convenience methods for CRUD (create, read, update, delete)\noperations.\"\"\"\nclass CRUDMixin(object):\n\t\"\"\"Create a new record and save it the database.\"\"\"\n\tfn create(cls)\n\t\"\"\"Update specific fields of a record.\"\"\"\n\tfn update(self, commit)\n\t\"\"\"Save the record.\"\"\"\n\tfn save(self, commit)\n\t\"\"\"Remove the record from the database.\"\"\"\n\tfn delete(self, commit)\n\n\n\"\"\"Base model class that includes CRUD convenience methods.\"\"\"\nclass Model(CRUDMixin, db.Model):\n\t__abstract__ = True\n\n\n\"\"\"A mixin that adds a surrogate integer 'primary key' column named\n``id`` to any declarative-mapped class.\"\"\"\nclass SurrogatePK(object):\n\t__table_args__ = {'extend_existing': True}\n\tid = db.Column(db.Integer, primary_key=True)\n\tfn get_by_id(cls, id)\n\n\n\"\"\"Column that adds primary key foreign key reference.\n\nUsage: ::\n\n    category_id = ReferenceCol('category')\n    category = relationship('Category', backref='categories')\"\"\"\nfn ReferenceCol(tablename, nullable, pk_name)\n\nclass Config(object):\n\tSECRET_KEY = os_env.get('CCFLASKTEST_SECRET', 'secret-key')\n\tAPP_DIR = os.path.abspath(os.path.dirname(__file__))\n\tPROJECT_ROOT = os.path.abspath(os.path.join(APP_DIR, os.pardir))\n\tBCRYPT_LOG_ROUNDS = 13\n\tASSETS_DEBUG = False\n\tDEBUG_TB_ENABLED = False\n\tDEBUG_TB_INTERCEPT_REDIRECTS = False\n\tCACHE_TYPE = 'simple'\n\n\n\"\"\"Production configuration.\"\"\"\nclass ProdConfig(Config):\n\tENV = 'prod'\n\tDEBUG = False\n\tSQLALCHEMY_DATABASE_URI = 'postgresql://localhost/'\n\tDEBUG_TB_ENABLED = False\n\n\n\"\"\"Development configuration.\"\"\"\nclass DevConfig(Config):\n\tENV = 'dev'\n\tDEBUG = True\n\tDB_NAME = 'dev.db'\n\tDB_PATH = os.path.join(Config.PROJECT_ROOT, DB_NAME)\n\tSQLALCHEMY_DATABASE_URI = 'sqlite:///{0}'.format(DB_PATH)\n\tDEBUG_TB_ENABLED = True\n\tASSETS_DEBUG = True\n\tCACHE_TYPE = 'simple'\n\n\nclass TestConfig(Config):\n\tTESTING = True\n\tDEBUG = True\n\tSQLALCHEMY_DATABASE_URI = 'sqlite://'\n\tBCRYPT_LOG_ROUNDS = 1\n\tWTF_CSRF_ENABLED = False\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport os\nfrom flask_script import Manager, Shell, Server\nfrom flask_script.commands import Clean, ShowUrls\nfrom flask_migrate import MigrateCommand\nfrom flask.ext.login import login_required, make_secure_token, get_auth_token\n\nfrom ccflasktest.app import create_app\nfrom ccflasktest.user.models import User\nfrom ccflasktest.settings import DevConfig, ProdConfig\nfrom ccflasktest.database import db\n\nif os.environ.get(\"CCFLASKTEST_ENV\") == 'prod':\n    app = create_app(ProdConfig)\nelse:\n    app = create_app(DevConfig)\n\nHERE = os.path.abspath(os.path.dirname(__file__))\nTEST_PATH = os.path.join(HERE, 'tests')\n\nmanager = Manager(app)\n\n\ndef _make_context():\n    \"\"\"Return context dict for a shell session so you can access\n    app, db, and the User model by default.\n    \"\"\"\n    return {'app': app, 'db': db, 'User': User}\n\n\n@manager.command\ndef test():\n    \"\"\"Run the tests.\"\"\"\n    import pytest\n    exit_code = pytest.main([TEST_PATH, '--verbose'])\n    return exit_code\n\n\n@app.route('/token', methods=['POST'])\n#  @login_required\ndef get_token():\n    # this will make a secure token based on the current user's ID\n    # cannot get a token without being first logged in...\n    # maybe we want to accept username/password for every POST /token\n    # and then validate the user.  If valid, then return token ?\n    #  return make_secure_token(current_user.id)\n    return get_auth_token()\n    #  s = Serializer(app.config['SECRET_KEY'], expires_in=600)\n    #  return s.dumps({'id': current_user.id})\n\nmanager.add_command('server', Server())\nmanager.add_command('shell', Shell(make_context=_make_context))\nmanager.add_command('db', MigrateCommand)\nmanager.add_command(\"urls\", ShowUrls())\nmanager.add_command(\"clean\", Clean())\n\nif __name__ == '__main__':\n    manager.run()\n<|endoftext|>",
    "repo": "chrcoe/ccflasktest"
  },
  {
    "prompt": "\"\"\"Factory for the user model\"\"\"\nclass UserFactory(DjangoModelFactory):\n\tusername = Faker('user_name')\n\temail = Faker('email')\n\tname = Faker('name')\n\tfn password(self, create: bool, extracted: Sequence[Any])\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import pytest\n\nfrom index_auth_service.users.forms import UserCreationForm\nfrom index_auth_service.users.tests.factories import UserFactory\n\npytestmark = pytest.mark.django_db\n\n\nclass TestUserCreationForm:\n    def test_clean_username(self):\n        # A user with proto_user params does not exist yet.\n        proto_user = UserFactory.build()\n\n        form = UserCreationForm(\n            {\n                \"username\": proto_user.username,\n                \"password1\": proto_user._password,\n                \"password2\": proto_user._password,\n            }\n        )\n\n        assert form.is_valid()\n        assert form.clean_username() == proto_user.username\n\n        # Creating a user.\n        form.save()\n\n        # The user with proto_user params already exists,\n        # hence cannot be created.\n        form = UserCreationForm(\n            {\n                \"username\": proto_user.username,\n                \"password1\": proto_user._password,\n                \"password2\": proto_user._password,\n            }\n        )\n\n        assert not form.is_valid()\n        assert len(form.errors) == 1\n        assert \"username\" in form.errors\n<|endoftext|>",
    "repo": "Silver3310/Index-auth-service"
  },
  {
    "prompt": "\"\"\"An event's location as a textual description and a list of places.\"\"\"\nclass EventLocation:\n\t\"\"\"Initialize with the location description and a list of places.\"\"\"\n\tfn __init__(self, description, places)\n\t\"\"\"Return the represention the event location.\"\"\"\n\tfn __repr__(self)\n\t\"\"\"Return the string version of the event location, represented the\n\tevent location solely by its description for compatibility with simple\n\tlocation strings.\"\"\"\n\tfn __str__(self)\n\t\"\"\"Return true if this event location is equal the other. Otherwise\n\treturn false. Used only for unit testing.\"\"\"\n\tfn __eq__(self, other)\n\t\"\"\"Used for truth testing. Return true if the locatiion is non-blank;\n\tfalse otherwise.\"\"\"\n\tfn __bool__(self)\n\t\"\"\"Make an event location from a list of places.\"\"\"\n\tfn from_places(cls, places)\n\t\"\"\"Format a list of places as an English phrase.\"\"\"\n\tfn format_place_list(places)\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "\"\"\"Test event locations.\"\"\"\n\n\nfrom meetup2xibo.updater.event_location import EventLocation\nimport pytest\n\nSAMPLE_PLACES = [\"Woodshop\", \"Classroom A\"]\nOTHER_PLACES = [\"Conference Room 1\"]\n\n\ndef test_repr():\n    \"\"\"Test that repr returns repr(description string).\"\"\"\n    location = EventLocation(\"test\", SAMPLE_PLACES)\n    assert repr(location) == \"EventLocation('test', ['Woodshop', 'Classroom A'])\"\n\ndef test_str():\n    \"\"\"Test that str returns str(description string).\"\"\"\n    location = EventLocation(\"test\", SAMPLE_PLACES)\n    assert str(location) == str(\"test\")\n\ndef test_equals_same():\n    \"\"\"Test that two identical locations are equals.\"\"\"\n    location1 = EventLocation(\"test\", SAMPLE_PLACES)\n    location2 = EventLocation(\"test\", SAMPLE_PLACES)\n    assert location1 == location2\n\ndef test_equals_different_descriptions():\n    \"\"\"Test that two different locations are not equals.\"\"\"\n    location1 = EventLocation(\"test1\", SAMPLE_PLACES)\n    location2 = EventLocation(\"test2\", SAMPLE_PLACES)\n    assert location1 != location2\n\ndef test_equals_different_places():\n    \"\"\"Test that two different locations are not equals.\"\"\"\n    location1 = EventLocation(\"test\", SAMPLE_PLACES)\n    location2 = EventLocation(\"test\", OTHER_PLACES)\n    assert location1 != location2\n\ndef test_bool_true():\n    \"\"\"Test that bool returns true for a non-empty string.\"\"\"\n    location = EventLocation(\"test\", SAMPLE_PLACES)\n    assert location\n\ndef test_bool_false():\n    \"\"\"Test that bool returns false for an empty string.\"\"\"\n    location = EventLocation(\"\", SAMPLE_PLACES)\n    assert not location\n\ntest_place_lists = [\n    ([], \"\"),\n    ([\"abc\"], \"abc\"),\n    ([\"def\", \"abc\"], \"def and abc\"),\n    ([\"def\", \"abc\", \"ghi\"], \"def, abc, and ghi\"),\n    ([\"def\", \"jkl\", \"abc\", \"ghi\"], \"def, jkl, abc, and ghi\"),\n]\n\n@pytest.mark.parametrize(\"place_list,expected_phrase\", test_place_lists)\ndef test_format_place_list(place_list, expected_phrase):\n    \"\"\"Test formatting place lists as a phrase retaining order.\"\"\"\n    phrase = EventLocation.format_place_list(place_list)\n    assert expected_phrase == phrase\n\n@pytest.mark.parametrize(\"place_list,expected_phrase\", test_place_lists)\ndef test_from_places(place_list, expected_phrase):\n    \"\"\"Test making an event location from a list of places.\"\"\"\n    event_location = EventLocation.from_places(place_list)\n    assert event_location.description == expected_phrase\n    assert event_location.places == place_list\n\n# vim: tabstop=8 expandtab shiftwidth=4 softtabstop=4 autoindent\n<|endoftext|>",
    "repo": "jshprentz/meetup2xibo"
  },
  {
    "prompt": "\"\"\"An overridden ArgumentParser class which is build to accommodate mnamer's\nsetting patterns and delineation of parameter, directive, and positional\narguments.\"\"\"\nclass ArgLoader(argparse.ArgumentParser):\n\tfn __init__(self)\n\tfn _add_spec(self, spec: SettingSpec)\n\t__iadd__ = _add_spec\n\tfn load(self) -> Dict[(str, Any)]\n\t\"\"\"Overrides ArgumentParser's format_help to dynamically generate a help\n\tmessage for use with the `--help` flag.\"\"\"\n\tfn format_help(self) -> str\n\n\nclass MediaType(Enum):\n\tEPISODE = 'episode'\n\tMOVIE = 'movie'\n\n\nclass MessageType(Enum):\n\tINFO = None\n\tALERT = 'yellow'\n\tERROR = 'red'\n\tSUCCESS = 'green'\n\tHEADING = 'bold'\n\n\nclass ProviderType(Enum):\n\tTVDB = 'tvdb'\n\tTVMAZE = 'tvmaze'\n\tTMDB = 'tmdb'\n\tOMDB = 'omdb'\n\n\nclass SettingType(Enum):\n\tDIRECTIVE = 'directive'\n\tPARAMETER = 'parameter'\n\tPOSITIONAL = 'positional'\n\tCONFIGURATION = 'configuration'\n\n\n\"\"\"A dataclass which contains the argparse and metadata fields used to describe\narguments and configuration settings.\"\"\"\nclass SettingSpec:\n\tgroup: SettingType\n\tdest: Optional[str] = None\n\taction: Optional[str] = None\n\tchoices: Optional[List[str]] = None\n\tflags: Optional[List[str]] = None\n\thelp: Optional[str] = None\n\tnargs: Optional[str] = None\n\ttype: Optional[type] = None\n\t\"\"\"Converts ArgSpec instance into a Python dictionary.\"\"\"\n\tfn as_dict(self) -> Dict[(str, Any)]\n\t__call__ = as_dict\n\tfn registration(self) -> Tuple[(List[str], Dict[(str, Any)])]\n\tfn name(self) -> Optional[str]\n\n\n### Unit tests for the above files using pytest. Make sure they are concise and complete. ###\n\n",
    "completion": "import sys\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom mnamer.argument import ArgLoader\nfrom mnamer.setting_spec import SettingSpec\nfrom mnamer.types import SettingType\n\npytestmark = pytest.mark.local\n\n\n@pytest.mark.parametrize(\n    \"settings_type\",\n    (SettingType.DIRECTIVE, SettingType.PARAMETER, SettingType.POSITIONAL),\n    ids=(\"directive\", \"parameter\", \"positional\"),\n)\ndef test_arg_loader__add_spec(settings_type: SettingType):\n    arg_loader = ArgLoader()\n    spec = SettingSpec(group=settings_type, flags=[\"-f\"], help=\"foo\")\n    actions = getattr(arg_loader, f\"_{settings_type.value}_group\")\n    assert len(actions._group_actions) == 0\n    arg_loader += spec\n    assert len(actions._group_actions) == 1\n\n\ndef test_arg_loader__add_spec_other():\n    arg_loader = ArgLoader()\n    spec = SettingSpec(group=SettingType.CONFIGURATION, flags=[\"-f\"], help=\"foo\")\n    with pytest.raises(RuntimeError):\n        arg_loader += spec\n\n\ndef test_arg_loader__format_help():\n    arg_loader = ArgLoader()\n    for spec in (\n        SettingSpec(SettingType.POSITIONAL, flags=[\"--foo1\"], help=\"foo1\"),\n        SettingSpec(SettingType.POSITIONAL, flags=[\"--foo2\"], help=\"foo2\"),\n        SettingSpec(SettingType.POSITIONAL, flags=[\"--foo3\"], help=\"foo3\"),\n        SettingSpec(SettingType.PARAMETER, flags=[\"--bar1\"], help=\"bar1\"),\n        SettingSpec(SettingType.PARAMETER, flags=[\"--bar2\"], help=\"bar2\"),\n        SettingSpec(SettingType.PARAMETER, flags=[\"--bar3\"], help=\"bar3\"),\n        SettingSpec(SettingType.DIRECTIVE, flags=[\"--baz1\"], help=\"baz1\"),\n        SettingSpec(SettingType.DIRECTIVE, flags=[\"--baz2\"], help=\"baz2\"),\n        SettingSpec(SettingType.DIRECTIVE, flags=[\"--baz3\"], help=\"baz3\"),\n    ):\n        arg_loader._add_spec(spec)\n    assert (\n        arg_loader.format_help()\n        == \"\"\"\nUSAGE: mnamer [preferences] [directives] target [targets ...]\n\nPOSITIONAL:\n  foo1\n  foo2\n  foo3\n\nPARAMETERS:\n  The following flags can be used to customize mnamer's behaviour. Their long\n  forms may also be set in a '.mnamer-v2.json' config file, in which case cli\n  arguments will take precedence.\n\n  bar1\n  bar2\n  bar3\n\nDIRECTIVES:\n  Directives are one-off arguments that are used to perform secondary tasks\n  like overriding media detection. They can't be used in '.mnamer-v2.json'.\n\n  baz1\n  baz2\n  baz3\n\nVisit https://github.com/jkwill87/mnamer for more information.\n\"\"\"\n    )\n\n\ndef test_arg_parser__load__valid_parameter():\n    spec = SettingSpec(group=SettingType.PARAMETER, flags=[\"-f\"], help=\"foo\", type=int)\n    arg_parser = ArgLoader(spec)\n    with patch.object(sys, \"argv\", [\"mnamer\", \"-f\", \"01\"]):\n        assert arg_parser.load() == {\"f\": 1}\n\n\ndef test_arg_parser__load__valid_directive():\n    spec = SettingSpec(group=SettingType.DIRECTIVE, flags=[\"-f\"], help=\"foo\", type=int)\n    arg_parser = ArgLoader(spec)\n    with patch.object(sys, \"argv\", [\"mnamer\", \"-f\", \"01\"]):\n        assert arg_parser.load() == {\"f\": 1}\n\n\ndef test_arg_parser__load__valid_positional():\n    spec = SettingSpec(group=SettingType.POSITIONAL, flags=[\"f\"], help=\"foo\", type=int)\n    arg_parser = ArgLoader(spec)\n    with patch.object(sys, \"argv\", [\"mnamer\", \"01\"]):\n        assert arg_parser.load() == {\"f\": 1}\n\n\ndef test_arg_parser__load__invalid_configuration():\n    spec = SettingSpec(group=SettingType.CONFIGURATION, flags=[\"-f\"], help=\"foo\")\n    arg_parser = ArgLoader(spec)\n    with patch.object(sys, \"argv\", [\"mnamer\", \"-f\", \"1\"]):\n        with pytest.raises(RuntimeError):\n            arg_parser.load()\n\n\ndef test_arg_parser__missing_help():\n    spec = SettingSpec(group=SettingType.DIRECTIVE, flags=[\"-f\"])\n    with pytest.raises(RuntimeError):\n        ArgLoader(spec)\n<|endoftext|>",
    "repo": "pennersr/mnamer"
  }
]
