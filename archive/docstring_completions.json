[
  {
    "prompt": "def process_user_csv(params, user_csv_file, algorithm, dev_mode, sandboxing, jail):\n    username = os.path.splitext(os.path.basename(user_csv_file))[0]\n    globals_dict = {'params': params}\n    user_specific_code = textwrap.dedent(\"\\n        def run_code():\\n            import bandicoot\\n\\n            algorithmobj = {}()\\n            bandicoot_user = bandicoot.read_csv(\\n               '{}', '', describe={}, warnings={})\\n            return algorithmobj.map(params, bandicoot_user)\\n        result = run_code()\\n        \".format(algorithm['className'], username, str(dev_mode), str(dev_mode)))\n    code = '{}\\n{}'.format(algorithm['code'], user_specific_code)\n    if sandboxing:\n        jail.safe_exec(code, globals_dict, files=[user_csv_file])\n    else:\n        not_safe_exec(code, globals_dict, files=[user_csv_file])\n    result = globals_dict['result']\n    return result",
    "completion": "Process a single user csv file.\n\n    Args:\n        params (dict): Parameters for the request.\n        user_csv_file (string): Path to user csv file.\n        algorithm (dict): Dictionary with keys `code` and `className`\n            specifying algorithm code and className.\n        dev_mode (bool): Should the algorithm run in development mode or\n            production mode.\n        sandboxing (bool): Should sandboxing be used or not.\n        jail (codejail.Jail): Jail object.\n\n    Returns:\n        Result of the execution.\n\n    Raises:\n        SafeExecException: If the execution wasn't successful.\n\n    END"
  },
  {
    "prompt": "def scale_result(result, scaler):\n    scaled_result = {}\n    for (key, val) in six.iteritems(result):\n        scaled_result[key] = scaler * val\n    return scaled_result",
    "completion": "Return scaled result.\n\n    Args:\n        result (dict): Result.\n        scaler (number): Factor by which results need to be scaled.\n\n    Returns:\n        dict: Scaled result.\n\n    END"
  },
  {
    "prompt": "def collector(writing_queue, params, dev_mode=False):\n    result_processor = ResultProcessor(params, dev_mode)\n    while True:\n        processed_result = writing_queue.get()\n        if processed_result == 'kill':\n            break\n        (result, scaler) = processed_result\n        result_processor(result, scaler=scaler)\n    return result_processor.get_result()",
    "completion": "Collect the results in writing queue and post to aggregator.\n\n    Args:\n        writing_queue (mp.manager.Queue): Queue from which collect results.\n        results_csv_path (str): CSV where we have to save results.\n        dev_mode (bool): Whether to run algorithm in development mode.\n\n    Returns:\n        bool: True on successful exit if `dev_mode` is set to False.\n\n    Note:\n        If `dev_mode` is set to true, then collector will just return all the\n        results in a list format.\n\n    END"
  },
  {
    "prompt": "def is_valid_result(result):\n    if not isinstance(result, dict):\n        return False\n    if not all([isinstance(x, six.integer_types) or isinstance(x, float) for x in six.itervalues(result)]):\n        return False\n    if not all([isinstance(x, six.string_types) for x in six.iterkeys(result)]):\n        return False\n    return True",
    "completion": "Check if result is valid.\n\n    Args:\n        result: Output of the algorithm.\n\n    Note:\n        Result is valid if it is a dict. All keys of the dict must be\n        be a string. All values must be numbers. These results are sent to\n        reducer which will sum, count, mean, median, mode of the values\n        belonging to same key.\n\n        Example:\n            - {\"alpha1\": 1, \"ant199\": 1, ..}\n\n    Returns:\n        bool: Specifying if the result is valid or not.\n\n    Todo:\n        * Define what is valid with privacy and other concerns\n\n    END"
  },
  {
    "prompt": "@pytest.fixture\ndef dbconn():\n    sqlite_db_file = '/tmp/test_qbo.db'\n    if os.path.exists(sqlite_db_file):\n        os.remove(sqlite_db_file)\n    conn = sqlite3.connect(sqlite_db_file, detect_types=sqlite3.PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES)\n    return conn",
    "completion": "\n    Make DB Connection\n    :return: DB Connection\n    END"
  },
  {
    "prompt": "def get_prediction_udf(model, method='predict', feature_type='numpy', names=None):\n    if not _is_pyspark_installed():\n        raise ImportError('Module pyspark not found')\n    if not _is_pyarrow_installed():\n        raise ImportError('Module pyarrow not found')\n    from pyspark.sql import functions as F\n    from pyspark.sql.types import DoubleType, StringType, IntegerType, ArrayType\n    if method == 'predict':\n\n        def predict_func(*cols):\n            vals = _get_vals(*cols, feature_type=feature_type, names=names)\n            return pd.Series(model.predict(vals))\n        return_type = StringType() if isinstance(model.classes_[0], str) else IntegerType()\n        predict = F.pandas_udf(predict_func, returnType=return_type)\n    elif method == 'predict_proba':\n\n        def predict_func(*cols):\n            vals = _get_vals(*cols, feature_type=feature_type, names=names)\n            return pd.Series(list(model.predict_proba(vals)))\n        predict = F.pandas_udf(predict_func, returnType=ArrayType(DoubleType()))\n    else:\n        raise ValueError('Unknown method: {0}'.format(method))\n    return predict",
    "completion": "\n    Build a vectorized PySpark UDF to apply a sklearn model's `predict` or\n    `predict_proba` methods columns in a PySpark DataFrame. Handles\n    flexible types of feature data for prediction including 2-D numpy\n    arrays ('numpy'), single field text data ('text') and mixed type\n    pandas DataFrames ('pandas'). The UDF can then be applied as shown in the\n    example below.\n\n    NOTE: This function requires pyarrow and pyspark with appropriate\n    versions for vectorized pandas UDFs and appropriate spark configuration\n    to use pyarrow. Ths requires pyarrow>=0.8.0 and pyspark>=2.3.0.\n    Additionally, the spark version must be 2.3 or higher. These requirements\n    are not enforced by the sk-dist package at setup time.\n\n    Args:\n        model (sklearn Estimator): sklearn model to distribute\n            predictions with PySpark\n        method (str): name of prediction method; either 'predict'\n            or 'predict_proba'\n        feature_type (str): name of feature type; either 'numpy',\n            'pandas' or 'text'\n        names (array-like): list of ordered column names\n            (only necessary for 'pandas' feature_type\n    Returns:\n        PySpark pandas UDF (pyspark.sql.functions.pandas_udf)\n    Example:\n    >>> import pandas as pd\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from pyspark.sql import SparkSession\n    >>> spark = (\n    >>>     SparkSession\n    >>>     .builder\n    >>>     .getOrCreate()\n    >>>     )\n    >>> data = load_digits()\n    >>> X = data[\"data\"]\n    >>> y = data[\"target\"]\n    >>> model = LogisticRegression()\n    >>> model.fit(X, y)\n    >>> predict = get_prediction_udf(model, method=\"predict\")\n    >>> predict_proba = get_prediction_udf(model, method=\"predict_proba\")\n    >>> pdf = pd.DataFrame(X)\n    >>> sdf = spark.createDataFrame(pdf)\n    >>> cols = [F.col(str(c)) for c in sdf.columns]\n    >>> prediction_df = (\n    >>>     sdf\n    >>>     .withColumn(\"scores\", predict_proba(*cols))\n    >>>     .withColumn(\"preds\", predict(*cols))\n    >>>     .select(\"preds\", \"scores\")\n    >>>     )\n    >>> prediction_df.show()\n    ... +-----+--------------------+\n    ... |preds|              scores|\n    ... +-----+--------------------+\n    ... |    0|[0.99988026795692...|\n    ... |    1|[4.75035277837040...|\n    ... |    2|[2.94811218592164...|\n    ... |    3|[1.63438595023762...|\n    ... |    4|[1.11339868338047...|\n    ... |    5|[1.47300432716012...|\n    ... |    6|[1.08560009259480...|\n    ... |    7|[3.02428232165044...|\n    ... |    8|[7.65445972596079...|\n    ... |    9|[3.97610488897298...|\n    ... |    0|[0.99918670844137...|\n    ... |    1|[2.65336456879078...|\n    ... |    2|[1.85886361541580...|\n    ... |    3|[2.89824009324990...|\n    ... |    4|[2.84813979824305...|\n    ... |    5|[2.70090567992820...|\n    ... |    6|[1.10907772018062...|\n    ... |    7|[3.06455862370095...|\n    ... |    8|[2.38739344440480...|\n    ... |    9|[8.23628591704589...|\n    ... +-----+--------------------+\n    ... only showing top 20 rows\n    END"
  },
  {
    "prompt": "def StepIsSupportedForMaster(step_name, master_name):\n    if not MasterIsSupported(master_name):\n        return False\n    steps_for_masters_rules = GetStepsForMastersRules()\n    supported_masters = steps_for_masters_rules['supported_masters']\n    supported_master = supported_masters[master_name]\n    check_global = supported_master.get('check_global', True)\n    if not check_global:\n        supported_steps = supported_master['supported_steps']\n        return step_name in supported_steps\n    supported_steps = supported_master.get('supported_steps', [])\n    unsupported_steps = supported_master.get('unsupported_steps', [])\n    global_unsupported_steps = steps_for_masters_rules['global'].get('unsupported_steps', [])\n    return step_name in supported_steps or (step_name not in unsupported_steps and step_name not in global_unsupported_steps)",
    "completion": "Determines whether or not a step is supported for the given build master.\n\n  Args:\n    step_name: The name of the step to check.\n    master_name: The name of the build master to check.\n\n  Returns:\n    True if Findit supports analyzing the failure, False otherwise.\n    Rules:\n      1. If a master is not supported, then neither are any of its steps.\n      2. If a master specifies check_global = True, then all of its steps are\n         supported except those according to those blacklisted under global.\n      3. If a master specifies check_global = True, but also specifies a\n         supported_steps, then supported_steps is to override any blacklisted\n         steps under global.\n      4. If a master specifies check_global = True, but also species its own\n         unsupported_list, those unsupported_steps are in addition to those\n         under global.\n      5. If a master specifies check_global = False, then all steps under\n         'supported_steps' are always supported and nothing else.\n         'unsupported_steps' is not allowed.\n  END"
  },
  {
    "prompt": "def ShouldSkipTestTryJobs(wf_mastername, wf_buildername):\n    trybot_config = FinditConfig.Get().builders_to_trybots.get(wf_mastername, {}).get(wf_buildername, {})\n    return trybot_config.get('not_run_tests', False)",
    "completion": "Returns True if test try jobs should be triggered.\n\n    By default, test try jobs should be supported unless the master/builder\n    specifies to bail out.\n\n  Args:\n    wf_mastername: The mastername of a waterfall builder.\n    wf_buildername: The buildername of a waterfall builder.\n\n  Returns:\n    True if test try jobs are to be skipped, False otherwise.\n  END"
  },
  {
    "prompt": "def recvjson(sock):\n    size = struct.unpack('<i', recvall(sock, 4))[0]\n    data = json.loads(py_str(recvall(sock, size)))\n    return data",
    "completion": "receive python value from remote via json\n\n    Parameters\n    ----------\n    sock : Socket\n        The socket\n\n    Returns\n    -------\n    value : object\n        The value received.\n    END"
  },
  {
    "prompt": "def random_key(prefix, cmap=None):\n    if cmap:\n        while True:\n            key = prefix + str(random.random())\n            if key not in cmap:\n                return key\n    else:\n        return prefix + str(random.random())",
    "completion": "Generate a random key\n\n    Parameters\n    ----------\n    prefix : str\n        The string prefix\n\n    cmap : dict\n        Conflict map\n\n    Returns\n    -------\n    key : str\n        The generated random key\n    END"
  },
  {
    "prompt": "def datetime_filter(t):\n    delta = int(time.time() - t)\n    if delta < 60:\n        return u'1\u5206\u949f\u524d'\n    if delta < 3600:\n        return u'%s\u5206\u949f\u524d' % (delta // 60)\n    if delta < 86400:\n        return u'%s\u5c0f\u65f6\u524d' % (delta // 3600)\n    if delta < 604800:\n        return u'%s\u5929\u524d' % (delta // 86400)\n    dt = datetime.fromtimestamp(t)\n    return u'%s\u5e74%s\u6708%s\u65e5' % (dt.year, dt.month, dt.day)",
    "completion": "\n     jinja2\u81ea\u5b9a\u4e49filter\uff0c\u6a21\u677f\u6587\u4ef6\u4e2d\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u65b9\u6cd5\u6765\u8f93\u51fa\u5185\u5bb9\n    :param t:\n    :return:\n    END"
  },
  {
    "prompt": "def get_package_version():\n    try:\n        return get_distribution(__name__.split('.')[0]).version\n    except DistributionNotFound:\n        return '0.0.0.dev'",
    "completion": "Get package version\n\n    Returns:\n         str: Installed package version, or 0.0.0.dev if not fully installed\n    END"
  },
  {
    "prompt": "def Server(host='127.0.0.1', porta=8585):\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    msg = f'[+]Voc\u00ea se conector ao {host}.'\n    s.bind((host, porta))\n    s.listen(1)\n    while True:\n        (c, e) = s.accept()\n        print('Conectado com ', e)\n        c.send(msg.encode('utf-8'))\n        c.close()",
    "completion": "\n        -> Servido TCP\n        :param host: Ip para o Servido\n        :param porta: Porta de Comunica\u00e7\u00e3o\n        :return: None\n        END"
  },
  {
    "prompt": "def weighted_avg(values, weights):\n    denom = sum([1 / w ** 2 for w in weights])\n    num = sum([1 / w ** 2 * v for (v, w) in zip(values, weights)])\n    return num / denom",
    "completion": "\n    Takes a list of values and a list of weights associated\n    with those values (index-to-index) and returns a weighted\n    averaged of those values as a float.\n\n    :param values: `list` of values to be averaged\n    :param weights: `list` of weights for each value (index-to-index)\n\n    :return: `float` The weighted average of the values\n    END"
  },
  {
    "prompt": "def K_separator_Watkins(x, rhol, rhog, horizontal=False, method='spline'):\n    factor = (1.0 - x) / x * sqrt(rhog / rhol)\n    if method == 'spline':\n        K = exp(float(splev(log(factor), tck_Watkins)))\n    elif method == 'blackwell':\n        X = log(factor)\n        A = -1.877478097\n        B = -0.81145804597\n        C = -0.1870744085\n        D = -0.0145228667\n        E = -0.00101148518\n        K = exp(A + X * (B + X * (C + X * (D + E * X))))\n    elif method == 'branan':\n        X = log(factor)\n        A = -1.942936\n        B = -0.814894\n        C = -0.17939\n        D = -0.012379\n        E = 0.000386235\n        F = 0.00025955\n        K = exp(A + X * (B + X * (C + X * (D + X * (E + F * X)))))\n    else:\n        raise ValueError(\"Only methods 'spline', 'branan', and 'blackwell' are supported.\")\n    K *= foot\n    if horizontal:\n        K *= 1.25\n    return K",
    "completion": "Calculates the Sounders-Brown `K` factor as used in determining maximum\n    allowable gas velocity in a two-phase separator in either a horizontal or\n    vertical orientation. This function approximates a graph published in [1]_\n    to determine `K` as used in the following equation:\n\n    .. math::\n        v_{max} =  K_{SB}\\sqrt{\\frac{\\rho_l-\\rho_g}{\\rho_g}}\n\n    The graph has `K_{SB}` on its y-axis, and the following as its x-axis:\n\n    .. math::\n        \\frac{m_l}{m_g}\\sqrt{\\rho_g/\\rho_l}\n        = \\frac{(1-x)}{x}\\sqrt{\\rho_g/\\rho_l}\n\n    Cubic spline interpolation is the default method of retrieving a value\n    from the graph, which was digitized with Engauge-Digitizer.\n\n    Also supported are two published curve fits to the graph. The first is that\n    of Blackwell (1984) [2]_, as follows:\n\n    .. math::\n        K_{SB} = \\exp(-1.942936 -0.814894X -0.179390 X^2 -0.0123790 X^3\n        + 0.000386235 X^4 + 0.000259550 X^5)\n\n        X = \\ln\\left[\\frac{(1-x)}{x}\\sqrt{\\rho_g/\\rho_l}\\right]\n\n    The second is that of Branan (1999), as follows:\n\n    .. math::\n        K_{SB} = \\exp(-1.877478097 -0.81145804597X -0.1870744085 X^2\n        -0.0145228667 X^3 -0.00101148518 X^4)\n\n        X = \\ln\\left[\\frac{(1-x)}{x}\\sqrt{\\rho_g/\\rho_l}\\right]\n\n    Parameters\n    ----------\n    x : float\n        Quality of fluid entering separator, [-]\n    rhol : float\n        Density of liquid phase [kg/m^3]\n    rhog : float\n        Density of gas phase [kg/m^3]\n    horizontal : bool, optional\n        Whether to use the vertical or horizontal value; horizontal is 1.25\n        higher\n    method : str\n        One of 'spline, 'blackwell', or 'branan'\n\n    Returns\n    -------\n    K : float\n        Sounders Brown horizontal or vertical `K` factor for two-phase\n        separator design only, [m/s]\n\n    Notes\n    -----\n    Both the 'branan' and 'blackwell' models are used frequently. However,\n    the spline is much more accurate.\n\n    No limits checking is enforced. However, the x-axis spans only 0.006 to\n    5.4, and the function should not be used outside those limits.\n\n    Examples\n    --------\n    >>> K_separator_Watkins(0.88, 985.4, 1.3, horizontal=True)\n    0.07951613600476297\n\n    References\n    ----------\n    .. [1] Watkins (1967). Sizing Separators and Accumulators, Hydrocarbon\n       Processing, November 1967.\n    .. [2] Blackwell, W. Wayne. Chemical Process Design on a Programmable\n       Calculator. New York: Mcgraw-Hill, 1984.\n    .. [3] Branan, Carl R. Pocket Guide to Chemical Engineering. 1st edition.\n       Houston, Tex: Gulf Professional Publishing, 1999.\n    END"
  },
  {
    "prompt": "def K_separator_demister_York(P, horizontal=False):\n    P = P / psi\n    if P < 15:\n        if P < 1:\n            P = 1\n        K = 0.1821 + 0.0029 * P + 0.046 * log(P)\n    elif P < 40:\n        K = 0.35\n    else:\n        if P > 5500:\n            P = 5500\n        K = 0.43 - 0.023 * log(P)\n    K *= foot\n    if horizontal:\n        K *= 1.25\n    return K",
    "completion": "Calculates the Sounders Brown `K` factor as used in determining maximum\n    permissible gas velocity in a two-phase separator in either a horizontal or\n    vertical orientation, *with a demister*.\n    This function is a curve fit to [1]_ published in [2]_ and is widely used.\n\n    For 1 < P < 15 psia:\n\n    .. math::\n        K = 0.1821 + 0.0029P + 0.0460\\ln P\n\n    For 15 <= P <= 40 psia:\n\n    .. math::\n        K = 0.35\n\n    For P < 5500 psia:\n\n    .. math::\n        K = 0.430 - 0.023\\ln P\n\n    In the above equations, P is in units of psia.\n\n    Parameters\n    ----------\n    P : float\n        Pressure of separator, [Pa]\n    horizontal : bool, optional\n        Whether to use the vertical or horizontal value; horizontal is 1.25\n        times higher, [-]\n\n    Returns\n    -------\n    K : float\n        Sounders Brown Horizontal or vertical `K` factor for two-phase\n        separator design with a demister, [m/s]\n\n    Notes\n    -----\n    If the input pressure is under 1 psia, 1 psia is used. If the\n    input pressure is over 5500 psia, 5500 psia is used.\n\n    Examples\n    --------\n    >>> K_separator_demister_York(975*psi)\n    0.08281536035331669\n\n    References\n    ----------\n    .. [2] Otto H. York Company, \"Mist Elimination in Gas Treatment Plants and\n       Refineries,\" Engineering, Parsippany, NJ.\n    .. [1] Svrcek, W. Y., and W. D. Monnery. \"Design Two-Phase Separators\n       within the Right Limits\" Chemical Engineering Progress, (October 1,\n       1993): 53-60.\n    END"
  },
  {
    "prompt": "def v_Sounders_Brown(K, rhol, rhog):\n    return K * sqrt((rhol - rhog) / rhog)",
    "completion": "Calculates the maximum allowable vapor velocity in a two-phase\n    separator to permit separation between entrained droplets and the gas\n    using an empirical `K` factor, named after Sounders and Brown [1]_.\n    This is a simplifying expression for terminal velocity and drag on\n    particles.\n\n    .. math::\n        v_{max} =  K_{SB} \\sqrt{\\frac{\\rho_l-\\rho_g}{\\rho_g}}\n\n    Parameters\n    ----------\n    K : float\n        Sounders Brown `K` factor for two-phase separator design, [m/s]\n    rhol : float\n        Density of liquid phase [kg/m^3]\n    rhog : float\n        Density of gas phase [kg/m^3]\n\n    Returns\n    -------\n    v_max : float\n        Maximum allowable vapor velocity in a two-phase separator to permit\n        separation between entrained droplets and the gas, [m/s]\n\n    Notes\n    -----\n    The Sounders Brown K factor is related to the terminal velocity as shown in\n    the following expression.\n\n    .. math::\n        v_{term} = v_{max} = \\sqrt{\\frac{4 g d_p (\\rho_p-\\rho_f)}{3 C_D \\rho_f }}\n\n        v_{term} = \\sqrt{\\frac{(\\rho_p-\\rho_f)}{\\rho_f}} \\sqrt{\\frac{4 g d_p}{3 C_D}}\n\n        v_{term} = K_{SB} \\sqrt{\\frac{4 g d_p}{3 C_D}}\n\n    Note this form corresponds to the Newton's law range (Re > 500), but in\n    reality droplets are normally in the intermediate or Stoke's law region\n    [2]_. For this reason using the drag coefficient expression directly is\n    cleaner, but identical results can be found with the Sounders Brown\n    equation.\n\n    Examples\n    --------\n    >>> v_Sounders_Brown(K=0.08, rhol=985.4, rhog=1.3)\n    2.2010906387516167\n\n    References\n    ----------\n    .. [1] Souders, Mott., and George Granger. Brown. \"Design of Fractionating\n       Columns I. Entrainment and Capacity.\" Industrial & Engineering Chemistry\n       26, no. 1 (January 1, 1934): 98-103. https://doi.org/10.1021/ie50289a025.\n    .. [2] Vasude, Gael D. Ulrich and Palligarnai T. Chemical Engineering\n       Process Design and Economics\u202f: A Practical Guide. 2nd edition. Durham,\n       N.H: Process Publishing, 2004.\n    END"
  },
  {
    "prompt": "def K_Sounders_Brown_theoretical(D, Cd, g=g):\n    return sqrt(4.0 / 3.0 * g * D / Cd)",
    "completion": "Converts a known drag coefficient into a Sounders-Brown `K` factor\n    for two-phase separator design. This factor is the traditional way for\n    separator diameters to be obtained although it is unnecessary and the\n    theoretical drag coefficient method can be used instead.\n\n    .. math::\n        K_{SB} = \\sqrt{\\frac{(\\rho_p-\\rho_f)}{\\rho_f}}\n        = \\sqrt{\\frac{4 g d_p}{3 C_D}}\n\n    Parameters\n    ----------\n    D : float\n        Design diameter of the droplets, [m]\n    Cd : float\n        Drag coefficient [-]\n    g : float, optional\n        Acceleration due to gravity, [m/s^2]\n\n    Returns\n    -------\n    K : float\n        Sounders Brown `K` factor for two-phase separator design, [m/s]\n\n    Notes\n    -----\n    Drag coefficient is a function of velocity; so iteration is needed to\n    obtain the most correct answer. The following example shows the use of\n    iteration to obtain the final velocity:\n\n    >>> from fluids import *\n    >>> V = 2.0\n    >>> D = 150E-6\n    >>> rho = 1.3\n    >>> rhol = 700.\n    >>> mu = 1E-5\n    >>> for i in range(10):\n    ...     Re = Reynolds(V=V, rho=rho, mu=mu, D=D)\n    ...     Cd = drag_sphere(Re)\n    ...     K = K_Sounders_Brown_theoretical(D=D, Cd=Cd)\n    ...     V = v_Sounders_Brown(K, rhol=rhol, rhog=rho)\n    ...     print('%.14f' %V)\n    0.76093307417658\n    0.56242939340131\n    0.50732895050696\n    0.48957142095508\n    0.48356021946899\n    0.48149076033622\n    0.48077414934614\n    0.48052549959141\n    0.48043916249756\n    0.48040917690193\n\n    The use of Sounders-Brown constants can be replaced as follows (the\n    v_terminal method includes its own solver for terminal velocity):\n\n    >>> from fluids.drag import v_terminal\n    >>> v_terminal(D=D, rhop=rhol, rho=rho, mu=mu)\n    0.4803932186998\n\n    Examples\n    --------\n    >>> K_Sounders_Brown_theoretical(D=150E-6, Cd=0.5)\n    0.06263114241333939\n\n    References\n    ----------\n    .. [1] Svrcek, W. Y., and W. D. Monnery. \"Design Two-Phase Separators\n       within the Right Limits\" Chemical Engineering Progress, (October 1,\n       1993): 53-60.\n    END"
  },
  {
    "prompt": "def import_key(encoded, passphrase=None):\n    encoded = tobytes(encoded)\n    if passphrase is not None:\n        passphrase = tobytes(passphrase)\n    if encoded.startswith(b'-----'):\n        (der_encoded, marker, enc_flag) = PEM.decode(tostr(encoded), passphrase)\n        if enc_flag:\n            passphrase = None\n        try:\n            result = _import_der(der_encoded, passphrase)\n        except UnsupportedEccFeature as uef:\n            raise uef\n        except ValueError:\n            raise ValueError('Invalid DER encoding inside the PEM file')\n        return result\n    if encoded.startswith(b'ecdsa-sha2-'):\n        return _import_openssh(encoded)\n    if bord(encoded[0]) == 48:\n        return _import_der(encoded, passphrase)\n    raise ValueError('ECC key format is not supported')",
    "completion": "Import an ECC key (public or private).\n\n    Args:\n      encoded (bytes or multi-line string):\n        The ECC key to import.\n\n        An ECC **public** key can be:\n\n        - An X.509 certificate, binary (DER) or ASCII (PEM)\n        - An X.509 ``subjectPublicKeyInfo``, binary (DER) or ASCII (PEM)\n        - An OpenSSH line (e.g. the content of ``~/.ssh/id_ecdsa``, ASCII)\n\n        An ECC **private** key can be:\n\n        - In binary format (DER, see section 3 of `RFC5915`_ or `PKCS#8`_)\n        - In ASCII format (PEM or OpenSSH)\n\n        Private keys can be in the clear or password-protected.\n\n        For details about the PEM encoding, see `RFC1421`_/`RFC1423`_.\n\n      passphrase (byte string):\n        The passphrase to use for decrypting a private key.\n        Encryption may be applied protected at the PEM level or at the PKCS#8 level.\n        This parameter is ignored if the key in input is not encrypted.\n\n    Returns:\n      :class:`EccKey` : a new ECC key object\n\n    Raises:\n      ValueError: when the given key cannot be parsed (possibly because\n        the pass phrase is wrong).\n\n    .. _RFC1421: http://www.ietf.org/rfc/rfc1421.txt\n    .. _RFC1423: http://www.ietf.org/rfc/rfc1423.txt\n    .. _RFC5915: http://www.ietf.org/rfc/rfc5915.txt\n    .. _`PKCS#8`: http://www.ietf.org/rfc/rfc5208.txt\n    END"
  },
  {
    "prompt": "def normalizeData(data, min=0, max=1):\n    import numpy as np\n    data = np.array(data)\n    new_data = data.copy()\n    max_value = data.max()\n    min_value = data.min()\n    diff_pp = max_value - min_value\n    diff_new_pp = max - min\n    new_data = new_data - min_value\n    new_data = new_data / diff_pp\n    new_data = new_data * diff_new_pp\n    new_data = new_data + min\n    return new_data",
    "completion": "\n    Its almost a map function. This function normalize the data between a\n    min and max values.\n\n    Parameters\n    ----------\n\n    data: list or array-like\n        The values that desire normalize.\n    min: int or float\n        Optional, is -1 by default. The min value correspond, in the end,\n        of the min value of data.\n    max: int or float\n        Optional, is 1 by default. The max value correspond, in the end,\n        of the max value of data.\n\n    Returns\n    -------\n    numpy.array:\n        The data normalized between min and max values.\n\n    END"
  },
  {
    "prompt": "def load_from_arff(filename, labelcount, endian='big', input_feature_type='float', encode_nominal=True, load_sparse=False, return_attribute_definitions=False):\n    matrix = None\n    if not load_sparse:\n        arff_frame = arff.load(open(filename, 'r'), encode_nominal=encode_nominal, return_type=arff.DENSE)\n        try:\n            matrix = sparse.csr_matrix(arff_frame['data'], dtype=input_feature_type)\n        except:\n            print(arff_frame['data'])\n    else:\n        arff_frame = arff.load(open(filename, 'r'), encode_nominal=encode_nominal, return_type=arff.COO)\n        data = arff_frame['data'][0]\n        row = arff_frame['data'][1]\n        col = arff_frame['data'][2]\n        matrix = sparse.coo_matrix((data, (row, col)), shape=(max(row) + 1, max(col) + 1))\n    (X, y) = (None, None)\n    if endian == 'big':\n        (X, y) = (matrix.tocsc()[:, labelcount:].tolil(), matrix.tocsc()[:, :labelcount].astype(int).tolil())\n    elif endian == 'little':\n        (X, y) = (matrix.tocsc()[:, :-labelcount].tolil(), matrix.tocsc()[:, -labelcount:].astype(int).tolil())\n    else:\n        return None\n    if return_attribute_definitions:\n        return (X, y, arff_frame['attributes'])\n    else:\n        return (X, y)",
    "completion": "Method for loading ARFF files as numpy array\n    Parameters\n    ----------\n    filename : str\n        path to ARFF file\n    labelcount: integer\n        number of labels in the ARFF file\n    endian: str {\"big\", \"little\"} (default is \"big\")\n        whether the ARFF file contains labels at the beginning of the\n        attributes list (\"big\" endianness, MEKA format) or at the end\n        (\"little\" endianness, MULAN format)\n    input_feature_type: numpy.type as string (default is \"float\")\n        the desire type of the contents of the return 'X' array-likes,\n        default 'i8', should be a numpy type,\n        see http://docs.scipy.org/doc/numpy/user/basics.types.html\n    encode_nominal: bool (default is True)\n        whether convert categorical data into numeric factors - required\n        for some scikit classifiers that can't handle non-numeric\n        input features.\n    load_sparse: boolean (default is False)\n        whether to read arff file as a sparse file format, liac-arff\n        breaks if sparse reading is enabled for non-sparse ARFFs.\n    return_attribute_definitions: boolean (default is False)\n        whether to return the definitions for each attribute in the\n        dataset\n    Returns\n    -------\n    X : scipy.sparse\n        matrix with :code:`input_feature_type` elements\n    y: scipy.sparse\n        matrix of binary label indicator matrix\n    END"
  },
  {
    "prompt": "def generate_random_graph(n_nodes, n_edges, namespace='NS'):\n    graph = BELGraph()\n    nodes = [protein(namespace=namespace, name=str(i)) for i in range(1, n_nodes)]\n    edges = list(itt.combinations(nodes, r=2))\n    edge_sample = random.sample(edges, n_edges)\n    for (u, v) in edge_sample:\n        graph.add_increases(u, v, citation=n(), evidence=n())\n    return graph",
    "completion": "Generate a sub-graph with random nodes and edges.\n\n    :param int n_nodes: Number of nodes to make\n    :param int n_edges: Number of edges to make\n    :param str namespace: The namespace of the nodes to use\n    :rtype: pybel.BELGraph\n    END"
  },
  {
    "prompt": "def build_sac_model_and_action_dist(policy: Policy, obs_space: gym.spaces.Space, action_space: gym.spaces.Space, config: TrainerConfigDict) -> Tuple[ModelV2, Type[TorchDistributionWrapper]]:\n    model = build_sac_model(policy, obs_space, action_space, config)\n    action_dist_class = _get_dist_class(config, action_space)\n    return (model, action_dist_class)",
    "completion": "Constructs the necessary ModelV2 and action dist class for the Policy.\n\n    Args:\n        policy (Policy): The TFPolicy that will use the models.\n        obs_space (gym.spaces.Space): The observation space.\n        action_space (gym.spaces.Space): The action space.\n        config (TrainerConfigDict): The SAC trainer's config dict.\n\n    Returns:\n        ModelV2: The ModelV2 to be used by the Policy. Note: An additional\n            target model will be created in this function and assigned to\n            `policy.target_model`.\n    END"
  },
  {
    "prompt": "def _get_dist_class(config: TrainerConfigDict, action_space: gym.spaces.Space) -> Type[TorchDistributionWrapper]:\n    if isinstance(action_space, Discrete):\n        return TorchCategorical\n    elif isinstance(action_space, Simplex):\n        return TorchDirichlet\n    elif config['normalize_actions']:\n        return TorchSquashedGaussian if not config['_use_beta_distribution'] else TorchBeta\n    else:\n        return TorchDiagGaussian",
    "completion": "Helper function to return a dist class based on config and action space.\n\n    Args:\n        config (TrainerConfigDict): The Trainer's config dict.\n        action_space (gym.spaces.Space): The action space used.\n\n    Returns:\n        Type[TFActionDistribution]: A TF distribution class.\n    END"
  },
  {
    "prompt": "def stats(policy: Policy, train_batch: SampleBatch) -> Dict[str, TensorType]:\n    return {'td_error': policy.td_error, 'mean_td_error': torch.mean(policy.td_error), 'actor_loss': torch.mean(policy.actor_loss), 'critic_loss': torch.mean(torch.stack(policy.critic_loss)), 'alpha_loss': torch.mean(policy.alpha_loss), 'alpha_value': torch.mean(policy.alpha_value), 'log_alpha_value': torch.mean(policy.log_alpha_value), 'target_entropy': policy.target_entropy, 'policy_t': torch.mean(policy.policy_t), 'mean_q': torch.mean(policy.q_t), 'max_q': torch.max(policy.q_t), 'min_q': torch.min(policy.q_t)}",
    "completion": "Stats function for SAC. Returns a dict with important loss stats.\n\n    Args:\n        policy (Policy): The Policy to generate stats for.\n        train_batch (SampleBatch): The SampleBatch (already) used for training.\n\n    Returns:\n        Dict[str, TensorType]: The stats dict.\n    END"
  },
  {
    "prompt": "def optimizer_fn(policy: Policy, config: TrainerConfigDict) -> Tuple[LocalOptimizer]:\n    policy.actor_optim = torch.optim.Adam(params=policy.model.policy_variables(), lr=config['optimization']['actor_learning_rate'], eps=1e-07)\n    critic_split = len(policy.model.q_variables())\n    if config['twin_q']:\n        critic_split //= 2\n    policy.critic_optims = [torch.optim.Adam(params=policy.model.q_variables()[:critic_split], lr=config['optimization']['critic_learning_rate'], eps=1e-07)]\n    if config['twin_q']:\n        policy.critic_optims.append(torch.optim.Adam(params=policy.model.q_variables()[critic_split:], lr=config['optimization']['critic_learning_rate'], eps=1e-07))\n    policy.alpha_optim = torch.optim.Adam(params=[policy.model.log_alpha], lr=config['optimization']['entropy_learning_rate'], eps=1e-07)\n    return tuple([policy.actor_optim] + policy.critic_optims + [policy.alpha_optim])",
    "completion": "Creates all necessary optimizers for SAC learning.\n\n    The 3 or 4 (twin_q=True) optimizers returned here correspond to the\n    number of loss terms returned by the loss function.\n\n    Args:\n        policy (Policy): The policy object to be trained.\n        config (TrainerConfigDict): The Trainer's config dict.\n\n    Returns:\n        Tuple[LocalOptimizer]: The local optimizers to use for policy training.\n    END"
  },
  {
    "prompt": "def plot_param_importances(study: Study, evaluator: BaseImportanceEvaluator=None, params: Optional[List[str]]=None) -> 'go.Figure':\n    _imports.check()\n    layout = go.Layout(title='Hyperparameter Importances', xaxis={'title': 'Importance'}, yaxis={'title': 'Hyperparameter'}, showlegend=False)\n    trials = [trial for trial in study.trials if trial.state == TrialState.COMPLETE]\n    if len(trials) == 0:\n        logger.warning('Study instance does not contain completed trials.')\n        return go.Figure(data=[], layout=layout)\n    importances = optuna.importance.get_param_importances(study, evaluator=evaluator, params=params)\n    importances = OrderedDict(reversed(list(importances.items())))\n    importance_values = list(importances.values())\n    param_names = list(importances.keys())\n    fig = go.Figure(data=[go.Bar(x=importance_values, y=param_names, text=importance_values, texttemplate='%{text:.2f}', textposition='outside', cliponaxis=False, hovertemplate=[_make_hovertext(param_name, importance, study) for (param_name, importance) in importances.items()], marker_color=[_get_color(param_name, study) for param_name in param_names], orientation='h')], layout=layout)\n    return fig",
    "completion": "Plot hyperparameter importances.\n\n    Example:\n\n        The following code snippet shows how to plot hyperparameter importances.\n\n        .. testcode::\n\n            import optuna\n\n            def objective(trial):\n                x = trial.suggest_int(\"x\", 0, 2)\n                y = trial.suggest_float(\"y\", -1.0, 1.0)\n                z = trial.suggest_float(\"z\", 0.0, 1.5)\n                return x ** 2 + y ** 3 - z ** 4\n\n            study = optuna.create_study(sampler=optuna.samplers.RandomSampler())\n            study.optimize(objective, n_trials=100)\n\n            optuna.visualization.plot_param_importances(study)\n\n        .. raw:: html\n\n            <iframe src=\"../../_static/plot_param_importances.html\"\n             width=\"100%\" height=\"500px\" frameborder=\"0\">\n            </iframe>\n\n    .. seealso::\n\n        This function visualizes the results of :func:`optuna.importance.get_param_importances`.\n\n    Args:\n        study:\n            An optimized study.\n        evaluator:\n            An importance evaluator object that specifies which algorithm to base the importance\n            assessment on.\n            Defaults to\n            :class:`~optuna.importance._mean_decrease_impurity.MeanDecreaseImpurityImportanceEvaluator`.\n        params:\n            A list of names of parameters to assess.\n            If :obj:`None`, all parameters that are present in all of the completed trials are\n            assessed.\n\n    Returns:\n        A :class:`plotly.graph_objs.Figure` object.\n    END"
  },
  {
    "prompt": "def format_filters(params, filters, range_filters):\n    _filters = {}\n    query = []\n    for (k, v) in params.items():\n        if k not in filters:\n            continue\n        if k in range_filters:\n            match = re.match('[[{].+ TO .+[]}]', v)\n            if match:\n                query.append(f'+{range_filters[k]}:{v}')\n            continue\n        _filters[k] = v\n    return (_filters, ' '.join(query))",
    "completion": "Extract any special placeholder filter, such as ranges, from the query params.\n\n    https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html#_ranges\n\n    :param params: The filter query params\n    :param filters: The filter whitelist\n    :param range_filters: The range filter whitelist\n    :returns: filters, extracted placeholders\n    END"
  },
  {
    "prompt": "def run_fuzzers(config):\n    fuzz_target_runner = get_fuzz_target_runner(config)\n    if not fuzz_target_runner.initialize():\n        return RunFuzzersResult.ERROR\n    if not fuzz_target_runner.run_fuzz_targets():\n        return RunFuzzersResult.NO_BUG_FOUND\n    return RunFuzzersResult.BUG_FOUND",
    "completion": "Runs fuzzers for a specific OSS-Fuzz project.\n\n  Args:\n    config: A RunFuzzTargetsConfig.\n\n  Returns:\n    A RunFuzzersResult enum value indicating what happened during fuzzing.\n  END"
  },
  {
    "prompt": "def one_hot(labels, classes):\n    return np.eye(classes)[labels]",
    "completion": "\n    One Hot encode a vector.\n\n    Args:\n        labels (list):  List of labels to onehot encode\n        classes (int): Total number of categorical classes\n\n    Returns:\n        np.array: Matrix of one-hot encoded labels\n    END"
  },
  {
    "prompt": "def from_pymc3_predictions(predictions, posterior_trace: Optional[MultiTrace]=None, model: Optional[Model]=None, coords=None, dims=None, idata_orig: Optional[InferenceData]=None, inplace: bool=False) -> InferenceData:\n    if inplace and (not idata_orig):\n        raise ValueError('Do not pass True for inplace unless passingan existing InferenceData as idata_orig')\n    new_idata = PyMC3Converter(trace=posterior_trace, predictions=predictions, model=model, coords=coords, dims=dims).to_inference_data()\n    if idata_orig is None:\n        return new_idata\n    elif inplace:\n        concat([idata_orig, new_idata], dim=None, inplace=True)\n        return idata_orig\n    else:\n        concat([new_idata, idata_orig], dim=None, copy=True, inplace=True)\n        return new_idata",
    "completion": "Translate out-of-sample predictions into ``InferenceData``.\n\n    Parameters\n    ----------\n    predictions: Dict[str, np.ndarray]\n        The predictions are the return value of ``pymc3.sample_posterior_predictive``,\n        a dictionary of strings (variable names) to numpy ndarrays (draws).\n    posterior_trace: pm.MultiTrace\n        This should be a trace that has been thinned appropriately for\n        ``pymc3.sample_posterior_predictive``. Specifically, any variable whose shape is\n        a deterministic function of the shape of any predictor (explanatory, independent, etc.)\n        variables must be *removed* from this trace.\n    model: pymc3.Model\n        This argument is *not* optional, unlike in conventional uses of ``from_pymc3``.\n        The reason is that the posterior_trace argument is likely to supply an incorrect\n        value of model.\n    coords: Dict[str, array-like[Any]]\n        Coordinates for the variables.  Map from coordinate names to coordinate values.\n    dims: Dict[str, array-like[str]]\n        Map from variable name to ordered set of coordinate names.\n    idata_orig: InferenceData, optional\n        If supplied, then modify this inference data in place, adding ``predictions`` and\n        (if available) ``predictions_constant_data`` groups. If this is not supplied, make a\n        fresh InferenceData\n    inplace: boolean, optional\n        If idata_orig is supplied and inplace is True, merge the predictions into idata_orig,\n        rather than returning a fresh InferenceData object.\n\n    Returns\n    -------\n    InferenceData:\n        May be modified ``idata_orig``.\n    END"
  },
  {
    "prompt": "def repeated_checkpoint_run(tensor_dict, summary_dir, evaluators, batch_processor=None, checkpoint_dirs=None, variables_to_restore=None, restore_fn=None, num_batches=1, eval_interval_secs=120, max_number_of_evaluations=None, max_evaluation_global_step=None, master='', save_graph=False, save_graph_dir='', losses_dict=None, eval_export_path=None, process_metrics_fn=None):\n    if max_number_of_evaluations and max_number_of_evaluations <= 0:\n        raise ValueError('`max_number_of_evaluations` must be either None or a positive number.')\n    if max_evaluation_global_step and max_evaluation_global_step <= 0:\n        raise ValueError('`max_evaluation_global_step` must be either None or positive.')\n    if not checkpoint_dirs:\n        raise ValueError('`checkpoint_dirs` must have at least one entry.')\n    last_evaluated_model_path = None\n    number_of_evaluations = 0\n    while True:\n        start = time.time()\n        tf.logging.info('Starting evaluation at ' + time.strftime('%Y-%m-%d-%H:%M:%S', time.gmtime()))\n        model_path = tf.train.latest_checkpoint(checkpoint_dirs[0])\n        if not model_path:\n            tf.logging.info('No model found in %s. Will try again in %d seconds', checkpoint_dirs[0], eval_interval_secs)\n        elif model_path == last_evaluated_model_path:\n            tf.logging.info('Found already evaluated checkpoint. Will try again in %d seconds', eval_interval_secs)\n        else:\n            last_evaluated_model_path = model_path\n            (global_step, metrics) = _run_checkpoint_once(tensor_dict, evaluators, batch_processor, checkpoint_dirs, variables_to_restore, restore_fn, num_batches, master, save_graph, save_graph_dir, losses_dict=losses_dict, eval_export_path=eval_export_path, process_metrics_fn=process_metrics_fn)\n            write_metrics(metrics, global_step, summary_dir)\n            if max_evaluation_global_step and global_step >= max_evaluation_global_step:\n                tf.logging.info('Finished evaluation!')\n                break\n        number_of_evaluations += 1\n        if max_number_of_evaluations and number_of_evaluations >= max_number_of_evaluations:\n            tf.logging.info('Finished evaluation!')\n            break\n        time_to_next_eval = start + eval_interval_secs - time.time()\n        if time_to_next_eval > 0:\n            time.sleep(time_to_next_eval)\n    return metrics",
    "completion": "Periodically evaluates desired tensors using checkpoint_dirs or restore_fn.\n\n  This function repeatedly loads a checkpoint and evaluates a desired\n  set of tensors (provided by tensor_dict) and hands the resulting numpy\n  arrays to a function result_processor which can be used to further\n  process/save/visualize the results.\n\n  Args:\n    tensor_dict: a dictionary holding tensors representing a batch of detections\n      and corresponding groundtruth annotations.\n    summary_dir: a directory to write metrics summaries.\n    evaluators: a list of object of type DetectionEvaluator to be used for\n      evaluation. Note that the metric names produced by different evaluators\n      must be unique.\n    batch_processor: a function taking three arguments:\n      1. tensor_dict: the same tensor_dict that is passed in as the first\n        argument to this function.\n      2. sess: a tensorflow session\n      3. batch_index: an integer representing the index of the batch amongst\n        all batches\n      By default, batch_processor is None, which defaults to running:\n        return sess.run(tensor_dict)\n    checkpoint_dirs: list of directories to load into a DetectionModel or an\n      EnsembleModel if restore_fn isn't set. Also used to determine when to run\n      next evaluation. Must have at least one element.\n    variables_to_restore: None, or a dictionary mapping variable names found in\n      a checkpoint to model variables. The dictionary would normally be\n      generated by creating a tf.train.ExponentialMovingAverage object and\n      calling its variables_to_restore() method. Not used if restore_fn is set.\n    restore_fn: a function that takes a tf.Session object and correctly restores\n      all necessary variables from the correct checkpoint file.\n    num_batches: the number of batches to use for evaluation.\n    eval_interval_secs: the number of seconds between each evaluation run.\n    max_number_of_evaluations: the max number of iterations of the evaluation.\n      If the value is left as None the evaluation continues indefinitely.\n    max_evaluation_global_step: global step when evaluation stops.\n    master: the location of the Tensorflow session.\n    save_graph: whether or not the Tensorflow graph is saved as a pbtxt file.\n    save_graph_dir: where to save on disk the Tensorflow graph. If store_graph\n      is True this must be non-empty.\n    losses_dict: optional dictionary of scalar detection losses.\n    eval_export_path: Path for saving a json file that contains the detection\n      results in json format.\n    process_metrics_fn: a callback called with evaluation results after each\n      evaluation is done.  It could be used e.g. to back up checkpoints with\n      best evaluation scores, or to call an external system to update evaluation\n      results in order to drive best hyper-parameter search.  Parameters are:\n      int checkpoint_number, Dict[str, ObjectDetectionEvalMetrics] metrics,\n      str checkpoint_file path.\n\n  Returns:\n    metrics: A dictionary containing metric names and values in the latest\n      evaluation.\n\n  Raises:\n    ValueError: if max_num_of_evaluations is not None or a positive number.\n    ValueError: if checkpoint_dirs doesn't have at least one element.\n  END"
  },
  {
    "prompt": "def _resize_detection_masks(arg_tuple):\n    (detection_boxes, detection_masks, image_shape, pad_shape) = arg_tuple\n    detection_masks_reframed = ops.reframe_box_masks_to_image_masks(detection_masks, detection_boxes, image_shape[0], image_shape[1])\n    pad_instance_dim = tf.zeros([3, 1], dtype=tf.int32)\n    pad_hw_dim = tf.concat([tf.zeros([1], dtype=tf.int32), pad_shape - image_shape], axis=0)\n    pad_hw_dim = tf.expand_dims(pad_hw_dim, 1)\n    paddings = tf.concat([pad_instance_dim, pad_hw_dim], axis=1)\n    detection_masks_reframed = tf.pad(detection_masks_reframed, paddings)\n    if detection_masks_reframed.dtype == tf.float32:\n        detection_masks_reframed = tf.greater(detection_masks_reframed, 0.5)\n    return tf.cast(detection_masks_reframed, tf.uint8)",
    "completion": "Resizes detection masks.\n\n  Args:\n    arg_tuple: A (detection_boxes, detection_masks, image_shape, pad_shape)\n      tuple where\n      detection_boxes is a tf.float32 tensor of size [num_masks, 4] containing\n        the box corners. Row i contains [ymin, xmin, ymax, xmax] of the box\n        corresponding to mask i. Note that the box corners are in\n        normalized coordinates.\n      detection_masks is a tensor of size\n        [num_masks, mask_height, mask_width].\n      image_shape is a tensor of shape [2]\n      pad_shape is a tensor of shape [2] --- this is assumed to be greater\n        than or equal to image_shape along both dimensions and represents a\n        shape to-be-padded-to.\n\n  Returns:\n  END"
  },
  {
    "prompt": "def resize_detection_masks(detection_boxes, detection_masks, original_image_spatial_shapes):\n    max_spatial_shape = tf.reduce_max(original_image_spatial_shapes, axis=0, keep_dims=True)\n    tiled_max_spatial_shape = tf.tile(max_spatial_shape, multiples=[tf.shape(original_image_spatial_shapes)[0], 1])\n    return shape_utils.static_or_dynamic_map_fn(_resize_detection_masks, elems=[detection_boxes, detection_masks, original_image_spatial_shapes, tiled_max_spatial_shape], dtype=tf.uint8)",
    "completion": "Resizes per-box detection masks to be relative to the entire image.\n\n  Note that this function only works when the spatial size of all images in\n  the batch is the same. If not, this function should be used with batch_size=1.\n\n  Args:\n    detection_boxes: A [batch_size, num_instances, 4] float tensor containing\n      bounding boxes.\n    detection_masks: A [batch_size, num_instances, height, width] float tensor\n      containing binary instance masks per box.\n    original_image_spatial_shapes: a [batch_size, 3] shaped int tensor\n      holding the spatial dimensions of each image in the batch.\n  Returns:\n    masks: Masks resized to the spatial extents given by\n      (original_image_spatial_shapes[0, 0], original_image_spatial_shapes[0, 1])\n  END"
  },
  {
    "prompt": "def result_dict_for_single_example(image, key, detections, groundtruth=None, class_agnostic=False, scale_to_absolute=False):\n    if groundtruth:\n        max_gt_boxes = tf.shape(groundtruth[fields.InputDataFields.groundtruth_boxes])[0]\n        for gt_key in groundtruth:\n            groundtruth[gt_key] = tf.expand_dims(groundtruth[gt_key], 0)\n    for detection_key in detections:\n        detections[detection_key] = tf.expand_dims(detections[detection_key][0], axis=0)\n    batched_output_dict = result_dict_for_batched_example(image, tf.expand_dims(key, 0), detections, groundtruth, class_agnostic, scale_to_absolute, max_gt_boxes=max_gt_boxes)\n    exclude_keys = [fields.InputDataFields.original_image, fields.DetectionResultFields.num_detections, fields.InputDataFields.num_groundtruth_boxes]\n    output_dict = {fields.InputDataFields.original_image: batched_output_dict[fields.InputDataFields.original_image]}\n    for key in batched_output_dict:\n        if key not in exclude_keys:\n            output_dict[key] = tf.squeeze(batched_output_dict[key], 0)\n    return output_dict",
    "completion": "Merges all detection and groundtruth information for a single example.\n\n  Note that evaluation tools require classes that are 1-indexed, and so this\n  function performs the offset. If `class_agnostic` is True, all output classes\n  have label 1.\n\n  Args:\n    image: A single 4D uint8 image tensor of shape [1, H, W, C].\n    key: A single string tensor identifying the image.\n    detections: A dictionary of detections, returned from\n      DetectionModel.postprocess().\n    groundtruth: (Optional) Dictionary of groundtruth items, with fields:\n      'groundtruth_boxes': [num_boxes, 4] float32 tensor of boxes, in\n        normalized coordinates.\n      'groundtruth_classes': [num_boxes] int64 tensor of 1-indexed classes.\n      'groundtruth_area': [num_boxes] float32 tensor of bbox area. (Optional)\n      'groundtruth_is_crowd': [num_boxes] int64 tensor. (Optional)\n      'groundtruth_difficult': [num_boxes] int64 tensor. (Optional)\n      'groundtruth_group_of': [num_boxes] int64 tensor. (Optional)\n      'groundtruth_instance_masks': 3D int64 tensor of instance masks\n        (Optional).\n      'groundtruth_keypoints': [num_boxes, num_keypoints, 2] float32 tensor with\n        keypoints (Optional).\n    class_agnostic: Boolean indicating whether the detections are class-agnostic\n      (i.e. binary). Default False.\n    scale_to_absolute: Boolean indicating whether boxes and keypoints should be\n      scaled to absolute coordinates. Note that for IoU based evaluations, it\n      does not matter whether boxes are expressed in absolute or relative\n      coordinates. Default False.\n\n  Returns:\n    A dictionary with:\n    'original_image': A [1, H, W, C] uint8 image tensor.\n    'key': A string tensor with image identifier.\n    'detection_boxes': [max_detections, 4] float32 tensor of boxes, in\n      normalized or absolute coordinates, depending on the value of\n      `scale_to_absolute`.\n    'detection_scores': [max_detections] float32 tensor of scores.\n    'detection_classes': [max_detections] int64 tensor of 1-indexed classes.\n    'detection_masks': [max_detections, H, W] float32 tensor of binarized\n      masks, reframed to full image masks.\n    'groundtruth_boxes': [num_boxes, 4] float32 tensor of boxes, in\n      normalized or absolute coordinates, depending on the value of\n      `scale_to_absolute`. (Optional)\n    'groundtruth_classes': [num_boxes] int64 tensor of 1-indexed classes.\n      (Optional)\n    'groundtruth_area': [num_boxes] float32 tensor of bbox area. (Optional)\n    'groundtruth_is_crowd': [num_boxes] int64 tensor. (Optional)\n    'groundtruth_difficult': [num_boxes] int64 tensor. (Optional)\n    'groundtruth_group_of': [num_boxes] int64 tensor. (Optional)\n    'groundtruth_instance_masks': 3D int64 tensor of instance masks\n      (Optional).\n    'groundtruth_keypoints': [num_boxes, num_keypoints, 2] float32 tensor with\n      keypoints (Optional).\n  END"
  },
  {
    "prompt": "def get_evaluators(eval_config, categories, evaluator_options=None):\n    evaluator_options = evaluator_options or {}\n    eval_metric_fn_keys = eval_config.metrics_set\n    if not eval_metric_fn_keys:\n        eval_metric_fn_keys = [EVAL_DEFAULT_METRIC]\n    evaluators_list = []\n    for eval_metric_fn_key in eval_metric_fn_keys:\n        if eval_metric_fn_key not in EVAL_METRICS_CLASS_DICT:\n            raise ValueError('Metric not found: {}'.format(eval_metric_fn_key))\n        kwargs_dict = evaluator_options[eval_metric_fn_key] if eval_metric_fn_key in evaluator_options else {}\n        evaluators_list.append(EVAL_METRICS_CLASS_DICT[eval_metric_fn_key](categories, **kwargs_dict))\n    if isinstance(eval_config, eval_pb2.EvalConfig):\n        parameterized_metrics = eval_config.parameterized_metric\n        for parameterized_metric in parameterized_metrics:\n            assert parameterized_metric.HasField('parameterized_metric')\n            if parameterized_metric.WhichOneof('parameterized_metric') == EVAL_KEYPOINT_METRIC:\n                keypoint_metrics = parameterized_metric.coco_keypoint_metrics\n                category_keypoints = {}\n                class_label = keypoint_metrics.class_label\n                category = None\n                for cat in categories:\n                    if cat['name'] == class_label:\n                        category = cat\n                        break\n                if not category:\n                    continue\n                keypoints_for_this_class = category['keypoints']\n                category_keypoints = [{'id': keypoints_for_this_class[kp_name], 'name': kp_name} for kp_name in keypoints_for_this_class]\n                evaluators_list.append(EVAL_METRICS_CLASS_DICT[EVAL_KEYPOINT_METRIC](category['id'], category_keypoints, class_label, keypoint_metrics.keypoint_label_to_sigmas))\n    return evaluators_list",
    "completion": "Returns the evaluator class according to eval_config, valid for categories.\n\n  Args:\n    eval_config: An `eval_pb2.EvalConfig`.\n    categories: A list of dicts, each of which has the following keys -\n        'id': (required) an integer id uniquely identifying this category.\n        'name': (required) string representing category name e.g., 'cat', 'dog'.\n        'keypoints': (optional) dict mapping this category's keypoints to unique\n          ids.\n    evaluator_options: A dictionary of metric names (see\n      EVAL_METRICS_CLASS_DICT) to `DetectionEvaluator` initialization\n      keyword arguments. For example:\n      evalator_options = {\n        'coco_detection_metrics': {'include_metrics_per_category': True}\n      }\n\n  Returns:\n    An list of instances of DetectionEvaluator.\n\n  Raises:\n    ValueError: if metric is not in the metric class dictionary.\n  END"
  },
  {
    "prompt": "def get_eval_metric_ops_for_evaluators(eval_config, categories, eval_dict):\n    eval_metric_ops = {}\n    evaluator_options = evaluator_options_from_eval_config(eval_config)\n    evaluators_list = get_evaluators(eval_config, categories, evaluator_options)\n    for evaluator in evaluators_list:\n        eval_metric_ops.update(evaluator.get_estimator_eval_metric_ops(eval_dict))\n    return eval_metric_ops",
    "completion": "Returns eval metrics ops to use with `tf.estimator.EstimatorSpec`.\n\n  Args:\n    eval_config: An `eval_pb2.EvalConfig`.\n    categories: A list of dicts, each of which has the following keys -\n        'id': (required) an integer id uniquely identifying this category.\n        'name': (required) string representing category name e.g., 'cat', 'dog'.\n    eval_dict: An evaluation dictionary, returned from\n      result_dict_for_single_example().\n\n  Returns:\n    A dictionary of metric names to tuple of value_op and update_op that can be\n    used as eval metric ops in tf.EstimatorSpec.\n  END"
  },
  {
    "prompt": "def evaluator_options_from_eval_config(eval_config):\n    eval_metric_fn_keys = eval_config.metrics_set\n    evaluator_options = {}\n    for eval_metric_fn_key in eval_metric_fn_keys:\n        if eval_metric_fn_key in ('coco_detection_metrics', 'coco_mask_metrics', 'lvis_mask_metrics'):\n            evaluator_options[eval_metric_fn_key] = {'include_metrics_per_category': eval_config.include_metrics_per_category}\n            if hasattr(eval_config, 'all_metrics_per_category') and eval_config.all_metrics_per_category:\n                evaluator_options[eval_metric_fn_key].update({'all_metrics_per_category': eval_config.all_metrics_per_category})\n            if eval_metric_fn_key == 'coco_detection_metrics' and hasattr(eval_config, 'skip_predictions_for_unlabeled_class'):\n                evaluator_options[eval_metric_fn_key].update({'skip_predictions_for_unlabeled_class': eval_config.skip_predictions_for_unlabeled_class})\n            for super_category in eval_config.super_categories:\n                if 'super_categories' not in evaluator_options[eval_metric_fn_key]:\n                    evaluator_options[eval_metric_fn_key]['super_categories'] = {}\n                key = super_category\n                value = eval_config.super_categories[key].split(',')\n                evaluator_options[eval_metric_fn_key]['super_categories'][key] = value\n            if eval_metric_fn_key == 'lvis_mask_metrics' and hasattr(eval_config, 'export_path'):\n                evaluator_options[eval_metric_fn_key].update({'export_path': eval_config.export_path})\n        elif eval_metric_fn_key == 'precision_at_recall_detection_metrics':\n            evaluator_options[eval_metric_fn_key] = {'recall_lower_bound': eval_config.recall_lower_bound, 'recall_upper_bound': eval_config.recall_upper_bound}\n    return evaluator_options",
    "completion": "Produces a dictionary of evaluation options for each eval metric.\n\n  Args:\n    eval_config: An `eval_pb2.EvalConfig`.\n\n  Returns:\n    evaluator_options: A dictionary of metric names (see\n      EVAL_METRICS_CLASS_DICT) to `DetectionEvaluator` initialization\n      keyword arguments. For example:\n      evalator_options = {\n        'coco_detection_metrics': {'include_metrics_per_category': True}\n      }\n  END"
  },
  {
    "prompt": "def _parse_pages(pages):\n    if pages:\n        if '-' in pages:\n            if not pages.count('-') == 2:\n                pages = re.sub('-+', '--', pages)\n        return pages\n    return None",
    "completion": "\n    Parse the page number to a 2-dashes format (e.g. 100--120).\n    :param pages: entry page numbers\n    :return:\n    END"
  },
  {
    "prompt": "def _print_field(field_name, field_value, capitals=False):\n    if field_value:\n        field_value = str(field_value).replace('_', '\\\\_')\n        field_value = str(field_value).replace('\\\\\\\\_', '\\\\_')\n        field_value = str(field_value).replace('#', '\\\\#')\n        field_value = str(field_value).replace('\\\\\\\\#', '\\\\#')\n        field_value = str(field_value).replace('$', '')\n        if capitals:\n            return '\\t%s = {{%s}},\\n' % (field_name, field_value)\n        else:\n            return '\\t%s = {%s},\\n' % (field_name, field_value)\n    return ''",
    "completion": "\n    Print a field in bib format if value is not none.\n    :param field_name: name of the field\n    :param field_value: value of the field\n    :param capitals: whether to add\n    :return: field in bib format or blank if field is None\n    END"
  },
  {
    "prompt": "def _merge_field(f1, f2, is_int=False):\n    if not f1 and (not f2):\n        return None\n    if not f2 and f1 is not None:\n        return f1\n    if not f1 and f2 is not None:\n        return f2\n    if is_int:\n        try:\n            if int(f1) >= int(f2):\n                return f1\n            else:\n                return f2\n        except ValueError:\n            pass\n    if len(f1) >= len(f2):\n        return f1\n    else:\n        return f2\n    return f1",
    "completion": "\n    Merge field contents from two entries.\n    :param f1: first field\n    :param f2: second field\n    :param is_int: whether the field is an integer\n    :return: merged field contents\n    END"
  },
  {
    "prompt": "def _merge_entry_type(t1, t2):\n    if t1 == EntryType.ARTICLE or t2 == EntryType.ARTICLE:\n        return EntryType.ARTICLE\n    if t1 == EntryType.INPROCEEDINGS or t2 == EntryType.INPROCEEDINGS:\n        return EntryType.INPROCEEDINGS\n    if t1 == EntryType.INCOLLECTION or t2 == EntryType.INCOLLECTION:\n        return EntryType.INCOLLECTION\n    if t1 == EntryType.PROCEEDINGS or t2 == EntryType.PROCEEDINGS:\n        return EntryType.PROCEEDINGS\n    if t1 == EntryType.BOOK or t2 == EntryType.BOOK:\n        return EntryType.BOOK\n    if t1 == EntryType.PHDTHESIS or t2 == EntryType.PHDTHESIS:\n        return EntryType.PHDTHESIS\n    if t1 == EntryType.MASTERTHESIS or t2 == EntryType.MASTERTHESIS:\n        return EntryType.MASTERTHESIS\n    if t1 == EntryType.TECHREPORT or t2 == EntryType.TECHREPORT:\n        return EntryType.TECHREPORT\n    return EntryType.MISC",
    "completion": "\n    Merge entry type field from two entries according to entry type level.\n    :param t1: first entry type\n    :param t2: second entry type\n    :return: merged entry type\n    END"
  },
  {
    "prompt": "def _toggle_selected(selected_data, value):\n    if value in selected_data:\n        selected_data.remove(value)\n    else:\n        selected_data.add(value)\n    return selected_data",
    "completion": "Add or remove value from the selected data set.\n\n    Parameters\n    ----------\n    selected_data : set\n        Set of selected data points to be modified.\n    value : int\n        Index of point to add or remove from selected data set.\n\n    Returns\n    -------\n    set\n        Modified selected_data set.\n    END"
  },
  {
    "prompt": "def frame_fix_badpix_isolated(array, bpm_mask=None, sigma_clip=3, num_neig=5, size=5, protect_mask=False, radius=30, verbose=True):\n    if array.ndim != 2:\n        raise TypeError('Array is not a 2d array or single frame')\n    if size % 2 == 0:\n        raise TypeError('Size of the median blur kernel must be an odd integer')\n    if bpm_mask is not None:\n        bpm_mask = bpm_mask.astype('bool')\n    if verbose:\n        start = time_ini()\n    if num_neig > 0:\n        neigh = True\n    else:\n        neigh = False\n    frame = array.copy()\n    (cy, cx) = frame_center(frame)\n    if bpm_mask is None:\n        ind = clip_array(frame, sigma_clip, sigma_clip, neighbor=neigh, num_neighbor=num_neig, mad=True)\n        bpm_mask = np.zeros_like(frame)\n        bpm_mask[ind] = 1\n        if protect_mask:\n            cir = circle(cy, cx, radius)\n            bpm_mask[cir] = 0\n        bpm_mask = bpm_mask.astype('bool')\n    smoothed = median_filter(frame, size, mode='mirror')\n    frame[np.where(bpm_mask)] = smoothed[np.where(bpm_mask)]\n    array_out = frame\n    count_bp = np.sum(bpm_mask)\n    if verbose:\n        msg = '/nDone replacing {} bad pixels using the median of neighbors'\n        print(msg.format(count_bp))\n        timing(start)\n    return array_out",
    "completion": " Corrects the bad pixels, marked in the bad pixel mask. The bad pixel is\n     replaced by the median of the adjacent pixels. This function is very fast\n     but works only with isolated (sparse) pixels.\n\n     Parameters\n     ----------\n     array : numpy ndarray\n         Input 2d array.\n     bpm_mask : numpy ndarray, optional\n         Input bad pixel map. Zeros frame where the bad pixels have a value of\n         1. If None is provided a bad pixel map will be created using\n         sigma clip statistics. \n     sigma_clip : int, optional\n         In case no bad pixel mask is provided all the pixels above and below\n         sigma_clip*STDDEV will be marked as bad.\n     num_neig : int, optional\n         The side of the square window around each pixel where the sigma clipped\n         statistics are calculated (STDDEV and MEDIAN). If the value is equal to\n         0 then the statistics are computed in the whole frame.\n     size : odd int, optional\n         The size the box (size x size) of adjacent pixels for the median\n         filter.\n     protect_mask : bool, optional\n         If True a circular aperture at the center of the frames will be\n         protected from any operation. With this we protect the star and\n         vicinity.\n     radius : int, optional\n         Radius of the circular aperture (at the center of the frames) for the\n         protection mask.\n     verbose : bool, optional\n         If True additional information will be printed.\n\n     Return\n     ------\n     frame : numpy ndarray\n         Frame with bad pixels corrected.\n     END"
  },
  {
    "prompt": "def cube_fix_badpix_isolated(array, bpm_mask=None, sigma_clip=3, num_neig=5, size=5, frame_by_frame=False, protect_mask=False, radius=30, verbose=True):\n    if array.ndim != 3:\n        raise TypeError('Array is not a 3d array or cube')\n    if size % 2 == 0:\n        raise TypeError('Size of the median blur kernel must be an odd integer')\n    if bpm_mask is not None:\n        bpm_mask = bpm_mask.astype('bool')\n    if verbose:\n        start = time_ini()\n    if num_neig > 0:\n        neigh = True\n    else:\n        neigh = False\n    (cy, cx) = frame_center(array[0])\n    array_out = array.copy()\n    n_frames = array.shape[0]\n    count_bp = 0\n    if frame_by_frame:\n        for i in Progressbar(range(n_frames), desc='processing frames'):\n            array_out[i] = frame_fix_badpix_isolated(array[i], bpm_mask=bpm_mask, sigma_clip=sigma_clip, num_neig=num_neig, size=size, protect_mask=protect_mask, radius=radius, verbose=False, debug=False)\n            if verbose:\n                bpm = np.where(array_out[i] != array[i])\n                count_bp += np.sum(np.ones_like(array_out[i])[bpm])\n    else:\n        if bpm_mask is None:\n            ind = clip_array(np.mean(array, axis=0), sigma_clip, sigma_clip, neighbor=neigh, num_neighbor=num_neig, mad=True)\n            bpm_mask = np.zeros_like(array[0])\n            bpm_mask[ind] = 1\n            if protect_mask:\n                cir = circle(cy, cx, radius)\n                bpm_mask[cir] = 0\n            bpm_mask = bpm_mask.astype('bool')\n        for i in Progressbar(range(n_frames), desc='processing frames'):\n            frame = array_out[i]\n            smoothed = median_filter(frame, size, mode='mirror')\n            frame[np.where(bpm_mask)] = smoothed[np.where(bpm_mask)]\n            if verbose:\n                count_bp += np.sum(bpm_mask)\n    if verbose:\n        msg = '/nDone replacing {} bad pixels using the median of neighbors'\n        print(msg.format(count_bp))\n        timing(start)\n    return array_out",
    "completion": " Corrects the bad pixels, marked in the bad pixel mask. The bad pixel is \n    replaced by the median of the adjacent pixels. This function is very fast\n    but works only with isolated (sparse) pixels. \n     \n    Parameters\n    ----------\n    array : numpy ndarray\n        Input 3d array.\n    bpm_mask : numpy ndarray, optional\n        Input bad pixel map. Zeros frame where the bad pixels have a value of 1.\n        If None is provided a bad pixel map will be created per frame using \n        sigma clip statistics.\n    sigma_clip : int, optional\n        In case no bad pixel mask is provided all the pixels above and below\n        sigma_clip*STDDEV will be marked as bad. \n    num_neig : int, optional\n        The side of the square window around each pixel where the sigma clipped\n        statistics are calculated (STDDEV and MEDIAN). If the value is equal to\n        0 then the statistics are computed in the whole frame.\n    size : odd int, optional\n        The size the box (size x size) of adjacent pixels for the median filter. \n    frame_by_frame: bool, optional\n        Whether to correct bad pixels frame by frame in the cube. By default it\n        is set to False; the bad pixels are computed on the mean frame of the \n        stack (faster but not necessarily optimal).\n    protect_mask : bool, optional\n        If True a circular aperture at the center of the frames will be \n        protected from any operation. With this we protect the star and its\n        vicinity.\n    radius : int, optional \n        Radius of the circular aperture (at the center of the frames) for the \n        protection mask.\n    verbose : bool, optional\n        If True additional information will be printed.\n    \n    Return\n    ------\n    array_out : numpy ndarray\n        Cube with bad pixels corrected.\n    END"
  },
  {
    "prompt": "def _validate_nep5_args(wallet, token_str, from_addr, to_addr, amount):\n    try:\n        token = PromptUtils.get_token(wallet, token_str)\n    except ValueError:\n        raise\n    if not isValidPublicAddress(from_addr):\n        raise ValueError('send_from is not a valid address')\n    if not isValidPublicAddress(to_addr):\n        raise ValueError('send_to is not a valid address')\n    try:\n        amount = amount_from_string(token, amount)\n    except Exception:\n        raise ValueError(f'{amount} is not a valid amount')\n    return token",
    "completion": "\n    A helper function to validate common arguments used in NEP-5 functions\n\n    Args:\n        wallet (Wallet): a UserWallet instance\n        token_str (str): symbol name or script_hash\n        from_addr (str): a wallet address\n        to_addr (str): a wallet address\n        amount (float): the number of tokens to send\n\n    Raises:\n        ValueError: for invalid arguments\n\n    Returns:\n        token (NEP5Token): instance\n    END"
  },
  {
    "prompt": "def token_send(wallet, token_str, from_addr, to_addr, amount, fee=Fixed8.Zero(), user_tx_attributes=None):\n    if not user_tx_attributes:\n        user_tx_attributes = []\n    try:\n        token = _validate_nep5_args(wallet, token_str, from_addr, to_addr, amount)\n    except ValueError:\n        raise\n    for attr in user_tx_attributes:\n        if not isinstance(attr, TransactionAttribute):\n            raise ValueError(f'{attr} is not a valid transaction attribute')\n    decimal_amount = amount_from_string(token, amount)\n    return do_token_transfer(token, wallet, from_addr, to_addr, decimal_amount, fee=fee, tx_attributes=user_tx_attributes)",
    "completion": "\n    Send `amount` of tokens from `from_addr` to `to_addr`\n\n    Args:\n        wallet (Wallet): a UserWallet instance\n        token_str (str): symbol name or script_hash\n        from_addr (str): a wallet address\n        to_addr (str): a wallet address\n        amount (float): the number of tokens to send\n        fee (Fixed8): (optional) a fee to give the transaction priority (> 0.001) \n        user_tx_attributes (list): a list of ``TransactionAttribute``s.\n\n    Raises:\n        ValueError: for invalid arguments\n\n    Returns:\n        a Transaction object if successful, False otherwise.\n    END"
  },
  {
    "prompt": "def test_token_send_from(wallet, token_str, from_addr, to_addr, amount):\n    try:\n        token = _validate_nep5_args(wallet, token_str, from_addr, to_addr, amount)\n        allowance = token_get_allowance(wallet, token_str, from_addr, to_addr, verbose=False)\n        if allowance < amount:\n            raise ValueError(f'Insufficient allowance: {allowance}')\n    except ValueError:\n        raise\n    (tx, fees, results) = token.TransferFrom(wallet, from_addr, to_addr, amount)\n    return (token, tx, fees, results)",
    "completion": "\n    Test sending funds from `addr_from` to `addr_to` without commiting to the network.\n\n    This does a local test to validate all supplied arguments and if the blockchain state allows for the transfer.\n\n    Args:\n        wallet (Wallet): a UserWallet instance\n        token_str (str): symbol name or script_hash\n        from_addr (str): a wallet address\n        to_addr (str): a wallet address\n        amount (float): the number of tokens to send\n\n    Raises:\n        ValueError: for invalid arguments or if allowance is insufficient.\n\n    Returns:\n        tuple:\n            token (NEP5Token): instance\n            InvocationTransaction: the transaction.\n            int: the transaction fee.\n            list: the neo VM evaluation stack results.\n    END"
  },
  {
    "prompt": "def token_get_allowance(wallet, token_str, from_addr, to_addr, verbose=False):\n    try:\n        token = _validate_nep5_args(wallet, token_str, from_addr, to_addr, amount=0)\n    except ValueError:\n        raise\n    (tx, fee, results) = token.Allowance(wallet, from_addr, to_addr)\n    if tx is not None and results is not None:\n        allowance = results[0].GetBigInteger()\n        if verbose:\n            print('%s allowance for %s from %s : %s ' % (token.symbol, from_addr, to_addr, allowance))\n        return allowance\n    else:\n        if verbose:\n            print('Could not get allowance for token %s ' % token.symbol)\n        raise ValueError(f'Could not get allowance for token {token.symbol}')",
    "completion": "\n    Query the smart contract for the amount from_addr is allowed to send to to_addr\n\n    Requires amount to be `approved`.\n\n    Args:\n        wallet (Wallet): a UserWallet instance\n        token_str (str): symbol name or script_hash\n        from_addr (str): a wallet address\n        to_addr (str): a wallet address\n        verbose (bool): flag indicating whether to print VM results\n\n    Raises:\n        ValueError: for invalid arguments or if allowance could not be queried\n\n    Returns:\n        int: allowance\n    END"
  },
  {
    "prompt": "def ellipse_extent(a, b, theta):\n    t = np.arctan2(-b * np.tan(theta), a)\n    dx = a * np.cos(t) * np.cos(theta) - b * np.sin(t) * np.sin(theta)\n    t = np.arctan2(b, a * np.tan(theta))\n    dy = b * np.sin(t) * np.cos(theta) + a * np.cos(t) * np.sin(theta)\n    if isinstance(dx, u.Quantity) or isinstance(dy, u.Quantity):\n        return np.abs(u.Quantity([dx, dy]))\n    else:\n        return np.abs([dx, dy])",
    "completion": "\n    Calculates the extent of a box encapsulating a rotated 2D ellipse.\n\n    Parameters\n    ----------\n    a : float or `~astropy.units.Quantity`\n        Major axis.\n    b : float or `~astropy.units.Quantity`\n        Minor axis.\n    theta : float or `~astropy.units.Quantity`\n        Rotation angle. If given as a floating-point value, it is assumed to be\n        in radians.\n\n    Returns\n    -------\n    offsets : tuple\n        The absolute value of the offset distances from the ellipse center that\n        define its bounding box region, ``(dx, dy)``.\n\n    Examples\n    --------\n    .. plot::\n        :include-source:\n\n        import numpy as np\n        import matplotlib.pyplot as plt\n        from astropy.modeling.models import Ellipse2D\n        from astropy.modeling.utils import ellipse_extent, render_model\n\n        amplitude = 1\n        x0 = 50\n        y0 = 50\n        a = 30\n        b = 10\n        theta = np.pi/4\n\n        model = Ellipse2D(amplitude, x0, y0, a, b, theta)\n\n        dx, dy = ellipse_extent(a, b, theta)\n\n        limits = [x0 - dx, x0 + dx, y0 - dy, y0 + dy]\n\n        model.bounding_box = limits\n\n        image = render_model(model)\n\n        plt.imshow(image, cmap='binary', interpolation='nearest', alpha=.5,\n                  extent = limits)\n        plt.show()\n    END"
  },
  {
    "prompt": "def get_inputs_and_params(func):\n    sig = signature(func)\n    inputs = []\n    params = []\n    for param in sig.parameters.values():\n        if param.kind in (param.VAR_POSITIONAL, param.VAR_KEYWORD):\n            raise ValueError('Signature must not have *args or **kwargs')\n        if param.default == param.empty:\n            inputs.append(param)\n        else:\n            params.append(param)\n    return (inputs, params)",
    "completion": "\n    Given a callable, determine the input variables and the\n    parameters.\n\n    Parameters\n    ----------\n    func : callable\n\n    Returns\n    -------\n    inputs, params : tuple\n        Each entry is a list of inspect.Parameter objects\n    END"
  },
  {
    "prompt": "def quantity_from_hdf5(dset):\n    if 'unit' in dset.attrs and dset.attrs['unit'] is not None:\n        unit = u.Unit(dset.attrs['unit'])\n    else:\n        unit = 1.0\n    return dset[:] * unit",
    "completion": "\n    Return an Astropy Quantity object from a key in an HDF5 file,\n    group, or dataset. This checks to see if the input file/group/dataset\n    contains a ``'unit'`` attribute (e.g., in `f.attrs`).\n\n    Parameters\n    ----------\n    dset : :class:`h5py.DataSet`\n\n    Returns\n    -------\n    q : `astropy.units.Quantity`, `numpy.ndarray`\n        If a unit attribute exists, this returns a Quantity. Otherwise, it\n        returns a numpy array.\n    END"
  },
  {
    "prompt": "def _filter_kwargs_to_query_params(filter_kwargs):\n    query_params = {}\n    for (kwarg, arglist) in six.iteritems(filter_kwargs):\n        if not arglist:\n            continue\n        if not hasattr(arglist, '__iter__') or isinstance(arglist, six.string_types):\n            arglist = (arglist,)\n        if kwarg == 'version':\n            query_params['match[version]'] = ','.join((_ensure_datetime_to_string(val) for val in arglist))\n        elif kwarg == 'added_after':\n            if len(arglist) > 1:\n                raise InvalidArgumentsError(\"No more than one value for filter 'added_after' may be given\")\n            query_params['added_after'] = ','.join((_ensure_datetime_to_string(val) for val in arglist))\n        else:\n            query_params['match[' + kwarg + ']'] = ','.join(arglist)\n    return query_params",
    "completion": "\n    Convert API keyword args to a mapping of URL query parameters.  Except for\n    \"added_after\", all keywords are mapped to match filters, i.e. to a query\n    parameter of the form \"match[<kwarg>]\".  \"added_after\" is left alone, since\n    it's a special filter, as defined in the spec.\n\n    Each value can be a single value or iterable of values.  \"version\" and\n    \"added_after\" get special treatment, since they are timestamp-valued:\n    datetime.datetime instances are supported and automatically converted to\n    STIX-compliant strings.  Other than that, all values must be strings.  None\n    values, empty lists, etc are silently ignored.\n\n    Args:\n        filter_kwargs: The filter information, as a mapping.\n\n    Returns:\n        query_params (dict): The query parameter map, mapping strings to\n            strings.\n\n    END"
  },
  {
    "prompt": "def _to_json(resp):\n    try:\n        return resp.json()\n    except ValueError as e:\n        six.raise_from(InvalidJSONError('Invalid JSON was received from ' + resp.request.url), e)",
    "completion": "\n    Factors out some JSON parse code with error handling, to hopefully improve\n    error messages.\n\n    :param resp: A \"requests\" library response\n    :return: Parsed JSON.\n    :raises: InvalidJSONError If JSON parsing failed.\n    END"
  },
  {
    "prompt": "def homogenize(series_dict):\n    index = None\n    need_reindex = False\n    for (_, series) in series_dict.iteritems():\n        if not np.isnan(series.fill_value):\n            raise Exception('this method is only valid with NaN fill values')\n        if index is None:\n            index = series.sp_index\n        elif not series.sp_index.equals(index):\n            need_reindex = True\n            index = index.intersect(series.sp_index)\n    if need_reindex:\n        output = {}\n        for (name, series) in series_dict.iteritems():\n            if not series.sp_index.equals(index):\n                series = series.sparse_reindex(index)\n            output[name] = series\n    else:\n        output = series_dict\n    return output",
    "completion": "\n    Conform a set of SparseSeries (with NaN fill_value) to a common SparseIndex\n    corresponding to the locations where they all have data\n\n    Parameters\n    ----------\n    series_dict : dict or DataFrame\n\n    Notes\n    -----\n    Using the dumbest algorithm I could think of. Should put some more thought\n    into this\n\n    Returns\n    -------\n    homogenized : dict of SparseSeries\n    END"
  },
  {
    "prompt": "def get_split_size(dim_size, chunks):\n    return (dim_size + chunks - 1) // chunks",
    "completion": "\n    Computes the split size inline with ``torch.chunk``\n\n    Args:\n        dim_size(int): Size of the dimension being chunked.\n        chunks(int): Number of chunks to create for ``dim_size``.\n\n    Returns:\n        An int indicating the split size to use.\n    END"
  },
  {
    "prompt": "def get_chunked_dim_size(dim_size, split_size, idx):\n    return max(min(dim_size, split_size * (idx + 1)) - split_size * idx, 0)",
    "completion": "\n    Computes the dim size of the chunk for provided ``idx`` given ``dim_size``\n    and ``split_size``.\n\n    Args:\n        dim_size(int): Size of the dimension being chunked.\n        split_size(int): The chunk size for each chunk of ``dim_size``.\n        idx(int): The index of chunk whose dim size is being requested.\n\n    Returns:\n        An int indicating the dim size of the chunk.\n    END"
  },
  {
    "prompt": "def get_chunk_sharding_params(sharding_dim_size, world_size, spec, rank):\n    split_size = get_split_size(sharding_dim_size, world_size)\n    current_offsets = 0\n    start_pos = current_offsets\n    for (idx, placement) in enumerate(spec.placements):\n        chunk_size = get_chunked_dim_size(sharding_dim_size, split_size, idx)\n        if rank == placement.rank():\n            start_pos = current_offsets\n            break\n        current_offsets += chunk_size\n    return (start_pos, chunk_size)",
    "completion": "\n    Generate the start pos and offset length for the current rank for\n    chunk sharding.\n\n    Args:\n        sharding_dim_size(int): The dimension length which we shard on.\n        world_size(int): number of ranks.\n        spec (:class:`torch.distributed._shard.sharding_spec.ChunkShardingSpec`):\n            sharding spec.\n        rank(int): # of cuda process.\n\n    Returns:\n        start_pos(int): start position of sharded tensor on the given rank.\n        chunk_size(int): chunk size of sharded tensor on the given rank.\n    END"
  },
  {
    "prompt": "def get_list(get_fn):\n\n    def get_list_fn(identifier):\n        \"\"\"Retrieves a list of objects.\n\n    Args:\n      identifier: An object identifier. Must be a string, a dictionary, an\n        `ObjectConfig` or `None`.\n\n    Returns:\n      A list of Keras objects as class instances.\n    \"\"\"\n        if isinstance(identifier, list):\n            return [get_fn(ident) for ident in identifier]\n        return get_fn(identifier)\n    return get_list_fn",
    "completion": "Returns a function that retrieves a list of objects.\n\n  Args:\n    get_fn: The get function to be used for individual identifiers.\n\n  Returns:\n    A function that retrieves an object or a list of objects.\n  END"
  },
  {
    "prompt": "def get_nest(get_fn):\n\n    def get_nest_fn(identifier):\n        \"\"\"Retrieves a nested structure of objects.\n\n    Args:\n      identifier: An object identifier. Must be a string, a dictionary, an\n        `ObjectConfig` or `None`.\n\n    Returns:\n      A list of Keras objects as class instances.\n    \"\"\"\n        if isinstance(identifier, hyperparams.ParamsDict):\n            identifier = identifier.as_dict()\n\n        def _parse_nest(nest):\n            if is_object_config(nest):\n                return get_fn(nest)\n            if isinstance(nest, dict):\n                return {key: _parse_nest(value) for (key, value) in nest.items()}\n            if isinstance(nest, list):\n                return [_parse_nest(value) for value in nest]\n            return get_fn(nest)\n        return _parse_nest(identifier)\n    return get_nest_fn",
    "completion": "Returns a function that retrieves a nested structure of objects.\n\n  Nests include lists and dictionaries.\n\n  Args:\n    get_fn: The get function to be used for individual identifiers.\n\n  Returns:\n    A function that retrieves an object or a list of objects.\n  END"
  },
  {
    "prompt": "def get_callback(identifier):\n    return _get(identifier, _CALLBACK_OBJECTS, 'callback')",
    "completion": "Retrieve a Keras callback as a class instance.\n\n  Args:\n    identifier: A callback identifier. Must be a string, a dictionary, an\n      `ObjectConfig` or `None`.\n\n  Returns:\n    A Keras callback as a class instance.\n  END"
  },
  {
    "prompt": "def get_layer(identifier):\n    return _get(identifier, _LAYER_OBJECTS, 'layer')",
    "completion": "Retrieve a Keras layer as a class instance.\n\n  Args:\n    identifier: A layer identifier. Must be a string, a dictionary, an\n      `ObjectConfig` or `None`.\n\n  Returns:\n    A Keras layer as a class instance.\n  END"
  },
  {
    "prompt": "def get_loss(identifier):\n    return _get(identifier, _LOSS_OBJECTS, 'loss')",
    "completion": "Retrieve a Keras loss as a class instance.\n\n  Args:\n    identifier: A loss identifier. Must be a string, a dictionary, an\n      `ObjectConfig` or `None`.\n\n  Returns:\n    A Keras loss as a class instance.\n  END"
  },
  {
    "prompt": "def get_metric(identifier):\n    return _get(identifier, _METRIC_OBJECTS, 'metric')",
    "completion": "Retrieve a Keras metric as a class instance.\n\n  Args:\n    identifier: A metric identifier. Must be a string, a dictionary, an\n      `ObjectConfig` or `None`.\n\n  Returns:\n    A Keras metric as a class instance.\n  END"
  },
  {
    "prompt": "def get_optimizer(identifier):\n    return _get(identifier, _OPTIMIZER_OBJECTS, 'optimizer')",
    "completion": "Retrieve a Keras optimizer as a class instance.\n\n  Args:\n    identifier: An optimizer identifier. Must be a string, a dictionary, an\n      `ObjectConfig` or `None`.\n\n  Returns:\n    A Keras optimizer as a class instance.\n  END"
  },
  {
    "prompt": "def get_predicate(identifier):\n    return _get(identifier, _PREDICATE_OBJECTS, 'predicate')",
    "completion": "Retrieve a predicate as a class instance.\n\n  Args:\n    identifier: A predicate identifier. Must be a string, a dictionary, an\n      `ObjectConfig` or `None`.\n\n  Returns:\n    A predicate as a class instance.\n  END"
  },
  {
    "prompt": "def get_strategy(identifier):\n    return _get(identifier, _STRATEGY_OBJECTS, 'strategy')",
    "completion": "Retrieve a TF distribution strategy as a class instance.\n\n  Args:\n    identifier: A strategy identifier. Must be a string, a dictionary, an\n      `ObjectConfig` or `None`.\n\n  Returns:\n    A TF distribution strategy as a class instance.\n  END"
  },
  {
    "prompt": "def _get(identifier, objects, objtype):\n    if isinstance(identifier, util.ExternalObject):\n        return identifier\n    if isinstance(identifier, config_module.ObjectConfig):\n        identifier = identifier.as_dict()\n    if not identifier:\n        return None\n    (class_name, config) = class_and_config_for_serialized_object(identifier)\n    if class_name not in objects:\n        raise ValueError(f'No known {objtype} with name: {class_name}')\n    obj = objects[class_name]\n    try:\n        return obj(**config)\n    except Exception as e:\n        raise RuntimeError(f'An error occurred while initializing {class_name} with parameters: {config}') from e",
    "completion": "Retrieve an object as a class instance.\n\n  Args:\n    identifier: An object identifier. Must be a string, a dictionary, an\n      `ObjectConfig` or `None`.\n    objects: A dictionary with the registered objects.\n    objtype: A string with the type of object being retrieved. This is only used\n      to format error messages.\n\n  Returns:\n    An instance of the object identified by `identifier`.\n\n  Raises:\n    ValueError: If the identifier is invalid.\n    RuntimeError: If an error occurs while initializing the object.\n  END"
  },
  {
    "prompt": "def class_and_config_for_serialized_object(identifier):\n    if isinstance(identifier, config_module.ObjectConfig):\n        identifier = identifier.as_dict()\n    if isinstance(identifier, str):\n        (class_name, config) = (identifier, {})\n    elif isinstance(identifier, dict):\n        if 'class_name' not in identifier or 'config' not in identifier:\n            raise ValueError(f'Invalid identifier: {identifier}. Value is not a valid configuration dictionary.')\n        class_name = identifier['class_name']\n        config = identifier['config']\n    else:\n        raise ValueError(f'Invalid identifier: {identifier}. Value must be a string, a dictionary or an `ObjectConfig`.')\n    return (class_name, config)",
    "completion": "Returns the class name and config for a serialized object.\n\n  Args:\n    identifier: An object identifier. Must be a string, a dictionary or an\n      `ObjectConfig`.\n\n  Returns:\n    A tuple containing the class name and its keyword arguments.\n\n  Raises:\n    ValueError: If the identifier is invalid.\n  END"
  },
  {
    "prompt": "def is_object_config(config):\n    if isinstance(config, (str, type(None))):\n        return True\n    if not isinstance(config, (dict, hyperparams.ParamsDict)):\n        return False\n    d = config.as_dict() if isinstance(config, hyperparams.ParamsDict) else config\n    if set(d.keys()) != {'class_name', 'config'}:\n        return False\n    return True",
    "completion": "Check if input is a valid object configuration dict.\n\n  Args:\n    config: The object to check.\n\n  Returns:\n    True if input is a valid object configuration dict, false otherwise.\n  END"
  },
  {
    "prompt": "def _find_objects(modules, objtype):\n    objects = {}\n    for module in modules:\n        members = inspect.getmembers(module)\n        for (name, value) in members:\n            if inspect.isclass(value) and issubclass(value, objtype):\n                objects[name] = value\n    return objects",
    "completion": "Finds objects of a certain type on the given modules.\n\n  Args:\n    modules: A list of modules to search for objects.\n    objtype: The type of objects to be searched for.\n\n  Returns:\n    A dictionary containing the found objects.\n  END"
  },
  {
    "prompt": "def get_image_resizer_config(model_config):\n    meta_architecture = model_config.WhichOneof('model')\n    if meta_architecture == 'faster_rcnn':\n        return model_config.faster_rcnn.image_resizer\n    if meta_architecture == 'ssd':\n        return model_config.ssd.image_resizer\n    raise ValueError('Unknown model type: {}'.format(meta_architecture))",
    "completion": "Returns the image resizer config from a model config.\n\n  Args:\n    model_config: A model_pb2.DetectionModel.\n\n  Returns:\n    An image_resizer_pb2.ImageResizer.\n\n  Raises:\n    ValueError: If the model type is not recognized.\n  END"
  },
  {
    "prompt": "def get_spatial_image_size(image_resizer_config):\n    if image_resizer_config.HasField('fixed_shape_resizer'):\n        return [image_resizer_config.fixed_shape_resizer.height, image_resizer_config.fixed_shape_resizer.width]\n    if image_resizer_config.HasField('keep_aspect_ratio_resizer'):\n        if image_resizer_config.keep_aspect_ratio_resizer.pad_to_max_dimension:\n            return [image_resizer_config.keep_aspect_ratio_resizer.max_dimension] * 2\n        else:\n            return [-1, -1]\n    raise ValueError('Unknown image resizer type.')",
    "completion": "Returns expected spatial size of the output image from a given config.\n\n  Args:\n    image_resizer_config: An image_resizer_pb2.ImageResizer.\n\n  Returns:\n    A list of two integers of the form [height, width]. `height` and `width` are\n    set  -1 if they cannot be determined during graph construction.\n\n  Raises:\n    ValueError: If the model type is not recognized.\n  END"
  },
  {
    "prompt": "def get_configs_from_pipeline_file(pipeline_config_path):\n    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n    with tf.gfile.GFile(pipeline_config_path, 'r') as f:\n        proto_str = f.read()\n        text_format.Merge(proto_str, pipeline_config)\n    configs = {}\n    configs['model'] = pipeline_config.model\n    configs['train_config'] = pipeline_config.train_config\n    configs['train_input_config'] = pipeline_config.train_input_reader\n    configs['eval_config'] = pipeline_config.eval_config\n    configs['eval_input_config'] = pipeline_config.eval_input_reader\n    return configs",
    "completion": "Reads configuration from a pipeline_pb2.TrainEvalPipelineConfig.\n\n  Args:\n    pipeline_config_path: Path to pipeline_pb2.TrainEvalPipelineConfig text\n      proto.\n\n  Returns:\n    Dictionary of configuration objects. Keys are `model`, `train_config`,\n      `train_input_config`, `eval_config`, `eval_input_config`. Value are the\n      corresponding config objects.\n  END"
  },
  {
    "prompt": "def create_pipeline_proto_from_configs(configs):\n    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n    pipeline_config.model.CopyFrom(configs['model'])\n    pipeline_config.train_config.CopyFrom(configs['train_config'])\n    pipeline_config.train_input_reader.CopyFrom(configs['train_input_config'])\n    pipeline_config.eval_config.CopyFrom(configs['eval_config'])\n    pipeline_config.eval_input_reader.CopyFrom(configs['eval_input_config'])\n    return pipeline_config",
    "completion": "Creates a pipeline_pb2.TrainEvalPipelineConfig from configs dictionary.\n\n  This function nearly performs the inverse operation of\n  get_configs_from_pipeline_file(). Instead of returning a file path, it returns\n  a `TrainEvalPipelineConfig` object.\n\n  Args:\n    configs: Dictionary of configs. See get_configs_from_pipeline_file().\n\n  Returns:\n    A fully populated pipeline_pb2.TrainEvalPipelineConfig.\n  END"
  },
  {
    "prompt": "def get_configs_from_multiple_files(model_config_path='', train_config_path='', train_input_config_path='', eval_config_path='', eval_input_config_path=''):\n    configs = {}\n    if model_config_path:\n        model_config = model_pb2.DetectionModel()\n        with tf.gfile.GFile(model_config_path, 'r') as f:\n            text_format.Merge(f.read(), model_config)\n            configs['model'] = model_config\n    if train_config_path:\n        train_config = train_pb2.TrainConfig()\n        with tf.gfile.GFile(train_config_path, 'r') as f:\n            text_format.Merge(f.read(), train_config)\n            configs['train_config'] = train_config\n    if train_input_config_path:\n        train_input_config = input_reader_pb2.InputReader()\n        with tf.gfile.GFile(train_input_config_path, 'r') as f:\n            text_format.Merge(f.read(), train_input_config)\n            configs['train_input_config'] = train_input_config\n    if eval_config_path:\n        eval_config = eval_pb2.EvalConfig()\n        with tf.gfile.GFile(eval_config_path, 'r') as f:\n            text_format.Merge(f.read(), eval_config)\n            configs['eval_config'] = eval_config\n    if eval_input_config_path:\n        eval_input_config = input_reader_pb2.InputReader()\n        with tf.gfile.GFile(eval_input_config_path, 'r') as f:\n            text_format.Merge(f.read(), eval_input_config)\n            configs['eval_input_config'] = eval_input_config\n    return configs",
    "completion": "Reads training configuration from multiple config files.\n\n  Args:\n    model_config_path: Path to model_pb2.DetectionModel.\n    train_config_path: Path to train_pb2.TrainConfig.\n    train_input_config_path: Path to input_reader_pb2.InputReader.\n    eval_config_path: Path to eval_pb2.EvalConfig.\n    eval_input_config_path: Path to input_reader_pb2.InputReader.\n\n  Returns:\n    Dictionary of configuration objects. Keys are `model`, `train_config`,\n      `train_input_config`, `eval_config`, `eval_input_config`. Key/Values are\n        returned only for valid (non-empty) strings.\n  END"
  },
  {
    "prompt": "def get_number_of_classes(model_config):\n    meta_architecture = model_config.WhichOneof('model')\n    if meta_architecture == 'faster_rcnn':\n        return model_config.faster_rcnn.num_classes\n    if meta_architecture == 'ssd':\n        return model_config.ssd.num_classes\n    raise ValueError(\"Expected the model to be one of 'faster_rcnn' or 'ssd'.\")",
    "completion": "Returns the number of classes for a detection model.\n\n  Args:\n    model_config: A model_pb2.DetectionModel.\n\n  Returns:\n    Number of classes.\n\n  Raises:\n    ValueError: If the model type is not recognized.\n  END"
  },
  {
    "prompt": "def get_optimizer_type(train_config):\n    return train_config.optimizer.WhichOneof('optimizer')",
    "completion": "Returns the optimizer type for training.\n\n  Args:\n    train_config: A train_pb2.TrainConfig.\n\n  Returns:\n    The type of the optimizer\n  END"
  },
  {
    "prompt": "def get_learning_rate_type(optimizer_config):\n    return optimizer_config.learning_rate.WhichOneof('learning_rate')",
    "completion": "Returns the learning rate type for training.\n\n  Args:\n    optimizer_config: An optimizer_pb2.Optimizer.\n\n  Returns:\n    The type of the learning rate.\n  END"
  },
  {
    "prompt": "def get_pack_id_from_error_with_gcp_path(error: str) -> str:\n    return error.split('/packs/')[1].split('.zip')[0].split('/')[0]",
    "completion": "\n        Gets the id of the pack from the pack's path in GCP that is mentioned in the error msg.\n    Args:\n        error: path of pack in GCP.\n\n    Returns:\n        The id of given pack.\n    END"
  },
  {
    "prompt": "def get_pack_display_name(pack_id: str) -> str:\n    metadata_path = os.path.join(PACKS_FULL_PATH, pack_id, PACK_METADATA_FILE)\n    if pack_id and os.path.isfile(metadata_path):\n        with open(metadata_path, 'r') as json_file:\n            pack_metadata = json.load(json_file)\n        return pack_metadata.get('name')\n    return ''",
    "completion": "\n    Gets the display name of the pack from the pack ID.\n\n    :param pack_id: ID of the pack.\n    :return: Name found in the pack metadata, otherwise an empty string.\n    END"
  },
  {
    "prompt": "def is_pack_hidden(pack_id: str) -> bool:\n    metadata_path = os.path.join(PACKS_FULL_PATH, pack_id, PACK_METADATA_FILE)\n    if pack_id and os.path.isfile(metadata_path):\n        with open(metadata_path, 'r') as json_file:\n            pack_metadata = json.load(json_file)\n            return pack_metadata.get('hidden', False)\n    else:\n        logging.warning(f'Could not open metadata file of pack {pack_id}')\n    return False",
    "completion": "\n    Check if the given pack is deprecated.\n\n    :param pack_id: ID of the pack.\n    :return: True if the pack is deprecated, i.e. has 'hidden: true' field, False otherwise.\n    END"
  },
  {
    "prompt": "def get_pack_dependencies(client: demisto_client, pack_data: dict, lock: Lock):\n    pack_id = pack_data['id']\n    logging.debug(f'Getting dependencies for pack {pack_id}')\n    try:\n        (response_data, status_code, _) = demisto_client.generic_request_func(client, path='/contentpacks/marketplace/search/dependencies', method='POST', body=[pack_data], accept='application/json', _request_timeout=None)\n        if 200 <= status_code < 300:\n            dependencies_data: list = []\n            dependants_ids = [pack_id]\n            reseponse_data = ast.literal_eval(response_data).get('dependencies', [])\n            create_dependencies_data_structure(reseponse_data, dependants_ids, dependencies_data, dependants_ids)\n            dependencies_str = ', '.join([dep['id'] for dep in dependencies_data])\n            if dependencies_data:\n                logging.debug(f'Found the following dependencies for pack {pack_id}: {dependencies_str}')\n            return dependencies_data\n        if status_code == 400:\n            logging.error(f'Unable to find dependencies for {pack_id}.')\n            return []\n        else:\n            result_object = ast.literal_eval(response_data)\n            msg = result_object.get('message', '')\n            raise Exception(f'Failed to get pack {pack_id} dependencies - with status code {status_code}\\n{msg}\\n')\n    except Exception:\n        logging.exception(f'The request to get pack {pack_id} dependencies has failed.')\n        lock.acquire()\n        global SUCCESS_FLAG\n        SUCCESS_FLAG = False\n        lock.release()",
    "completion": " Get the pack's required dependencies.\n\n    Args:\n        client (demisto_client): The configured client to use.\n        pack_data (dict): Contains the pack ID and version.\n        lock (Lock): A lock object.\n    Returns:\n        (list) The pack's dependencies.\n    END"
  },
  {
    "prompt": "def search_pack(client: demisto_client, pack_display_name: str, pack_id: str, lock: Lock) -> dict:\n    try:\n        (response_data, status_code, _) = demisto_client.generic_request_func(client, path=f'/contentpacks/marketplace/{pack_id}', method='GET', accept='application/json', _request_timeout=None)\n        if 200 <= status_code < 300:\n            result_object = ast.literal_eval(response_data)\n            if result_object and result_object.get('currentVersion'):\n                logging.debug(f'Found pack \"{pack_display_name}\" by its ID \"{pack_id}\" in bucket!')\n                pack_data = {'id': result_object.get('id'), 'version': result_object.get('currentVersion')}\n                return pack_data\n            else:\n                raise Exception(f'Did not find pack \"{pack_display_name}\" by its ID \"{pack_id}\" in bucket.')\n        else:\n            result_object = ast.literal_eval(response_data)\n            msg = result_object.get('message', '')\n            err_msg = f'Search request for pack \"{pack_display_name}\" with ID \"{pack_id}\", failed with status code {status_code}\\n{msg}'\n            raise Exception(err_msg)\n    except Exception:\n        logging.exception(f'Search request for pack \"{pack_display_name}\" with ID \"{pack_id}\", failed.')\n        lock.acquire()\n        global SUCCESS_FLAG\n        SUCCESS_FLAG = False\n        lock.release()\n        return {}",
    "completion": " Make a pack search request.\n\n    Args:\n        client (demisto_client): The configured client to use.\n        pack_display_name (string): The pack display name.\n        pack_id (string): The pack ID.\n        lock (Lock): A lock object.\n    Returns:\n        (dict): Returns the pack data if found, or empty dict otherwise.\n    END"
  },
  {
    "prompt": "def handle_malformed_pack_ids(malformed_pack_ids, packs_to_install):\n    for malformed_pack_id in malformed_pack_ids:\n        if malformed_pack_id not in {pack['id'] for pack in packs_to_install}:\n            raise Exception(f'The pack {malformed_pack_id} has failed to install even though it was not in the installation list')",
    "completion": "\n    Handles the case where the malformed id failed the installation but it was not a part of the initial installaion.\n    This is in order to prevent an infinite loop for this such edge case.\n    Args:\n        malformed_pack_ids: the ids found from the error msg\n        packs_to_install: list of packs that was already installed that caused the failure.\n\n    Returns:\n        raises an error.\n    END"
  },
  {
    "prompt": "def install_packs_from_artifacts(client: demisto_client, host: str, test_pack_path: str, pack_ids_to_install: List):\n    logging.info(f'Test pack path is: {test_pack_path}')\n    logging.info(f'Pack IDs to install are: {pack_ids_to_install}')\n    local_packs = glob.glob(f'{test_pack_path}/*.zip')\n    for local_pack in local_packs:\n        if any((pack_id in local_pack for pack_id in pack_ids_to_install)):\n            logging.info(f'Installing the following pack: {local_pack}')\n            upload_zipped_packs(client=client, host=host, pack_path=local_pack)",
    "completion": "\n    Installs all the packs located in the artifacts folder of the BitHub actions build. Please note:\n    The server always returns a 200 status even if the pack was not installed.\n\n    :param client: Demisto-py client to connect to the server.\n    :param host: FQDN of the server.\n    :param test_pack_path: Path the the test pack directory.\n    :param pack_ids_to_install: List of pack IDs to install.\n    :return: None. Call to server waits until a successful response.\n    END"
  },
  {
    "prompt": "def get_pack_installation_request_data(pack_id: str, pack_version: str):\n    return {'id': pack_id, 'version': pack_version}",
    "completion": "\n    Returns the installation request data of a given pack and its version. The request must have the ID and Version.\n\n    :param pack_id: Id of the pack to add.\n    :param pack_version: Version of the pack to add.\n    :return: The request data part of the pack\n    END"
  },
  {
    "prompt": "def install_all_content_packs_for_nightly(client: demisto_client, host: str, service_account: str):\n    all_packs = []\n    storage_client = init_storage_client(service_account)\n    production_bucket = storage_client.bucket(GCPConfig.PRODUCTION_BUCKET)\n    logging.debug(f'Installing all content packs for nightly flow in server {host}')\n    for pack_id in os.listdir(PACKS_FULL_PATH):\n        if is_pack_hidden(pack_id):\n            logging.debug(f'Skipping installation of hidden pack \"{pack_id}\"')\n            IGNORED_FILES.append(pack_id)\n    for pack_id in os.listdir(PACKS_FULL_PATH):\n        if pack_id not in IGNORED_FILES:\n            pack_version = get_latest_version_from_bucket(pack_id, production_bucket)\n            if pack_version:\n                all_packs.append(get_pack_installation_request_data(pack_id, pack_version))\n    install_packs(client, host, all_packs)",
    "completion": " Iterates over the packs currently located in the Packs directory. Wrapper for install_packs.\n    Retrieving the latest version of each pack from the production bucket.\n\n    :param client: Demisto-py client to connect to the server.\n    :param host: FQDN of the server.\n    :param service_account: The full path to the service account json.\n    :return: None. Prints the response from the server in the build.\n    END"
  },
  {
    "prompt": "def install_all_content_packs_from_build_bucket(client: demisto_client, host: str, server_version: str, bucket_packs_root_path: str, service_account: str, extract_destination_path: str):\n    all_packs = []\n    logging.debug(f'Installing all content packs in server {host} from packs path {bucket_packs_root_path}')\n    storage_client = init_storage_client(service_account)\n    build_bucket = storage_client.bucket(GCPConfig.CI_BUILD_BUCKET)\n    (index_folder_path, _, _) = download_and_extract_index(build_bucket, extract_destination_path, bucket_packs_root_path)\n    for pack_id in os.listdir(index_folder_path):\n        if os.path.isdir(os.path.join(index_folder_path, pack_id)):\n            metadata_path = os.path.join(index_folder_path, pack_id, Pack.METADATA)\n            pack_metadata = load_json(metadata_path)\n            if 'partnerId' in pack_metadata:\n                continue\n            pack_version = pack_metadata.get(Metadata.CURRENT_VERSION, Metadata.SERVER_DEFAULT_MIN_VERSION)\n            server_min_version = pack_metadata.get(Metadata.SERVER_MIN_VERSION, Metadata.SERVER_DEFAULT_MIN_VERSION)\n            hidden = pack_metadata.get(Metadata.HIDDEN, False)\n            if ('Master' in server_version or Version(server_version) >= Version(server_min_version)) and (not hidden):\n                logging.debug(f'Appending pack id {pack_id}')\n                all_packs.append(get_pack_installation_request_data(pack_id, pack_version))\n            else:\n                reason = 'Is hidden' if hidden else f'min server version is {server_min_version}'\n                logging.debug(f'Pack: {pack_id} with version: {pack_version} will not be installed on {host}. Pack {reason}.')\n    return install_packs(client, host, all_packs)",
    "completion": " Iterates over the packs currently located in the Build bucket. Wrapper for install_packs.\n    Retrieving the metadata of the latest version of each pack from the index.zip of the build bucket.\n\n    :param client: Demisto-py client to connect to the server.\n    :param host: FQDN of the server.\n    :param server_version: The version of the server the packs are installed on.\n    :param bucket_packs_root_path: The prefix to the root of packs in the bucket\n    :param service_account: Google Service Account\n    :param extract_destination_path: the full path of extract folder for the index.\n    :return: None. Prints the response from the server in the build.\n    END"
  },
  {
    "prompt": "def _get_static_ndims(x, expect_static=False, expect_ndims=None, expect_ndims_no_more_than=None, expect_ndims_at_least=None):\n    ndims = x.get_shape().ndims\n    if ndims is None:\n        shape_const = tensor_util.constant_value(array_ops.shape(x))\n        if shape_const is not None:\n            ndims = shape_const.ndim\n    if ndims is None:\n        if expect_static:\n            raise ValueError(\"Expected argument 'x' to have statically defined 'ndims'.  Found: \" % x)\n        return\n    if expect_ndims is not None:\n        ndims_message = \"Expected argument 'x' to have ndims %s.  Found tensor %s\" % (expect_ndims, x)\n        if ndims != expect_ndims:\n            raise ValueError(ndims_message)\n    if expect_ndims_at_least is not None:\n        ndims_at_least_message = \"Expected argument 'x' to have ndims >= %d.  Found tensor %s\" % (expect_ndims_at_least, x)\n        if ndims < expect_ndims_at_least:\n            raise ValueError(ndims_at_least_message)\n    if expect_ndims_no_more_than is not None:\n        ndims_no_more_than_message = \"Expected argument 'x' to have ndims <= %d.  Found tensor %s\" % (expect_ndims_no_more_than, x)\n        if ndims > expect_ndims_no_more_than:\n            raise ValueError(ndims_no_more_than_message)\n    return ndims",
    "completion": "Get static number of dimensions and assert that some expectations are met.\n\n  This function returns the number of dimensions \"ndims\" of x, as a Python int.\n\n  The optional expect arguments are used to check the ndims of x, but this is\n  only done if the static ndims of x is not None.\n\n  Args:\n    x:  A Tensor.\n    expect_static:  Expect `x` to have statically defined `ndims`.\n    expect_ndims:  Optional Python integer.  If provided, assert that x has\n      number of dimensions equal to this.\n    expect_ndims_no_more_than:  Optional Python integer.  If provided, assert\n      that x has no more than this many dimensions.\n    expect_ndims_at_least:  Optional Python integer.  If provided, assert that\n      x has at least this many dimensions.\n\n  Returns:\n    ndims:  A Python integer.\n\n  Raises:\n    ValueError:  If any of the expectations above are violated.\n  END"
  },
  {
    "prompt": "def _insert_back_keep_dims(x, axis):\n    for i in sorted(axis):\n        x = array_ops.expand_dims(x, axis=i)\n    return x",
    "completion": "Insert the dims in `axis` back as singletons after being removed.\n\n  Args:\n    x:  `Tensor`.\n    axis:  Python list of integers.\n\n  Returns:\n    `Tensor` with same values as `x`, but additional singleton dimensions.\n  END"
  },
  {
    "prompt": "def _make_static_axis_non_negative(axis, ndims):\n    non_negative_axis = []\n    for d in axis:\n        if d >= 0:\n            if d >= ndims:\n                raise ValueError('dim %d not in the interval [0, %d].' % (d, ndims - 1))\n            non_negative_axis.append(d)\n        else:\n            if d < -1 * ndims:\n                raise ValueError('Negatively indexed dim %d not in the interval [-%d, -1]' % (d, ndims))\n            non_negative_axis.append(ndims + d)\n    return non_negative_axis",
    "completion": "Convert possibly negatively indexed axis to non-negative.\n\n  Args:\n    axis:  Iterable over Python integers.\n    ndims:  Number of dimensions into which axis indexes.\n\n  Returns:\n    A list of non-negative Python integers.\n\n  Raises:\n    ValueError: If values in `axis` are too big/small to index into `ndims`.\n  END"
  },
  {
    "prompt": "def _move_dims_to_flat_end(x, axis, x_ndims):\n    front_dims = sorted(set(range(x_ndims)).difference(axis))\n    x_permed = array_ops.transpose(x, perm=front_dims + list(axis))\n    if x.get_shape().is_fully_defined():\n        x_shape = x.get_shape().as_list()\n        front_shape = [x_shape[i] for i in front_dims]\n        end_shape = [np.prod([x_shape[i] for i in axis])]\n        full_shape = front_shape + end_shape\n    else:\n        front_shape = array_ops.shape(x_permed)[:x_ndims - len(axis)]\n        end_shape = [-1]\n        full_shape = array_ops.concat([front_shape, end_shape], axis=0)\n    return array_ops.reshape(x_permed, shape=full_shape)",
    "completion": "Move dims corresponding to `axis` in `x` to the end, then flatten.\n\n  Args:\n    x: `Tensor` with shape `[B0,B1,...,Bb]`.\n    axis:  Python list of indices into dimensions of `x`.\n    x_ndims:  Python integer holding number of dimensions in `x`.\n\n  Returns:\n    `Tensor` with value from `x` and dims in `axis` moved to end into one single\n      dimension.\n  END"
  },
  {
    "prompt": "def get_dataloaders(opt):\n    num_workers = 1\n    train = pd.read_csv(os.path.join(opt.data_input_dir, 'train.csv'))\n    valid = pd.read_csv(os.path.join(opt.data_input_dir, 'valid.csv'))\n    test = pd.read_csv(os.path.join(opt.data_input_dir, 'test.csv'))\n    loader = None\n    if opt.bar_type == 'time':\n        loader = default_stock_loader\n    elif opt.bar_type == 'volume':\n        loader = volume_stock_loader\n    elif opt.bar_type == 'dollars':\n        loader = dollars_stock_loader\n    else:\n        raise ValueError('Incorrect Bar Type!')\n    if opt.validate:\n        logging.info('Using Train / Val Split')\n        train_dataset = StockDataset(opt, train, loader=loader)\n        test_dataset = StockDataset(opt, valid, loader=loader)\n    else:\n        logging.info('Using Train+Val / Test Split')\n        train_dataset = StockDataset(opt, pd.concat([train, valid], ignore_index=True), loader=loader)\n        test_dataset = StockDataset(opt, test, loader=loader)\n    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=opt.batch_size_multiGPU, shuffle=not opt.noshuffle, drop_last=True, num_workers=num_workers)\n    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=opt.batch_size_multiGPU, shuffle=False, drop_last=True, num_workers=num_workers)\n    return (train_loader, train_dataset, test_loader, test_dataset)",
    "completion": "creates and returns the stock dataset and dataloaders, either with\n    train/val split, or train+val/test split.\n\n    :param opt:\n    :return: train_loader, train_dataset,\n    test_loader, test_dataset - corresponds to validation or test set depending on opt.validate\n    END"
  },
  {
    "prompt": "def positionFactory(descriptor, data):\n    return {'point': lambda : PointPosition(*data)}.get(descriptor)()",
    "completion": "\n    Factory method for a text position object.\n    :param descriptor: Descriptor of the position type\n    :param data: List of data to be used for instantiation\n    :return: Text position object\n    :rtype: Position\n    END"
  },
  {
    "prompt": "def create_model(name, normalize, info, device):\n    if info['data'] in ['tiny-imagenet']:\n        assert 'preact-resnet' in name, 'Only preact-resnets are supported for this dataset!'\n        from .ti_preact_resnet import ti_preact_resnet\n        backbone = ti_preact_resnet(name, num_classes=info['num_classes'], device=device)\n    elif info['data'] in DATASETS and info['data'] not in ['tiny-imagenet']:\n        if 'preact-resnet' in name and 'swish' not in name:\n            backbone = preact_resnet(name, num_classes=info['num_classes'], pretrained=False, device=device)\n        elif 'preact-resnet' in name and 'swish' in name:\n            backbone = preact_resnetwithswish(name, dataset=info['data'], num_classes=info['num_classes'])\n        elif 'resnet' in name and 'preact' not in name:\n            backbone = resnet(name, num_classes=info['num_classes'], pretrained=False, device=device)\n        elif 'wrn' in name and 'swish' not in name:\n            backbone = wideresnet(name, num_classes=info['num_classes'], device=device)\n        elif 'wrn' in name and 'swish' in name:\n            backbone = wideresnetwithswish(name, dataset=info['data'], num_classes=info['num_classes'], device=device)\n        else:\n            raise ValueError('Invalid model name {}!'.format(name))\n    else:\n        raise ValueError('Models for {} not yet supported!'.format(info['data']))\n    if normalize:\n        model = torch.nn.Sequential(Normalization(info['mean'], info['std']), backbone)\n    else:\n        model = torch.nn.Sequential(backbone)\n    model = torch.nn.DataParallel(model)\n    model = model.to(device)\n    return model",
    "completion": "\n    Returns suitable model from its name.\n    Arguments:\n        name (str): name of resnet architecture.\n        normalize (bool): normalize input.\n        info (dict): dataset information.\n        device (str or torch.device): device to work on.\n    Returns:\n        torch.nn.Module.\n    END"
  },
  {
    "prompt": "def enforce(context, rule=None, target=None, do_raise=True, exc=None, *args, **kwargs):\n    enforcer = init()\n    credentials = context.to_dict()\n    if not exc:\n        exc = exception.PolicyNotAuthorized\n    if target is None:\n        target = {'project_id': context.project_id, 'user_id': context.user_id}\n    add_policy_attributes(target)\n    return enforcer.enforce(rule, target, credentials, *args, do_raise=do_raise, exc=exc, **kwargs)",
    "completion": "Checks authorization of a rule against the target and credentials.\n\n        :param dict context: As much information about the user performing the\n                             action as possible.\n        :param rule: The rule to evaluate.\n        :param dict target: As much information about the object being operated\n                            on as possible.\n        :param do_raise: Whether to raise an exception or not if check\n                         fails.\n        :param exc: Class of the exception to raise if the check fails.\n                    Any remaining arguments passed to :meth:`enforce` (both\n                    positional and keyword arguments) will be passed to\n                    the exception class. If not specified,\n                    :class:`PolicyNotAuthorized` will be used.\n\n        :return: ``False`` if the policy does not allow the action and `exc` is\n                 not provided; otherwise, returns a value that evaluates to\n                 ``True``.  Note: for rules using the \"case\" expression, this\n                 ``True`` value will be the specified string from the\n                 expression.\n    END"
  },
  {
    "prompt": "def flow_to_img(raw_flow, bound=20.0):\n    flow = np.clip(raw_flow, -bound, bound)\n    flow += bound\n    flow *= 255 / float(2 * bound)\n    flow = flow.astype(np.uint8)\n    return flow",
    "completion": "Convert flow to gray image.\n\n    Args:\n        raw_flow (np.ndarray[float]): Estimated flow with the shape (w, h).\n        bound (float): Bound for the flow-to-image normalization. Default: 20.\n\n    Returns:\n        np.ndarray[uint8]: The result list of np.ndarray[uint8], with shape\n                        (w, h).\n    END"
  },
  {
    "prompt": "def generate_flow(frames, method='tvl1'):\n    assert method in ['tvl1', 'farneback']\n    gray_frames = [cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) for frame in frames]\n    if method == 'tvl1':\n        tvl1 = cv2.optflow.DualTVL1OpticalFlow_create()\n\n        def op(x, y):\n            return tvl1.calc(x, y, None)\n    elif method == 'farneback':\n\n        def op(x, y):\n            return cv2.calcOpticalFlowFarneback(x, y, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n    gray_st = gray_frames[:-1]\n    gray_ed = gray_frames[1:]\n    flow = [op(x, y) for (x, y) in zip(gray_st, gray_ed)]\n    return flow",
    "completion": "Estimate flow with given frames.\n\n    Args:\n        frames (list[np.ndarray[uint8]]): List of rgb frames, with shape\n                                        (w, h, 3).\n        method (str): Use which method to generate flow. Options are 'tvl1'\n                    and 'farneback'. Default: 'tvl1'.\n\n    Returns:\n        list[np.ndarray[float]]: The result list of np.ndarray[float], with\n                                shape (w, h, 2).\n    END"
  },
  {
    "prompt": "def _get_asn1_time(timestamp):\n    string_timestamp = _ffi.cast('ASN1_STRING*', timestamp)\n    if _lib.ASN1_STRING_length(string_timestamp) == 0:\n        return None\n    elif _lib.ASN1_STRING_type(string_timestamp) == _lib.V_ASN1_GENERALIZEDTIME:\n        return _ffi.string(_lib.ASN1_STRING_data(string_timestamp))\n    else:\n        generalized_timestamp = _ffi.new('ASN1_GENERALIZEDTIME**')\n        _lib.ASN1_TIME_to_generalizedtime(timestamp, generalized_timestamp)\n        if generalized_timestamp[0] == _ffi.NULL:\n            _untested_error('ASN1_TIME_to_generalizedtime')\n        else:\n            string_timestamp = _ffi.cast('ASN1_STRING*', generalized_timestamp[0])\n            string_data = _lib.ASN1_STRING_data(string_timestamp)\n            string_result = _ffi.string(string_data)\n            _lib.ASN1_GENERALIZEDTIME_free(generalized_timestamp[0])\n            return string_result",
    "completion": "\n    Retrieve the time value of an ASN1 time object.\n\n    @param timestamp: An ASN1_GENERALIZEDTIME* (or an object safely castable to\n        that type) from which the time value will be retrieved.\n\n    @return: The time value from C{timestamp} as a L{bytes} string in a certain\n        format.  Or C{None} if the object contains no time value.\n    END"
  },
  {
    "prompt": "def load_certificate(type, buffer):\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode('ascii')\n    bio = _new_mem_buf(buffer)\n    if type == FILETYPE_PEM:\n        x509 = _lib.PEM_read_bio_X509(bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)\n    elif type == FILETYPE_ASN1:\n        x509 = _lib.d2i_X509_bio(bio, _ffi.NULL)\n    else:\n        raise ValueError('type argument must be FILETYPE_PEM or FILETYPE_ASN1')\n    if x509 == _ffi.NULL:\n        _raise_current_error()\n    cert = X509.__new__(X509)\n    cert._x509 = _ffi.gc(x509, _lib.X509_free)\n    return cert",
    "completion": "\n    Load a certificate from a buffer\n\n    :param type: The file type (one of FILETYPE_PEM, FILETYPE_ASN1)\n\n    :param buffer: The buffer the certificate is stored in\n    :type buffer: :py:class:`bytes`\n\n    :return: The X509 object\n    END"
  },
  {
    "prompt": "def dump_certificate(type, cert):\n    bio = _new_mem_buf()\n    if type == FILETYPE_PEM:\n        result_code = _lib.PEM_write_bio_X509(bio, cert._x509)\n    elif type == FILETYPE_ASN1:\n        result_code = _lib.i2d_X509_bio(bio, cert._x509)\n    elif type == FILETYPE_TEXT:\n        result_code = _lib.X509_print_ex(bio, cert._x509, 0, 0)\n    else:\n        raise ValueError('type argument must be FILETYPE_PEM, FILETYPE_ASN1, or FILETYPE_TEXT')\n    return _bio_to_string(bio)",
    "completion": "\n    Dump a certificate to a buffer\n\n    :param type: The file type (one of FILETYPE_PEM, FILETYPE_ASN1, or\n        FILETYPE_TEXT)\n    :param cert: The certificate to dump\n    :return: The buffer with the dumped certificate in\n    END"
  },
  {
    "prompt": "def dump_privatekey(type, pkey, cipher=None, passphrase=None):\n    bio = _new_mem_buf()\n    if cipher is not None:\n        if passphrase is None:\n            raise TypeError('if a value is given for cipher one must also be given for passphrase')\n        cipher_obj = _lib.EVP_get_cipherbyname(_byte_string(cipher))\n        if cipher_obj == _ffi.NULL:\n            raise ValueError('Invalid cipher name')\n    else:\n        cipher_obj = _ffi.NULL\n    helper = _PassphraseHelper(type, passphrase)\n    if type == FILETYPE_PEM:\n        result_code = _lib.PEM_write_bio_PrivateKey(bio, pkey._pkey, cipher_obj, _ffi.NULL, 0, helper.callback, helper.callback_args)\n        helper.raise_if_problem()\n    elif type == FILETYPE_ASN1:\n        result_code = _lib.i2d_PrivateKey_bio(bio, pkey._pkey)\n    elif type == FILETYPE_TEXT:\n        rsa = _lib.EVP_PKEY_get1_RSA(pkey._pkey)\n        result_code = _lib.RSA_print(bio, rsa, 0)\n    else:\n        raise ValueError('type argument must be FILETYPE_PEM, FILETYPE_ASN1, or FILETYPE_TEXT')\n    if result_code == 0:\n        _raise_current_error()\n    return _bio_to_string(bio)",
    "completion": "\n    Dump a private key to a buffer\n\n    :param type: The file type (one of FILETYPE_PEM, FILETYPE_ASN1, or\n        FILETYPE_TEXT)\n    :param pkey: The PKey to dump\n    :param cipher: (optional) if encrypted PEM format, the cipher to\n                   use\n    :param passphrase: (optional) if encrypted PEM format, this can be either\n                       the passphrase to use, or a callback for providing the\n                       passphrase.\n    :return: The buffer with the dumped key in\n    :rtype: :py:data:`str`\n    END"
  },
  {
    "prompt": "def load_privatekey(type, buffer, passphrase=None):\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode('ascii')\n    bio = _new_mem_buf(buffer)\n    helper = _PassphraseHelper(type, passphrase)\n    if type == FILETYPE_PEM:\n        evp_pkey = _lib.PEM_read_bio_PrivateKey(bio, _ffi.NULL, helper.callback, helper.callback_args)\n        helper.raise_if_problem()\n    elif type == FILETYPE_ASN1:\n        evp_pkey = _lib.d2i_PrivateKey_bio(bio, _ffi.NULL)\n    else:\n        raise ValueError('type argument must be FILETYPE_PEM or FILETYPE_ASN1')\n    if evp_pkey == _ffi.NULL:\n        _raise_current_error()\n    pkey = PKey.__new__(PKey)\n    pkey._pkey = _ffi.gc(evp_pkey, _lib.EVP_PKEY_free)\n    return pkey",
    "completion": "\n    Load a private key from a buffer\n\n    :param type: The file type (one of FILETYPE_PEM, FILETYPE_ASN1)\n    :param buffer: The buffer the key is stored in\n    :param passphrase: (optional) if encrypted PEM format, this can be\n                       either the passphrase to use, or a callback for\n                       providing the passphrase.\n\n    :return: The PKey object\n    END"
  },
  {
    "prompt": "def dump_certificate_request(type, req):\n    bio = _new_mem_buf()\n    if type == FILETYPE_PEM:\n        result_code = _lib.PEM_write_bio_X509_REQ(bio, req._req)\n    elif type == FILETYPE_ASN1:\n        result_code = _lib.i2d_X509_REQ_bio(bio, req._req)\n    elif type == FILETYPE_TEXT:\n        result_code = _lib.X509_REQ_print_ex(bio, req._req, 0, 0)\n    else:\n        raise ValueError('type argument must be FILETYPE_PEM, FILETYPE_ASN1, or FILETYPE_TEXT')\n    if result_code == 0:\n        _raise_current_error()\n    return _bio_to_string(bio)",
    "completion": "\n    Dump a certificate request to a buffer\n\n    :param type: The file type (one of FILETYPE_PEM, FILETYPE_ASN1)\n    :param req: The certificate request to dump\n    :return: The buffer with the dumped certificate request in\n    END"
  },
  {
    "prompt": "def load_certificate_request(type, buffer):\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode('ascii')\n    bio = _new_mem_buf(buffer)\n    if type == FILETYPE_PEM:\n        req = _lib.PEM_read_bio_X509_REQ(bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)\n    elif type == FILETYPE_ASN1:\n        req = _lib.d2i_X509_REQ_bio(bio, _ffi.NULL)\n    else:\n        raise ValueError('type argument must be FILETYPE_PEM or FILETYPE_ASN1')\n    if req == _ffi.NULL:\n        _raise_current_error()\n    x509req = X509Req.__new__(X509Req)\n    x509req._req = _ffi.gc(req, _lib.X509_REQ_free)\n    return x509req",
    "completion": "\n    Load a certificate request from a buffer\n\n    :param type: The file type (one of FILETYPE_PEM, FILETYPE_ASN1)\n    :param buffer: The buffer the certificate request is stored in\n    :return: The X509Req object\n    END"
  },
  {
    "prompt": "def sign(pkey, data, digest):\n    digest_obj = _lib.EVP_get_digestbyname(_byte_string(digest))\n    if digest_obj == _ffi.NULL:\n        raise ValueError('No such digest method')\n    md_ctx = _ffi.new('EVP_MD_CTX*')\n    md_ctx = _ffi.gc(md_ctx, _lib.EVP_MD_CTX_cleanup)\n    _lib.EVP_SignInit(md_ctx, digest_obj)\n    _lib.EVP_SignUpdate(md_ctx, data, len(data))\n    signature_buffer = _ffi.new('unsigned char[]', 512)\n    signature_length = _ffi.new('unsigned int*')\n    signature_length[0] = len(signature_buffer)\n    final_result = _lib.EVP_SignFinal(md_ctx, signature_buffer, signature_length, pkey._pkey)\n    if final_result != 1:\n        _raise_current_error()\n    return _ffi.buffer(signature_buffer, signature_length[0])[:]",
    "completion": "\n    Sign data with a digest\n\n    :param pkey: Pkey to sign with\n    :param data: data to be signed\n    :param digest: message digest to use\n    :return: signature\n    END"
  },
  {
    "prompt": "def verify(cert, signature, data, digest):\n    digest_obj = _lib.EVP_get_digestbyname(_byte_string(digest))\n    if digest_obj == _ffi.NULL:\n        raise ValueError('No such digest method')\n    pkey = _lib.X509_get_pubkey(cert._x509)\n    if pkey == _ffi.NULL:\n        _raise_current_error()\n    pkey = _ffi.gc(pkey, _lib.EVP_PKEY_free)\n    md_ctx = _ffi.new('EVP_MD_CTX*')\n    md_ctx = _ffi.gc(md_ctx, _lib.EVP_MD_CTX_cleanup)\n    _lib.EVP_VerifyInit(md_ctx, digest_obj)\n    _lib.EVP_VerifyUpdate(md_ctx, data, len(data))\n    verify_result = _lib.EVP_VerifyFinal(md_ctx, signature, len(signature), pkey)\n    if verify_result != 1:\n        _raise_current_error()",
    "completion": "\n    Verify a signature\n\n    :param cert: signing certificate (X509 object)\n    :param signature: signature returned by sign function\n    :param data: data to be verified\n    :param digest: message digest to use\n    :return: None if the signature is correct, raise exception otherwise\n    END"
  },
  {
    "prompt": "def load_crl(type, buffer):\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode('ascii')\n    bio = _new_mem_buf(buffer)\n    if type == FILETYPE_PEM:\n        crl = _lib.PEM_read_bio_X509_CRL(bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)\n    elif type == FILETYPE_ASN1:\n        crl = _lib.d2i_X509_CRL_bio(bio, _ffi.NULL)\n    else:\n        raise ValueError('type argument must be FILETYPE_PEM or FILETYPE_ASN1')\n    if crl == _ffi.NULL:\n        _raise_current_error()\n    result = CRL.__new__(CRL)\n    result._crl = crl\n    return result",
    "completion": "\n    Load a certificate revocation list from a buffer\n\n    :param type: The file type (one of FILETYPE_PEM, FILETYPE_ASN1)\n    :param buffer: The buffer the CRL is stored in\n\n    :return: The PKey object\n    END"
  },
  {
    "prompt": "def load_pkcs7_data(type, buffer):\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode('ascii')\n    bio = _new_mem_buf(buffer)\n    if type == FILETYPE_PEM:\n        pkcs7 = _lib.PEM_read_bio_PKCS7(bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)\n    elif type == FILETYPE_ASN1:\n        pass\n    else:\n        _raise_current_error()\n        raise ValueError('type argument must be FILETYPE_PEM or FILETYPE_ASN1')\n    if pkcs7 == _ffi.NULL:\n        _raise_current_error()\n    pypkcs7 = PKCS7.__new__(PKCS7)\n    pypkcs7._pkcs7 = _ffi.gc(pkcs7, _lib.PKCS7_free)\n    return pypkcs7",
    "completion": "\n    Load pkcs7 data from a buffer\n\n    :param type: The file type (one of FILETYPE_PEM or FILETYPE_ASN1)\n    :param buffer: The buffer with the pkcs7 data.\n    :return: The PKCS7 object\n    END"
  },
  {
    "prompt": "def load_pkcs12(buffer, passphrase):\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode('ascii')\n    bio = _new_mem_buf(buffer)\n    p12 = _lib.d2i_PKCS12_bio(bio, _ffi.NULL)\n    if p12 == _ffi.NULL:\n        _raise_current_error()\n    p12 = _ffi.gc(p12, _lib.PKCS12_free)\n    pkey = _ffi.new('EVP_PKEY**')\n    cert = _ffi.new('X509**')\n    cacerts = _ffi.new('Cryptography_STACK_OF_X509**')\n    parse_result = _lib.PKCS12_parse(p12, passphrase, pkey, cert, cacerts)\n    if not parse_result:\n        _raise_current_error()\n    cacerts = _ffi.gc(cacerts[0], _lib.sk_X509_free)\n    try:\n        _raise_current_error()\n    except Error:\n        pass\n    if pkey[0] == _ffi.NULL:\n        pykey = None\n    else:\n        pykey = PKey.__new__(PKey)\n        pykey._pkey = _ffi.gc(pkey[0], _lib.EVP_PKEY_free)\n    if cert[0] == _ffi.NULL:\n        pycert = None\n        friendlyname = None\n    else:\n        pycert = X509.__new__(X509)\n        pycert._x509 = _ffi.gc(cert[0], _lib.X509_free)\n        friendlyname_length = _ffi.new('int*')\n        friendlyname_buffer = _lib.X509_alias_get0(cert[0], friendlyname_length)\n        friendlyname = _ffi.buffer(friendlyname_buffer, friendlyname_length[0])[:]\n        if friendlyname_buffer == _ffi.NULL:\n            friendlyname = None\n    pycacerts = []\n    for i in range(_lib.sk_X509_num(cacerts)):\n        pycacert = X509.__new__(X509)\n        pycacert._x509 = _lib.sk_X509_value(cacerts, i)\n        pycacerts.append(pycacert)\n    if not pycacerts:\n        pycacerts = None\n    pkcs12 = PKCS12.__new__(PKCS12)\n    pkcs12._pkey = pykey\n    pkcs12._cert = pycert\n    pkcs12._cacerts = pycacerts\n    pkcs12._friendlyname = friendlyname\n    return pkcs12",
    "completion": "\n    Load a PKCS12 object from a buffer\n\n    :param buffer: The buffer the certificate is stored in\n    :param passphrase: (Optional) The password to decrypt the PKCS12 lump\n    :returns: The PKCS12 object\n    END"
  },
  {
    "prompt": "def crosstab(index, columns, values=None, rownames=None, colnames=None, aggfunc=None, margins=False, margins_name: str='All', dropna: bool=True, normalize=False) -> DataFrame:\n    if values is None and aggfunc is not None:\n        raise ValueError('aggfunc cannot be used without values.')\n    if values is not None and aggfunc is None:\n        raise ValueError('values cannot be used without an aggfunc.')\n    index = com.maybe_make_list(index)\n    columns = com.maybe_make_list(columns)\n    common_idx = None\n    pass_objs = [x for x in index + columns if isinstance(x, (ABCSeries, ABCDataFrame))]\n    if pass_objs:\n        common_idx = get_objs_combined_axis(pass_objs, intersect=True, sort=False)\n    rownames = _get_names(index, rownames, prefix='row')\n    colnames = _get_names(columns, colnames, prefix='col')\n    (rownames_mapper, unique_rownames, colnames_mapper, unique_colnames) = _build_names_mapper(rownames, colnames)\n    from pandas import DataFrame\n    data = {**dict(zip(unique_rownames, index)), **dict(zip(unique_colnames, columns))}\n    df = DataFrame(data, index=common_idx)\n    if values is None:\n        df['__dummy__'] = 0\n        kwargs = {'aggfunc': len, 'fill_value': 0}\n    else:\n        df['__dummy__'] = values\n        kwargs = {'aggfunc': aggfunc}\n    table = df.pivot_table('__dummy__', index=unique_rownames, columns=unique_colnames, margins=margins, margins_name=margins_name, dropna=dropna, **kwargs)\n    if normalize is not False:\n        table = _normalize(table, normalize=normalize, margins=margins, margins_name=margins_name)\n    table = table.rename_axis(index=rownames_mapper, axis=0)\n    table = table.rename_axis(columns=colnames_mapper, axis=1)\n    return table",
    "completion": "\n    Compute a simple cross tabulation of two (or more) factors. By default\n    computes a frequency table of the factors unless an array of values and an\n    aggregation function are passed.\n\n    Parameters\n    ----------\n    index : array-like, Series, or list of arrays/Series\n        Values to group by in the rows.\n    columns : array-like, Series, or list of arrays/Series\n        Values to group by in the columns.\n    values : array-like, optional\n        Array of values to aggregate according to the factors.\n        Requires `aggfunc` be specified.\n    rownames : sequence, default None\n        If passed, must match number of row arrays passed.\n    colnames : sequence, default None\n        If passed, must match number of column arrays passed.\n    aggfunc : function, optional\n        If specified, requires `values` be specified as well.\n    margins : bool, default False\n        Add row/column margins (subtotals).\n    margins_name : str, default 'All'\n        Name of the row/column that will contain the totals\n        when margins is True.\n    dropna : bool, default True\n        Do not include columns whose entries are all NaN.\n    normalize : bool, {'all', 'index', 'columns'}, or {0,1}, default False\n        Normalize by dividing all values by the sum of values.\n\n        - If passed 'all' or `True`, will normalize over all values.\n        - If passed 'index' will normalize over each row.\n        - If passed 'columns' will normalize over each column.\n        - If margins is `True`, will also normalize margin values.\n\n    Returns\n    -------\n    DataFrame\n        Cross tabulation of the data.\n\n    See Also\n    --------\n    DataFrame.pivot : Reshape data based on column values.\n    pivot_table : Create a pivot table as a DataFrame.\n\n    Notes\n    -----\n    Any Series passed will have their name attributes used unless row or column\n    names for the cross-tabulation are specified.\n\n    Any input passed containing Categorical data will have **all** of its\n    categories included in the cross-tabulation, even if the actual data does\n    not contain any instances of a particular category.\n\n    In the event that there aren't overlapping indexes an empty DataFrame will\n    be returned.\n\n    Examples\n    --------\n    >>> a = np.array([\"foo\", \"foo\", \"foo\", \"foo\", \"bar\", \"bar\",\n    ...               \"bar\", \"bar\", \"foo\", \"foo\", \"foo\"], dtype=object)\n    >>> b = np.array([\"one\", \"one\", \"one\", \"two\", \"one\", \"one\",\n    ...               \"one\", \"two\", \"two\", \"two\", \"one\"], dtype=object)\n    >>> c = np.array([\"dull\", \"dull\", \"shiny\", \"dull\", \"dull\", \"shiny\",\n    ...               \"shiny\", \"dull\", \"shiny\", \"shiny\", \"shiny\"],\n    ...              dtype=object)\n    >>> pd.crosstab(a, [b, c], rownames=['a'], colnames=['b', 'c'])\n    b   one        two\n    c   dull shiny dull shiny\n    a\n    bar    1     2    1     0\n    foo    2     2    1     2\n\n    Here 'c' and 'f' are not represented in the data and will not be\n    shown in the output because dropna is True by default. Set\n    dropna=False to preserve categories with no data.\n\n    >>> foo = pd.Categorical(['a', 'b'], categories=['a', 'b', 'c'])\n    >>> bar = pd.Categorical(['d', 'e'], categories=['d', 'e', 'f'])\n    >>> pd.crosstab(foo, bar)\n    col_0  d  e\n    row_0\n    a      1  0\n    b      0  1\n    >>> pd.crosstab(foo, bar, dropna=False)\n    col_0  d  e  f\n    row_0\n    a      1  0  0\n    b      0  1  0\n    c      0  0  0\n    END"
  },
  {
    "prompt": "def _build_names_mapper(rownames: List[str], colnames: List[str]) -> Tuple[Dict[str, str], List[str], Dict[str, str], List[str]]:\n\n    def get_duplicates(names):\n        seen: Set = set()\n        return {name for name in names if name not in seen}\n    shared_names = set(rownames).intersection(set(colnames))\n    dup_names = get_duplicates(rownames) | get_duplicates(colnames) | shared_names\n    rownames_mapper = {f'row_{i}': name for (i, name) in enumerate(rownames) if name in dup_names}\n    unique_rownames = [f'row_{i}' if name in dup_names else name for (i, name) in enumerate(rownames)]\n    colnames_mapper = {f'col_{i}': name for (i, name) in enumerate(colnames) if name in dup_names}\n    unique_colnames = [f'col_{i}' if name in dup_names else name for (i, name) in enumerate(colnames)]\n    return (rownames_mapper, unique_rownames, colnames_mapper, unique_colnames)",
    "completion": "\n    Given the names of a DataFrame's rows and columns, returns a set of unique row\n    and column names and mappers that convert to original names.\n\n    A row or column name is replaced if it is duplicate among the rows of the inputs,\n    among the columns of the inputs or between the rows and the columns.\n\n    Parameters\n    ----------\n    rownames: list[str]\n    colnames: list[str]\n\n    Returns\n    -------\n    Tuple(Dict[str, str], List[str], Dict[str, str], List[str])\n\n    rownames_mapper: dict[str, str]\n        a dictionary with new row names as keys and original rownames as values\n    unique_rownames: list[str]\n        a list of rownames with duplicate names replaced by dummy names\n    colnames_mapper: dict[str, str]\n        a dictionary with new column names as keys and original column names as values\n    unique_colnames: list[str]\n        a list of column names with duplicate names replaced by dummy names\n\n    END"
  },
  {
    "prompt": "def decode_jpeg_exif(contents, name=None):\n    return core_ops.io_decode_jpeg_exif(contents, name=name)",
    "completion": "\n    Decode Exif information from an JPEG image.\n\n    TODO: Add additional fields besides orientation.\n\n    Args:\n      contents: A `Tensor` of type `string`. 0-D.  The JPEG-encoded image.\n      name: A name for the operation (optional).\n\n    Returns:\n      A `Tensor` of type `int64` for orientation.\n    END"
  },
  {
    "prompt": "def decode_tiff_info(contents, name=None):\n    (shape, dtype) = core_ops.io_decode_tiff_info(contents, name=name)\n    return (shape, dtype)",
    "completion": "\n    Decode a TIFF-encoded image meta data.\n\n    Args:\n      contents: A `Tensor` of type `string`. 0-D.  The TIFF-encoded image.\n      name: A name for the operation (optional).\n\n    Returns:\n      A `Tensor` of type `uint8` and shape of `[height, width, 4]` (RGBA).\n    END"
  },
  {
    "prompt": "def decode_tiff(contents, index=0, name=None):\n    return core_ops.io_decode_tiff(contents, index, name=name)",
    "completion": "\n    Decode a TIFF-encoded image to a uint8 tensor.\n\n    Args:\n      contents: A `Tensor` of type `string`. 0-D.  The TIFF-encoded image.\n      index: A `Tensor` of type int64. 0-D. The 0-based index of the frame\n        inside TIFF-encoded image.\n      name: A name for the operation (optional).\n\n    Returns:\n      A `Tensor` of type `uint8` and shape of `[height, width, 4]` (RGBA).\n    END"
  },
  {
    "prompt": "def decode_exr_info(contents, name=None):\n    (shape, dtype, channel) = core_ops.io_decode_exr_info(contents, name=name)\n    return (shape, dtype, channel)",
    "completion": "\n    Decode a EXR-encoded image meta data.\n\n    Args:\n      contents: A `Tensor` of type `string`. 0-D.  The EXR-encoded image.\n      name: A name for the operation (optional).\n\n    Returns:\n      A `Tensor` of type `uint8` and shape of `[height, width, 4]` (RGBA).\n    END"
  },
  {
    "prompt": "def decode_hdr(contents, name=None):\n    return core_ops.io_decode_hdr(contents, name=name)",
    "completion": "\n    Decode a HDR-encoded image to a tf.float tensor.\n\n    Args:\n      contents: A `Tensor` of type `string`. 0-D.  The HDR-encoded image.\n      name: A name for the operation (optional).\n\n    Returns:\n      A `Tensor` of type `float` and shape of `[height, width, 3]` (RGB).\n    END"
  },
  {
    "prompt": "def decode_nv12(contents, size, name=None):\n    return core_ops.io_decode_nv12(contents, size=size, name=name)",
    "completion": "\n    Decode a NV12-encoded image to a uint8 tensor.\n\n    Args:\n      contents: A `Tensor` of type `string`. 0-D.  The NV12-encoded image.\n      size: A 1-D int32 Tensor of 2 elements: height, width. The size\n        for the images.\n      name: A name for the operation (optional).\n\n    Returns:\n      A `Tensor` of type `uint8` and shape of `[height, width, 3]` (RGB).\n    END"
  },
  {
    "prompt": "def decode_yuy2(contents, size, name=None):\n    return core_ops.io_decode_yuy2(contents, size=size, name=name)",
    "completion": "\n    Decode a YUY2-encoded image to a uint8 tensor.\n\n    Args:\n      contents: A `Tensor` of type `string`. 0-D.  The YUY2-encoded image.\n      size: A 1-D int32 Tensor of 2 elements: height, width. The size\n        for the images.\n      name: A name for the operation (optional).\n\n    Returns:\n      A `Tensor` of type `uint8` and shape of `[height, width, 3]` (RGB).\n    END"
  },
  {
    "prompt": "def decode_avif(contents, name=None):\n    return core_ops.io_decode_avif(contents, name=name)",
    "completion": "\n    Decode a AVIF-encoded image to a uint8 tensor.\n\n    Args:\n      contents: A `Tensor` of type `string`. 0-D.  The AVIF-encoded image.\n      name: A name for the operation (optional).\n\n    Returns:\n      A `Tensor` of type `uint8` and shape of `[height, width, 3]` (RGB).\n    END"
  },
  {
    "prompt": "def add_index(body):\n    return Indexes.add_index(connexion)",
    "completion": "\n    Added a new Index\n    \n    :param body: Object that needs to be added to the db.\n    :type body: dict | bytes\n\n    :rtype: AddIndexResponse\n    END"
  },
  {
    "prompt": "def get_index_by_index_id(indexId):\n    return Indexes.get_index_by_index_id(indexId)",
    "completion": "\n    Get Index\n    Returns a single Index\n    :param indexId: ID of Index to return\n    :type indexId: str\n\n    :rtype: GetIndexResponse\n    END"
  },
  {
    "prompt": "def get_index_by_object_id(objectId):\n    return Indexes.get_index_by_object_id(objectId)",
    "completion": "\n    Get Index by objectId\n    Returns a single Index\n    :param objectId: ID of Object to return\n    :type objectId: str\n\n    :rtype: GetIndexResponse\n    END"
  }
]
